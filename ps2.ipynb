{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent's INITIAL Location: [0 3], Target Location: [0 0]\n",
      "Agent's Location: [0 3], Target Location: [0 0]\n",
      "Agent's Location: [0 3], Target Location: [0 0]\n",
      "Agent's Location: [1 3], Target Location: [0 0]\n",
      "Agent's Location: [1 3], Target Location: [0 0]\n",
      "Agent's Location: [1 3], Target Location: [0 0]\n",
      "Agent's Location: [1 3], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [1 3], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [1 3], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 3], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [1 3], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 3], Target Location: [0 0]\n",
      "Agent's Location: [0 3], Target Location: [0 0]\n",
      "Agent's Location: [0 3], Target Location: [0 0]\n",
      "Agent's Location: [1 3], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [1 3], Target Location: [0 0]\n",
      "Agent's Location: [0 3], Target Location: [0 0]\n",
      "Agent's Location: [0 3], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 1], Target Location: [0 0]\n",
      "Agent's Location: [0 1], Target Location: [0 0]\n",
      "Agent's Location: [0 0], Target Location: [0 0]\n",
      "Episode finished!\n"
     ]
    }
   ],
   "source": [
    "# import gym_examples\n",
    "# import gym\n",
    "\n",
    "# env = gym.make('gym_examples/GridWorld-v0', size=10)\n",
    "# print(env.reset())\n",
    "# print(env.action_space.sample())\n",
    "\n",
    "import gym\n",
    "import gym_examples\n",
    "# import numpy as np\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make('gym_examples/GridWorld-v0', size=4)\n",
    "\n",
    "# Reset the environment to start a new episode\n",
    "state = env.reset()\n",
    "agent_location = env.get_agent_location()\n",
    "target_location = env.get_target_location()\n",
    "print(f\"Agent's INITIAL Location: {agent_location}, Target Location: {target_location}\")\n",
    "# Initialize done to False\n",
    "done = False\n",
    "\n",
    "# Loop until the episode ends\n",
    "while not done:\n",
    "    # Select an action randomly\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # Apply the action to the environment\n",
    "    next_state, reward, done, truncated, info = env.step(action)\n",
    "    \n",
    "    # Optionally, render the environment to visualize it\n",
    "    # env.render()\n",
    "    agent_location = env.get_agent_location()\n",
    "    target_location = env.get_target_location()\n",
    "    print(f\"Agent's Location: {agent_location}, Target Location: {target_location}\")\n",
    "\n",
    "    # Update state\n",
    "    state = next_state\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "\n",
    "print(\"Episode finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'agent': array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  'target': array([0, 0])},\n",
       " -1,\n",
       " False,\n",
       " False,\n",
       " {'distance': 2.0})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_location = env.get_agent_location()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def one_hot_encode(index, vector_length=16):\n",
    "    vector = np.zeros(vector_length)\n",
    "    vector[index] = 1\n",
    "    return vector\n",
    "one_hot_encode(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import gym_examples\n",
    "# import numpy as np\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make('gym_examples/GridWorld-v0', size=4)\n",
    "\n",
    "# Reset the environment to start a new episode\n",
    "state = env.reset()\n",
    "action = env.action_space.sample()\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'agent': array([3, 0]), 'target': array([0, 0])}, {'distance': 3.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andyyang/Desktop/DS598 Reinforcement Learning/gym-examples/.env/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "next_state, reward, done, truncated, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agent': array([3, 1]), 'target': array([0, 0])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.0180,  0.0788, -0.0027,  0.1204], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.0174,  0.0785, -0.0424,  0.0629], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.0164,  0.0616, -0.0169,  0.1060], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.0164,  0.0616, -0.0169,  0.1060], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([ 0.0619,  0.1204,  0.0180,  0.0984,  0.1060,  0.0996, -0.0333,  0.0785,\n",
      "         0.0861,  0.1060], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0775, 0.0706, 0.1025, 0.0656, 0.0896, 0.0954, 0.0954, 0.1084, 0.0775,\n",
      "        0.0896], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.002904254011809826\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.0089,  0.1110, -0.0126, -0.0102], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.0827,  0.0024,  0.1110,  0.0334, -0.0301, -0.0102, -0.0006,  0.1452,\n",
      "         0.0624,  0.1528], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.1086, 0.1375, 0.1260, 0.1307, 0.0999, 0.0999, 0.1260, 0.1031, 0.0876,\n",
      "        0.0876], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0080345394089818\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.0873,  0.0760, -0.0389,  0.0220], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0422, 0.0710, 0.1285, 0.0404, 0.0339, 0.0889, 0.0873, 0.1154, 0.0220,\n",
      "        0.0440], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0786, 0.0786, 0.1039, 0.0800, 0.0379, 0.0717, 0.1157, 0.0717, 0.0379,\n",
      "        0.0786], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0008036725339479744\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-0.0045,  0.0803,  0.0942,  0.1038,  0.0803,  0.0380,  0.1357,  0.1432,\n",
      "         0.0831,  0.0477], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0848, 0.0748, 0.0865, 0.0670, 0.0748, 0.0949, 0.0949, 0.1221, 0.0848,\n",
      "        0.1288], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.002136481925845146\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-0.0173,  0.1114,  0.0398,  0.0208,  0.1003,  0.1283,  0.1025,  0.0285,\n",
      "         0.1082,  0.0369], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0902, 0.0902, 0.1171, 0.0974, 0.0923, 0.0974, 0.0923, 0.0918, 0.1003,\n",
      "        0.0866], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.003152836812660098\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.0790,  0.0184, -0.0440,  0.0980], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.0042,  0.0495,  0.0980,  0.0514,  0.1134,  0.1045,  0.1121,  0.1127,\n",
      "         0.1194,  0.1127], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0941, 0.1015, 0.0924, 0.0882, 0.1014, 0.1009, 0.1009, 0.1074, 0.0941,\n",
      "        0.1074], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0014713482232764363\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([ 0.0572,  0.0699, -0.0117,  0.0950], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0965, 0.0886, 0.1169, 0.1043, 0.0422, 0.0157, 0.1004, 0.0699, 0.1069,\n",
      "        0.1069], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.1007, 0.0855, 0.0869, 0.0962, 0.0962, 0.0869, 0.0955, 0.0798, 0.1052,\n",
      "        0.1052], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0009108082158491015\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0950, 0.0740, 0.0833, 0.1079, 0.0954, 0.0609, 0.0890, 0.0898, 0.0303,\n",
      "        0.1034], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0971, 0.0801, 0.0956, 0.0750, 0.0855, 0.0855, 0.0666, 0.0872, 0.0774,\n",
      "        0.0778], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0005363273085094988\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([ 0.0479,  0.0836, -0.0187,  0.0817], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0844, 0.0645, 0.0897, 0.0998, 0.0836, 0.1024, 0.0844, 0.0710, 0.0917,\n",
      "        0.1012], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0880, 0.0921, 0.0788, 0.0898, 0.0888, 0.0640, 0.0880, 0.0752, 0.0911,\n",
      "        0.0826], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0002875521604437381\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([ 0.0787,  0.1043, -0.0064,  0.0631], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.1043, 0.1101, 0.0853, 0.0730, 0.1043, 0.0607, 0.0864, 0.0960, 0.0665,\n",
      "        0.0787], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0958, 0.0768, 0.0991, 0.0599, 0.0958, 0.0958, 0.0788, 0.0864, 0.0864,\n",
      "        0.0778], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00033903849544003606\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0977, 0.0835, 0.0799, 0.0762, 0.0638, 0.0647, 0.0667, 0.0887, 0.1033,\n",
      "        0.0615], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0930, 0.0793, 0.0600, 0.0808, 0.0953, 0.0930, 0.0857, 0.0600, 0.0953,\n",
      "        0.0879], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0004191040061414242\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([ 0.0798,  0.0971, -0.0103,  0.0680], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0971, 0.0900, 0.0715, 0.0972, 0.0680, 0.0730, 0.0805, 0.0971, 0.1121,\n",
      "        0.0900], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0910, 0.0654, 0.0910, 0.0875, 0.0835, 0.0791, 0.0795, 0.0910, 0.0824,\n",
      "        0.0795], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00024284487881232053\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([ 0.0491,  0.0951, -0.0028,  0.0734], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0809, 0.0923, 0.0716, 0.0829, 0.0769, 0.0895, 0.0951, 0.0895, 0.0734,\n",
      "        0.0809], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0903, 0.0728, 0.0833, 0.0830, 0.0779, 0.0856, 0.0725, 0.0856, 0.0805,\n",
      "        0.0728], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00012681884982157499\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.0797,  0.0733, -0.0477,  0.0889], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0790, 0.1033, 0.0772, 0.0713, 0.0903, 0.0876, 0.0944, 0.0293, 0.0826,\n",
      "        0.0790], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0763, 0.0930, 0.0819, 0.0875, 0.0930, 0.0849, 0.0813, 0.0763, 0.0799,\n",
      "        0.0763], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00028049616958014667\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([ 0.0555,  0.0836, -0.0041,  0.0823], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0823, 0.0879, 0.0836, 0.0773, 0.0974, 0.0743, 0.0794, 0.0836, 0.0823,\n",
      "        0.0902], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0791, 0.0845, 0.0845, 0.0753, 0.0937, 0.0891, 0.0854, 0.0845, 0.0791,\n",
      "        0.0850], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.31546543748118e-05\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.0857,  0.0674, -0.0487,  0.0978], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0921, 0.0793, 0.1002, 0.0823, 0.0978, 0.0430, 0.0725, 0.0718, 0.0793,\n",
      "        0.1031], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0848, 0.0881, 0.1021, 0.0885, 0.0760, 0.0814, 0.0760, 0.0814, 0.0881,\n",
      "        0.0943], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00023870619770605117\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([ 0.0657,  0.0770, -0.0048,  0.0852], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0698, 0.0883, 0.0852, 0.1141, 0.0705, 0.0873, 0.0807, 0.0770, 0.0852,\n",
      "        0.1040], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0767, 0.0867, 0.0827, 0.0885, 0.0827, 0.0927, 0.0927, 0.0877, 0.0827,\n",
      "        0.0936], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00012609190889634192\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([ 0.0946,  0.0693, -0.0189,  0.0847], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0954, 0.0954, 0.0770, 0.0693, 0.0842, 0.0770, 0.1059, 0.0895, 0.0913,\n",
      "        0.1020], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0757, 0.0757, 0.0859, 0.0757, 0.0852, 0.0859, 0.0918, 0.0809, 0.0952,\n",
      "        0.0918], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00013624204439111054\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([ 0.0970,  0.0696, -0.0185,  0.0801], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0801, 0.0953, 0.0782, 0.0832, 0.0696, 0.0868, 0.0905, 0.0995, 0.0868,\n",
      "        0.0782], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0836, 0.0974, 0.0866, 0.0873, 0.0749, 0.0781, 0.0749, 0.0995, 0.0781,\n",
      "        0.0866], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.0058995586587116e-05\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([ 0.0980,  0.0704, -0.0179,  0.0757], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0704, 0.0704, 0.0925, 0.1004, 0.0979, 0.0840, 0.0862, 0.0799, 0.0840,\n",
      "        0.0981], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0742, 0.0742, 0.0883, 0.0976, 0.0881, 0.0775, 0.0990, 0.0877, 0.0775,\n",
      "        0.0990], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.586794239003211e-05\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([ 0.0981,  0.0722, -0.0170,  0.0712], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0811, 0.1019, 0.1061, 0.0897, 0.0981, 0.0712, 0.0828, 0.0803, 0.0803,\n",
      "        0.0804], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0919, 0.0955, 0.0917, 0.0992, 0.0883, 0.0795, 0.0878, 0.0745, 0.0745,\n",
      "        0.0807], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.133446342777461e-05\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([ 0.0974,  0.0744, -0.0161,  0.0668], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0974, 0.0974, 0.0785, 0.0766, 0.0744, 0.0865, 0.0748, 0.0974, 0.0996,\n",
      "        0.0668], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0877, 0.0877, 0.0877, 0.0841, 0.0779, 0.0873, 0.0779, 0.0877, 0.0835,\n",
      "        0.0759], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.87150565884076e-05\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0772, 0.0761, 0.0705, 0.0761, 0.0732, 0.0941, 0.1062, 0.1017, 0.0958,\n",
      "        0.0638], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0862, 0.0805, 0.0805, 0.0805, 0.0875, 0.0875, 0.0908, 0.0974, 0.0862,\n",
      "        0.0736], grad_fn=<AddBackward0>)\n",
      "LOSS: 9.104387572733685e-05\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([ 0.0936,  0.0768, -0.0147,  0.0628], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0953, 0.0959, 0.0936, 0.0936, 0.0936, 0.0936, 0.0948, 0.0742, 0.0904,\n",
      "        0.0936], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0866, 0.0963, 0.0842, 0.0842, 0.0842, 0.0842, 0.0915, 0.0866, 0.0850,\n",
      "        0.0842], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.073352026054636e-05\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([ 0.0908,  0.0768, -0.0138,  0.0632], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0786, 0.0943, 0.0907, 0.0908, 0.0908, 0.0825, 0.0958, 0.0907, 0.0632,\n",
      "        0.0951], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0817, 0.0917, 0.0840, 0.0817, 0.0817, 0.0817, 0.0856, 0.0840, 0.0742,\n",
      "        0.0862], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.783341475762427e-05\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([ 0.0871,  0.0750, -0.0121,  0.0656], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0871, 0.0812, 0.0800, 0.0871, 0.0871, 0.0660, 0.0942, 0.0935, 0.0660,\n",
      "        0.0899], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0784, 0.0784, 0.0784, 0.0784, 0.0784, 0.0809, 0.0854, 0.0940, 0.0809,\n",
      "        0.0839], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.9488439951092e-05\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0670, 0.0933, 0.0681, 0.0670, 0.0845, 0.0925, 0.0730, 0.0845, 0.0734,\n",
      "        0.0990], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0785, 0.0839, 0.0708, 0.0785, 0.0760, 0.0839, 0.0785, 0.0760, 0.0823,\n",
      "        0.0932], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.182099216151983e-05\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.0801,  0.0518, -0.0326,  0.0918], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0984, 0.0747, 0.0818, 0.0918, 0.0817, 0.0686, 0.0710, 0.0753, 0.0930,\n",
      "        0.0686], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0913, 0.0736, 0.0840, 0.0837, 0.0736, 0.0759, 0.0759, 0.0794, 0.0837,\n",
      "        0.0759], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.211288978694938e-05\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([ 0.0801,  0.0761,  0.0781,  0.0801,  0.0894,  0.0861,  0.0868,  0.0844,\n",
      "        -0.0097,  0.0894], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0721, 0.0781, 0.0830, 0.0721, 0.0836, 0.0901, 0.0804, 0.0721, 0.0804,\n",
      "        0.0836], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.000855582591611892\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.0748,  0.0898, -0.0117,  0.0910], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0765, 0.0806, 0.0988, 0.0765, 0.0709, 0.0910, 0.0898, 0.0698, 0.0893,\n",
      "        0.0765], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0689, 0.0689, 0.0878, 0.0689, 0.0680, 0.0819, 0.0732, 0.0725, 0.0889,\n",
      "        0.0689], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.074424840742722e-05\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.0719,  0.0875, -0.0034,  0.0892], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0960, 0.0671, 0.0884, 0.0764, 0.0683, 0.0713, 0.0671, 0.0713, 0.0824,\n",
      "        0.0202], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0857, 0.0688, 0.0882, 0.0659, 0.0654, 0.0778, 0.0688, 0.0778, 0.0857,\n",
      "        0.0681], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00026211398653686047\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0663, 0.0855, 0.0090, 0.0876], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0694, 0.0669, 0.0659, 0.0876, 0.0674, 0.0894, 0.0876, 0.0669, 0.0815,\n",
      "        0.0966], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0723, 0.0607, 0.0647, 0.0788, 0.0647, 0.0806, 0.0788, 0.0607, 0.0804,\n",
      "        0.0764], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.36267538741231e-05\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0600, 0.0828, 0.0206, 0.0867], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0680, 0.0657, 0.0594, 0.0656, 0.0690, 0.0937, 0.0656, 0.0867, 0.0657,\n",
      "        0.0604], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0702, 0.0621, 0.0591, 0.0621, 0.0591, 0.0749, 0.0621, 0.0781, 0.0621,\n",
      "        0.0591], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.837878052261658e-05\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0523, 0.0670, 0.0864, 0.0864, 0.0654, 0.0526, 0.0864, 0.0634, 0.0523,\n",
      "        0.0523], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0677, 0.0677, 0.0778, 0.0778, 0.0603, 0.0677, 0.0778, 0.0603, 0.0677,\n",
      "        0.0677], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00012006227916572243\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0444, 0.0573, 0.0464, 0.0661], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0573, 0.0596, 0.0823, 0.0596, 0.0855, 0.0601, 0.0646, 0.0646, 0.0543,\n",
      "        0.0543], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0708, 0.0587, 0.0780, 0.0587, 0.0770, 0.0595, 0.0589, 0.0589, 0.0708,\n",
      "        0.0708], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.88759022927843e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0606, 0.0592, 0.0782, 0.0841, 0.0606, 0.0749, 0.0644, 0.0841, 0.0642,\n",
      "        0.0606], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0674, 0.0606, 0.0525, 0.0757, 0.0674, 0.0525, 0.0578, 0.0757, 0.0674,\n",
      "        0.0674], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00015008478658273816\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0511, 0.0568, 0.0373, 0.0657], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0685, 0.0762, 0.0576, 0.0636, 0.0657, 0.0667, 0.0576, 0.0685, 0.0568,\n",
      "        0.0555], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0616, 0.0542, 0.0631, 0.0592, 0.0616, 0.0684, 0.0631, 0.0616, 0.0616,\n",
      "        0.0616], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.383069896604866e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0862, 0.0676, 0.0561, 0.0743, 0.0676, 0.0862, 0.0687, 0.0682, 0.0635,\n",
      "        0.0812], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0776, 0.0608, 0.0641, 0.0669, 0.0608, 0.0776, 0.0700, 0.0781, 0.0776,\n",
      "        0.0771], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.745387508999556e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0554, 0.0550, 0.0261, 0.0691], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0786, 0.0869, 0.0556, 0.0653, 0.0691, 0.0579, 0.0659, 0.0696, 0.0869,\n",
      "        0.0786], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0707, 0.0782, 0.0640, 0.0622, 0.0622, 0.0707, 0.0626, 0.0707, 0.0782,\n",
      "        0.0707], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.781245999969542e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0565, 0.0548, 0.0213, 0.0705], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0705, 0.0812, 0.0874, 0.0705, 0.0558, 0.0712, 0.0874, 0.0712, 0.0613,\n",
      "        0.0704], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0635, 0.0731, 0.0787, 0.0635, 0.0631, 0.0731, 0.0787, 0.0731, 0.0635,\n",
      "        0.0787], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.527001874521375e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0772, 0.0715, 0.0633, 0.0715, 0.0169, 0.0370, 0.0715, 0.0633, 0.0740,\n",
      "        0.0678], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0655, 0.0644, 0.0655, 0.0644, 0.0787, 0.0662, 0.0644, 0.0655, 0.0736,\n",
      "        0.0644], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0004988565924577415\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0609, 0.0676, 0.0148, 0.0827], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0676, 0.0841, 0.0790, 0.0684, 0.0688, 0.0644, 0.0575, 0.0827, 0.0841,\n",
      "        0.0720], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0648, 0.0757, 0.0648, 0.0791, 0.0619, 0.0638, 0.0602, 0.0745, 0.0757,\n",
      "        0.0745], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.924660945311189e-05\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0689, 0.0590, 0.0842, 0.0677, 0.0621, 0.0705, 0.0900, 0.0782, 0.0753,\n",
      "        0.0441], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0758, 0.0581, 0.0758, 0.0642, 0.0620, 0.0682, 0.0690, 0.0704, 0.0642,\n",
      "        0.0642], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00011625798651948571\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0585, 0.0722, 0.0835, 0.0585, 0.0733, 0.0870, 0.0627, 0.0677, 0.0685,\n",
      "        0.0556], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0565, 0.0764, 0.0752, 0.0565, 0.0638, 0.0704, 0.0565, 0.0650, 0.0658,\n",
      "        0.0565], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.154967584530823e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0474, 0.0589, 0.0679, 0.0635, 0.0545, 0.0618, 0.0589, 0.0679, 0.0568,\n",
      "        0.0562], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0629, 0.0557, 0.0629, 0.0540, 0.0571, 0.0742, 0.0557, 0.0629, 0.0557,\n",
      "        0.0557], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.610483276541345e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0548, 0.0630, 0.0505, 0.0543, 0.0657, 0.0623, 0.0651, 0.0543, 0.0785,\n",
      "        0.0808], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0586, 0.0548, 0.0591, 0.0586, 0.0524, 0.0642, 0.0727, 0.0586, 0.0729,\n",
      "        0.0727], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.2761955885216594e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0497, 0.0724, 0.0595, 0.0855, 0.0795, 0.0795, 0.0542, 0.0515, 0.0481,\n",
      "        0.0582], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0597, 0.0589, 0.0527, 0.0686, 0.0716, 0.0716, 0.0716, 0.0597, 0.0589,\n",
      "        0.0640], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00012593290011864156\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0572, 0.0632, 0.0526, 0.0592, 0.0502, 0.0526, 0.0718, 0.0712, 0.0592,\n",
      "        0.0632], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0619, 0.0512, 0.0646, 0.0619, 0.0646, 0.0646, 0.0619, 0.0710, 0.0619,\n",
      "        0.0512], grad_fn=<AddBackward0>)\n",
      "LOSS: 9.149364632321522e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0577, 0.0780, 0.0685, 0.0531, 0.0714, 0.0780, 0.0531, 0.0557, 0.0590,\n",
      "        0.0599], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0702, 0.0702, 0.0539, 0.0643, 0.0596, 0.0702, 0.0643, 0.0643, 0.0702,\n",
      "        0.0541], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00011099338007625192\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0596, 0.0647, 0.0391, 0.0584], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0781, 0.0781, 0.0814, 0.0530, 0.0596, 0.0781, 0.0575, 0.0574, 0.0675,\n",
      "        0.0658], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0703, 0.0703, 0.0715, 0.0557, 0.0591, 0.0703, 0.0591, 0.0573, 0.0701,\n",
      "        0.0557], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.9767932321410626e-05\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0659, 0.0284, 0.0418, 0.0674], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0603, 0.0602, 0.0623, 0.0602, 0.0655, 0.0655, 0.0659, 0.0573, 0.0674,\n",
      "        0.0625], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0567, 0.0544, 0.0589, 0.0544, 0.0701, 0.0701, 0.0701, 0.0701, 0.0567,\n",
      "        0.0606], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.377480217954144e-05\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0612, 0.0626, 0.0593, 0.0613, 0.0693, 0.0613, 0.0571, 0.0765, 0.0620,\n",
      "        0.0765], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0626, 0.0564, 0.0618, 0.0597, 0.0689, 0.0597, 0.0597, 0.0689, 0.0564,\n",
      "        0.0689], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.0832823793170974e-05\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0690, 0.0601, 0.0313, 0.0621], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0601, 0.0690, 0.0748, 0.0703, 0.0501, 0.0621, 0.0748, 0.0862, 0.0648,\n",
      "        0.0621], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0647, 0.0583, 0.0673, 0.0763, 0.0635, 0.0621, 0.0673, 0.0685, 0.0583,\n",
      "        0.0621], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.191434608306736e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0625, 0.0544, 0.0524, 0.0666], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0625, 0.0524, 0.0718, 0.0602, 0.0666, 0.0718, 0.0625, 0.0724, 0.0733,\n",
      "        0.0733], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0599, 0.0626, 0.0647, 0.0653, 0.0599, 0.0647, 0.0626, 0.0626, 0.0647,\n",
      "        0.0647], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.286657324177213e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0619, 0.0845, 0.0718, 0.0627, 0.0609, 0.0671, 0.0609, 0.0704, 0.0859,\n",
      "        0.0619], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0634, 0.0771, 0.0634, 0.0660, 0.0659, 0.0604, 0.0659, 0.0604, 0.0693,\n",
      "        0.0634], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.133693386800587e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([0.0512, 0.0596, 0.0609, 0.0677, 0.0671, 0.0540, 0.0702, 0.0609, 0.0610,\n",
      "        0.0677], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0633, 0.0653, 0.0633, 0.0609, 0.0604, 0.0654, 0.0633, 0.0633, 0.0604,\n",
      "        0.0609], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.0812337576644495e-05\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([0.0647, 0.0546, 0.0613, 0.0659], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0706, 0.0529, 0.0647, 0.0679, 0.0666, 0.0618, 0.0666, 0.0666, 0.0666,\n",
      "        0.0529], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0593, 0.0620, 0.0593, 0.0620, 0.0599, 0.0640, 0.0599, 0.0599, 0.0599,\n",
      "        0.0620], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.407758726505563e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0548, 0.0575, 0.0571, 0.0664], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0699, 0.0699, 0.0664, 0.0652, 0.0601, 0.0571, 0.0748, 0.0591, 0.0637,\n",
      "        0.0825], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0595, 0.0595, 0.0597, 0.0597, 0.0595, 0.0588, 0.0718, 0.0588, 0.0697,\n",
      "        0.0717], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.5182994654169306e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0513, 0.0574, 0.0612, 0.0665], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0677, 0.0637, 0.0665, 0.0646, 0.0622, 0.0561, 0.0561, 0.0614, 0.0561,\n",
      "        0.0622], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0599, 0.0672, 0.0599, 0.0586, 0.0614, 0.0609, 0.0609, 0.0666, 0.0609,\n",
      "        0.0614], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.5076016754610464e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0492, 0.0576, 0.0648, 0.0660], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0578, 0.0576, 0.0662, 0.0604, 0.0831, 0.0662, 0.0627, 0.0660, 0.0601,\n",
      "        0.0648], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0590, 0.0630, 0.0630, 0.0643, 0.0694, 0.0630, 0.0673, 0.0594, 0.0594,\n",
      "        0.0590], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.538021701388061e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0472, 0.0576, 0.0674, 0.0660], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0590, 0.0582, 0.0657, 0.0610, 0.0660, 0.0660, 0.0625, 0.0645, 0.0574,\n",
      "        0.0574], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0648, 0.0607, 0.0549, 0.0591, 0.0607, 0.0607, 0.0668, 0.0648, 0.0591,\n",
      "        0.0591], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.410156957921572e-05\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0660, 0.0703, 0.0509, 0.0509, 0.0620, 0.0667, 0.0531, 0.0592, 0.0738,\n",
      "        0.0574], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0632, 0.0583, 0.0664, 0.0664, 0.0570, 0.0661, 0.0558, 0.0600, 0.0544,\n",
      "        0.0583], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00010385209316154942\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0526, 0.0578, 0.0662, 0.0578, 0.0662, 0.0610, 0.0662, 0.0662, 0.0628,\n",
      "        0.0473], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0566, 0.0605, 0.0605, 0.0605, 0.0605, 0.0601, 0.0605, 0.0605, 0.0625,\n",
      "        0.0605], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.3793920010793954e-05\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0436, 0.0684, 0.0602, 0.0516], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0602, 0.0615, 0.0602, 0.0658, 0.0647, 0.0564, 0.0503, 0.0531, 0.0566,\n",
      "        0.0647], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0584, 0.0550, 0.0584, 0.0556, 0.0584, 0.0592, 0.0584, 0.0550, 0.0624,\n",
      "        0.0584], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.4042099287034944e-05\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0513, 0.0760, 0.0399, 0.0740], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0623, 0.0544, 0.0635, 0.0658, 0.0631, 0.0623, 0.0607, 0.0623, 0.0705,\n",
      "        0.0658], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0571, 0.0568, 0.0593, 0.0592, 0.0571, 0.0571, 0.0592, 0.0571, 0.0715,\n",
      "        0.0592], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.288339237566106e-05\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0864, 0.0606, 0.0340, 0.0674], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0547, 0.0573, 0.0573, 0.0654, 0.0686, 0.0649, 0.0567, 0.0532, 0.0761,\n",
      "        0.0532], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0594, 0.0589, 0.0589, 0.0634, 0.0685, 0.0599, 0.0599, 0.0589, 0.0778,\n",
      "        0.0589], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.3425732504401822e-05\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([0.0744, 0.0658, 0.0567, 0.0671, 0.0550, 0.0648, 0.0528, 0.0658, 0.0528,\n",
      "        0.0732], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0757, 0.0583, 0.0591, 0.0604, 0.0604, 0.0592, 0.0592, 0.0583, 0.0592,\n",
      "        0.0729], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.055469278478995e-05\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0843, 0.0467, 0.0394, 0.0607], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0679, 0.0679, 0.0556, 0.0533, 0.0556, 0.0719, 0.0533, 0.0556, 0.0556,\n",
      "        0.0679], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0611, 0.0611, 0.0601, 0.0577, 0.0600, 0.0759, 0.0577, 0.0601, 0.0601,\n",
      "        0.0611], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.7319314540363848e-05\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([0.0555, 0.0564, 0.0555, 0.0500, 0.0670, 0.0735, 0.0548, 0.0548, 0.0805,\n",
      "        0.0653], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0603, 0.0586, 0.0603, 0.0586, 0.0564, 0.0751, 0.0564, 0.0564, 0.0658,\n",
      "        0.0603], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.862155765295029e-05\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0575, 0.0575, 0.0575, 0.0560, 0.0560, 0.0565, 0.0724, 0.0560, 0.0735,\n",
      "        0.0571], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0585, 0.0585, 0.0571, 0.0586, 0.0586, 0.0559, 0.0747, 0.0586, 0.0719,\n",
      "        0.0559], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.277287305536447e-06\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([0.0625, 0.0508, 0.0531, 0.0585, 0.0587, 0.0633, 0.0585, 0.0572, 0.0676,\n",
      "        0.0583], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0557, 0.0558, 0.0676, 0.0581, 0.0557, 0.0658, 0.0581, 0.0570, 0.0609,\n",
      "        0.0557], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.506119173835032e-05\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([0.0601, 0.0601, 0.0742, 0.0647, 0.0580, 0.0546, 0.0602, 0.0641, 0.0601,\n",
      "        0.0515], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0570, 0.0562, 0.0727, 0.0667, 0.0577, 0.0577, 0.0577, 0.0601, 0.0541,\n",
      "        0.0541], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.056812016031472e-05\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([0.0575, 0.0527, 0.0170, 0.0610], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0616, 0.0616, 0.0527, 0.0694, 0.0527, 0.0746, 0.0750, 0.0629, 0.0596,\n",
      "        0.0658], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0566, 0.0566, 0.0579, 0.0718, 0.0579, 0.0624, 0.0672, 0.0638, 0.0566,\n",
      "        0.0609], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.5378885513637215e-05\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([0.0616, 0.0621, 0.0410, 0.0574], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0622, 0.0597, 0.0574, 0.0647, 0.0622, 0.0647, 0.0574, 0.0617, 0.0687,\n",
      "        0.0597], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0580, 0.0583, 0.0613, 0.0613, 0.0580, 0.0613, 0.0559, 0.0559, 0.0613,\n",
      "        0.0583], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.6706766473362222e-05\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0595, 0.0515, 0.0576, 0.0864, 0.0575, 0.0627, 0.0621, 0.0282, 0.0576,\n",
      "        0.0562], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0591, 0.0601, 0.0616, 0.0615, 0.0601, 0.0616, 0.0601, 0.0507, 0.0616,\n",
      "        0.0591], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00012502408935688436\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0745, 0.0495, 0.0300, 0.0693], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0634, 0.0636, 0.0476, 0.0598, 0.0476, 0.0636, 0.0653, 0.0476, 0.0825,\n",
      "        0.0600], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0571, 0.0596, 0.0570, 0.0631, 0.0570, 0.0596, 0.0693, 0.0570, 0.0652,\n",
      "        0.0570], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.657424091827124e-05\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([0.0485, 0.0522, 0.0148, 0.0633], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0599, 0.0511, 0.0641, 0.0610, 0.0633, 0.0599, 0.0633, 0.0539, 0.0605,\n",
      "        0.0786], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0539, 0.0539, 0.0587, 0.0570, 0.0549, 0.0539, 0.0518, 0.0587, 0.0645,\n",
      "        0.0673], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.9570342525839806e-05\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0355, 0.0605, 0.0621, 0.0706, 0.0641, 0.0479, 0.0733, 0.0160, 0.0564,\n",
      "        0.0587], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0542, 0.0641, 0.0577, 0.0659, 0.0514, 0.0528, 0.0660, 0.0660, 0.0641,\n",
      "        0.0528], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0003233865718357265\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0633, 0.0742, 0.0562, 0.0678, 0.0534, 0.0634, 0.0582, 0.0669, 0.0265,\n",
      "        0.0587], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0557, 0.0628, 0.0610, 0.0618, 0.0610, 0.0611, 0.0514, 0.0524, 0.0618,\n",
      "        0.0557], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0001826142834033817\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0505, 0.0525, 0.0578, 0.0516, 0.0686, 0.0516, 0.0502, 0.0595, 0.0517,\n",
      "        0.0535], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0535, 0.0535, 0.0582, 0.0558, 0.0618, 0.0558, 0.0651, 0.0537, 0.0558,\n",
      "        0.0553], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.672707316582091e-05\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0475, 0.0480, 0.0283, 0.0475, 0.0526, 0.0502, 0.0555, 0.0609, 0.0475,\n",
      "        0.0502], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0707, 0.0544, 0.0582, 0.0707, 0.0519, 0.0562, 0.0473, 0.0582, 0.0707,\n",
      "        0.0562], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0002696820010896772\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0529, 0.0553, 0.0529, 0.0574, 0.0529, 0.0589, 0.0509, 0.0529, 0.0499,\n",
      "        0.0507], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0645, 0.0489, 0.0548, 0.0532, 0.0645, 0.0544, 0.0489, 0.0548, 0.0531,\n",
      "        0.0565], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.003117646789178e-05\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0376, 0.0516, 0.0497, 0.0782], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0434, 0.0546, 0.0385, 0.0568, 0.0603, 0.0597, 0.0499, 0.0531, 0.0376,\n",
      "        0.0576], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0568, 0.0494, 0.0494, 0.0543, 0.0492, 0.0568, 0.0494, 0.0492, 0.0492,\n",
      "        0.0554], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.179537740536034e-05\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0174, 0.0119, 0.0738, 0.0493], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0399, 0.0738, 0.0541, 0.0572, 0.0506, 0.0483, 0.0535, 0.0483, 0.0560,\n",
      "        0.0521], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0515, 0.0664, 0.0515, 0.0555, 0.0515, 0.0592, 0.0542, 0.0592, 0.0576,\n",
      "        0.0805], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0001252468500752002\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0611, 0.0678, 0.0569, 0.0448, 0.0516, 0.0528, 0.0166, 0.0610, 0.0611,\n",
      "        0.0610], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0571, 0.0611, 0.0566, 0.0562, 0.0571, 0.0566, 0.0721, 0.0562, 0.0571,\n",
      "        0.0562], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00033775786869227886\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0288, 0.0309, 0.0656, 0.0411], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0651, 0.0651, 0.0538, 0.0502, 0.0606, 0.0586, 0.0648, 0.0740, 0.0590,\n",
      "        0.0590], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0586, 0.0586, 0.0560, 0.0484, 0.0603, 0.0609, 0.0643, 0.0622, 0.0609,\n",
      "        0.0609], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.4554992705816403e-05\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0684, 0.0565, 0.0672, 0.0672, 0.0582, 0.0670, 0.0585, 0.0672, 0.0758,\n",
      "        0.0522], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0615, 0.0497, 0.0605, 0.0605, 0.0640, 0.0605, 0.0640, 0.0605, 0.0525,\n",
      "        0.0591], grad_fn=<AddBackward0>)\n",
      "LOSS: 9.283256076741964e-05\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0360, 0.0506, 0.0595, 0.0360], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0529, 0.0327, 0.0684, 0.0639, 0.0583, 0.0529, 0.0592, 0.0648, 0.0592,\n",
      "        0.0694], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0602, 0.0572, 0.0536, 0.0602, 0.0525, 0.0602, 0.0624, 0.0525, 0.0647,\n",
      "        0.0624], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00012141167826484889\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0714, 0.0544, 0.0610, 0.0673, 0.0262, 0.0592, 0.0616, 0.0714, 0.0544,\n",
      "        0.0714], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0643, 0.0584, 0.0549, 0.0618, 0.0643, 0.0568, 0.0554, 0.0643, 0.0584,\n",
      "        0.0643], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00017475863569416106\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0334, 0.0604, 0.0661, 0.0353], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0565, 0.0619, 0.0680, 0.0624, 0.0571, 0.0737, 0.0600, 0.0671, 0.0737,\n",
      "        0.0624], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0572, 0.0554, 0.0664, 0.0615, 0.0603, 0.0664, 0.0590, 0.0603, 0.0664,\n",
      "        0.0615], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.1170028048800305e-05\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0320, 0.0624, 0.0702, 0.0359], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0582, 0.0643, 0.0644, 0.0618, 0.0757, 0.0651, 0.0583, 0.0638, 0.0757,\n",
      "        0.0741], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0586, 0.0586, 0.0681, 0.0584, 0.0681, 0.0586, 0.0579, 0.0597, 0.0681,\n",
      "        0.0697], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.51863275479991e-05\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0315, 0.0638, 0.0739, 0.0360], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0646, 0.0688, 0.0519, 0.0772, 0.0765, 0.0595, 0.0591, 0.0642, 0.0642,\n",
      "        0.0360], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0665, 0.0591, 0.0577, 0.0670, 0.0689, 0.0628, 0.0577, 0.0591, 0.0591,\n",
      "        0.0665], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00012909052020404488\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0336, 0.0645, 0.0720, 0.0395], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0645, 0.0672, 0.0653, 0.0653, 0.0781, 0.0710, 0.0720, 0.0781, 0.0710,\n",
      "        0.0781], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0703, 0.0597, 0.0608, 0.0608, 0.0703, 0.0608, 0.0648, 0.0703, 0.0608,\n",
      "        0.0703], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.739520929637365e-05\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0648, 0.0780, 0.0765, 0.0725, 0.0669, 0.0338, 0.0608, 0.0780, 0.0620,\n",
      "        0.0338], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0653, 0.0702, 0.0685, 0.0702, 0.0602, 0.0653, 0.0635, 0.0702, 0.0583,\n",
      "        0.0653], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00022320446441881359\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0490, 0.0542, 0.0402, 0.0432, 0.0638, 0.0693, 0.0773, 0.0638, 0.0810,\n",
      "        0.0686], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0624, 0.0636, 0.0630, 0.0636, 0.0585, 0.0630, 0.0696, 0.0585, 0.0735,\n",
      "        0.0624], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00014528646715916693\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0741, 0.0743, 0.0407, 0.0759, 0.0759, 0.0759, 0.0474, 0.0759, 0.0579,\n",
      "        0.0538], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0667, 0.0682, 0.0667, 0.0683, 0.0683, 0.0683, 0.0607, 0.0683, 0.0601,\n",
      "        0.0659], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00013293485972099006\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0728, 0.0654, 0.0632, 0.0607, 0.0728, 0.0577, 0.0607, 0.0750, 0.0750,\n",
      "        0.0651], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0656, 0.0586, 0.0584, 0.0579, 0.0656, 0.0588, 0.0579, 0.0675, 0.0675,\n",
      "        0.0575], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.631431172834709e-05\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([0.0617, 0.0604, 0.0596, 0.0613, 0.0589, 0.0613, 0.0635, 0.0702, 0.0617,\n",
      "        0.0714], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0595, 0.0529, 0.0572, 0.0663, 0.0663, 0.0663, 0.0560, 0.0659, 0.0595,\n",
      "        0.0643], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.017426934093237e-05\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([0.0604, 0.0716, 0.0577, 0.0563, 0.0740, 0.0609, 0.0604, 0.0716, 0.0716,\n",
      "        0.0574], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0607, 0.0644, 0.0644, 0.0647, 0.0567, 0.0585, 0.0607, 0.0644, 0.0644,\n",
      "        0.0666], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.589256372535601e-05\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0572, 0.0638, 0.0494, 0.0785], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0729, 0.0729, 0.0572, 0.0702, 0.0578, 0.0575, 0.0717, 0.0702, 0.0579,\n",
      "        0.0607], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0752, 0.0752, 0.0596, 0.0631, 0.0631, 0.0631, 0.0569, 0.0631, 0.0534,\n",
      "        0.0616], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.150874519837089e-05\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0469, 0.0571, 0.0646, 0.0691, 0.0593, 0.0363, 0.0664, 0.0582, 0.0578,\n",
      "        0.0646], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0753, 0.0622, 0.0665, 0.0622, 0.0622, 0.0598, 0.0665, 0.0541, 0.0529,\n",
      "        0.0665], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00014912504411768168\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0458, 0.0379, 0.0496, 0.0589], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0682, 0.0546, 0.0496, 0.0554, 0.0663, 0.0682, 0.0546, 0.0661, 0.0602,\n",
      "        0.0609], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0614, 0.0595, 0.0741, 0.0621, 0.0596, 0.0614, 0.0595, 0.0666, 0.0614,\n",
      "        0.0621], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.352072472916916e-05\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0556, 0.0556, 0.0684, 0.0622, 0.0806, 0.0794, 0.0684, 0.0673, 0.0642,\n",
      "        0.0689], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0601, 0.0601, 0.0639, 0.0605, 0.0714, 0.0725, 0.0639, 0.0553, 0.0543,\n",
      "        0.0576], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.849112130817957e-05\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([0.0621, 0.0653, 0.0683, 0.0625, 0.0618, 0.0671, 0.0574, 0.0683, 0.0617,\n",
      "        0.0597], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0580, 0.0582, 0.0675, 0.0604, 0.0691, 0.0556, 0.0604, 0.0675, 0.0610,\n",
      "        0.0582], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.712673449423164e-05\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0624, 0.0737, 0.0609, 0.0588], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0689, 0.0689, 0.0627, 0.0621, 0.0602, 0.0804, 0.0703, 0.0682, 0.0689,\n",
      "        0.0682], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0630, 0.0630, 0.0627, 0.0631, 0.0627, 0.0633, 0.0630, 0.0723, 0.0630,\n",
      "        0.0723], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.906082176603377e-05\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([0.0682, 0.0752, 0.0591, 0.0685, 0.0677, 0.0576, 0.0636, 0.0582, 0.0611,\n",
      "        0.0677], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0614, 0.0645, 0.0637, 0.0735, 0.0639, 0.0614, 0.0735, 0.0632, 0.0641,\n",
      "        0.0639], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.799659316428006e-05\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0644, 0.0364, 0.0780, 0.0691], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0696, 0.0630, 0.0792, 0.0749, 0.0841, 0.0629, 0.0630, 0.0630, 0.0609,\n",
      "        0.0614], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0713, 0.0658, 0.0612, 0.0635, 0.0635, 0.0626, 0.0658, 0.0658, 0.0612,\n",
      "        0.0608], grad_fn=<AddBackward0>)\n",
      "LOSS: 9.055667032953352e-05\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0627, 0.0697, 0.0711, 0.0587, 0.0618, 0.0633, 0.0607, 0.0598, 0.0624,\n",
      "        0.0619], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0601, 0.0683, 0.0747, 0.0677, 0.0659, 0.0606, 0.0678, 0.0622, 0.0677,\n",
      "        0.0592], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.1914060198469087e-05\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0628, 0.0699, 0.0611, 0.0627, 0.0784, 0.0723, 0.0628, 0.0628, 0.0611,\n",
      "        0.0627], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0579, 0.0659, 0.0626, 0.0611, 0.0573, 0.0673, 0.0579, 0.0611, 0.0626,\n",
      "        0.0611], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.487199450726621e-05\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0580, 0.0772, 0.0410, 0.0481], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0534, 0.0631, 0.0687, 0.0534, 0.0639, 0.0631, 0.0728, 0.0771, 0.0714,\n",
      "        0.0678], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0700, 0.0585, 0.0643, 0.0700, 0.0700, 0.0585, 0.0717, 0.0597, 0.0575,\n",
      "        0.0575], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00012536122812889516\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0543, 0.0574, 0.0722, 0.0726], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0687, 0.0649, 0.0726, 0.0661, 0.0617, 0.0778, 0.0649, 0.0572, 0.0605,\n",
      "        0.0603], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0692, 0.0639, 0.0700, 0.0661, 0.0597, 0.0653, 0.0639, 0.0692, 0.0613,\n",
      "        0.0597], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.130823461106047e-05\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0569, 0.0770, 0.0461, 0.0445], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0644, 0.0655, 0.0530, 0.0633, 0.0622, 0.0697, 0.0622, 0.0596, 0.0596,\n",
      "        0.0770], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0685, 0.0633, 0.0715, 0.0685, 0.0670, 0.0640, 0.0670, 0.0664, 0.0664,\n",
      "        0.0683], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.390213093254715e-05\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0531, 0.0560, 0.0784, 0.0719], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0655, 0.0591, 0.0532, 0.0590, 0.0638, 0.0600, 0.0604, 0.0697, 0.0630,\n",
      "        0.0592], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0662, 0.0593, 0.0730, 0.0730, 0.0692, 0.0684, 0.0593, 0.0630, 0.0593,\n",
      "        0.0663], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.951374573167413e-05\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0649, 0.0700, 0.0613, 0.0612, 0.0631, 0.0600, 0.0738, 0.0727, 0.0643,\n",
      "        0.0731], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0627, 0.0630, 0.0665, 0.0603, 0.0665, 0.0603, 0.0665, 0.0675, 0.0664,\n",
      "        0.0644], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.523612965887878e-05\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0683, 0.0709, 0.0627, 0.0589, 0.0711, 0.0683, 0.0633, 0.0731, 0.0621,\n",
      "        0.0633], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0657, 0.0629, 0.0639, 0.0621, 0.0657, 0.0657, 0.0615, 0.0647, 0.0640,\n",
      "        0.0615], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.970576704479754e-05\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0628, 0.0659, 0.0487, 0.0734], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0741, 0.0710, 0.0636, 0.0727, 0.0648, 0.0636, 0.0606, 0.0675, 0.0659,\n",
      "        0.0649], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0674, 0.0637, 0.0586, 0.0660, 0.0608, 0.0586, 0.0613, 0.0621, 0.0604,\n",
      "        0.0654], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.676205986062996e-05\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0637, 0.0716, 0.0499, 0.0631], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0619, 0.0577, 0.0671, 0.0773, 0.0625, 0.0647, 0.0585, 0.0759, 0.0577,\n",
      "        0.0683], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0608, 0.0601, 0.0608, 0.0654, 0.0651, 0.0651, 0.0640, 0.0645, 0.0601,\n",
      "        0.0615], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.096034535905346e-05\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0637, 0.0646, 0.0443, 0.0759], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0675, 0.0644, 0.0675, 0.0688, 0.0627, 0.0759, 0.0623, 0.0570, 0.0675,\n",
      "        0.0685], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0624, 0.0611, 0.0624, 0.0619, 0.0645, 0.0646, 0.0602, 0.0645, 0.0624,\n",
      "        0.0616], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.7544647057075053e-05\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0613, 0.0565, 0.0728, 0.0653, 0.0761, 0.0609, 0.0671, 0.0735, 0.0606,\n",
      "        0.0671], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0600, 0.0632, 0.0663, 0.0600, 0.0662, 0.0632, 0.0630, 0.0685, 0.0610,\n",
      "        0.0630], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.7838283131131902e-05\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0658, 0.0734, 0.0464, 0.0610], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0593, 0.0685, 0.0656, 0.0569, 0.0464, 0.0588, 0.0670, 0.0656, 0.0550,\n",
      "        0.0654], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0628, 0.0638, 0.0631, 0.0614, 0.0661, 0.0616, 0.0646, 0.0631, 0.0673,\n",
      "        0.0646], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.175189628265798e-05\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0641, 0.0655, 0.0703, 0.0612, 0.0596, 0.0641, 0.0583, 0.0612, 0.0755,\n",
      "        0.0575], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0651, 0.0623, 0.0656, 0.0599, 0.0626, 0.0651, 0.0640, 0.0599, 0.0651,\n",
      "        0.0614], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.0161947759333998e-05\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0633, 0.0740, 0.0590, 0.0603], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0603, 0.0677, 0.0533, 0.0640, 0.0651, 0.0677, 0.0660, 0.0731, 0.0655,\n",
      "        0.0724], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0571, 0.0638, 0.0571, 0.0571, 0.0619, 0.0638, 0.0666, 0.0658, 0.0589,\n",
      "        0.0621], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.1455121643375605e-05\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([0.0634, 0.0587, 0.0639, 0.0580, 0.0601, 0.0730, 0.0691, 0.0664, 0.0647,\n",
      "        0.0622], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0587, 0.0597, 0.0622, 0.0678, 0.0616, 0.0657, 0.0640, 0.0633, 0.0660,\n",
      "        0.0678], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.453615343256388e-05\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0586, 0.0520, 0.0483, 0.0657], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0625, 0.0625, 0.0725, 0.0586, 0.0625, 0.0719, 0.0734, 0.0620, 0.0727,\n",
      "        0.0637], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0661, 0.0661, 0.0659, 0.0628, 0.0661, 0.0647, 0.0647, 0.0627, 0.0591,\n",
      "        0.0647], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.100053774891421e-05\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0648, 0.0703, 0.0696, 0.0589, 0.0684, 0.0582, 0.0659, 0.0706, 0.0708,\n",
      "        0.0565], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0635, 0.0635, 0.0627, 0.0593, 0.0616, 0.0592, 0.0633, 0.0635, 0.0635,\n",
      "        0.0576], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.543299888202455e-05\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0717, 0.0717, 0.0705, 0.0615, 0.0554, 0.0615, 0.0711, 0.0652, 0.0652,\n",
      "        0.0585], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0645, 0.0645, 0.0634, 0.0606, 0.0585, 0.0606, 0.0634, 0.0634, 0.0634,\n",
      "        0.0640], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.5914472644217312e-05\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0574, 0.0675, 0.0671, 0.0610], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0603, 0.0584, 0.0493, 0.0550, 0.0702, 0.0619, 0.0577, 0.0731, 0.0633,\n",
      "        0.0646], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0600, 0.0632, 0.0608, 0.0589, 0.0645, 0.0607, 0.0589, 0.0658, 0.0616,\n",
      "        0.0616], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.6942167096422054e-05\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0595, 0.0502, 0.0512, 0.0659], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0634, 0.0635, 0.0641, 0.0601, 0.0648, 0.0634, 0.0634, 0.0601, 0.0634,\n",
      "        0.0637], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0662, 0.0622, 0.0593, 0.0604, 0.0622, 0.0662, 0.0662, 0.0604, 0.0662,\n",
      "        0.0622], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.319692147371825e-06\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0570, 0.0669, 0.0684, 0.0587], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0672, 0.0613, 0.0649, 0.0588, 0.0629, 0.0669, 0.0703, 0.0557, 0.0663,\n",
      "        0.0629], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0605, 0.0599, 0.0666, 0.0599, 0.0666, 0.0586, 0.0677, 0.0589, 0.0599,\n",
      "        0.0633], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.936074477271177e-05\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0633, 0.0579, 0.0625, 0.0633, 0.0635, 0.0643, 0.0562, 0.0561, 0.0586,\n",
      "        0.0705], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0639, 0.0590, 0.0659, 0.0639, 0.0659, 0.0639, 0.0587, 0.0587, 0.0598,\n",
      "        0.0634], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.37988409330137e-06\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([0.0641, 0.0563, 0.0770, 0.0621, 0.0589, 0.0660, 0.0641, 0.0563, 0.0690,\n",
      "        0.0634], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0693, 0.0578, 0.0693, 0.0646, 0.0571, 0.0594, 0.0594, 0.0578, 0.0608,\n",
      "        0.0608], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.3924554625409655e-05\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0582, 0.0583, 0.0567, 0.0636, 0.0597, 0.0631, 0.0501, 0.0616, 0.0561,\n",
      "        0.0606], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0568, 0.0580, 0.0589, 0.0625, 0.0583, 0.0589, 0.0592, 0.0636, 0.0568,\n",
      "        0.0685], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.7784437659429386e-05\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0597, 0.0310, 0.0572, 0.0655], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0627, 0.0665, 0.0627, 0.0597, 0.0661, 0.0583, 0.0557, 0.0661, 0.0613,\n",
      "        0.0556], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0641, 0.0657, 0.0641, 0.0622, 0.0623, 0.0605, 0.0579, 0.0623, 0.0623,\n",
      "        0.0605], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.469867796316976e-06\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0605, 0.0574, 0.0515, 0.0600, 0.0670, 0.0600, 0.0600, 0.0600, 0.0555,\n",
      "        0.0723], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0610, 0.0610, 0.0610, 0.0578, 0.0666, 0.0578, 0.0626, 0.0578, 0.0578,\n",
      "        0.0651], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.8357888620812446e-05\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0630, 0.0573, 0.0480, 0.0720], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0556, 0.0624, 0.0606, 0.0595, 0.0501, 0.0556, 0.0438, 0.0667, 0.0630,\n",
      "        0.0667], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0580, 0.0648, 0.0586, 0.0574, 0.0568, 0.0580, 0.0622, 0.0677, 0.0555,\n",
      "        0.0677], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.6550092520192266e-05\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0537, 0.0629, 0.0608, 0.0624], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0627, 0.0711, 0.0597, 0.0629, 0.0659, 0.0629, 0.0673, 0.0629, 0.0579,\n",
      "        0.0608], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0553, 0.0640, 0.0614, 0.0641, 0.0680, 0.0641, 0.0553, 0.0581, 0.0576,\n",
      "        0.0566], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.9830680432496592e-05\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0647, 0.0647, 0.0637, 0.0628, 0.0609, 0.0597, 0.0647, 0.0465, 0.0647,\n",
      "        0.0597], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0676, 0.0676, 0.0628, 0.0625, 0.0625, 0.0564, 0.0676, 0.0581, 0.0676,\n",
      "        0.0676], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.4583589038229547e-05\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0590, 0.0592, 0.0508, 0.0685], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0593, 0.0741, 0.0644, 0.0617, 0.0741, 0.0601, 0.0608, 0.0644, 0.0616,\n",
      "        0.0602], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0549, 0.0667, 0.0616, 0.0640, 0.0667, 0.0541, 0.0554, 0.0616, 0.0592,\n",
      "        0.0548], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.4929744540713727e-05\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0524, 0.0642, 0.0630, 0.0610], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0567, 0.0530, 0.0646, 0.0642, 0.0616, 0.0642, 0.0682, 0.0607, 0.0599,\n",
      "        0.0620], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0614, 0.0577, 0.0644, 0.0619, 0.0644, 0.0619, 0.0619, 0.0653, 0.0614,\n",
      "        0.0653], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.3595342352346051e-05\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([0.0650, 0.0635, 0.0646, 0.0578, 0.0698, 0.0660, 0.0631, 0.0722, 0.0602,\n",
      "        0.0560], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0639, 0.0635, 0.0581, 0.0565, 0.0628, 0.0628, 0.0650, 0.0682, 0.0565,\n",
      "        0.0628], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.829963548516389e-05\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0567, 0.0316, 0.0593, 0.0631], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0640, 0.0672, 0.0628, 0.0565, 0.0585, 0.0633, 0.0565, 0.0763, 0.0640,\n",
      "        0.0583], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0643, 0.0614, 0.0648, 0.0571, 0.0561, 0.0614, 0.0571, 0.0686, 0.0643,\n",
      "        0.0529], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.3496918654709589e-05\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0535, 0.0564, 0.0530, 0.0732], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0629, 0.0633, 0.0620, 0.0681, 0.0576, 0.0585, 0.0654, 0.0648, 0.0620,\n",
      "        0.0648], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0566, 0.0636, 0.0659, 0.0612, 0.0569, 0.0570, 0.0636, 0.0636, 0.0659,\n",
      "        0.0636], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.2574741958815139e-05\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0511, 0.0617, 0.0624, 0.0639], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0516, 0.0642, 0.0685, 0.0656, 0.0554, 0.0736, 0.0511, 0.0736, 0.0534,\n",
      "        0.0652], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0575, 0.0662, 0.0616, 0.0632, 0.0616, 0.0575, 0.0573, 0.0575, 0.0577,\n",
      "        0.0632], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.06013961462304e-05\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([0.0616, 0.0528, 0.0534, 0.0646, 0.0577, 0.0532, 0.0569, 0.0512, 0.0524,\n",
      "        0.0773], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0638, 0.0623, 0.0638, 0.0632, 0.0567, 0.0623, 0.0567, 0.0570, 0.0578,\n",
      "        0.0696], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.142019315622747e-05\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0546, 0.0617, 0.0625, 0.0634], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0672, 0.0631, 0.0617, 0.0577, 0.0631, 0.0631, 0.0556, 0.0587, 0.0570,\n",
      "        0.0546], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0571, 0.0634, 0.0618, 0.0583, 0.0634, 0.0634, 0.0551, 0.0618, 0.0556,\n",
      "        0.0544], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.1491991244838573e-05\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0582, 0.0535, 0.0790, 0.0599], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0599, 0.0694, 0.0585, 0.0632, 0.0545, 0.0591, 0.0515, 0.0508, 0.0591,\n",
      "        0.0643], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0711, 0.0631, 0.0527, 0.0567, 0.0559, 0.0586, 0.0625, 0.0561, 0.0586,\n",
      "        0.0629], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.958494198741391e-05\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0603, 0.0532, 0.0778, 0.0601], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0619, 0.0588, 0.0601, 0.0639, 0.0778, 0.0537, 0.0697, 0.0611, 0.0627,\n",
      "        0.0778], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0627, 0.0545, 0.0700, 0.0643, 0.0700, 0.0573, 0.0700, 0.0618, 0.0643,\n",
      "        0.0700], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.5397082936251536e-05\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0614, 0.0641, 0.0532, 0.0531, 0.0681, 0.0624, 0.0549, 0.0648, 0.0605,\n",
      "        0.0621], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0679, 0.0654, 0.0583, 0.0583, 0.0679, 0.0613, 0.0533, 0.0551, 0.0654,\n",
      "        0.0568], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.4639273760840297e-05\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0648, 0.0574, 0.0668, 0.0630], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0549, 0.0648, 0.0612, 0.0633, 0.0729, 0.0569, 0.0518, 0.0630, 0.0628,\n",
      "        0.0566], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0549, 0.0650, 0.0538, 0.0601, 0.0656, 0.0559, 0.0567, 0.0601, 0.0601,\n",
      "        0.0549], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.61858133651549e-05\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0630, 0.0549, 0.0706, 0.0645], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0570, 0.0612, 0.0635, 0.0547, 0.0608, 0.0594, 0.0549, 0.0635, 0.0518,\n",
      "        0.0612], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0614, 0.0550, 0.0594, 0.0561, 0.0560, 0.0547, 0.0599, 0.0594, 0.0575,\n",
      "        0.0550], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.331729410798289e-05\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0633, 0.0647, 0.0619, 0.0527, 0.0599, 0.0684, 0.0602, 0.0546, 0.0633,\n",
      "        0.0615], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0602, 0.0636, 0.0602, 0.0541, 0.0570, 0.0616, 0.0570, 0.0602, 0.0565,\n",
      "        0.0557], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.925855212903116e-05\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0628, 0.0574, 0.0689, 0.0659], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0609, 0.0552, 0.0596, 0.0677, 0.0574, 0.0540, 0.0643, 0.0533, 0.0528,\n",
      "        0.0565], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0587, 0.0548, 0.0587, 0.0620, 0.0548, 0.0587, 0.0632, 0.0601, 0.0533,\n",
      "        0.0535], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.251898083864944e-05\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0632, 0.0582, 0.0685, 0.0667], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0605, 0.0550, 0.0528, 0.0632, 0.0624, 0.0595, 0.0682, 0.0582, 0.0551,\n",
      "        0.0624], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0597, 0.0547, 0.0600, 0.0614, 0.0630, 0.0562, 0.0617, 0.0600, 0.0547,\n",
      "        0.0630], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.1469453056633938e-05\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0630, 0.0589, 0.0686, 0.0676], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0638, 0.0615, 0.0641, 0.0578, 0.0686, 0.0608, 0.0588, 0.0621, 0.0588,\n",
      "        0.0630], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0624, 0.0616, 0.0624, 0.0568, 0.0618, 0.0567, 0.0567, 0.0616, 0.0567,\n",
      "        0.0624], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.8064786066534e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0628, 0.0596, 0.0686, 0.0685], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0578, 0.0532, 0.0641, 0.0641, 0.0578, 0.0571, 0.0532, 0.0563, 0.0588,\n",
      "        0.0532], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0575, 0.0596, 0.0619, 0.0619, 0.0575, 0.0573, 0.0596, 0.0576, 0.0620,\n",
      "        0.0596], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.4300678230938502e-05\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0644, 0.0602, 0.0564, 0.0597, 0.0616, 0.0614, 0.0689, 0.0655, 0.0556,\n",
      "        0.0547], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0620, 0.0589, 0.0576, 0.0620, 0.0620, 0.0620, 0.0559, 0.0626, 0.0517,\n",
      "        0.0517], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.1409427063190378e-05\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0636, 0.0572, 0.0694, 0.0614], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0558, 0.0706, 0.0551, 0.0617, 0.0691, 0.0561, 0.0561, 0.0639, 0.0648,\n",
      "        0.0556], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0531, 0.0635, 0.0520, 0.0624, 0.0635, 0.0582, 0.0582, 0.0621, 0.0624,\n",
      "        0.0543], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.185958808491705e-05\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0643, 0.0600, 0.0691, 0.0716], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0559, 0.0689, 0.0577, 0.0559, 0.0668, 0.0553, 0.0600, 0.0553, 0.0619,\n",
      "        0.0716], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0541, 0.0620, 0.0575, 0.0541, 0.0585, 0.0521, 0.0575, 0.0521, 0.0629,\n",
      "        0.0644], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.996073842747137e-05\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0654, 0.0593, 0.0694, 0.0723], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0560, 0.0611, 0.0606, 0.0560, 0.0587, 0.0587, 0.0577, 0.0558, 0.0674,\n",
      "        0.0536], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0584, 0.0584, 0.0635, 0.0584, 0.0568, 0.0568, 0.0597, 0.0540, 0.0623,\n",
      "        0.0568], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.857604941818863e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0665, 0.0587, 0.0701, 0.0723], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0544, 0.0557, 0.0574, 0.0534, 0.0587, 0.0620, 0.0584, 0.0584, 0.0599,\n",
      "        0.0723], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0558, 0.0596, 0.0571, 0.0570, 0.0558, 0.0651, 0.0595, 0.0595, 0.0641,\n",
      "        0.0651], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.2085308298992459e-05\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0622, 0.0712, 0.0636, 0.0558, 0.0622, 0.0702, 0.0636, 0.0636, 0.0723,\n",
      "        0.0593], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0648, 0.0650, 0.0595, 0.0604, 0.0648, 0.0650, 0.0641, 0.0595, 0.0650,\n",
      "        0.0553], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.0347541067167185e-05\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0598, 0.0659, 0.0598, 0.0630, 0.0591, 0.0700, 0.0619, 0.0542, 0.0586,\n",
      "        0.0560], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0568, 0.0648, 0.0568, 0.0630, 0.0630, 0.0655, 0.0648, 0.0556, 0.0556,\n",
      "        0.0537], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.966486009536311e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0678, 0.0582, 0.0681, 0.0735], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0598, 0.0591, 0.0541, 0.0568, 0.0591, 0.0625, 0.0619, 0.0586, 0.0679,\n",
      "        0.0590], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0549, 0.0593, 0.0530, 0.0575, 0.0593, 0.0597, 0.0648, 0.0558, 0.0662,\n",
      "        0.0611], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.728023097617552e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0677, 0.0585, 0.0669, 0.0742], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0612, 0.0620, 0.0598, 0.0518, 0.0610, 0.0588, 0.0570, 0.0613, 0.0570,\n",
      "        0.0620], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0611, 0.0644, 0.0658, 0.0538, 0.0668, 0.0593, 0.0577, 0.0658, 0.0577,\n",
      "        0.0644], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.0694185220927466e-05\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0584, 0.0658, 0.0589, 0.0625, 0.0658, 0.0595, 0.0593, 0.0580, 0.0580,\n",
      "        0.0539], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0587, 0.0673, 0.0553, 0.0633, 0.0673, 0.0605, 0.0649, 0.0595, 0.0595,\n",
      "        0.0531], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.480691925185965e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0660, 0.0590, 0.0652, 0.0751], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0652, 0.0620, 0.0575, 0.0555, 0.0652, 0.0667, 0.0575, 0.0524, 0.0561,\n",
      "        0.0629], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0676, 0.0578, 0.0596, 0.0572, 0.0676, 0.0624, 0.0596, 0.0523, 0.0557,\n",
      "        0.0624], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.848104592587333e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0657, 0.0591, 0.0650, 0.0748], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0650, 0.0588, 0.0509, 0.0598, 0.0650, 0.0564, 0.0650, 0.0614, 0.0598,\n",
      "        0.0601], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0673, 0.0569, 0.0521, 0.0599, 0.0673, 0.0555, 0.0673, 0.0577, 0.0599,\n",
      "        0.0613], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.741576165339211e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0596, 0.0593, 0.0592, 0.0622, 0.0622, 0.0593, 0.0596, 0.0637, 0.0613,\n",
      "        0.0525], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0600, 0.0616, 0.0558, 0.0628, 0.0628, 0.0616, 0.0600, 0.0520, 0.0600,\n",
      "        0.0527], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.608256388863083e-05\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0523, 0.0563, 0.0549, 0.0738, 0.0738, 0.0523, 0.0669, 0.0549, 0.0523,\n",
      "        0.0591], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0528, 0.0578, 0.0578, 0.0665, 0.0665, 0.0528, 0.0623, 0.0578, 0.0528,\n",
      "        0.0614], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.550751221657265e-05\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0671, 0.0599, 0.0591, 0.0600], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0600, 0.0599, 0.0600, 0.0632, 0.0736, 0.0589, 0.0736, 0.0629, 0.0591,\n",
      "        0.0677], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0604, 0.0588, 0.0604, 0.0627, 0.0663, 0.0604, 0.0663, 0.0529, 0.0663,\n",
      "        0.0607], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.128246316919103e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0598, 0.0593, 0.0670, 0.0572, 0.0622, 0.0604, 0.0632, 0.0519, 0.0582,\n",
      "        0.0607], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0582, 0.0591, 0.0629, 0.0567, 0.0623, 0.0603, 0.0629, 0.0528, 0.0567,\n",
      "        0.0591], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.597141929072677e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0702, 0.0550, 0.0606, 0.0633], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0603, 0.0562, 0.0720, 0.0577, 0.0562, 0.0680, 0.0658, 0.0550, 0.0562,\n",
      "        0.0550], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0601, 0.0569, 0.0648, 0.0569, 0.0548, 0.0648, 0.0601, 0.0552, 0.0548,\n",
      "        0.0552], grad_fn=<AddBackward0>)\n",
      "LOSS: 9.905468687065877e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0707, 0.0544, 0.0611, 0.0634], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0604, 0.0592, 0.0553, 0.0684, 0.0536, 0.0684, 0.0634, 0.0707, 0.0553,\n",
      "        0.0548], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0580, 0.0622, 0.0572, 0.0643, 0.0589, 0.0643, 0.0636, 0.0636, 0.0572,\n",
      "        0.0589], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.5164545402512886e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0708, 0.0544, 0.0605, 0.0637], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0708, 0.0493, 0.0608, 0.0511, 0.0588, 0.0647, 0.0669, 0.0597, 0.0604,\n",
      "        0.0715], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0637, 0.0554, 0.0602, 0.0529, 0.0582, 0.0637, 0.0637, 0.0602, 0.0643,\n",
      "        0.0643], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.6887115634744987e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0541, 0.0605, 0.0646, 0.0568, 0.0584, 0.0548, 0.0715, 0.0496, 0.0712,\n",
      "        0.0639], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0565, 0.0641, 0.0595, 0.0576, 0.0534, 0.0540, 0.0644, 0.0551, 0.0641,\n",
      "        0.0635], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.032614611380268e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0547, 0.0596, 0.0701, 0.0554, 0.0522, 0.0554, 0.0628, 0.0552, 0.0502,\n",
      "        0.0586], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0550, 0.0589, 0.0631, 0.0540, 0.0527, 0.0540, 0.0638, 0.0541, 0.0543,\n",
      "        0.0584], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.349463885475416e-06\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([0.0564, 0.0652, 0.0652, 0.0554, 0.0571, 0.0608, 0.0510, 0.0641, 0.0603,\n",
      "        0.0653], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0578, 0.0633, 0.0633, 0.0547, 0.0547, 0.0609, 0.0532, 0.0632, 0.0633,\n",
      "        0.0536], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.679928391240537e-05\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0630, 0.0374, 0.0522, 0.0600], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0555, 0.0595, 0.0610, 0.0646, 0.0587, 0.0649, 0.0561, 0.0555, 0.0645,\n",
      "        0.0558], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0555, 0.0565, 0.0631, 0.0611, 0.0631, 0.0631, 0.0580, 0.0555, 0.0631,\n",
      "        0.0541], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.683587914973032e-06\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([0.0579, 0.0545, 0.0575, 0.0672], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0535, 0.0646, 0.0628, 0.0579, 0.0642, 0.0562, 0.0646, 0.0698, 0.0564,\n",
      "        0.0564], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0547, 0.0604, 0.0640, 0.0604, 0.0640, 0.0545, 0.0604, 0.0628, 0.0579,\n",
      "        0.0579], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.0091909643961117e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0673, 0.0563, 0.0593, 0.0639], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0640, 0.0569, 0.0536, 0.0690, 0.0560, 0.0640, 0.0558, 0.0568, 0.0640,\n",
      "        0.0638], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0621, 0.0574, 0.0550, 0.0621, 0.0574, 0.0621, 0.0581, 0.0550, 0.0621,\n",
      "        0.0621], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.448741143889492e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0676, 0.0568, 0.0589, 0.0633], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0547, 0.0710, 0.0633, 0.0568, 0.0522, 0.0688, 0.0676, 0.0534, 0.0589,\n",
      "        0.0522], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0550, 0.0639, 0.0609, 0.0576, 0.0523, 0.0619, 0.0609, 0.0514, 0.0559,\n",
      "        0.0523], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.6319692804245278e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0681, 0.0575, 0.0583, 0.0625], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0561, 0.0621, 0.0621, 0.0588, 0.0581, 0.0576, 0.0541, 0.0682, 0.0588,\n",
      "        0.0621], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0584, 0.0614, 0.0614, 0.0560, 0.0549, 0.0568, 0.0519, 0.0614, 0.0560,\n",
      "        0.0614], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.534376320312731e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0501, 0.0687, 0.0582, 0.0619, 0.0575, 0.0687, 0.0512, 0.0560, 0.0563,\n",
      "        0.0610], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0524, 0.0619, 0.0549, 0.0557, 0.0557, 0.0619, 0.0518, 0.0564, 0.0588,\n",
      "        0.0608], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.589704697835259e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0565, 0.0609, 0.0603, 0.0549, 0.0574, 0.0571, 0.0531, 0.0581, 0.0563,\n",
      "        0.0574], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0553, 0.0624, 0.0604, 0.0551, 0.0557, 0.0555, 0.0521, 0.0555, 0.0587,\n",
      "        0.0557], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.6271709430147894e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0695, 0.0579, 0.0573, 0.0605], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0597, 0.0642, 0.0637, 0.0617, 0.0568, 0.0562, 0.0596, 0.0621, 0.0562,\n",
      "        0.0617], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0602, 0.0634, 0.0602, 0.0625, 0.0574, 0.0583, 0.0574, 0.0561, 0.0583,\n",
      "        0.0625], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.570827736140927e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0697, 0.0577, 0.0570, 0.0603], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0484, 0.0551, 0.0570, 0.0534, 0.0499, 0.0697, 0.0565, 0.0619, 0.0616,\n",
      "        0.0579], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0577, 0.0545, 0.0557, 0.0513, 0.0513, 0.0627, 0.0554, 0.0565, 0.0554,\n",
      "        0.0574], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.0976478481316008e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0697, 0.0570, 0.0569, 0.0605], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0498, 0.0570, 0.0576, 0.0697, 0.0559, 0.0572, 0.0569, 0.0615, 0.0605,\n",
      "        0.0554], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0503, 0.0554, 0.0566, 0.0628, 0.0551, 0.0554, 0.0578, 0.0628, 0.0628,\n",
      "        0.0578], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.965462489461061e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0618, 0.0600, 0.0553, 0.0606, 0.0554, 0.0563, 0.0567, 0.0642, 0.0553,\n",
      "        0.0553], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0626, 0.0577, 0.0545, 0.0580, 0.0548, 0.0559, 0.0577, 0.0614, 0.0545,\n",
      "        0.0577], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.9132174859114457e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0614, 0.0579, 0.0579, 0.0547, 0.0553, 0.0564, 0.0582, 0.0613, 0.0517,\n",
      "        0.0575], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0627, 0.0622, 0.0622, 0.0529, 0.0500, 0.0574, 0.0565, 0.0622, 0.0552,\n",
      "        0.0552], grad_fn=<AddBackward0>)\n",
      "LOSS: 9.060869160748553e-06\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([0.0575, 0.0602, 0.0602, 0.0544, 0.0582, 0.0519, 0.0531, 0.0621, 0.0519,\n",
      "        0.0614], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0551, 0.0578, 0.0578, 0.0562, 0.0567, 0.0518, 0.0545, 0.0625, 0.0518,\n",
      "        0.0625], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.7146788852405734e-06\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0500, 0.0604, 0.0518, 0.0515, 0.0558, 0.0601, 0.0530, 0.0687, 0.0537,\n",
      "        0.0601], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0521, 0.0622, 0.0525, 0.0550, 0.0561, 0.0618, 0.0550, 0.0618, 0.0515,\n",
      "        0.0618], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.183489626389928e-06\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0621, 0.0559, 0.0659, 0.0591], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0719, 0.0591, 0.0536, 0.0591, 0.0551, 0.0690, 0.0531, 0.0545, 0.0591,\n",
      "        0.0608], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0647, 0.0593, 0.0515, 0.0593, 0.0549, 0.0621, 0.0546, 0.0546, 0.0593,\n",
      "        0.0621], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.0759955330286175e-05\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0553, 0.0527, 0.0623, 0.0672], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0585, 0.0562, 0.0597, 0.0590, 0.0605, 0.0605, 0.0623, 0.0533, 0.0605,\n",
      "        0.0533], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0600, 0.0600, 0.0600, 0.0652, 0.0618, 0.0618, 0.0605, 0.0524, 0.0618,\n",
      "        0.0524], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.537015906360466e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0551, 0.0527, 0.0626, 0.0672], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0557, 0.0680, 0.0566, 0.0585, 0.0542, 0.0672, 0.0608, 0.0557, 0.0585,\n",
      "        0.0569], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0556, 0.0612, 0.0565, 0.0597, 0.0543, 0.0605, 0.0612, 0.0556, 0.0597,\n",
      "        0.0565], grad_fn=<AddBackward0>)\n",
      "LOSS: 9.48376145970542e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0547, 0.0526, 0.0626, 0.0673], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0587, 0.0596, 0.0556, 0.0673, 0.0612, 0.0528, 0.0512, 0.0626, 0.0506,\n",
      "        0.0567], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0592, 0.0592, 0.0553, 0.0605, 0.0606, 0.0527, 0.0510, 0.0605, 0.0520,\n",
      "        0.0568], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.235422577243298e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0668, 0.0616, 0.0589, 0.0651, 0.0572, 0.0515, 0.0581, 0.0589, 0.0608,\n",
      "        0.0668], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0601, 0.0601, 0.0586, 0.0607, 0.0586, 0.0511, 0.0550, 0.0620, 0.0658,\n",
      "        0.0601], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.5636398529750295e-05\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0630, 0.0577, 0.0526, 0.0518, 0.0661, 0.0621, 0.0490, 0.0592, 0.0609,\n",
      "        0.0592], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0613, 0.0547, 0.0547, 0.0514, 0.0595, 0.0603, 0.0508, 0.0595, 0.0613,\n",
      "        0.0567], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.28664508642396e-06\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0628, 0.0558, 0.0610, 0.0590], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0519, 0.0508, 0.0481, 0.0557, 0.0610, 0.0579, 0.0525, 0.0596, 0.0620,\n",
      "        0.0530], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0576, 0.0489, 0.0507, 0.0565, 0.0614, 0.0565, 0.0518, 0.0614, 0.0607,\n",
      "        0.0547], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.343822522263508e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0681, 0.0526, 0.0556, 0.0611], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0580, 0.0506, 0.0611, 0.0676, 0.0590, 0.0676, 0.0506, 0.0676, 0.0532,\n",
      "        0.0554], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0543, 0.0499, 0.0613, 0.0608, 0.0608, 0.0608, 0.0499, 0.0608, 0.0511,\n",
      "        0.0577], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.6484334992128424e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0587, 0.0681, 0.0501, 0.0546, 0.0667, 0.0525, 0.0522, 0.0639, 0.0681,\n",
      "        0.0681], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0600, 0.0613, 0.0499, 0.0535, 0.0600, 0.0505, 0.0505, 0.0613, 0.0613,\n",
      "        0.0613], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.008237424888648e-05\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0638, 0.0577, 0.0578, 0.0564], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0588, 0.0546, 0.0520, 0.0619, 0.0681, 0.0552, 0.0584, 0.0524, 0.0579,\n",
      "        0.0584], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0593, 0.0578, 0.0538, 0.0589, 0.0589, 0.0529, 0.0593, 0.0480, 0.0554,\n",
      "        0.0593], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.3748624951404054e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0684, 0.0540, 0.0534, 0.0592], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0469, 0.0559, 0.0652, 0.0521, 0.0521, 0.0583, 0.0540, 0.0619, 0.0534,\n",
      "        0.0554], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0527, 0.0579, 0.0586, 0.0483, 0.0537, 0.0550, 0.0550, 0.0604, 0.0498,\n",
      "        0.0527], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.3122926247888245e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0685, 0.0539, 0.0529, 0.0589], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0589, 0.0553, 0.0514, 0.0549, 0.0589, 0.0530, 0.0569, 0.0556, 0.0571,\n",
      "        0.0589], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0617, 0.0525, 0.0485, 0.0580, 0.0617, 0.0497, 0.0580, 0.0580, 0.0581,\n",
      "        0.0617], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.522806415887317e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0674, 0.0533, 0.0527, 0.0597], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0674, 0.0545, 0.0649, 0.0649, 0.0552, 0.0570, 0.0563, 0.0563, 0.0574,\n",
      "        0.0617], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0607, 0.0555, 0.0584, 0.0584, 0.0577, 0.0584, 0.0569, 0.0569, 0.0555,\n",
      "        0.0607], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.4468964764091652e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0662, 0.0527, 0.0526, 0.0604], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0630, 0.0688, 0.0619, 0.0568, 0.0496, 0.0619, 0.0604, 0.0533, 0.0568,\n",
      "        0.0548], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0560, 0.0568, 0.0596, 0.0557, 0.0485, 0.0596, 0.0596, 0.0508, 0.0557,\n",
      "        0.0567], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.1685522369807586e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0640, 0.0521, 0.0531, 0.0612], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0640, 0.0601, 0.0594, 0.0612, 0.0612, 0.0570, 0.0554, 0.0651, 0.0535,\n",
      "        0.0651], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0576, 0.0586, 0.0576, 0.0576, 0.0576, 0.0534, 0.0586, 0.0586, 0.0534,\n",
      "        0.0586], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.797484583221376e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0586, 0.0610, 0.0528, 0.0643, 0.0586, 0.0538, 0.0643, 0.0528, 0.0518,\n",
      "        0.0629], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0579, 0.0566, 0.0570, 0.0579, 0.0579, 0.0520, 0.0579, 0.0570, 0.0517,\n",
      "        0.0566], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.8100219676853158e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0466, 0.0590, 0.0565, 0.0590, 0.0565, 0.0564, 0.0594, 0.0594, 0.0519,\n",
      "        0.0630], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0491, 0.0567, 0.0535, 0.0567, 0.0535, 0.0517, 0.0567, 0.0567, 0.0517,\n",
      "        0.0567], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.1153419109177776e-05\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0579, 0.0547, 0.0560, 0.0553], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0560, 0.0620, 0.0500, 0.0459, 0.0539, 0.0554, 0.0620, 0.0547, 0.0637,\n",
      "        0.0637], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0558, 0.0558, 0.0479, 0.0488, 0.0499, 0.0528, 0.0558, 0.0506, 0.0573,\n",
      "        0.0573], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.1077761630294845e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0644, 0.0521, 0.0531, 0.0580], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0543, 0.0644, 0.0527, 0.0546, 0.0581, 0.0544, 0.0580, 0.0582, 0.0492,\n",
      "        0.0543], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0547, 0.0580, 0.0547, 0.0524, 0.0551, 0.0526, 0.0580, 0.0551, 0.0492,\n",
      "        0.0591], grad_fn=<AddBackward0>)\n",
      "LOSS: 9.540012797515374e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0649, 0.0520, 0.0527, 0.0571], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0552, 0.0548, 0.0527, 0.0571, 0.0649, 0.0506, 0.0507, 0.0649, 0.0516,\n",
      "        0.0608], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0548, 0.0540, 0.0529, 0.0584, 0.0584, 0.0508, 0.0498, 0.0584, 0.0520,\n",
      "        0.0548], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.247306772711454e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0649, 0.0520, 0.0524, 0.0565], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0588, 0.0517, 0.0524, 0.0540, 0.0530, 0.0471, 0.0485, 0.0467, 0.0540,\n",
      "        0.0588], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0584, 0.0534, 0.0529, 0.0504, 0.0529, 0.0488, 0.0465, 0.0465, 0.0504,\n",
      "        0.0584], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.643890067905886e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0566, 0.0573, 0.0545, 0.0465, 0.0512, 0.0515, 0.0566, 0.0604, 0.0604,\n",
      "        0.0544], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0544, 0.0544, 0.0491, 0.0466, 0.0475, 0.0530, 0.0544, 0.0544, 0.0544,\n",
      "        0.0586], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.5502642781939358e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0568, 0.0560, 0.0538, 0.0475, 0.0609, 0.0557, 0.0516, 0.0655, 0.0522,\n",
      "        0.0560], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0528, 0.0590, 0.0531, 0.0476, 0.0548, 0.0548, 0.0531, 0.0590, 0.0511,\n",
      "        0.0590], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.1824180546682328e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0618, 0.0526, 0.0480, 0.0646, 0.0526, 0.0512, 0.0548, 0.0547, 0.0465,\n",
      "        0.0471], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0556, 0.0522, 0.0519, 0.0582, 0.0533, 0.0502, 0.0556, 0.0533, 0.0474,\n",
      "        0.0474], grad_fn=<AddBackward0>)\n",
      "LOSS: 9.946370482794009e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([0.0514, 0.0623, 0.0637, 0.0577, 0.0637, 0.0529, 0.0623, 0.0541, 0.0637,\n",
      "        0.0526], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0514, 0.0561, 0.0573, 0.0561, 0.0573, 0.0514, 0.0561, 0.0561, 0.0573,\n",
      "        0.0494], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.186000710935332e-05\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([0.0533, 0.0455, 0.0492, 0.0593], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0545, 0.0533, 0.0474, 0.0614, 0.0517, 0.0630, 0.0522, 0.0545, 0.0537,\n",
      "        0.0409], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0560, 0.0534, 0.0471, 0.0584, 0.0466, 0.0567, 0.0508, 0.0560, 0.0515,\n",
      "        0.0465], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.184567554446403e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0620, 0.0488, 0.0523, 0.0564], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0460, 0.0525, 0.0452, 0.0526, 0.0620, 0.0550, 0.0550, 0.0525, 0.0521,\n",
      "        0.0526], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0461, 0.0530, 0.0478, 0.0501, 0.0558, 0.0555, 0.0555, 0.0530, 0.0501,\n",
      "        0.0501], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.324396963464096e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0518, 0.0604, 0.0554, 0.0459, 0.0459, 0.0480, 0.0486, 0.0523, 0.0473,\n",
      "        0.0554], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0502, 0.0544, 0.0555, 0.0460, 0.0460, 0.0460, 0.0481, 0.0521, 0.0502,\n",
      "        0.0555], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.212189535086509e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([0.0535, 0.0461, 0.0612, 0.0495, 0.0612, 0.0487, 0.0504, 0.0612, 0.0506,\n",
      "        0.0504], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0506, 0.0461, 0.0551, 0.0453, 0.0551, 0.0478, 0.0492, 0.0551, 0.0511,\n",
      "        0.0492], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.4219573131413199e-05\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0413, 0.0456, 0.0535, 0.0467, 0.0504, 0.0565, 0.0477, 0.0486, 0.0562,\n",
      "        0.0562], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0454, 0.0613, 0.0546, 0.0435, 0.0508, 0.0522, 0.0485, 0.0475, 0.0522,\n",
      "        0.0522], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.2560892577748746e-05\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0533, 0.0515, 0.0557, 0.0470, 0.0564, 0.0536, 0.0428, 0.0489, 0.0437,\n",
      "        0.0490], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0544, 0.0501, 0.0544, 0.0471, 0.0520, 0.0524, 0.0460, 0.0484, 0.0429,\n",
      "        0.0501], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.88965554520837e-06\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([0.0530, 0.0535, 0.0554, 0.0439, 0.0442, 0.0511, 0.0600, 0.0501, 0.0600,\n",
      "        0.0588], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0529, 0.0529, 0.0540, 0.0462, 0.0444, 0.0502, 0.0540, 0.0502, 0.0540,\n",
      "        0.0529], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.1453876140876673e-05\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0489, 0.0482, 0.0504, 0.0549, 0.0426, 0.0505, 0.0505, 0.0549, 0.0495,\n",
      "        0.0516], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0494, 0.0494, 0.0494, 0.0534, 0.0534, 0.0481, 0.0504, 0.0534, 0.0471,\n",
      "        0.0533], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.3926587598689366e-05\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0447, 0.0458, 0.0413, 0.0476], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0590, 0.0590, 0.0439, 0.0535, 0.0486, 0.0445, 0.0573, 0.0451, 0.0507,\n",
      "        0.0472], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0531, 0.0531, 0.0428, 0.0531, 0.0481, 0.0466, 0.0500, 0.0471, 0.0516,\n",
      "        0.0489], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.3705493074667174e-05\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0565, 0.0458, 0.0499, 0.0588, 0.0467, 0.0501, 0.0467, 0.0470, 0.0466,\n",
      "        0.0437], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0530, 0.0451, 0.0509, 0.0530, 0.0541, 0.0509, 0.0541, 0.0451, 0.0484,\n",
      "        0.0483], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.8674445527722128e-05\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([0.0483, 0.0590, 0.0480, 0.0533, 0.0479, 0.0533, 0.0467, 0.0480, 0.0483,\n",
      "        0.0471], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0531, 0.0531, 0.0475, 0.0470, 0.0477, 0.0526, 0.0447, 0.0475, 0.0531,\n",
      "        0.0470], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.2593698556884192e-05\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0440, 0.0479, 0.0583, 0.0423, 0.0526, 0.0511, 0.0511, 0.0467, 0.0423,\n",
      "        0.0524], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0440, 0.0484, 0.0525, 0.0440, 0.0473, 0.0513, 0.0513, 0.0484, 0.0413,\n",
      "        0.0525], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.9405696194735356e-06\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0467, 0.0499, 0.0472, 0.0441, 0.0554, 0.0491, 0.0581, 0.0517, 0.0458,\n",
      "        0.0517], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0473, 0.0464, 0.0498, 0.0410, 0.0499, 0.0485, 0.0523, 0.0523, 0.0434,\n",
      "        0.0523], grad_fn=<AddBackward0>)\n",
      "LOSS: 9.98298310150858e-06\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0538, 0.0310, 0.0513, 0.0452], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0552, 0.0493, 0.0542, 0.0515, 0.0454, 0.0448, 0.0552, 0.0460, 0.0552,\n",
      "        0.0580], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0497, 0.0458, 0.0497, 0.0522, 0.0471, 0.0417, 0.0497, 0.0485, 0.0497,\n",
      "        0.0522], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.774819247657433e-05\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([0.0485, 0.0443, 0.0509, 0.0528], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0504, 0.0463, 0.0439, 0.0485, 0.0553, 0.0444, 0.0511, 0.0511, 0.0485,\n",
      "        0.0454], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0487, 0.0457, 0.0454, 0.0513, 0.0512, 0.0513, 0.0519, 0.0519, 0.0475,\n",
      "        0.0487], grad_fn=<AddBackward0>)\n",
      "LOSS: 9.267527275369503e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0569, 0.0474, 0.0544, 0.0513], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0569, 0.0436, 0.0544, 0.0446, 0.0473, 0.0569, 0.0569, 0.0421, 0.0537,\n",
      "        0.0569], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0512, 0.0431, 0.0513, 0.0513, 0.0464, 0.0512, 0.0512, 0.0453, 0.0475,\n",
      "        0.0512], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.3245822376338765e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0495, 0.0545, 0.0451, 0.0461, 0.0476, 0.0520, 0.0425, 0.0451, 0.0458,\n",
      "        0.0451], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0500, 0.0505, 0.0459, 0.0505, 0.0505, 0.0535, 0.0439, 0.0459, 0.0440,\n",
      "        0.0459], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.349736966309138e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0544, 0.0457, 0.0537, 0.0536], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0544, 0.0405, 0.0457, 0.0398, 0.0398, 0.0457, 0.0544, 0.0447, 0.0422,\n",
      "        0.0426], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0489, 0.0443, 0.0496, 0.0436, 0.0436, 0.0496, 0.0489, 0.0453, 0.0451,\n",
      "        0.0495], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.8927537894342095e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0542, 0.0542, 0.0578, 0.0548, 0.0542, 0.0440, 0.0578, 0.0542, 0.0448,\n",
      "        0.0517], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0493, 0.0493, 0.0520, 0.0493, 0.0493, 0.0474, 0.0520, 0.0493, 0.0453,\n",
      "        0.0520], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.0430332369869575e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0572, 0.0469, 0.0489, 0.0531], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0488, 0.0558, 0.0497, 0.0517, 0.0572, 0.0423, 0.0489, 0.0525, 0.0549,\n",
      "        0.0464], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0489, 0.0502, 0.0468, 0.0515, 0.0515, 0.0451, 0.0465, 0.0502, 0.0468,\n",
      "        0.0459], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.5754765627207235e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0588, 0.0477, 0.0471, 0.0519], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0418, 0.0454, 0.0496, 0.0588, 0.0496, 0.0519, 0.0591, 0.0510, 0.0547,\n",
      "        0.0418], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0444, 0.0501, 0.0479, 0.0529, 0.0479, 0.0532, 0.0532, 0.0532, 0.0492,\n",
      "        0.0444], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.4658222426078282e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0508, 0.0465, 0.0598, 0.0529, 0.0525, 0.0598, 0.0475, 0.0506, 0.0488,\n",
      "        0.0598], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0538, 0.0504, 0.0538, 0.0538, 0.0517, 0.0538, 0.0490, 0.0481, 0.0476,\n",
      "        0.0538], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.4289580576587468e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0443, 0.0500, 0.0437, 0.0467, 0.0500, 0.0415, 0.0504, 0.0497, 0.0528,\n",
      "        0.0596], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0447, 0.0536, 0.0428, 0.0432, 0.0536, 0.0428, 0.0454, 0.0496, 0.0475,\n",
      "        0.0536], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.2980772226001136e-05\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0539, 0.0486, 0.0408, 0.0471], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0471, 0.0408, 0.0443, 0.0408, 0.0441, 0.0494, 0.0583, 0.0583, 0.0489,\n",
      "        0.0471], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0485, 0.0469, 0.0445, 0.0469, 0.0426, 0.0469, 0.0524, 0.0524, 0.0485,\n",
      "        0.0485], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.5478663044632412e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0444, 0.0487, 0.0492, 0.0421, 0.0512, 0.0481, 0.0475, 0.0502, 0.0527,\n",
      "        0.0424], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0446, 0.0492, 0.0461, 0.0474, 0.0461, 0.0477, 0.0474, 0.0484, 0.0511,\n",
      "        0.0428], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.0497872002306394e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0553, 0.0511, 0.0420, 0.0502], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0502, 0.0429, 0.0420, 0.0506, 0.0484, 0.0484, 0.0435, 0.0496, 0.0553,\n",
      "        0.0435], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0498, 0.0447, 0.0460, 0.0420, 0.0457, 0.0479, 0.0455, 0.0474, 0.0498,\n",
      "        0.0455], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.4321913113235496e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([0.0432, 0.0445, 0.0481, 0.0478, 0.0536, 0.0493, 0.0511, 0.0380, 0.0426,\n",
      "        0.0491], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0430, 0.0446, 0.0444, 0.0456, 0.0482, 0.0446, 0.0476, 0.0478, 0.0444,\n",
      "        0.0482], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.817994416342117e-05\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([0.0529, 0.0427, 0.0428, 0.0487, 0.0487, 0.0487, 0.0518, 0.0478, 0.0518,\n",
      "        0.0508], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0502, 0.0425, 0.0471, 0.0417, 0.0417, 0.0438, 0.0467, 0.0471, 0.0467,\n",
      "        0.0467], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.1898566046729684e-05\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([0.0394, 0.0454, 0.0346, 0.0473], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0406, 0.0502, 0.0505, 0.0435, 0.0451, 0.0475, 0.0534, 0.0457, 0.0435,\n",
      "        0.0451], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0485, 0.0481, 0.0458, 0.0485, 0.0451, 0.0481, 0.0481, 0.0440, 0.0485,\n",
      "        0.0451], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.7026293789967895e-05\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([0.0410, 0.0408, 0.0458, 0.0528], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0505, 0.0410, 0.0428, 0.0408, 0.0444, 0.0528, 0.0543, 0.0501, 0.0503,\n",
      "        0.0468], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0455, 0.0475, 0.0438, 0.0421, 0.0416, 0.0455, 0.0528, 0.0447, 0.0455,\n",
      "        0.0475], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.8807846572599374e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0502, 0.0471, 0.0476, 0.0504], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0455, 0.0465, 0.0483, 0.0483, 0.0548, 0.0449, 0.0548, 0.0471, 0.0416,\n",
      "        0.0446], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0445, 0.0419, 0.0435, 0.0435, 0.0493, 0.0426, 0.0493, 0.0460, 0.0392,\n",
      "        0.0454], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.4178073797666002e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0500, 0.0461, 0.0480, 0.0504], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0463, 0.0463, 0.0485, 0.0454, 0.0472, 0.0463, 0.0485, 0.0423, 0.0440,\n",
      "        0.0481], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0448, 0.0456, 0.0539, 0.0494, 0.0494, 0.0448, 0.0539, 0.0448, 0.0444,\n",
      "        0.0433], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.1305755833745934e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0501, 0.0459, 0.0473, 0.0504], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0493, 0.0504, 0.0504, 0.0504, 0.0501, 0.0456, 0.0501, 0.0468, 0.0423,\n",
      "        0.0424], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0482, 0.0454, 0.0454, 0.0454, 0.0454, 0.0437, 0.0454, 0.0482, 0.0490,\n",
      "        0.0411], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.7408314306521788e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0488, 0.0454, 0.0468, 0.0510], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0414, 0.0393, 0.0488, 0.0453, 0.0485, 0.0487, 0.0433, 0.0488, 0.0510,\n",
      "        0.0510], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0432, 0.0434, 0.0459, 0.0472, 0.0438, 0.0512, 0.0407, 0.0459, 0.0459,\n",
      "        0.0459], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.273997168027563e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0473, 0.0452, 0.0463, 0.0514], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0509, 0.0552, 0.0407, 0.0473, 0.0458, 0.0454, 0.0473, 0.0473, 0.0432,\n",
      "        0.0514], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0433, 0.0497, 0.0412, 0.0463, 0.0426, 0.0461, 0.0463, 0.0463, 0.0408,\n",
      "        0.0463], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.3440951079246588e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0423, 0.0406, 0.0435, 0.0440, 0.0406, 0.0435, 0.0453, 0.0456, 0.0457,\n",
      "        0.0499], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0422, 0.0467, 0.0456, 0.0448, 0.0467, 0.0456, 0.0449, 0.0467, 0.0436,\n",
      "        0.0449], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.1459042070782743e-05\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0414, 0.0443, 0.0455, 0.0478], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0478, 0.0453, 0.0431, 0.0450, 0.0428, 0.0443, 0.0453, 0.0453, 0.0485,\n",
      "        0.0453], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0430, 0.0459, 0.0416, 0.0442, 0.0418, 0.0437, 0.0459, 0.0459, 0.0437,\n",
      "        0.0411], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.995435796852689e-06\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0422, 0.0447, 0.0447, 0.0474], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0436, 0.0503, 0.0420, 0.0472, 0.0486, 0.0512, 0.0439, 0.0435, 0.0431,\n",
      "        0.0503], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0403, 0.0453, 0.0431, 0.0425, 0.0425, 0.0460, 0.0426, 0.0425, 0.0403,\n",
      "        0.0453], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.5919835277600214e-05\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0468, 0.0464, 0.0430, 0.0431, 0.0476, 0.0494, 0.0468, 0.0415, 0.0436,\n",
      "        0.0472], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0421, 0.0420, 0.0425, 0.0469, 0.0421, 0.0445, 0.0421, 0.0428, 0.0420,\n",
      "        0.0431], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.516179781901883e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0520, 0.0464, 0.0411, 0.0449, 0.0449, 0.0486, 0.0486, 0.0486, 0.0472,\n",
      "        0.0472], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0440, 0.0417, 0.0421, 0.0425, 0.0425, 0.0437, 0.0437, 0.0437, 0.0425,\n",
      "        0.0425], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.1367392037063837e-05\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0432, 0.0444, 0.0453, 0.0460], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0379, 0.0483, 0.0437, 0.0419, 0.0338, 0.0338, 0.0357, 0.0460, 0.0387,\n",
      "        0.0483], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0413, 0.0435, 0.0430, 0.0413, 0.0370, 0.0370, 0.0461, 0.0414, 0.0414,\n",
      "        0.0435], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.1401916455943137e-05\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0500, 0.0439, 0.0454, 0.0434, 0.0469, 0.0379, 0.0395, 0.0439, 0.0390,\n",
      "        0.0404], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0450, 0.0395, 0.0413, 0.0422, 0.0422, 0.0395, 0.0445, 0.0404, 0.0413,\n",
      "        0.0365], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.4400333384401165e-05\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0458, 0.0392, 0.0450, 0.0429, 0.0408, 0.0395, 0.0393, 0.0393, 0.0441,\n",
      "        0.0429], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0419, 0.0390, 0.0419, 0.0418, 0.0390, 0.0419, 0.0453, 0.0453, 0.0413,\n",
      "        0.0418], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.1705441465892363e-05\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0454, 0.0425, 0.0389, 0.0439, 0.0497, 0.0425, 0.0497, 0.0396, 0.0407,\n",
      "        0.0380], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0419, 0.0421, 0.0350, 0.0421, 0.0447, 0.0421, 0.0447, 0.0392, 0.0447,\n",
      "        0.0350], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.0591467798803933e-05\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0410, 0.0387, 0.0484, 0.0423], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0394, 0.0437, 0.0389, 0.0473, 0.0426, 0.0421, 0.0385, 0.0421, 0.0472,\n",
      "        0.0387], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0347, 0.0425, 0.0394, 0.0461, 0.0417, 0.0425, 0.0394, 0.0425, 0.0425,\n",
      "        0.0393], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.011866051063407e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0382, 0.0438, 0.0432, 0.0435, 0.0403, 0.0463, 0.0472, 0.0416, 0.0438,\n",
      "        0.0432], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0391, 0.0417, 0.0389, 0.0425, 0.0447, 0.0425, 0.0425, 0.0425, 0.0417,\n",
      "        0.0425], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.65781657921616e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0452, 0.0421, 0.0360, 0.0436, 0.0413, 0.0470, 0.0429, 0.0408, 0.0421,\n",
      "        0.0407], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0413, 0.0413, 0.0354, 0.0389, 0.0423, 0.0423, 0.0382, 0.0419, 0.0413,\n",
      "        0.0386], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.940327461459674e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0419, 0.0385, 0.0451, 0.0440], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0464, 0.0440, 0.0405, 0.0400, 0.0419, 0.0420, 0.0384, 0.0405, 0.0440,\n",
      "        0.0380], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0418, 0.0406, 0.0355, 0.0381, 0.0414, 0.0414, 0.0377, 0.0418, 0.0406,\n",
      "        0.0361], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.932512744446285e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0411, 0.0388, 0.0452, 0.0434], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0435, 0.0395, 0.0399, 0.0452, 0.0422, 0.0468, 0.0434, 0.0416, 0.0349,\n",
      "        0.0433], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0421, 0.0410, 0.0380, 0.0407, 0.0380, 0.0407, 0.0407, 0.0407, 0.0349,\n",
      "        0.0374], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.2606421478267293e-05\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0406, 0.0389, 0.0453, 0.0427], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0425, 0.0411, 0.0359, 0.0411, 0.0388, 0.0407, 0.0419, 0.0427, 0.0419,\n",
      "        0.0447], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0421, 0.0423, 0.0375, 0.0421, 0.0402, 0.0402, 0.0377, 0.0408, 0.0377,\n",
      "        0.0402], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.625131391047034e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0401, 0.0391, 0.0457, 0.0418], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0376, 0.0413, 0.0421, 0.0401, 0.0408, 0.0401, 0.0457, 0.0457, 0.0415,\n",
      "        0.0437], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0334, 0.0384, 0.0400, 0.0422, 0.0422, 0.0374, 0.0411, 0.0411, 0.0374,\n",
      "        0.0393], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.2274256732780486e-05\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0398, 0.0389, 0.0460, 0.0414], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0414, 0.0376, 0.0367, 0.0395, 0.0460, 0.0406, 0.0413, 0.0377, 0.0376,\n",
      "        0.0342], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0414, 0.0388, 0.0376, 0.0388, 0.0414, 0.0422, 0.0371, 0.0362, 0.0388,\n",
      "        0.0378], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.023597961757332e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0396, 0.0384, 0.0463, 0.0412], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0427, 0.0411, 0.0373, 0.0373, 0.0404, 0.0401, 0.0373, 0.0396, 0.0380,\n",
      "        0.0338], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0384, 0.0370, 0.0384, 0.0384, 0.0421, 0.0384, 0.0384, 0.0421, 0.0375,\n",
      "        0.0399], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.736747076909523e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0464, 0.0416, 0.0353, 0.0464, 0.0413, 0.0373, 0.0421, 0.0462, 0.0316,\n",
      "        0.0376], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0417, 0.0374, 0.0331, 0.0417, 0.0395, 0.0413, 0.0379, 0.0417, 0.0373,\n",
      "        0.0379], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.5407229511765763e-05\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0400, 0.0376, 0.0467, 0.0411], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0421, 0.0408, 0.0421, 0.0421, 0.0467, 0.0415, 0.0425, 0.0467, 0.0369,\n",
      "        0.0451], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0379, 0.0367, 0.0379, 0.0379, 0.0421, 0.0373, 0.0364, 0.0421, 0.0364,\n",
      "        0.0450], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.680121749814134e-05\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0399, 0.0370, 0.0462, 0.0416], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0423, 0.0423, 0.0423, 0.0281, 0.0398, 0.0370, 0.0462, 0.0443, 0.0462,\n",
      "        0.0462], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0381, 0.0381, 0.0381, 0.0339, 0.0398, 0.0381, 0.0416, 0.0444, 0.0416,\n",
      "        0.0416], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.524528215668397e-05\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0398, 0.0392, 0.0386, 0.0392, 0.0394, 0.0368, 0.0396, 0.0414, 0.0459,\n",
      "        0.0368], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0390, 0.0364, 0.0390, 0.0383, 0.0390, 0.0373, 0.0376, 0.0413, 0.0413,\n",
      "        0.0376], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.5692282835952938e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0488, 0.0391, 0.0363, 0.0392, 0.0414, 0.0339, 0.0375, 0.0366, 0.0414,\n",
      "        0.0330], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0439, 0.0372, 0.0384, 0.0353, 0.0372, 0.0348, 0.0396, 0.0372, 0.0372,\n",
      "        0.0353], grad_fn=<AddBackward0>)\n",
      "LOSS: 9.219393177772872e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0396, 0.0356, 0.0453, 0.0412], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0412, 0.0419, 0.0426, 0.0412, 0.0419, 0.0412, 0.0453, 0.0380, 0.0340,\n",
      "        0.0348], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0408, 0.0408, 0.0434, 0.0408, 0.0377, 0.0371, 0.0408, 0.0377, 0.0342,\n",
      "        0.0354], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.786619112768676e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0395, 0.0352, 0.0449, 0.0410], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0395, 0.0384, 0.0380, 0.0384, 0.0275, 0.0410, 0.0380, 0.0339, 0.0334,\n",
      "        0.0352], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0365, 0.0368, 0.0368, 0.0368, 0.0329, 0.0404, 0.0368, 0.0337, 0.0329,\n",
      "        0.0378], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.316049737302819e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0387, 0.0352, 0.0453, 0.0402], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0372, 0.0354, 0.0403, 0.0290, 0.0376, 0.0374, 0.0453, 0.0402, 0.0406,\n",
      "        0.0376], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0363, 0.0374, 0.0363, 0.0320, 0.0366, 0.0363, 0.0408, 0.0408, 0.0408,\n",
      "        0.0366], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.414338374976069e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0381, 0.0352, 0.0457, 0.0393], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0457, 0.0306, 0.0343, 0.0360, 0.0366, 0.0396, 0.0366, 0.0457, 0.0362,\n",
      "        0.0362], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0412, 0.0333, 0.0352, 0.0352, 0.0337, 0.0356, 0.0349, 0.0412, 0.0356,\n",
      "        0.0356], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.767311217321549e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0373, 0.0351, 0.0463, 0.0383], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0463, 0.0346, 0.0388, 0.0463, 0.0388, 0.0346, 0.0388, 0.0356, 0.0463,\n",
      "        0.0340], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0417, 0.0349, 0.0349, 0.0417, 0.0349, 0.0349, 0.0349, 0.0371, 0.0417,\n",
      "        0.0360], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.1611062291194685e-05\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0381, 0.0347, 0.0416, 0.0487, 0.0347, 0.0465, 0.0351, 0.0323, 0.0347,\n",
      "        0.0384], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0346, 0.0372, 0.0438, 0.0438, 0.0346, 0.0418, 0.0352, 0.0347, 0.0341,\n",
      "        0.0372], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.5890552579949144e-06\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0339, 0.0362, 0.0410, 0.0338], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0339, 0.0410, 0.0383, 0.0372, 0.0300, 0.0296, 0.0336, 0.0314, 0.0410,\n",
      "        0.0462], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0345, 0.0416, 0.0369, 0.0345, 0.0326, 0.0327, 0.0345, 0.0340, 0.0416,\n",
      "        0.0416], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.5596365200472064e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0455, 0.0360, 0.0294, 0.0321, 0.0333, 0.0368, 0.0455, 0.0455, 0.0378,\n",
      "        0.0344], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0410, 0.0377, 0.0324, 0.0334, 0.0341, 0.0341, 0.0410, 0.0410, 0.0364,\n",
      "        0.0350], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.59373358252924e-06\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0333, 0.0295, 0.0333, 0.0409, 0.0366, 0.0447, 0.0339, 0.0447, 0.0364,\n",
      "        0.0447], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0335, 0.0320, 0.0335, 0.0420, 0.0368, 0.0402, 0.0368, 0.0402, 0.0339,\n",
      "        0.0402], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.244214768637903e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0357, 0.0352, 0.0435, 0.0358], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0435, 0.0435, 0.0363, 0.0305, 0.0361, 0.0308, 0.0305, 0.0325, 0.0358,\n",
      "        0.0352], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0391, 0.0391, 0.0353, 0.0314, 0.0327, 0.0372, 0.0314, 0.0318, 0.0391,\n",
      "        0.0338], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.0597264918033034e-05\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([0.0333, 0.0344, 0.0401, 0.0313, 0.0344, 0.0344, 0.0334, 0.0298, 0.0356,\n",
      "        0.0333], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0340, 0.0323, 0.0361, 0.0323, 0.0323, 0.0323, 0.0309, 0.0303, 0.0368,\n",
      "        0.0340], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.892926088155946e-06\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0332, 0.0324, 0.0328, 0.0383], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0364, 0.0410, 0.0363, 0.0339, 0.0331, 0.0410, 0.0379, 0.0364, 0.0410,\n",
      "        0.0339], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0327, 0.0369, 0.0341, 0.0327, 0.0341, 0.0369, 0.0369, 0.0327, 0.0369,\n",
      "        0.0341], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.53634537634207e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0357, 0.0324, 0.0400, 0.0378], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0331, 0.0341, 0.0369, 0.0331, 0.0317, 0.0369, 0.0331, 0.0323, 0.0400,\n",
      "        0.0323], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0332, 0.0349, 0.0332, 0.0332, 0.0337, 0.0332, 0.0332, 0.0298, 0.0360,\n",
      "        0.0332], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.4873844419489615e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0350, 0.0313, 0.0393, 0.0384], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0393, 0.0297, 0.0319, 0.0350, 0.0350, 0.0372, 0.0372, 0.0314, 0.0324,\n",
      "        0.0260], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0353, 0.0325, 0.0335, 0.0335, 0.0335, 0.0335, 0.0335, 0.0295, 0.0296,\n",
      "        0.0277], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.242043011501664e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0340, 0.0302, 0.0389, 0.0387], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0373, 0.0382, 0.0310, 0.0340, 0.0284, 0.0343, 0.0310, 0.0354, 0.0310,\n",
      "        0.0284], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0336, 0.0332, 0.0336, 0.0337, 0.0302, 0.0318, 0.0336, 0.0337, 0.0336,\n",
      "        0.0297], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.232695679704193e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0339, 0.0359, 0.0281, 0.0388, 0.0313, 0.0358, 0.0295, 0.0344, 0.0284,\n",
      "        0.0313], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0347, 0.0323, 0.0304, 0.0349, 0.0323, 0.0347, 0.0322, 0.0340, 0.0297,\n",
      "        0.0323], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.6319846660480835e-06\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0322, 0.0303, 0.0382, 0.0330], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0315, 0.0342, 0.0341, 0.0307, 0.0316, 0.0364, 0.0315, 0.0299, 0.0338,\n",
      "        0.0338], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0312, 0.0309, 0.0312, 0.0301, 0.0308, 0.0348, 0.0312, 0.0336, 0.0344,\n",
      "        0.0344], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.6765486584044993e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([0.0310, 0.0347, 0.0347, 0.0304, 0.0304, 0.0329, 0.0319, 0.0320, 0.0327,\n",
      "        0.0261], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0312, 0.0350, 0.0350, 0.0308, 0.0308, 0.0322, 0.0304, 0.0317, 0.0317,\n",
      "        0.0312], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.0472858725261176e-06\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0317, 0.0310, 0.0321, 0.0346], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0354, 0.0296, 0.0330, 0.0278, 0.0320, 0.0316, 0.0386, 0.0316, 0.0330,\n",
      "        0.0311], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0350, 0.0279, 0.0318, 0.0318, 0.0311, 0.0318, 0.0347, 0.0310, 0.0318,\n",
      "        0.0311], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.8047196539992e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0335, 0.0316, 0.0383, 0.0326], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0335, 0.0292, 0.0311, 0.0292, 0.0383, 0.0390, 0.0354, 0.0292, 0.0313,\n",
      "        0.0243], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0351, 0.0319, 0.0304, 0.0319, 0.0345, 0.0345, 0.0351, 0.0319, 0.0319,\n",
      "        0.0308], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.038428490574006e-05\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0337, 0.0320, 0.0365, 0.0330], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0299, 0.0315, 0.0365, 0.0365, 0.0365, 0.0365, 0.0299, 0.0335, 0.0294,\n",
      "        0.0365], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0303, 0.0302, 0.0329, 0.0329, 0.0329, 0.0329, 0.0305, 0.0335, 0.0302,\n",
      "        0.0329], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.963010491745081e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0338, 0.0323, 0.0347, 0.0334], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0307, 0.0297, 0.0347, 0.0297, 0.0322, 0.0309, 0.0296, 0.0300, 0.0319,\n",
      "        0.0315], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0281, 0.0281, 0.0312, 0.0285, 0.0356, 0.0281, 0.0309, 0.0319, 0.0309,\n",
      "        0.0285], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.789063834527042e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0330, 0.0322, 0.0339, 0.0338], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0339, 0.0339, 0.0305, 0.0311, 0.0347, 0.0326, 0.0295, 0.0330, 0.0305,\n",
      "        0.0305], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0305, 0.0305, 0.0280, 0.0312, 0.0305, 0.0354, 0.0280, 0.0312, 0.0280,\n",
      "        0.0312], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.77427578921197e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0339, 0.0278, 0.0312, 0.0311, 0.0385, 0.0313, 0.0313, 0.0293, 0.0331,\n",
      "        0.0339], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0309, 0.0271, 0.0309, 0.0277, 0.0335, 0.0280, 0.0280, 0.0290, 0.0296,\n",
      "        0.0309], grad_fn=<AddBackward0>)\n",
      "LOSS: 9.01334988157032e-06\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0303, 0.0319, 0.0345, 0.0254, 0.0325, 0.0295, 0.0318, 0.0304, 0.0303,\n",
      "        0.0303], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0323, 0.0311, 0.0321, 0.0293, 0.0293, 0.0296, 0.0292, 0.0303, 0.0323,\n",
      "        0.0323], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.165858510736143e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0250, 0.0273, 0.0322, 0.0317], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0288, 0.0343, 0.0270, 0.0298, 0.0322, 0.0339, 0.0336, 0.0310, 0.0339,\n",
      "        0.0250], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0289, 0.0331, 0.0289, 0.0309, 0.0309, 0.0331, 0.0295, 0.0289, 0.0331,\n",
      "        0.0289], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.621889729605755e-06\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0269, 0.0281, 0.0337, 0.0330], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0245, 0.0245, 0.0269, 0.0337, 0.0334, 0.0318, 0.0334, 0.0237, 0.0375,\n",
      "        0.0334], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0288, 0.0288, 0.0288, 0.0297, 0.0337, 0.0288, 0.0337, 0.0297, 0.0337,\n",
      "        0.0337], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.158868872153107e-05\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0299, 0.0298, 0.0323, 0.0373], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0302, 0.0242, 0.0250, 0.0226, 0.0373, 0.0373, 0.0256, 0.0373, 0.0311,\n",
      "        0.0275], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0288, 0.0294, 0.0294, 0.0272, 0.0336, 0.0336, 0.0280, 0.0336, 0.0280,\n",
      "        0.0287], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.2858229638368357e-05\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0325, 0.0303, 0.0305, 0.0365], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0299, 0.0269, 0.0302, 0.0291, 0.0239, 0.0336, 0.0279, 0.0328, 0.0330,\n",
      "        0.0325], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0269, 0.0297, 0.0297, 0.0269, 0.0259, 0.0328, 0.0293, 0.0297, 0.0303,\n",
      "        0.0280], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.608807780139614e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0339, 0.0309, 0.0294, 0.0355], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0288, 0.0300, 0.0277, 0.0355, 0.0294, 0.0307, 0.0300, 0.0317, 0.0294,\n",
      "        0.0298], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0272, 0.0289, 0.0328, 0.0319, 0.0319, 0.0276, 0.0289, 0.0291, 0.0319,\n",
      "        0.0272], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.95820051280316e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0351, 0.0314, 0.0296, 0.0334], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0268, 0.0306, 0.0337, 0.0289, 0.0250, 0.0337, 0.0283, 0.0268, 0.0337,\n",
      "        0.0289], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0293, 0.0315, 0.0293, 0.0321, 0.0291, 0.0293, 0.0303, 0.0293, 0.0293,\n",
      "        0.0315], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.1077356248279102e-05\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0324, 0.0295, 0.0293, 0.0277], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0300, 0.0308, 0.0320, 0.0288, 0.0299, 0.0263, 0.0262, 0.0326, 0.0326,\n",
      "        0.0324], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0307, 0.0341, 0.0295, 0.0267, 0.0307, 0.0254, 0.0293, 0.0293, 0.0293,\n",
      "        0.0293], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.413407390937209e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0317, 0.0292, 0.0286, 0.0259], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0273, 0.0259, 0.0259, 0.0259, 0.0299, 0.0293, 0.0299, 0.0259, 0.0317,\n",
      "        0.0273], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0272, 0.0286, 0.0286, 0.0286, 0.0294, 0.0280, 0.0294, 0.0286, 0.0286,\n",
      "        0.0272], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.095030362805119e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0295, 0.0295, 0.0290, 0.0266], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0266, 0.0290, 0.0270, 0.0288, 0.0270, 0.0237, 0.0295, 0.0272, 0.0280,\n",
      "        0.0245], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0266, 0.0273, 0.0266, 0.0266, 0.0266, 0.0249, 0.0266, 0.0284, 0.0292,\n",
      "        0.0283], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.5261850825918373e-06\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([0.0250, 0.0290, 0.0323, 0.0250], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0376, 0.0268, 0.0250, 0.0250, 0.0254, 0.0297, 0.0297, 0.0319, 0.0274,\n",
      "        0.0274], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0339, 0.0281, 0.0291, 0.0291, 0.0271, 0.0281, 0.0281, 0.0302, 0.0269,\n",
      "        0.0269], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.024131835147273e-06\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0233, 0.0123, 0.0254, 0.0273], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0280, 0.0253, 0.0253, 0.0287, 0.0341, 0.0315, 0.0280, 0.0257, 0.0215,\n",
      "        0.0287], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0273, 0.0281, 0.0281, 0.0281, 0.0303, 0.0307, 0.0273, 0.0273, 0.0273,\n",
      "        0.0281], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.9124848778301384e-06\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0215, 0.0298, 0.0302, 0.0290], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0215, 0.0273, 0.0307, 0.0302, 0.0302, 0.0286, 0.0258, 0.0290, 0.0281,\n",
      "        0.0215], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0272, 0.0306, 0.0307, 0.0307, 0.0307, 0.0272, 0.0277, 0.0272, 0.0272,\n",
      "        0.0272], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.634130608697888e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([0.0274, 0.0330, 0.0294, 0.0296, 0.0270, 0.0280, 0.0280, 0.0325, 0.0288,\n",
      "        0.0296], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0266, 0.0314, 0.0264, 0.0266, 0.0271, 0.0299, 0.0299, 0.0298, 0.0271,\n",
      "        0.0298], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.829828983725747e-06\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0262, 0.0286, 0.0282, 0.0301, 0.0306, 0.0329, 0.0315, 0.0290, 0.0322,\n",
      "        0.0263], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0257, 0.0271, 0.0266, 0.0271, 0.0282, 0.0296, 0.0298, 0.0271, 0.0283,\n",
      "        0.0271], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.223651896812953e-06\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0275, 0.0085, 0.0231, 0.0289], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0294, 0.0241, 0.0275, 0.0287, 0.0287, 0.0295, 0.0231, 0.0305, 0.0253,\n",
      "        0.0321], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0260, 0.0262, 0.0270, 0.0298, 0.0298, 0.0290, 0.0290, 0.0309, 0.0309,\n",
      "        0.0309], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.753224392421544e-06\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0277, 0.0257, 0.0298, 0.0309], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0307, 0.0307, 0.0330, 0.0290, 0.0277, 0.0241, 0.0310, 0.0310, 0.0295,\n",
      "        0.0253], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0279, 0.0297, 0.0297, 0.0278, 0.0279, 0.0277, 0.0279, 0.0279, 0.0268,\n",
      "        0.0256], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.129479970695684e-06\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0278, 0.0247, 0.0308, 0.0315], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0295, 0.0278, 0.0295, 0.0270, 0.0282, 0.0227, 0.0301, 0.0316, 0.0339,\n",
      "        0.0308], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0283, 0.0284, 0.0283, 0.0284, 0.0283, 0.0278, 0.0283, 0.0298, 0.0273,\n",
      "        0.0298], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.107849680527579e-06\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0286, 0.0302, 0.0282, 0.0309, 0.0316, 0.0307, 0.0307, 0.0317, 0.0297,\n",
      "        0.0330], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0276, 0.0285, 0.0286, 0.0297, 0.0285, 0.0276, 0.0276, 0.0297, 0.0277,\n",
      "        0.0297], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.381549726735102e-06\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0280, 0.0245, 0.0305, 0.0323], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0311, 0.0320, 0.0305, 0.0274, 0.0280, 0.0315, 0.0250, 0.0274, 0.0277,\n",
      "        0.0333], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0300, 0.0288, 0.0300, 0.0288, 0.0288, 0.0297, 0.0258, 0.0288, 0.0250,\n",
      "        0.0300], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.924823431589175e-06\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0279, 0.0245, 0.0300, 0.0329], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0324, 0.0301, 0.0293, 0.0322, 0.0300, 0.0304, 0.0324, 0.0290, 0.0269,\n",
      "        0.0329], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0292, 0.0271, 0.0292, 0.0294, 0.0305, 0.0305, 0.0292, 0.0292, 0.0290,\n",
      "        0.0296], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.402610440796707e-06\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0278, 0.0246, 0.0296, 0.0332], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0225, 0.0253, 0.0340, 0.0287, 0.0319, 0.0292, 0.0332, 0.0268, 0.0292,\n",
      "        0.0286], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0267, 0.0267, 0.0306, 0.0287, 0.0317, 0.0258, 0.0299, 0.0299, 0.0258,\n",
      "        0.0258], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.235844688897487e-06\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0330, 0.0296, 0.0277, 0.0337, 0.0270, 0.0330, 0.0287, 0.0268, 0.0296,\n",
      "        0.0284], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0297, 0.0303, 0.0289, 0.0303, 0.0287, 0.0297, 0.0259, 0.0297, 0.0303,\n",
      "        0.0260], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.1163432292232756e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([0.0330, 0.0270, 0.0297, 0.0322, 0.0272, 0.0297, 0.0282, 0.0238, 0.0272,\n",
      "        0.0314], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0297, 0.0283, 0.0297, 0.0290, 0.0290, 0.0297, 0.0290, 0.0254, 0.0290,\n",
      "        0.0283], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.239200734446058e-06\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([0.0274, 0.0265, 0.0282, 0.0264], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0280, 0.0267, 0.0312, 0.0266, 0.0303, 0.0303, 0.0293, 0.0276, 0.0267,\n",
      "        0.0260], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0280, 0.0273, 0.0287, 0.0278, 0.0273, 0.0273, 0.0299, 0.0299, 0.0273,\n",
      "        0.0252], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.3828910090960562e-06\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([0.0274, 0.0301, 0.0301, 0.0310, 0.0256, 0.0301, 0.0267, 0.0254, 0.0291,\n",
      "        0.0269], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0253, 0.0279, 0.0279, 0.0279, 0.0272, 0.0279, 0.0255, 0.0262, 0.0262,\n",
      "        0.0247], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.7307235035987105e-06\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0228, 0.0273, 0.0266, 0.0273], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0262, 0.0286, 0.0252, 0.0273, 0.0312, 0.0286, 0.0299, 0.0285, 0.0295,\n",
      "        0.0295], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0268, 0.0257, 0.0249, 0.0257, 0.0281, 0.0257, 0.0281, 0.0287, 0.0281,\n",
      "        0.0281], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.658646619442152e-06\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0234, 0.0094, 0.0268, 0.0255], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0284, 0.0284, 0.0273, 0.0266, 0.0284, 0.0284, 0.0266, 0.0256, 0.0341,\n",
      "        0.0284], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0255, 0.0262, 0.0270, 0.0242, 0.0255, 0.0287, 0.0255, 0.0267, 0.0315,\n",
      "        0.0287], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.5612270039564464e-06\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0289, 0.0284, 0.0259, 0.0293], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0355, 0.0282, 0.0278, 0.0286, 0.0259, 0.0259, 0.0256, 0.0233, 0.0255,\n",
      "        0.0283], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0320, 0.0290, 0.0263, 0.0257, 0.0255, 0.0255, 0.0244, 0.0250, 0.0263,\n",
      "        0.0263], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.2538505365664605e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0264, 0.0303, 0.0283, 0.0323], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0264, 0.0267, 0.0243, 0.0269, 0.0252, 0.0262, 0.0267, 0.0273, 0.0275,\n",
      "        0.0269], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0268, 0.0256, 0.0247, 0.0286, 0.0260, 0.0247, 0.0280, 0.0260, 0.0250,\n",
      "        0.0268], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.6854448858794058e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0263, 0.0300, 0.0285, 0.0325], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0239, 0.0268, 0.0262, 0.0285, 0.0295, 0.0277, 0.0303, 0.0283, 0.0268,\n",
      "        0.0264], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0250, 0.0266, 0.0272, 0.0292, 0.0272, 0.0256, 0.0292, 0.0292, 0.0266,\n",
      "        0.0284], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.8479171330909594e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0262, 0.0298, 0.0285, 0.0328], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0272, 0.0274, 0.0257, 0.0232, 0.0290, 0.0257, 0.0298, 0.0262, 0.0262,\n",
      "        0.0242], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0279, 0.0261, 0.0250, 0.0264, 0.0253, 0.0275, 0.0275, 0.0275, 0.0275,\n",
      "        0.0268], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.608054496202385e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0270, 0.0254, 0.0287, 0.0274, 0.0274, 0.0246, 0.0246, 0.0254, 0.0274,\n",
      "        0.0253], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0258, 0.0258, 0.0267, 0.0304, 0.0304, 0.0258, 0.0258, 0.0258, 0.0304,\n",
      "        0.0242], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.770272769543226e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0277, 0.0290, 0.0277, 0.0335], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0284, 0.0296, 0.0255, 0.0277, 0.0277, 0.0269, 0.0277, 0.0284, 0.0290,\n",
      "        0.0277], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0266, 0.0302, 0.0275, 0.0302, 0.0266, 0.0269, 0.0302, 0.0258, 0.0258,\n",
      "        0.0302], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.410308065416757e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0280, 0.0285, 0.0290, 0.0323], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0245, 0.0273, 0.0260, 0.0231, 0.0249, 0.0285, 0.0288, 0.0288, 0.0244,\n",
      "        0.0290], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0240, 0.0262, 0.0259, 0.0246, 0.0274, 0.0260, 0.0274, 0.0274, 0.0246,\n",
      "        0.0291], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.9957371932832757e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0286, 0.0277, 0.0300, 0.0314], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0310, 0.0286, 0.0314, 0.0300, 0.0288, 0.0255, 0.0286, 0.0300, 0.0297,\n",
      "        0.0267], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0283, 0.0279, 0.0283, 0.0283, 0.0279, 0.0259, 0.0279, 0.0283, 0.0269,\n",
      "        0.0259], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.376115728315199e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0278, 0.0228, 0.0279, 0.0266, 0.0248, 0.0303, 0.0275, 0.0269, 0.0255,\n",
      "        0.0257], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0251, 0.0255, 0.0275, 0.0279, 0.0275, 0.0229, 0.0251, 0.0262, 0.0259,\n",
      "        0.0273], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.638545295980293e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0286, 0.0269, 0.0297, 0.0316], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0258, 0.0286, 0.0273, 0.0297, 0.0243, 0.0245, 0.0297, 0.0245, 0.0297,\n",
      "        0.0232], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0246, 0.0273, 0.0273, 0.0285, 0.0259, 0.0268, 0.0285, 0.0268, 0.0285,\n",
      "        0.0234], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.0829879758821335e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0287, 0.0253, 0.0263, 0.0241, 0.0295, 0.0287, 0.0322, 0.0253, 0.0262,\n",
      "        0.0261], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0290, 0.0247, 0.0256, 0.0238, 0.0290, 0.0290, 0.0290, 0.0247, 0.0235,\n",
      "        0.0265], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.9734275156224612e-06\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0250, 0.0253, 0.0287, 0.0260], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0251, 0.0279, 0.0266, 0.0312, 0.0269, 0.0247, 0.0282, 0.0247, 0.0328,\n",
      "        0.0247], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0240, 0.0295, 0.0259, 0.0312, 0.0259, 0.0228, 0.0240, 0.0228, 0.0295,\n",
      "        0.0240], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.157717739872169e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0257, 0.0282, 0.0279, 0.0326], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0249, 0.0238, 0.0267, 0.0260, 0.0279, 0.0279, 0.0249, 0.0346, 0.0262,\n",
      "        0.0272], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0260, 0.0240, 0.0240, 0.0260, 0.0294, 0.0294, 0.0260, 0.0311, 0.0280,\n",
      "        0.0265], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.9624079616041854e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0244, 0.0286, 0.0286, 0.0319], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0263, 0.0263, 0.0224, 0.0224, 0.0319, 0.0286, 0.0319, 0.0319, 0.0243,\n",
      "        0.0255], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0237, 0.0237, 0.0278, 0.0237, 0.0287, 0.0287, 0.0287, 0.0287, 0.0242,\n",
      "        0.0264], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.685768650844693e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0266, 0.0312, 0.0259, 0.0241, 0.0305, 0.0266, 0.0286, 0.0290, 0.0259,\n",
      "        0.0250], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0239, 0.0280, 0.0274, 0.0261, 0.0280, 0.0261, 0.0280, 0.0280, 0.0274,\n",
      "        0.0261], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.4167976536991773e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0242, 0.0292, 0.0281, 0.0312], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0225, 0.0227, 0.0225, 0.0221, 0.0266, 0.0282, 0.0312, 0.0191, 0.0191,\n",
      "        0.0312], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0233, 0.0236, 0.0233, 0.0233, 0.0243, 0.0280, 0.0280, 0.0236, 0.0236,\n",
      "        0.0280], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.8876715886290185e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0316, 0.0273, 0.0254, 0.0262, 0.0275, 0.0239, 0.0255, 0.0262, 0.0283,\n",
      "        0.0229], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0284, 0.0284, 0.0251, 0.0236, 0.0270, 0.0236, 0.0246, 0.0236, 0.0274,\n",
      "        0.0229], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.693381929930183e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0264, 0.0273, 0.0275, 0.0318], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0318, 0.0273, 0.0249, 0.0219, 0.0243, 0.0249, 0.0279, 0.0275, 0.0275,\n",
      "        0.0310], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0286, 0.0276, 0.0238, 0.0217, 0.0238, 0.0238, 0.0279, 0.0286, 0.0286,\n",
      "        0.0296], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.7263461131733493e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0259, 0.0247, 0.0244, 0.0269, 0.0231, 0.0243, 0.0269, 0.0244, 0.0278,\n",
      "        0.0312], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0255, 0.0242, 0.0242, 0.0286, 0.0263, 0.0250, 0.0286, 0.0242, 0.0286,\n",
      "        0.0263], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.233013441989897e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0224, 0.0287, 0.0043, 0.0287, 0.0252, 0.0221, 0.0314, 0.0277, 0.0277,\n",
      "        0.0252], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0267, 0.0282, 0.0244, 0.0282, 0.0243, 0.0229, 0.0282, 0.0282, 0.0282,\n",
      "        0.0243], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.376321521704085e-05\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0261, 0.0259, 0.0294, 0.0305], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0249, 0.0259, 0.0261, 0.0241, 0.0294, 0.0259, 0.0247, 0.0238, 0.0261,\n",
      "        0.0261], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0241, 0.0273, 0.0256, 0.0241, 0.0274, 0.0273, 0.0256, 0.0251, 0.0256,\n",
      "        0.0256], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.1854731383209582e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0301, 0.0301, 0.0246, 0.0294, 0.0301, 0.0245, 0.0245, 0.0257, 0.0254,\n",
      "        0.0254], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0270, 0.0270, 0.0261, 0.0270, 0.0270, 0.0238, 0.0216, 0.0264, 0.0264,\n",
      "        0.0264], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.592178811435588e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0252, 0.0268, 0.0306, 0.0290], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0306, 0.0222, 0.0239, 0.0306, 0.0240, 0.0306, 0.0254, 0.0240, 0.0240,\n",
      "        0.0218], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0275, 0.0241, 0.0236, 0.0275, 0.0236, 0.0275, 0.0267, 0.0236, 0.0236,\n",
      "        0.0241], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.948478024540236e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0248, 0.0234, 0.0265, 0.0288, 0.0288, 0.0296, 0.0234, 0.0264, 0.0285,\n",
      "        0.0248], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0263, 0.0237, 0.0241, 0.0273, 0.0273, 0.0319, 0.0237, 0.0237, 0.0258,\n",
      "        0.0263], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.5335947359271813e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0246, 0.0281, 0.0303, 0.0282], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0231, 0.0231, 0.0227, 0.0303, 0.0239, 0.0281, 0.0240, 0.0253, 0.0303,\n",
      "        0.0197], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0236, 0.0236, 0.0217, 0.0273, 0.0255, 0.0259, 0.0247, 0.0258, 0.0273,\n",
      "        0.0219], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.292380370112369e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0244, 0.0283, 0.0301, 0.0277], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0239, 0.0253, 0.0279, 0.0277, 0.0301, 0.0225, 0.0228, 0.0301, 0.0282,\n",
      "        0.0205], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0253, 0.0257, 0.0251, 0.0271, 0.0271, 0.0236, 0.0236, 0.0271, 0.0253,\n",
      "        0.0215], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.943462616007309e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0244, 0.0286, 0.0302, 0.0267], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0234, 0.0238, 0.0267, 0.0302, 0.0228, 0.0254, 0.0249, 0.0267, 0.0271,\n",
      "        0.0226], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0254, 0.0252, 0.0272, 0.0272, 0.0230, 0.0256, 0.0252, 0.0272, 0.0252,\n",
      "        0.0230], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.9466988305794075e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0247, 0.0287, 0.0299, 0.0259], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0247, 0.0299, 0.0254, 0.0252, 0.0278, 0.0247, 0.0299, 0.0259, 0.0251,\n",
      "        0.0259], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0250, 0.0270, 0.0251, 0.0250, 0.0270, 0.0250, 0.0270, 0.0270, 0.0226,\n",
      "        0.0270], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.751170768533484e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0251, 0.0288, 0.0293, 0.0255], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0240, 0.0234, 0.0234, 0.0233, 0.0241, 0.0231, 0.0240, 0.0285, 0.0234,\n",
      "        0.0212], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0216, 0.0224, 0.0224, 0.0216, 0.0207, 0.0227, 0.0224, 0.0247, 0.0224,\n",
      "        0.0216], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.045010427944362e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0245, 0.0254, 0.0266, 0.0300, 0.0287, 0.0287, 0.0254, 0.0249, 0.0226,\n",
      "        0.0287], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0239, 0.0239, 0.0258, 0.0296, 0.0258, 0.0258, 0.0239, 0.0247, 0.0231,\n",
      "        0.0258], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.004827931363252e-06\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0220, 0.0241, 0.0265, 0.0256], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0254, 0.0227, 0.0256, 0.0242, 0.0261, 0.0261, 0.0238, 0.0265, 0.0285,\n",
      "        0.0238], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0234, 0.0221, 0.0239, 0.0258, 0.0256, 0.0256, 0.0239, 0.0256, 0.0256,\n",
      "        0.0239], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.9470066945359576e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0233, 0.0280, 0.0285, 0.0260], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0211, 0.0266, 0.0238, 0.0285, 0.0285, 0.0213, 0.0213, 0.0256, 0.0256,\n",
      "        0.0233], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0235, 0.0256, 0.0239, 0.0256, 0.0256, 0.0235, 0.0235, 0.0239, 0.0239,\n",
      "        0.0239], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.90546574635664e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0241, 0.0268, 0.0286, 0.0241, 0.0231, 0.0228, 0.0286, 0.0286, 0.0229,\n",
      "        0.0252], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0235, 0.0258, 0.0258, 0.0235, 0.0235, 0.0225, 0.0258, 0.0258, 0.0239,\n",
      "        0.0258], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.7848873287439346e-06\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0224, 0.0240, 0.0267, 0.0238], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0245, 0.0240, 0.0224, 0.0247, 0.0239, 0.0224, 0.0286, 0.0238, 0.0240,\n",
      "        0.0239], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0257, 0.0240, 0.0222, 0.0222, 0.0232, 0.0222, 0.0257, 0.0240, 0.0240,\n",
      "        0.0240], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.6360338577214861e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0243, 0.0276, 0.0283, 0.0242], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0221, 0.0278, 0.0242, 0.0243, 0.0243, 0.0283, 0.0283, 0.0236, 0.0283,\n",
      "        0.0283], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0218, 0.0292, 0.0218, 0.0238, 0.0218, 0.0254, 0.0254, 0.0238, 0.0254,\n",
      "        0.0254], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.630997409549309e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0244, 0.0275, 0.0279, 0.0238], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0233, 0.0233, 0.0275, 0.0279, 0.0238, 0.0237, 0.0195, 0.0233, 0.0221,\n",
      "        0.0232], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0235, 0.0235, 0.0240, 0.0251, 0.0251, 0.0230, 0.0214, 0.0230, 0.0214,\n",
      "        0.0214], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.909263457695488e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0243, 0.0267, 0.0272, 0.0242], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0272, 0.0243, 0.0226, 0.0267, 0.0272, 0.0221, 0.0229, 0.0227, 0.0217,\n",
      "        0.0271], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0245, 0.0225, 0.0231, 0.0244, 0.0245, 0.0217, 0.0231, 0.0225, 0.0215,\n",
      "        0.0245], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.0686107947985874e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0241, 0.0259, 0.0268, 0.0243], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0243, 0.0256, 0.0259, 0.0226, 0.0230, 0.0259, 0.0268, 0.0174, 0.0243,\n",
      "        0.0268], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0218, 0.0249, 0.0246, 0.0228, 0.0222, 0.0246, 0.0241, 0.0240, 0.0241,\n",
      "        0.0241], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.741710876667639e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0238, 0.0249, 0.0263, 0.0246], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0204, 0.0226, 0.0238, 0.0241, 0.0244, 0.0216, 0.0249, 0.0170, 0.0216,\n",
      "        0.0213], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0217, 0.0221, 0.0224, 0.0221, 0.0245, 0.0221, 0.0237, 0.0232, 0.0221,\n",
      "        0.0217], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.732692104880698e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0234, 0.0242, 0.0261, 0.0247], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0261, 0.0224, 0.0247, 0.0261, 0.0248, 0.0261, 0.0243, 0.0223, 0.0193,\n",
      "        0.0250], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0235, 0.0217, 0.0235, 0.0235, 0.0223, 0.0235, 0.0243, 0.0221, 0.0225,\n",
      "        0.0244], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.9521537473774515e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0229, 0.0235, 0.0260, 0.0246], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0249, 0.0225, 0.0221, 0.0212, 0.0284, 0.0246, 0.0260, 0.0223, 0.0229,\n",
      "        0.0229], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0224, 0.0222, 0.0224, 0.0224, 0.0234, 0.0234, 0.0234, 0.0222, 0.0222,\n",
      "        0.0222], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.236547738400986e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0225, 0.0230, 0.0269, 0.0239], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0221, 0.0222, 0.0243, 0.0242, 0.0239, 0.0269, 0.0219, 0.0269, 0.0247,\n",
      "        0.0269], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0201, 0.0227, 0.0219, 0.0250, 0.0242, 0.0242, 0.0222, 0.0242, 0.0250,\n",
      "        0.0242], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.273892843935755e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0219, 0.0226, 0.0275, 0.0233], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0198, 0.0216, 0.0255, 0.0216, 0.0207, 0.0275, 0.0275, 0.0206, 0.0211,\n",
      "        0.0275], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0201, 0.0218, 0.0248, 0.0229, 0.0214, 0.0248, 0.0248, 0.0218, 0.0229,\n",
      "        0.0248], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.034563405890367e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0207, 0.0206, 0.0230, 0.0237, 0.0277, 0.0277, 0.0203, 0.0251, 0.0277,\n",
      "        0.0206], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0200, 0.0217, 0.0226, 0.0240, 0.0249, 0.0249, 0.0186, 0.0249, 0.0249,\n",
      "        0.0217], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.8776219096471323e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0211, 0.0221, 0.0276, 0.0225], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0213, 0.0221, 0.0209, 0.0231, 0.0212, 0.0197, 0.0276, 0.0181, 0.0211,\n",
      "        0.0220], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0208, 0.0234, 0.0216, 0.0208, 0.0208, 0.0187, 0.0249, 0.0191, 0.0222,\n",
      "        0.0225], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.909188313220511e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0209, 0.0224, 0.0273, 0.0219], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0229, 0.0205, 0.0201, 0.0240, 0.0273, 0.0240, 0.0220, 0.0273, 0.0284,\n",
      "        0.0218], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0227, 0.0203, 0.0216, 0.0246, 0.0246, 0.0246, 0.0220, 0.0246, 0.0270,\n",
      "        0.0217], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.9948352019127924e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0208, 0.0226, 0.0267, 0.0214], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0205, 0.0185, 0.0200, 0.0267, 0.0185, 0.0185, 0.0211, 0.0208, 0.0267,\n",
      "        0.0216], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0200, 0.0190, 0.0190, 0.0240, 0.0190, 0.0194, 0.0214, 0.0208, 0.0240,\n",
      "        0.0246], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.5566666863596765e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0208, 0.0225, 0.0260, 0.0214], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0197, 0.0191, 0.0187, 0.0213, 0.0206, 0.0218, 0.0223, 0.0184, 0.0180,\n",
      "        0.0222], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0179, 0.0185, 0.0193, 0.0200, 0.0207, 0.0218, 0.0200, 0.0193, 0.0179,\n",
      "        0.0216], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.1611161880864529e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([0.0212, 0.0206, 0.0216, 0.0208, 0.0223, 0.0234, 0.0254, 0.0206, 0.0212,\n",
      "        0.0223], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0201, 0.0201, 0.0229, 0.0201, 0.0201, 0.0202, 0.0229, 0.0201, 0.0201,\n",
      "        0.0201], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.200559831384453e-06\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0223, 0.0220, 0.0213, 0.0239], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0213, 0.0199, 0.0194, 0.0213, 0.0227, 0.0194, 0.0202, 0.0231, 0.0252,\n",
      "        0.0252], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0208, 0.0198, 0.0191, 0.0202, 0.0204, 0.0175, 0.0204, 0.0208, 0.0227,\n",
      "        0.0227], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.8428366931620985e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0191, 0.0212, 0.0251, 0.0216], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0192, 0.0191, 0.0251, 0.0186, 0.0172, 0.0251, 0.0216, 0.0191, 0.0207,\n",
      "        0.0192], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0206, 0.0193, 0.0225, 0.0181, 0.0181, 0.0225, 0.0225, 0.0181, 0.0193,\n",
      "        0.0206], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.177401484004804e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0189, 0.0227, 0.0200, 0.0212, 0.0190, 0.0190, 0.0190, 0.0214, 0.0247,\n",
      "        0.0227], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0191, 0.0204, 0.0187, 0.0228, 0.0204, 0.0204, 0.0204, 0.0222, 0.0222,\n",
      "        0.0204], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.745798610703787e-06\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0200, 0.0211, 0.0206, 0.0194], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0194, 0.0221, 0.0249, 0.0199, 0.0183, 0.0201, 0.0221, 0.0221, 0.0242,\n",
      "        0.0203], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0190, 0.0199, 0.0257, 0.0190, 0.0178, 0.0190, 0.0199, 0.0199, 0.0218,\n",
      "        0.0205], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.347801455471199e-06\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0184, 0.0124, 0.0218, 0.0194], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0204, 0.0208, 0.0188, 0.0197, 0.0197, 0.0236, 0.0177, 0.0218, 0.0193,\n",
      "        0.0194], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0212, 0.0191, 0.0193, 0.0191, 0.0191, 0.0212, 0.0229, 0.0198, 0.0175,\n",
      "        0.0193], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.435791652213084e-06\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0212, 0.0211, 0.0196, 0.0218], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0211, 0.0199, 0.0225, 0.0186, 0.0203, 0.0214, 0.0246, 0.0214, 0.0225,\n",
      "        0.0274], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0190, 0.0196, 0.0203, 0.0193, 0.0203, 0.0188, 0.0247, 0.0188, 0.0203,\n",
      "        0.0247], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.6545395687426208e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0197, 0.0190, 0.0222, 0.0205], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0197, 0.0206, 0.0205, 0.0197, 0.0222, 0.0219, 0.0178, 0.0190, 0.0152,\n",
      "        0.0178], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0186, 0.0191, 0.0200, 0.0186, 0.0200, 0.0200, 0.0170, 0.0200, 0.0176,\n",
      "        0.0170], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.1953537725494243e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([0.0227, 0.0172, 0.0190, 0.0182, 0.0227, 0.0186, 0.0197, 0.0188, 0.0227,\n",
      "        0.0227], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0204, 0.0176, 0.0179, 0.0194, 0.0204, 0.0189, 0.0204, 0.0179, 0.0204,\n",
      "        0.0204], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.5150220608338714e-06\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0194, 0.0228, 0.0103, 0.0181, 0.0228, 0.0228, 0.0207, 0.0174, 0.0228,\n",
      "        0.0200], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0186, 0.0205, 0.0169, 0.0186, 0.0205, 0.0205, 0.0186, 0.0186, 0.0205,\n",
      "        0.0202], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.094933152984595e-06\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0170, 0.0110, 0.0228, 0.0173], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0227, 0.0189, 0.0199, 0.0174, 0.0176, 0.0159, 0.0179, 0.0203, 0.0177,\n",
      "        0.0227], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0205, 0.0179, 0.0205, 0.0183, 0.0183, 0.0183, 0.0184, 0.0205, 0.0179,\n",
      "        0.0259], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.3562065507576335e-06\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0185, 0.0201, 0.0199, 0.0190], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0219, 0.0189, 0.0193, 0.0198, 0.0168, 0.0176, 0.0176, 0.0176, 0.0173,\n",
      "        0.0189], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0197, 0.0174, 0.0178, 0.0197, 0.0178, 0.0197, 0.0197, 0.0174, 0.0178,\n",
      "        0.0174], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.1687651496904436e-06\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0168, 0.0210, 0.0183, 0.0195], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0206, 0.0186, 0.0177, 0.0170, 0.0129, 0.0186, 0.0210, 0.0182, 0.0173,\n",
      "        0.0173], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0185, 0.0178, 0.0167, 0.0169, 0.0179, 0.0178, 0.0204, 0.0167, 0.0185,\n",
      "        0.0185], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.697514330269769e-06\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.0178, -0.0017,  0.0216,  0.0171], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0162, 0.0192, 0.0182, 0.0192, 0.0192, 0.0192, 0.0182, 0.0167, 0.0175,\n",
      "        0.0192], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0178, 0.0175, 0.0164, 0.0175, 0.0175, 0.0175, 0.0175, 0.0177, 0.0164,\n",
      "        0.0175], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.249412091259728e-06\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([0.0172, 0.0172, 0.0174, 0.0217, 0.0169, 0.0177, 0.0172, 0.0167, 0.0177,\n",
      "        0.0141], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0165, 0.0165, 0.0181, 0.0176, 0.0182, 0.0157, 0.0165, 0.0166, 0.0157,\n",
      "        0.0181], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.496096607908839e-06\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0199, 0.0235, 0.0225, 0.0214], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0144, 0.0143, 0.0195, 0.0165, 0.0207, 0.0151, 0.0196, 0.0117, 0.0143,\n",
      "        0.0185], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0188, 0.0190, 0.0161, 0.0174, 0.0190, 0.0188, 0.0188, 0.0157, 0.0190,\n",
      "        0.0157], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.1752199498005211e-05\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0196, 0.0161, 0.0218, 0.0196, 0.0153, 0.0153, 0.0187, 0.0207, 0.0187,\n",
      "        0.0156], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0176, 0.0168, 0.0210, 0.0176, 0.0186, 0.0186, 0.0168, 0.0186, 0.0168,\n",
      "        0.0146], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.334027380537009e-06\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.0164, -0.0037,  0.0223,  0.0184], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0174, 0.0156, 0.0184, 0.0182, 0.0194, 0.0174, 0.0150, 0.0174, 0.0148,\n",
      "        0.0174], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0181, 0.0141, 0.0170, 0.0170, 0.0181, 0.0181, 0.0172, 0.0181, 0.0171,\n",
      "        0.0181], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.9408789739827625e-06\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0191, 0.0200, 0.0278, 0.0204], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0212, 0.0146, 0.0178, 0.0178, 0.0164, 0.0159, 0.0144, 0.0189, 0.0160,\n",
      "        0.0169], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0177, 0.0212, 0.0191, 0.0191, 0.0222, 0.0191, 0.0212, 0.0177, 0.0158,\n",
      "        0.0190], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.5454401363967918e-05\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0197, 0.0207, 0.0266, 0.0202], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0151, 0.0189, 0.0200, 0.0179, 0.0189, 0.0189, 0.0189, 0.0156, 0.0200,\n",
      "        0.0167], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0155, 0.0170, 0.0170, 0.0180, 0.0170, 0.0170, 0.0170, 0.0177, 0.0197,\n",
      "        0.0180], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.9400036964943865e-06\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0181, 0.0166, 0.0171, 0.0179, 0.0159, 0.0147, 0.0183, 0.0180, 0.0179,\n",
      "        0.0180], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0168, 0.0143, 0.0169, 0.0168, 0.0179, 0.0169, 0.0163, 0.0165, 0.0168,\n",
      "        0.0165], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.7023907023249194e-06\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.0188, -0.0028,  0.0200,  0.0172], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0141, 0.0189, 0.0164, 0.0216, 0.0189, 0.0200, 0.0189, 0.0194, 0.0164,\n",
      "        0.0164], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0162, 0.0174, 0.0174, 0.0180, 0.0174, 0.0208, 0.0174, 0.0163, 0.0174,\n",
      "        0.0174], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.7747915939689847e-06\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0207, 0.0227, 0.0223, 0.0201], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0181, 0.0161, 0.0161, 0.0195, 0.0181, 0.0204, 0.0184, 0.0205, 0.0178,\n",
      "        0.0186], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0163, 0.0184, 0.0184, 0.0182, 0.0163, 0.0175, 0.0171, 0.0184, 0.0164,\n",
      "        0.0184], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.484733269942808e-06\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0180, 0.0165, 0.0161, 0.0181, 0.0181, 0.0190, 0.0155, 0.0151, 0.0141,\n",
      "        0.0153], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0162, 0.0183, 0.0143, 0.0183, 0.0163, 0.0162, 0.0162, 0.0164, 0.0183,\n",
      "        0.0164], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.164526217209641e-06\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([0.0153, 0.0171, 0.0175, 0.0147, 0.0149, 0.0159, 0.0167, 0.0158, 0.0171,\n",
      "        0.0167], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0162, 0.0168, 0.0159, 0.0173, 0.0180, 0.0159, 0.0165, 0.0160, 0.0168,\n",
      "        0.0165], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.073535370072932e-06\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0187, 0.0213, 0.0230, 0.0200], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0188, 0.0142, 0.0156, 0.0139, 0.0200, 0.0187, 0.0188, 0.0166, 0.0172,\n",
      "        0.0189], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0169, 0.0171, 0.0170, 0.0158, 0.0213, 0.0192, 0.0169, 0.0170, 0.0173,\n",
      "        0.0170], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.6373750188213307e-06\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([0.0135, 0.0177, 0.0171, 0.0160, 0.0164, 0.0140, 0.0168, 0.0183, 0.0197,\n",
      "        0.0188], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0160, 0.0178, 0.0160, 0.0174, 0.0156, 0.0173, 0.0174, 0.0194, 0.0177,\n",
      "        0.0177], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.751417014223989e-06\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0209, 0.0145, 0.0165, 0.0193, 0.0166, 0.0163, 0.0193, 0.0140, 0.0199,\n",
      "        0.0187], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0208, 0.0170, 0.0169, 0.0174, 0.0174, 0.0174, 0.0174, 0.0157, 0.0179,\n",
      "        0.0194], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.3226666598930024e-06\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0185, 0.0214, 0.0223, 0.0203], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0163, 0.0165, 0.0160, 0.0160, 0.0182, 0.0165, 0.0160, 0.0170, 0.0195,\n",
      "        0.0170], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0166, 0.0174, 0.0178, 0.0178, 0.0154, 0.0178, 0.0175, 0.0172, 0.0175,\n",
      "        0.0172], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.3461775526811834e-06\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0193, 0.0220, 0.0215, 0.0200], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0190, 0.0186, 0.0145, 0.0205, 0.0217, 0.0190, 0.0192, 0.0173, 0.0170,\n",
      "        0.0171], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0171, 0.0167, 0.0156, 0.0195, 0.0182, 0.0171, 0.0173, 0.0171, 0.0178,\n",
      "        0.0173], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.901083462347742e-06\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0198, 0.0224, 0.0211, 0.0196], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0188, 0.0188, 0.0161, 0.0185, 0.0167, 0.0188, 0.0188, 0.0188, 0.0220,\n",
      "        0.0167], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0169, 0.0169, 0.0185, 0.0171, 0.0169, 0.0169, 0.0169, 0.0169, 0.0181,\n",
      "        0.0169], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.097962573723635e-06\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0188, 0.0162, 0.0188, 0.0142, 0.0157, 0.0188, 0.0194, 0.0181, 0.0181,\n",
      "        0.0188], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0169, 0.0167, 0.0169, 0.0186, 0.0169, 0.0169, 0.0175, 0.0167, 0.0167,\n",
      "        0.0169], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.294598511478398e-06\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([0.0193, 0.0174, 0.0193, 0.0195, 0.0200, 0.0161, 0.0202, 0.0186, 0.0177,\n",
      "        0.0171], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0173, 0.0170, 0.0173, 0.0165, 0.0180, 0.0173, 0.0185, 0.0170, 0.0165,\n",
      "        0.0165], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.9109787647030316e-06\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([0.0192, 0.0206, 0.0145, 0.0160, 0.0155, 0.0241, 0.0216, 0.0175, 0.0169,\n",
      "        0.0192], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0173, 0.0186, 0.0186, 0.0152, 0.0177, 0.0194, 0.0194, 0.0171, 0.0157,\n",
      "        0.0173], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.172056600917131e-06\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.0191, -0.0023,  0.0221,  0.0157], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0169, 0.0152, 0.0190, 0.0190, 0.0190, 0.0185, 0.0152, 0.0169, 0.0188,\n",
      "        0.0190], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0167, 0.0172, 0.0171, 0.0171, 0.0171, 0.0167, 0.0172, 0.0167, 0.0174,\n",
      "        0.0171], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.8345928058115533e-06\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0220, 0.0172, 0.0220, 0.0190, 0.0201, 0.0164, 0.0167, 0.0134, 0.0188,\n",
      "        0.0186], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0198, 0.0189, 0.0198, 0.0156, 0.0181, 0.0173, 0.0173, 0.0157, 0.0169,\n",
      "        0.0173], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.932841536880005e-06\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([0.0164, 0.0159, 0.0154, 0.0154, 0.0172, 0.0150, 0.0186, 0.0175, 0.0190,\n",
      "        0.0171], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0166, 0.0167, 0.0166, 0.0166, 0.0171, 0.0162, 0.0167, 0.0178, 0.0171,\n",
      "        0.0158], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.39186920478096e-06\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0178, 0.0226, 0.0213, 0.0200], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0226, 0.0150, 0.0172, 0.0184, 0.0160, 0.0178, 0.0152, 0.0164, 0.0163,\n",
      "        0.0164], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0203, 0.0164, 0.0159, 0.0166, 0.0158, 0.0204, 0.0160, 0.0157, 0.0166,\n",
      "        0.0157], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.093717966999975e-06\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0171, 0.0231, 0.0212, 0.0204], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0181, 0.0181, 0.0181, 0.0166, 0.0166, 0.0168, 0.0166, 0.0167, 0.0181,\n",
      "        0.0124], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0163, 0.0163, 0.0163, 0.0161, 0.0161, 0.0163, 0.0161, 0.0159, 0.0163,\n",
      "        0.0158], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.6318027721572435e-06\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([ 0.0177,  0.0169,  0.0168,  0.0177, -0.0025,  0.0169,  0.0158,  0.0233,\n",
      "         0.0216,  0.0148], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0159, 0.0165, 0.0158, 0.0159, 0.0201, 0.0165, 0.0164, 0.0210, 0.0210,\n",
      "        0.0155], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.2485578635241836e-05\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0166, 0.0262, 0.0185, 0.0211], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0262, 0.0200, 0.0195, 0.0140, 0.0185, 0.0183, 0.0149, 0.0148, 0.0169,\n",
      "        0.0148], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0235, 0.0176, 0.0178, 0.0177, 0.0235, 0.0176, 0.0176, 0.0161, 0.0161,\n",
      "        0.0180], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.539927082689246e-06\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0151, 0.0151, 0.0178, 0.0153, 0.0183, 0.0132, 0.0172, 0.0132, 0.0158,\n",
      "        0.0183], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0165, 0.0165, 0.0244, 0.0179, 0.0183, 0.0170, 0.0170, 0.0170, 0.0183,\n",
      "        0.0189], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.968128895503469e-06\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([0.0140, 0.0124, 0.0140, 0.0142, 0.0140, 0.0140, 0.0190, 0.0168, 0.0140,\n",
      "        0.0190], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0162, 0.0166, 0.0162, 0.0166, 0.0162, 0.0162, 0.0171, 0.0171, 0.0162,\n",
      "        0.0176], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.343052180251107e-06\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0179, 0.0242, 0.0228, 0.0219], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0155, 0.0146, 0.0171, 0.0155, 0.0178, 0.0178, 0.0171, 0.0164, 0.0171,\n",
      "        0.0139], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0150, 0.0150, 0.0154, 0.0150, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154,\n",
      "        0.0160], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.6095221983268857e-06\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0186, 0.0225, 0.0258, 0.0216], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0200, 0.0160, 0.0200, 0.0152, 0.0200, 0.0200, 0.0178, 0.0178, 0.0199,\n",
      "        0.0153], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0180, 0.0168, 0.0180, 0.0155, 0.0180, 0.0180, 0.0170, 0.0170, 0.0233,\n",
      "        0.0176], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.4635327210708056e-06\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0138, 0.0213, 0.0188, 0.0163, 0.0168, 0.0213, 0.0221, 0.0221, 0.0171,\n",
      "        0.0150], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0199, 0.0249, 0.0185, 0.0189, 0.0185, 0.0249, 0.0199, 0.0199, 0.0185,\n",
      "        0.0189], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.0026853487943299e-05\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0227, 0.0191, 0.0132, 0.0077], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0173, 0.0210, 0.0195, 0.0178, 0.0195, 0.0210, 0.0213, 0.0264, 0.0139,\n",
      "        0.0178], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0156, 0.0189, 0.0189, 0.0166, 0.0189, 0.0189, 0.0172, 0.0237, 0.0177,\n",
      "        0.0166], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.32897229277296e-06\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0238, 0.0194, 0.0121, 0.0082], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0165, 0.0213, 0.0194, 0.0213, 0.0249, 0.0195, 0.0200, 0.0164, 0.0164,\n",
      "        0.0164], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0180, 0.0192, 0.0181, 0.0192, 0.0224, 0.0158, 0.0180, 0.0181, 0.0181,\n",
      "        0.0181], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.626628197001992e-06\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0240, 0.0195, 0.0108, 0.0092], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0132, 0.0201, 0.0204, 0.0179, 0.0187, 0.0187, 0.0187, 0.0144, 0.0145,\n",
      "        0.0179], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0175, 0.0193, 0.0155, 0.0181, 0.0178, 0.0178, 0.0178, 0.0181, 0.0166,\n",
      "        0.0181], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.415313691832125e-06\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0231, 0.0204, 0.0096, 0.0103], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0177, 0.0174, 0.0233, 0.0152, 0.0152, 0.0188, 0.0188, 0.0174, 0.0174,\n",
      "        0.0144], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0183, 0.0172, 0.0218, 0.0196, 0.0172, 0.0176, 0.0188, 0.0172, 0.0172,\n",
      "        0.0169], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.3753644856915344e-06\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0222, 0.0217, 0.0088, 0.0108], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0211, 0.0165, 0.0176, 0.0186, 0.0209, 0.0133, 0.0120, 0.0177, 0.0195,\n",
      "        0.0180], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0190, 0.0165, 0.0195, 0.0195, 0.0223, 0.0173, 0.0173, 0.0190, 0.0190,\n",
      "        0.0177], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.68451559956884e-06\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0217, 0.0230, 0.0089, 0.0105], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0148, 0.0196, 0.0180, 0.0215, 0.0208, 0.0175, 0.0193, 0.0180, 0.0181,\n",
      "        0.0217], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0181, 0.0174, 0.0166, 0.0192, 0.0187, 0.0162, 0.0181, 0.0166, 0.0192,\n",
      "        0.0207], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.470844148978358e-06\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0180, 0.0273, 0.0214, 0.0239], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0242, 0.0184, 0.0184, 0.0196, 0.0169, 0.0213, 0.0184, 0.0164, 0.0239,\n",
      "        0.0213], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0246, 0.0180, 0.0180, 0.0177, 0.0174, 0.0170, 0.0180, 0.0180, 0.0217,\n",
      "        0.0170], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.815206011699047e-06\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0183, 0.0277, 0.0216, 0.0230], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0173, 0.0211, 0.0168, 0.0183, 0.0188, 0.0173, 0.0168, 0.0190, 0.0188,\n",
      "        0.0166], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0178, 0.0194, 0.0179, 0.0182, 0.0179, 0.0178, 0.0179, 0.0171, 0.0179,\n",
      "        0.0150], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.3282399322633864e-06\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0186, 0.0278, 0.0218, 0.0223], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0278, 0.0278, 0.0278, 0.0191, 0.0176, 0.0193, 0.0176, 0.0174, 0.0185,\n",
      "        0.0176], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0251, 0.0251, 0.0251, 0.0181, 0.0181, 0.0174, 0.0181, 0.0181, 0.0181,\n",
      "        0.0181], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.9291636565176304e-06\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0187, 0.0277, 0.0222, 0.0216], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0277, 0.0212, 0.0182, 0.0143, 0.0202, 0.0252, 0.0160, 0.0192, 0.0182,\n",
      "        0.0165], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0250, 0.0199, 0.0164, 0.0153, 0.0177, 0.0250, 0.0199, 0.0182, 0.0164,\n",
      "        0.0202], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.31693740413175e-06\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0188, 0.0277, 0.0225, 0.0209], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0182, 0.0192, 0.0225, 0.0186, 0.0160, 0.0186, 0.0142, 0.0142, 0.0142,\n",
      "        0.0172], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0182, 0.0179, 0.0249, 0.0181, 0.0181, 0.0181, 0.0154, 0.0181, 0.0181,\n",
      "        0.0164], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.529543275566539e-06\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0188, 0.0265, 0.0232, 0.0214], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0192, 0.0192, 0.0191, 0.0211, 0.0165, 0.0168, 0.0190, 0.0176, 0.0149,\n",
      "        0.0192], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0174, 0.0174, 0.0180, 0.0190, 0.0180, 0.0179, 0.0173, 0.0173, 0.0156,\n",
      "        0.0173], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.281050228702952e-06\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0185, 0.0257, 0.0238, 0.0218], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0170, 0.0194, 0.0172, 0.0194, 0.0162, 0.0187, 0.0194, 0.0179, 0.0185,\n",
      "        0.0194], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0180, 0.0180, 0.0173, 0.0180, 0.0180, 0.0169, 0.0180, 0.0169, 0.0213,\n",
      "        0.0180], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.524055616959231e-06\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0190, 0.0250, 0.0238, 0.0223], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0167, 0.0163, 0.0250, 0.0178, 0.0184, 0.0172, 0.0193, 0.0162, 0.0190,\n",
      "        0.0120], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0150, 0.0150, 0.0225, 0.0176, 0.0176, 0.0162, 0.0174, 0.0150, 0.0170,\n",
      "        0.0217], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.1540691957634408e-05\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0182, 0.0240, 0.0251, 0.0227], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0197, 0.0181, 0.0197, 0.0176, 0.0134, 0.0188, 0.0179, 0.0185, 0.0201,\n",
      "        0.0134], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0181, 0.0174, 0.0181, 0.0179, 0.0226, 0.0169, 0.0174, 0.0174, 0.0178,\n",
      "        0.0226], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.830490327847656e-05\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0177, 0.0227, 0.0244, 0.0228], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0131, 0.0227, 0.0172, 0.0206, 0.0203, 0.0129, 0.0172, 0.0169, 0.0203,\n",
      "        0.0205], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0178, 0.0220, 0.0183, 0.0185, 0.0182, 0.0163, 0.0184, 0.0183, 0.0182,\n",
      "        0.0183], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.559554665524047e-06\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0178, 0.0223, 0.0235, 0.0222], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0196, 0.0167, 0.0196, 0.0195, 0.0204, 0.0208, 0.0205, 0.0167, 0.0204,\n",
      "        0.0204], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0184, 0.0188, 0.0184, 0.0185, 0.0184, 0.0185, 0.0185, 0.0184, 0.0184,\n",
      "        0.0184], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.352098701725481e-06\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0180, 0.0219, 0.0222, 0.0216], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0169, 0.0180, 0.0183, 0.0183, 0.0187, 0.0183, 0.0183, 0.0196, 0.0206,\n",
      "        0.0199], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0185, 0.0179, 0.0185, 0.0185, 0.0156, 0.0185, 0.0185, 0.0180, 0.0187,\n",
      "        0.0156], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.8085120195319178e-06\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0184, 0.0211, 0.0207, 0.0214], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0193, 0.0211, 0.0193, 0.0182, 0.0190, 0.0198, 0.0156, 0.0159, 0.0193,\n",
      "        0.0186], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0178, 0.0193, 0.0178, 0.0179, 0.0173, 0.0193, 0.0179, 0.0178, 0.0178,\n",
      "        0.0179], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.242700020360644e-06\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0223, 0.0210, 0.0121, 0.0109], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0183, 0.0208, 0.0214, 0.0113, 0.0183, 0.0136, 0.0149, 0.0187, 0.0188,\n",
      "        0.0161], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0169, 0.0197, 0.0201, 0.0157, 0.0176, 0.0160, 0.0168, 0.0192, 0.0169,\n",
      "        0.0175], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.9790415939933155e-06\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0231, 0.0217, 0.0111, 0.0105], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0207, 0.0195, 0.0191, 0.0208, 0.0104, 0.0184, 0.0119, 0.0191, 0.0166,\n",
      "        0.0164], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0208, 0.0195, 0.0172, 0.0187, 0.0180, 0.0172, 0.0154, 0.0172, 0.0176,\n",
      "        0.0195], grad_fn=<AddBackward0>)\n",
      "LOSS: 9.368295650347136e-06\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0232, 0.0237, 0.0106, 0.0093], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0158, 0.0211, 0.0158, 0.0168, 0.0211, 0.0180, 0.0193, 0.0180, 0.0171,\n",
      "        0.0222], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0195, 0.0190, 0.0195, 0.0141, 0.0190, 0.0195, 0.0174, 0.0195, 0.0174,\n",
      "        0.0200], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.7179145187546965e-06\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0191, 0.0232, 0.0151, 0.0182], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0159, 0.0156, 0.0182, 0.0159, 0.0190, 0.0159, 0.0173, 0.0232, 0.0184,\n",
      "        0.0156], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0181, 0.0181, 0.0159, 0.0181, 0.0209, 0.0181, 0.0181, 0.0209, 0.0182,\n",
      "        0.0164], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.5225871215516236e-06\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([0.0208, 0.0158, 0.0163, 0.0166, 0.0158, 0.0166, 0.0175, 0.0207, 0.0168,\n",
      "        0.0209], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0211, 0.0187, 0.0190, 0.0190, 0.0206, 0.0190, 0.0157, 0.0200, 0.0157,\n",
      "        0.0188], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.00510338699678e-06\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0171, 0.0224, 0.0160, 0.0176], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0157, 0.0160, 0.0190, 0.0190, 0.0190, 0.0169, 0.0168, 0.0216, 0.0168,\n",
      "        0.0139], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0195, 0.0167, 0.0171, 0.0171, 0.0171, 0.0171, 0.0183, 0.0195, 0.0198,\n",
      "        0.0169], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.967411314282799e-06\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0181, 0.0211, 0.0160, 0.0176], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0220, 0.0173, 0.0208, 0.0154, 0.0220, 0.0150, 0.0211, 0.0176, 0.0253,\n",
      "        0.0176], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0198, 0.0173, 0.0187, 0.0212, 0.0198, 0.0161, 0.0190, 0.0173, 0.0190,\n",
      "        0.0228], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.2028225683025084e-05\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0191, 0.0205, 0.0155, 0.0187], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0205, 0.0196, 0.0186, 0.0164, 0.0174, 0.0186, 0.0196, 0.0172, 0.0183,\n",
      "        0.0186], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0185, 0.0180, 0.0168, 0.0206, 0.0171, 0.0168, 0.0180, 0.0156, 0.0168,\n",
      "        0.0168], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.213718966639135e-06\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0197, 0.0199, 0.0161, 0.0192], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0189, 0.0156, 0.0195, 0.0212, 0.0228, 0.0184, 0.0192, 0.0161, 0.0192,\n",
      "        0.0183], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0175, 0.0169, 0.0205, 0.0190, 0.0205, 0.0175, 0.0175, 0.0179, 0.0175,\n",
      "        0.0190], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.4640094125061296e-06\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0197, 0.0188, 0.0176, 0.0196], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0184, 0.0185, 0.0165, 0.0212, 0.0185, 0.0215, 0.0159, 0.0183, 0.0213,\n",
      "        0.0184], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0192, 0.0208, 0.0203, 0.0194, 0.0208, 0.0177, 0.0182, 0.0203, 0.0203,\n",
      "        0.0192], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.339296876627486e-06\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0171, 0.0174, 0.0266, 0.0170], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0197, 0.0191, 0.0200, 0.0235, 0.0180, 0.0182, 0.0191, 0.0200, 0.0126,\n",
      "        0.0217], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0178, 0.0196, 0.0211, 0.0211, 0.0199, 0.0180, 0.0196, 0.0178, 0.0180,\n",
      "        0.0203], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.1452379921101965e-06\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0191, 0.0202, 0.0196, 0.0202, 0.0202, 0.0190, 0.0175, 0.0199, 0.0200,\n",
      "        0.0191], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0193, 0.0193, 0.0201, 0.0209, 0.0193, 0.0193, 0.0207, 0.0185, 0.0193,\n",
      "        0.0193], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.567862796036934e-06\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0187, 0.0171, 0.0266, 0.0174], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0221, 0.0155, 0.0136, 0.0234, 0.0188, 0.0187, 0.0204, 0.0234, 0.0266,\n",
      "        0.0174], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0199, 0.0217, 0.0217, 0.0211, 0.0180, 0.0217, 0.0211, 0.0211, 0.0212,\n",
      "        0.0212], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.7440004739910364e-05\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0252, 0.0195, 0.0158, 0.0227], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0215, 0.0218, 0.0209, 0.0192, 0.0165, 0.0155, 0.0224, 0.0195, 0.0208,\n",
      "        0.0174], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0193, 0.0197, 0.0201, 0.0186, 0.0188, 0.0172, 0.0201, 0.0227, 0.0208,\n",
      "        0.0185], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.4816869174392195e-06\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0208, 0.0211, 0.0214, 0.0176], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0200, 0.0217, 0.0169, 0.0187, 0.0205, 0.0209, 0.0187, 0.0229, 0.0208,\n",
      "        0.0209], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0180, 0.0196, 0.0209, 0.0209, 0.0180, 0.0232, 0.0209, 0.0209, 0.0184,\n",
      "        0.0224], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.6884705372795e-06\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0247, 0.0168, 0.0229, 0.0181, 0.0182, 0.0159, 0.0247, 0.0164, 0.0181,\n",
      "        0.0247], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0211, 0.0185, 0.0222, 0.0204, 0.0216, 0.0184, 0.0222, 0.0204, 0.0204,\n",
      "        0.0211], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.044800779316574e-06\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0178, 0.0265, 0.0209, 0.0162], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0193, 0.0210, 0.0190, 0.0181, 0.0193, 0.0168, 0.0163, 0.0210, 0.0193,\n",
      "        0.0210], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0211, 0.0211, 0.0181, 0.0191, 0.0211, 0.0185, 0.0226, 0.0211, 0.0211,\n",
      "        0.0211], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.428997155831894e-06\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0207, 0.0171, 0.0222, 0.0281, 0.0164, 0.0222, 0.0197, 0.0173, 0.0222,\n",
      "        0.0158], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0223, 0.0260, 0.0223, 0.0253, 0.0209, 0.0223, 0.0223, 0.0200, 0.0223,\n",
      "        0.0200], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.4164715139486361e-05\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0162, 0.0274, 0.0212, 0.0219], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0155, 0.0274, 0.0260, 0.0260, 0.0198, 0.0180, 0.0133, 0.0260, 0.0204,\n",
      "        0.0190], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0220, 0.0247, 0.0234, 0.0234, 0.0234, 0.0213, 0.0218, 0.0234, 0.0207,\n",
      "        0.0234], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.844006692408584e-05\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0165, 0.0259, 0.0224, 0.0234], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0195, 0.0156, 0.0224, 0.0219, 0.0224, 0.0191, 0.0156, 0.0248, 0.0233,\n",
      "        0.0244], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0220, 0.0220, 0.0233, 0.0184, 0.0233, 0.0222, 0.0220, 0.0221, 0.0212,\n",
      "        0.0222], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.2894420251541305e-05\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0185, 0.0240, 0.0218, 0.0256], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0256, 0.0179, 0.0223, 0.0248, 0.0210, 0.0198, 0.0248, 0.0248, 0.0206,\n",
      "        0.0248], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0199, 0.0201, 0.0201, 0.0223, 0.0200, 0.0204, 0.0223, 0.0223, 0.0201,\n",
      "        0.0223], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.8792605816270225e-06\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0203, 0.0187, 0.0225, 0.0253, 0.0225, 0.0230, 0.0250, 0.0244, 0.0253,\n",
      "        0.0231], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0219, 0.0213, 0.0208, 0.0227, 0.0208, 0.0225, 0.0225, 0.0227, 0.0227,\n",
      "        0.0227], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.7591494219668675e-06\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0259, 0.0212, 0.0145, 0.0141], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0252, 0.0260, 0.0232, 0.0260, 0.0170, 0.0232, 0.0179, 0.0247, 0.0191,\n",
      "        0.0222], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0236, 0.0234, 0.0225, 0.0234, 0.0242, 0.0225, 0.0227, 0.0234, 0.0227,\n",
      "        0.0240], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.1055920367653016e-05\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0274, 0.0222, 0.0140, 0.0136], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0216, 0.0273, 0.0258, 0.0213, 0.0226, 0.0222, 0.0274, 0.0213, 0.0244,\n",
      "        0.0213], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0232, 0.0254, 0.0247, 0.0242, 0.0245, 0.0232, 0.0247, 0.0242, 0.0254,\n",
      "        0.0242], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.49961089543649e-06\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0276, 0.0232, 0.0147, 0.0129], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0225, 0.0237, 0.0220, 0.0198, 0.0196, 0.0198, 0.0255, 0.0195, 0.0225,\n",
      "        0.0220], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0239, 0.0220, 0.0251, 0.0246, 0.0245, 0.0246, 0.0239, 0.0239, 0.0239,\n",
      "        0.0251], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.1713699677784462e-05\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0243, 0.0243, 0.0189, 0.0227, 0.0257, 0.0243, 0.0189, 0.0211, 0.0252,\n",
      "        0.0184], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0227, 0.0227, 0.0251, 0.0227, 0.0244, 0.0227, 0.0251, 0.0224, 0.0227,\n",
      "        0.0231], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.179782884719316e-05\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0211, 0.0283, 0.0172, 0.0126], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0279, 0.0236, 0.0236, 0.0262, 0.0237, 0.0258, 0.0258, 0.0223, 0.0226,\n",
      "        0.0265], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0251, 0.0221, 0.0221, 0.0227, 0.0236, 0.0232, 0.0232, 0.0222, 0.0232,\n",
      "        0.0238], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.536971573543269e-06\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0102, 0.0300, 0.0256, 0.0232], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0269, 0.0256, 0.0309, 0.0300, 0.0300, 0.0189, 0.0171, 0.0295, 0.0229,\n",
      "        0.0261], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0253, 0.0270, 0.0270, 0.0270, 0.0270, 0.0265, 0.0226, 0.0262, 0.0244,\n",
      "        0.0253], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.4043626833881717e-05\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0306, 0.0356, 0.0285, 0.0267, 0.0294, 0.0272, 0.0216, 0.0285, 0.0274,\n",
      "        0.0200], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0275, 0.0320, 0.0266, 0.0266, 0.0302, 0.0279, 0.0275, 0.0266, 0.0279,\n",
      "        0.0235], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.882205864007119e-06\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0077, 0.0362, 0.0286, 0.0212], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0145, 0.0215, 0.0249, 0.0090, 0.0304, 0.0306, 0.0112, 0.0243, 0.0298,\n",
      "        0.0115], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0246, 0.0271, 0.0255, 0.0255, 0.0275, 0.0284, 0.0292, 0.0273, 0.0273,\n",
      "        0.0241], grad_fn=<AddBackward0>)\n",
      "LOSS: 9.140606562141329e-05\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0119, 0.0345, 0.0278, 0.0231], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0287, 0.0161, 0.0145, 0.0283, 0.0288, 0.0283, 0.0168, 0.0131, 0.0257,\n",
      "        0.0252], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0275, 0.0240, 0.0246, 0.0270, 0.0241, 0.0270, 0.0267, 0.0283, 0.0267,\n",
      "        0.0240], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.2644532843260095e-05\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0283, 0.0202, 0.0307, 0.0273, 0.0213, 0.0283, 0.0283, 0.0283, 0.0238,\n",
      "        0.0236], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0276, 0.0278, 0.0276, 0.0302, 0.0237, 0.0276, 0.0276, 0.0276, 0.0249,\n",
      "        0.0237], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.416709533776157e-06\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0303, 0.0268, 0.0171, 0.0274], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0286, 0.0226, 0.0225, 0.0212, 0.0274, 0.0366, 0.0366, 0.0274, 0.0362,\n",
      "        0.0266], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0329, 0.0302, 0.0286, 0.0260, 0.0329, 0.0329, 0.0329, 0.0286, 0.0326,\n",
      "        0.0322], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.3798496840754524e-05\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([0.0321, 0.0176, 0.0190, 0.0299], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0251, 0.0186, 0.0407, 0.0148, 0.0186, 0.0363, 0.0114, 0.0303, 0.0336,\n",
      "        0.0285], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0315, 0.0272, 0.0302, 0.0320, 0.0272, 0.0367, 0.0320, 0.0367, 0.0378,\n",
      "        0.0367], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00011428521247580647\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([0.0320, 0.0191, 0.0204, 0.0307], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0281, 0.0182, 0.0307, 0.0349, 0.0350, 0.0189, 0.0363, 0.0403, 0.0307,\n",
      "        0.0303], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0315, 0.0313, 0.0314, 0.0378, 0.0329, 0.0273, 0.0362, 0.0314, 0.0314,\n",
      "        0.0362], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.815572563325986e-05\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([0.0300, 0.0224, 0.0216, 0.0315], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0322, 0.0322, 0.0306, 0.0239, 0.0269, 0.0293, 0.0322, 0.0316, 0.0264,\n",
      "        0.0251], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0334, 0.0334, 0.0316, 0.0283, 0.0389, 0.0361, 0.0334, 0.0285, 0.0313,\n",
      "        0.0296], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.685177059902344e-05\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0347, 0.0347, 0.0357, 0.0298, 0.0284, 0.0328, 0.0347, 0.0250, 0.0357,\n",
      "        0.0250], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0329, 0.0329, 0.0322, 0.0286, 0.0294, 0.0322, 0.0329, 0.0294, 0.0322,\n",
      "        0.0294], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.769301191729028e-06\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([0.0216, 0.0327, 0.0295, 0.0328], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0356, 0.0275, 0.0359, 0.0359, 0.0359, 0.0274, 0.0338, 0.0341, 0.0330,\n",
      "        0.0359], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0320, 0.0294, 0.0341, 0.0323, 0.0341, 0.0294, 0.0304, 0.0307, 0.0323,\n",
      "        0.0341], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.665139608230675e-06\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0394, 0.0363, 0.0363, 0.0294, 0.0364, 0.0361, 0.0279, 0.0331, 0.0155,\n",
      "        0.0185], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0355, 0.0359, 0.0359, 0.0338, 0.0338, 0.0335, 0.0355, 0.0355, 0.0328,\n",
      "        0.0328], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.118694000178948e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0343, 0.0333, 0.0405, 0.0412, 0.0174, 0.0322, 0.0373, 0.0371, 0.0385,\n",
      "        0.0376], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0375, 0.0336, 0.0314, 0.0382, 0.0419, 0.0371, 0.0323, 0.0347, 0.0334,\n",
      "        0.0362], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.909130363259465e-05\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0401, 0.0360, 0.0363, 0.0361, 0.0388, 0.0265, 0.0265, 0.0345, 0.0361,\n",
      "        0.0373], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0320, 0.0361, 0.0327, 0.0361, 0.0382, 0.0336, 0.0336, 0.0336, 0.0361,\n",
      "        0.0346], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.882821561594028e-05\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0403, 0.0347, 0.0307, 0.0368], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0368, 0.0388, 0.0371, 0.0409, 0.0383, 0.0307, 0.0459, 0.0383, 0.0298,\n",
      "        0.0368], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0363, 0.0413, 0.0361, 0.0413, 0.0345, 0.0363, 0.0413, 0.0345, 0.0363,\n",
      "        0.0363], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.3135238077666145e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0388, 0.0355, 0.0312, 0.0398], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0384, 0.0399, 0.0402, 0.0388, 0.0384, 0.0402, 0.0402, 0.0430, 0.0402,\n",
      "        0.0405], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0387, 0.0351, 0.0435, 0.0358, 0.0387, 0.0435, 0.0435, 0.0358, 0.0435,\n",
      "        0.0356], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.4967127754061949e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([0.0436, 0.0343, 0.0412, 0.0339, 0.0448, 0.0321, 0.0397, 0.0304, 0.0321,\n",
      "        0.0434], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0370, 0.0444, 0.0370, 0.0392, 0.0403, 0.0368, 0.0392, 0.0363, 0.0368,\n",
      "        0.0444], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.8928358005941845e-05\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([0.0312, 0.0234, 0.0365, 0.0400], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0489, 0.0298, 0.0424, 0.0442, 0.0353, 0.0412, 0.0412, 0.0399, 0.0391,\n",
      "        0.0442], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0440, 0.0379, 0.0370, 0.0373, 0.0373, 0.0370, 0.0370, 0.0440, 0.0381,\n",
      "        0.0373], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.665030478965491e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0414, 0.0303, 0.0416, 0.0417], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0417, 0.0257, 0.0393, 0.0221, 0.0389, 0.0432, 0.0401, 0.0248, 0.0438,\n",
      "        0.0471], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0375, 0.0384, 0.0389, 0.0409, 0.0384, 0.0461, 0.0361, 0.0415, 0.0384,\n",
      "        0.0461], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.663568587508053e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0419, 0.0326, 0.0430, 0.0403], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0290, 0.0408, 0.0525, 0.0403, 0.0336, 0.0430, 0.0411, 0.0264, 0.0347,\n",
      "        0.0525], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0389, 0.0431, 0.0473, 0.0387, 0.0385, 0.0411, 0.0385, 0.0417, 0.0357,\n",
      "        0.0473], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.294244718039408e-05\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([0.0365, 0.0368, 0.0514, 0.0342, 0.0358, 0.0459, 0.0360, 0.0413, 0.0390,\n",
      "        0.0387], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0399, 0.0348, 0.0462, 0.0369, 0.0453, 0.0462, 0.0413, 0.0462, 0.0396,\n",
      "        0.0382], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.9230003090342507e-05\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0398, 0.0355, 0.0410, 0.0430], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0375, 0.0410, 0.0404, 0.0410, 0.0402, 0.0410, 0.0372, 0.0430, 0.0410,\n",
      "        0.0399], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0450, 0.0439, 0.0382, 0.0439, 0.0439, 0.0439, 0.0372, 0.0401, 0.0439,\n",
      "        0.0401], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.1740036825358402e-05\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0376, 0.0441, 0.0402, 0.0432, 0.0427, 0.0455, 0.0359, 0.0455, 0.0448,\n",
      "        0.0374], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0390, 0.0390, 0.0389, 0.0340, 0.0394, 0.0419, 0.0446, 0.0419, 0.0403,\n",
      "        0.0402], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.544608105381485e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0406, 0.0451, 0.0362, 0.0415], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0336, 0.0406, 0.0384, 0.0415, 0.0415, 0.0370, 0.0370, 0.0384, 0.0462,\n",
      "        0.0421], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0440, 0.0406, 0.0387, 0.0406, 0.0406, 0.0406, 0.0424, 0.0387, 0.0422,\n",
      "        0.0387], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.79734306584578e-05\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([0.0365, 0.0394, 0.0329, 0.0412], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0481, 0.0485, 0.0448, 0.0408, 0.0430, 0.0408, 0.0404, 0.0412, 0.0448,\n",
      "        0.0513], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0445, 0.0437, 0.0461, 0.0412, 0.0386, 0.0412, 0.0461, 0.0412, 0.0461,\n",
      "        0.0427], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.6580834198975936e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0405, 0.0456, 0.0347, 0.0441], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0441, 0.0429, 0.0482, 0.0439, 0.0387, 0.0429, 0.0315, 0.0408, 0.0358,\n",
      "        0.0393], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0410, 0.0395, 0.0434, 0.0459, 0.0385, 0.0395, 0.0475, 0.0431, 0.0410,\n",
      "        0.0459], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.938403824577108e-05\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([0.0360, 0.0379, 0.0351, 0.0442], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0392, 0.0413, 0.0398, 0.0344, 0.0413, 0.0458, 0.0420, 0.0435, 0.0301,\n",
      "        0.0442], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0414, 0.0438, 0.0407, 0.0436, 0.0438, 0.0438, 0.0430, 0.0430, 0.0410,\n",
      "        0.0428], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.2844853447168134e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0394, 0.0407, 0.0419, 0.0462], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0455, 0.0426, 0.0451, 0.0429, 0.0485, 0.0392, 0.0455, 0.0485, 0.0455,\n",
      "        0.0449], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0437, 0.0425, 0.0421, 0.0393, 0.0437, 0.0425, 0.0437, 0.0437, 0.0437,\n",
      "        0.0395], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.1893750524905045e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0386, 0.0379, 0.0458, 0.0473], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0500, 0.0405, 0.0350, 0.0473, 0.0489, 0.0500, 0.0432, 0.0333, 0.0405,\n",
      "        0.0476], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0450, 0.0521, 0.0420, 0.0426, 0.0450, 0.0450, 0.0393, 0.0426, 0.0521,\n",
      "        0.0429], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.3124465921428055e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([0.0396, 0.0414, 0.0582, 0.0498, 0.0414, 0.0513, 0.0377, 0.0412, 0.0414,\n",
      "        0.0400], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0415, 0.0474, 0.0461, 0.0461, 0.0474, 0.0461, 0.0419, 0.0425, 0.0474,\n",
      "        0.0419], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.17415724566672e-05\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([0.0413, 0.0316, 0.0421, 0.0466], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0444, 0.0469, 0.0466, 0.0478, 0.0382, 0.0414, 0.0478, 0.0416, 0.0478,\n",
      "        0.0460], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0429, 0.0404, 0.0429, 0.0487, 0.0443, 0.0441, 0.0487, 0.0454, 0.0487,\n",
      "        0.0491], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.3017505807511043e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0466, 0.0352, 0.0444, 0.0478], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0468, 0.0493, 0.0352, 0.0511, 0.0383, 0.0460, 0.0465, 0.0468, 0.0493,\n",
      "        0.0478], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0504, 0.0438, 0.0414, 0.0466, 0.0455, 0.0430, 0.0466, 0.0504, 0.0455,\n",
      "        0.0404], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.4452188881696202e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0469, 0.0363, 0.0452, 0.0470], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0447, 0.0479, 0.0560, 0.0495, 0.0479, 0.0447, 0.0560, 0.0481, 0.0479,\n",
      "        0.0413], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0402, 0.0504, 0.0504, 0.0446, 0.0504, 0.0402, 0.0504, 0.0455, 0.0504,\n",
      "        0.0444], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.6325702745234594e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([0.0469, 0.0547, 0.0455, 0.0450, 0.0451, 0.0501, 0.0522, 0.0477, 0.0522,\n",
      "        0.0411], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0422, 0.0492, 0.0422, 0.0456, 0.0406, 0.0492, 0.0465, 0.0439, 0.0465,\n",
      "        0.0455], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.8221853679278865e-05\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([0.0441, 0.0369, 0.0457, 0.0393], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0452, 0.0436, 0.0542, 0.0437, 0.0384, 0.0457, 0.0416, 0.0450, 0.0452,\n",
      "        0.0475], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0452, 0.0483, 0.0482, 0.0488, 0.0412, 0.0441, 0.0483, 0.0452, 0.0452,\n",
      "        0.0488], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.4130468116491102e-05\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0422, 0.0379, 0.0502, 0.0465], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0399, 0.0424, 0.0542, 0.0446, 0.0519, 0.0422, 0.0519, 0.0417, 0.0542,\n",
      "        0.0542], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0417, 0.0434, 0.0497, 0.0452, 0.0497, 0.0417, 0.0497, 0.0452, 0.0497,\n",
      "        0.0497], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.730472472961992e-06\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0473, 0.0514, 0.0464, 0.0441], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0573, 0.0411, 0.0411, 0.0423, 0.0502, 0.0511, 0.0537, 0.0573, 0.0451,\n",
      "        0.0416], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0515, 0.0471, 0.0471, 0.0478, 0.0452, 0.0463, 0.0483, 0.0515, 0.0478,\n",
      "        0.0420], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.5309249394922517e-05\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0454, 0.0438, 0.0436, 0.0400], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0519, 0.0510, 0.0568, 0.0481, 0.0568, 0.0505, 0.0434, 0.0481, 0.0568,\n",
      "        0.0505], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0467, 0.0464, 0.0511, 0.0477, 0.0511, 0.0511, 0.0447, 0.0477, 0.0511,\n",
      "        0.0511], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.4783001461182721e-05\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0450, 0.0441, 0.0460, 0.0447], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0446, 0.0425, 0.0562, 0.0441, 0.0483, 0.0428, 0.0424, 0.0463, 0.0562,\n",
      "        0.0437], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0424, 0.0406, 0.0506, 0.0441, 0.0424, 0.0437, 0.0464, 0.0424, 0.0506,\n",
      "        0.0437], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.393730599374976e-05\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0450, 0.0455, 0.0410, 0.0399], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0393, 0.0511, 0.0553, 0.0553, 0.0553, 0.0482, 0.0444, 0.0512, 0.0440,\n",
      "        0.0503], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0409, 0.0477, 0.0497, 0.0497, 0.0497, 0.0473, 0.0497, 0.0460, 0.0419,\n",
      "        0.0498], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.6648235032334924e-05\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0473, 0.0488, 0.0440, 0.0466, 0.0467, 0.0440, 0.0464, 0.0464, 0.0443,\n",
      "        0.0513], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0447, 0.0426, 0.0406, 0.0426, 0.0447, 0.0406, 0.0476, 0.0476, 0.0406,\n",
      "        0.0485], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.131042699853424e-05\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0482, 0.0503, 0.0370, 0.0420], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0455, 0.0450, 0.0429, 0.0457, 0.0450, 0.0450, 0.0450, 0.0433, 0.0454,\n",
      "        0.0429], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0437, 0.0413, 0.0413, 0.0479, 0.0413, 0.0413, 0.0413, 0.0447, 0.0478,\n",
      "        0.0413], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.653496140846983e-06\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0470, 0.0517, 0.0363, 0.0416], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0528, 0.0440, 0.0475, 0.0507, 0.0475, 0.0510, 0.0449, 0.0455, 0.0446,\n",
      "        0.0471], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0475, 0.0438, 0.0469, 0.0486, 0.0469, 0.0475, 0.0486, 0.0475, 0.0469,\n",
      "        0.0433], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.29543023428414e-06\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0458, 0.0527, 0.0358, 0.0412], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0374, 0.0462, 0.0450, 0.0337, 0.0455, 0.0401, 0.0534, 0.0360, 0.0504,\n",
      "        0.0503], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0432, 0.0439, 0.0478, 0.0445, 0.0481, 0.0454, 0.0529, 0.0481, 0.0492,\n",
      "        0.0481], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.511406248435378e-05\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0450, 0.0516, 0.0365, 0.0421], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0534, 0.0483, 0.0344, 0.0427, 0.0431, 0.0514, 0.0382, 0.0531, 0.0514,\n",
      "        0.0516], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0480, 0.0454, 0.0450, 0.0454, 0.0450, 0.0479, 0.0510, 0.0478, 0.0479,\n",
      "        0.0464], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.027802060591057e-05\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0452, 0.0494, 0.0367, 0.0437], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0439, 0.0312, 0.0535, 0.0486, 0.0389, 0.0394, 0.0535, 0.0390, 0.0490,\n",
      "        0.0389], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0446, 0.0462, 0.0482, 0.0482, 0.0431, 0.0440, 0.0482, 0.0485, 0.0503,\n",
      "        0.0431], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.325061308918521e-05\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0406, 0.0435, 0.0466, 0.0533, 0.0421, 0.0452, 0.0409, 0.0491, 0.0412,\n",
      "        0.0533], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0388, 0.0481, 0.0421, 0.0480, 0.0388, 0.0400, 0.0435, 0.0461, 0.0481,\n",
      "        0.0480], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.9993833120679483e-05\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([0.0465, 0.0463, 0.0376, 0.0433], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0447, 0.0520, 0.0476, 0.0461, 0.0471, 0.0547, 0.0476, 0.0451, 0.0417,\n",
      "        0.0520], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0429, 0.0492, 0.0429, 0.0486, 0.0424, 0.0492, 0.0429, 0.0492, 0.0492,\n",
      "        0.0492], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.9509334379108623e-05\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0385, 0.0523, 0.0475, 0.0437, 0.0518, 0.0426, 0.0479, 0.0385, 0.0377,\n",
      "        0.0351], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0438, 0.0439, 0.0481, 0.0507, 0.0508, 0.0444, 0.0507, 0.0438, 0.0530,\n",
      "        0.0508], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.717289215885103e-05\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([0.0426, 0.0346, 0.0472, 0.0452], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0542, 0.0469, 0.0542, 0.0542, 0.0542, 0.0456, 0.0542, 0.0463, 0.0383,\n",
      "        0.0403], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0488, 0.0503, 0.0488, 0.0488, 0.0488, 0.0483, 0.0488, 0.0433, 0.0488,\n",
      "        0.0435], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.925059015979059e-05\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0476, 0.0458, 0.0542, 0.0458, 0.0324, 0.0314, 0.0553, 0.0459, 0.0542,\n",
      "        0.0556], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0437, 0.0437, 0.0497, 0.0437, 0.0451, 0.0462, 0.0437, 0.0435, 0.0488,\n",
      "        0.0501], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.256014603422955e-05\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([0.0576, 0.0505, 0.0527, 0.0527, 0.0527, 0.0419, 0.0477, 0.0459, 0.0376,\n",
      "        0.0436], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0519, 0.0519, 0.0473, 0.0473, 0.0445, 0.0473, 0.0430, 0.0473, 0.0425,\n",
      "        0.0473], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.5246812583645806e-05\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0440, 0.0349, 0.0494, 0.0494], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0426, 0.0415, 0.0582, 0.0524, 0.0539, 0.0392, 0.0524, 0.0582, 0.0398,\n",
      "        0.0415], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0493, 0.0493, 0.0524, 0.0470, 0.0528, 0.0440, 0.0470, 0.0524, 0.0447,\n",
      "        0.0493], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.4244629205204546e-05\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0391, 0.0364, 0.0520, 0.0511], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0479, 0.0435, 0.0433, 0.0565, 0.0479, 0.0407, 0.0479, 0.0415, 0.0415,\n",
      "        0.0415], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0521, 0.0468, 0.0470, 0.0508, 0.0521, 0.0468, 0.0521, 0.0483, 0.0483,\n",
      "        0.0483], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.8774094971595332e-05\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0455, 0.0476, 0.0423, 0.0526], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0450, 0.0498, 0.0492, 0.0446, 0.0454, 0.0479, 0.0433, 0.0443, 0.0498,\n",
      "        0.0513], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0479, 0.0479, 0.0462, 0.0462, 0.0436, 0.0414, 0.0449, 0.0473, 0.0479,\n",
      "        0.0462], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.1093527064076625e-05\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0468, 0.0449, 0.0475, 0.0478], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0473, 0.0478, 0.0449, 0.0444, 0.0475, 0.0475, 0.0478, 0.0475, 0.0557,\n",
      "        0.0473], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0426, 0.0430, 0.0467, 0.0406, 0.0430, 0.0430, 0.0430, 0.0430, 0.0501,\n",
      "        0.0473], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.7804550225264393e-05\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0459, 0.0494, 0.0440, 0.0484, 0.0503, 0.0381, 0.0503, 0.0447, 0.0513,\n",
      "        0.0467], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0449, 0.0445, 0.0468, 0.0436, 0.0470, 0.0449, 0.0470, 0.0419, 0.0452,\n",
      "        0.0466], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.693155172688421e-05\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0454, 0.0387, 0.0476, 0.0620, 0.0516, 0.0527, 0.0443, 0.0387, 0.0410,\n",
      "        0.0496], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0445, 0.0447, 0.0465, 0.0558, 0.0447, 0.0474, 0.0475, 0.0447, 0.0462,\n",
      "        0.0445], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.4928891434683464e-05\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0511, 0.0529, 0.0365, 0.0525], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0552, 0.0510, 0.0388, 0.0491, 0.0450, 0.0446, 0.0644, 0.0510, 0.0313,\n",
      "        0.0446], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0496, 0.0463, 0.0580, 0.0480, 0.0464, 0.0525, 0.0580, 0.0459, 0.0482,\n",
      "        0.0525], grad_fn=<AddBackward0>)\n",
      "LOSS: 9.009212953969836e-05\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0442, 0.0517, 0.0337, 0.0459], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0377, 0.0461, 0.0498, 0.0411, 0.0381, 0.0635, 0.0537, 0.0498, 0.0485,\n",
      "        0.0331], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0458, 0.0518, 0.0458, 0.0572, 0.0458, 0.0572, 0.0483, 0.0458, 0.0474,\n",
      "        0.0464], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.94979535182938e-05\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0475, 0.0487, 0.0378, 0.0510], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0422, 0.0514, 0.0422, 0.0516, 0.0488, 0.0503, 0.0516, 0.0422, 0.0607,\n",
      "        0.0503], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0452, 0.0452, 0.0452, 0.0497, 0.0462, 0.0497, 0.0497, 0.0452, 0.0547,\n",
      "        0.0452], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.4066907169762999e-05\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0534, 0.0501, 0.0474, 0.0474, 0.0421, 0.0500, 0.0450, 0.0501, 0.0474,\n",
      "        0.0446], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0459, 0.0481, 0.0450, 0.0450, 0.0465, 0.0450, 0.0462, 0.0481, 0.0450,\n",
      "        0.0465], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.3126860721968114e-05\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0413, 0.0450, 0.0457, 0.0512], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0483, 0.0591, 0.0562, 0.0490, 0.0591, 0.0509, 0.0501, 0.0512, 0.0498,\n",
      "        0.0513], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0461, 0.0531, 0.0506, 0.0461, 0.0531, 0.0461, 0.0461, 0.0473, 0.0506,\n",
      "        0.0461], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.959413566510193e-05\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0432, 0.0432, 0.0552, 0.0508], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0529, 0.0428, 0.0432, 0.0445, 0.0640, 0.0552, 0.0426, 0.0506, 0.0439,\n",
      "        0.0508], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0499, 0.0481, 0.0461, 0.0446, 0.0576, 0.0497, 0.0476, 0.0499, 0.0452,\n",
      "        0.0481], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.5089952285052277e-05\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0426, 0.0422, 0.0588, 0.0499], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0460, 0.0588, 0.0534, 0.0506, 0.0506, 0.0420, 0.0599, 0.0588, 0.0588,\n",
      "        0.0588], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0615, 0.0529, 0.0539, 0.0511, 0.0511, 0.0511, 0.0470, 0.0529, 0.0529,\n",
      "        0.0529], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.277307693380862e-05\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0428, 0.0424, 0.0597, 0.0490], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0482, 0.0574, 0.0490, 0.0603, 0.0409, 0.0598, 0.0503, 0.0503, 0.0570,\n",
      "        0.0428], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0486, 0.0496, 0.0537, 0.0486, 0.0479, 0.0537, 0.0513, 0.0513, 0.0538,\n",
      "        0.0538], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.428159445524216e-05\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0408, 0.0583, 0.0461, 0.0584, 0.0481, 0.0558, 0.0503, 0.0391, 0.0508,\n",
      "        0.0407], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0471, 0.0526, 0.0462, 0.0526, 0.0490, 0.0525, 0.0599, 0.0500, 0.0495,\n",
      "        0.0473], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.734630809049122e-05\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0476, 0.0445, 0.0554, 0.0492], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0509, 0.0476, 0.0492, 0.0517, 0.0551, 0.0419, 0.0497, 0.0554, 0.0554,\n",
      "        0.0494], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0461, 0.0559, 0.0498, 0.0559, 0.0498, 0.0443, 0.0479, 0.0498, 0.0498,\n",
      "        0.0559], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.4924598619691096e-05\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0520, 0.0521, 0.0469, 0.0464, 0.0525, 0.0478, 0.0410, 0.0487, 0.0537,\n",
      "        0.0442], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0468, 0.0510, 0.0472, 0.0491, 0.0472, 0.0485, 0.0491, 0.0472, 0.0510,\n",
      "        0.0510], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.8507664208300412e-05\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0519, 0.0490, 0.0503, 0.0480], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0503, 0.0538, 0.0519, 0.0471, 0.0557, 0.0456, 0.0519, 0.0503, 0.0503,\n",
      "        0.0545], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0467, 0.0484, 0.0484, 0.0442, 0.0501, 0.0442, 0.0484, 0.0467, 0.0467,\n",
      "        0.0496], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.5622204955434427e-05\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0527, 0.0527, 0.0459, 0.0535], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0460, 0.0527, 0.0470, 0.0400, 0.0447, 0.0470, 0.0459, 0.0447, 0.0470,\n",
      "        0.0497], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0481, 0.0501, 0.0512, 0.0447, 0.0485, 0.0485, 0.0485, 0.0485, 0.0485,\n",
      "        0.0475], grad_fn=<AddBackward0>)\n",
      "LOSS: 9.554165444569662e-06\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0530, 0.0540, 0.0441, 0.0535], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0498, 0.0453, 0.0441, 0.0538, 0.0441, 0.0478, 0.0481, 0.0453, 0.0453,\n",
      "        0.0542], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0503, 0.0484, 0.0484, 0.0486, 0.0484, 0.0478, 0.0444, 0.0484, 0.0476,\n",
      "        0.0494], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.2708640497294255e-05\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0469, 0.0542, 0.0372, 0.0441], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0532, 0.0515, 0.0451, 0.0451, 0.0493, 0.0548, 0.0510, 0.0555, 0.0532,\n",
      "        0.0541], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0514, 0.0500, 0.0487, 0.0487, 0.0444, 0.0514, 0.0491, 0.0488, 0.0509,\n",
      "        0.0443], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.1523263058043085e-05\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0399, 0.0380, 0.0487, 0.0453, 0.0517, 0.0462, 0.0462, 0.0462, 0.0380,\n",
      "        0.0458], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0500, 0.0510, 0.0510, 0.0454, 0.0522, 0.0492, 0.0492, 0.0492, 0.0510,\n",
      "        0.0554], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.620116280624643e-05\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([0.0495, 0.0546, 0.0381, 0.0423], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0508, 0.0380, 0.0508, 0.0590, 0.0498, 0.0495, 0.0445, 0.0477, 0.0492,\n",
      "        0.0401], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0513, 0.0457, 0.0513, 0.0531, 0.0513, 0.0509, 0.0493, 0.0493, 0.0451,\n",
      "        0.0451], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.6631598555250093e-05\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([0.0519, 0.0516, 0.0421, 0.0489], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0550, 0.0529, 0.0552, 0.0519, 0.0520, 0.0415, 0.0473, 0.0573, 0.0573,\n",
      "        0.0397], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0498, 0.0498, 0.0496, 0.0496, 0.0498, 0.0515, 0.0504, 0.0515, 0.0515,\n",
      "        0.0492], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.426710463827476e-05\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([0.0514, 0.0497, 0.0452, 0.0492], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0494, 0.0478, 0.0492, 0.0467, 0.0471, 0.0526, 0.0468, 0.0467, 0.0526,\n",
      "        0.0526], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0485, 0.0512, 0.0462, 0.0507, 0.0474, 0.0474, 0.0507, 0.0507, 0.0474,\n",
      "        0.0474], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.5232324585667811e-05\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([0.0513, 0.0481, 0.0479, 0.0491], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0484, 0.0492, 0.0505, 0.0524, 0.0478, 0.0446, 0.0478, 0.0523, 0.0491,\n",
      "        0.0527], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0498, 0.0474, 0.0466, 0.0474, 0.0476, 0.0496, 0.0476, 0.0500, 0.0451,\n",
      "        0.0496], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.0159557859878987e-05\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([0.0513, 0.0468, 0.0502, 0.0485], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0487, 0.0484, 0.0511, 0.0423, 0.0423, 0.0583, 0.0492, 0.0493, 0.0512,\n",
      "        0.0484], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0469, 0.0485, 0.0524, 0.0535, 0.0535, 0.0524, 0.0474, 0.0462, 0.0469,\n",
      "        0.0485], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.236335760448128e-05\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([0.0520, 0.0465, 0.0506, 0.0481], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0483, 0.0474, 0.0498, 0.0473, 0.0439, 0.0536, 0.0536, 0.0490, 0.0498,\n",
      "        0.0486], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0524, 0.0508, 0.0482, 0.0490, 0.0490, 0.0482, 0.0482, 0.0468, 0.0482,\n",
      "        0.0495], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.252098627446685e-05\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([0.0533, 0.0460, 0.0505, 0.0480], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0469, 0.0596, 0.0546, 0.0518, 0.0523, 0.0596, 0.0492, 0.0479, 0.0513,\n",
      "        0.0434], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0479, 0.0536, 0.0493, 0.0487, 0.0493, 0.0536, 0.0493, 0.0536, 0.0474,\n",
      "        0.0463], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.742763924994506e-05\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([0.0539, 0.0459, 0.0500, 0.0481], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0465, 0.0556, 0.0510, 0.0592, 0.0537, 0.0451, 0.0592, 0.0504, 0.0537,\n",
      "        0.0513], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0473, 0.0501, 0.0472, 0.0533, 0.0533, 0.0486, 0.0533, 0.0472, 0.0533,\n",
      "        0.0491], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.4326503332995344e-05\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([0.0545, 0.0461, 0.0493, 0.0479], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0540, 0.0510, 0.0521, 0.0492, 0.0563, 0.0442, 0.0515, 0.0563, 0.0535,\n",
      "        0.0515], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0527, 0.0499, 0.0487, 0.0527, 0.0506, 0.0474, 0.0506, 0.0506, 0.0506,\n",
      "        0.0506], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.1030494533770252e-05\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([0.0548, 0.0465, 0.0486, 0.0474], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0467, 0.0568, 0.0534, 0.0527, 0.0537, 0.0578, 0.0526, 0.0540, 0.0578,\n",
      "        0.0533], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0479, 0.0511, 0.0515, 0.0511, 0.0483, 0.0520, 0.0483, 0.0520, 0.0520,\n",
      "        0.0480], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.8618466128828004e-05\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([0.0551, 0.0468, 0.0483, 0.0465], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0575, 0.0516, 0.0508, 0.0528, 0.0512, 0.0575, 0.0574, 0.0470, 0.0574,\n",
      "        0.0427], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0518, 0.0483, 0.0468, 0.0483, 0.0483, 0.0518, 0.0517, 0.0521, 0.0517,\n",
      "        0.0468], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.299172047059983e-05\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0486, 0.0486, 0.0489, 0.0517, 0.0493, 0.0529, 0.0547, 0.0522, 0.0547,\n",
      "        0.0489], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0499, 0.0499, 0.0524, 0.0494, 0.0494, 0.0517, 0.0493, 0.0524, 0.0493,\n",
      "        0.0524], grad_fn=<AddBackward0>)\n",
      "LOSS: 9.39520396059379e-06\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0496, 0.0486, 0.0464, 0.0534], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0582, 0.0570, 0.0570, 0.0582, 0.0538, 0.0481, 0.0570, 0.0522, 0.0506,\n",
      "        0.0570], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0524, 0.0513, 0.0513, 0.0524, 0.0490, 0.0515, 0.0513, 0.0500, 0.0452,\n",
      "        0.0513], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.6674539185478352e-05\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0439, 0.0472, 0.0578, 0.0578, 0.0466, 0.0464, 0.0536, 0.0562, 0.0477,\n",
      "        0.0578], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0472, 0.0474, 0.0520, 0.0520, 0.0506, 0.0503, 0.0483, 0.0506, 0.0506,\n",
      "        0.0520], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.1205203665886074e-05\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0489, 0.0471, 0.0516, 0.0481], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0471, 0.0432, 0.0488, 0.0476, 0.0503, 0.0567, 0.0478, 0.0431, 0.0483,\n",
      "        0.0458], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0472, 0.0461, 0.0496, 0.0473, 0.0510, 0.0510, 0.0473, 0.0480, 0.0473,\n",
      "        0.0461], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.765990747226169e-06\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0474, 0.0476, 0.0508, 0.0486], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0531, 0.0498, 0.0508, 0.0485, 0.0531, 0.0552, 0.0430, 0.0533, 0.0556,\n",
      "        0.0461], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0499, 0.0497, 0.0500, 0.0497, 0.0499, 0.0500, 0.0477, 0.0468, 0.0500,\n",
      "        0.0497], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.5949408407323062e-05\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([0.0460, 0.0467, 0.0468, 0.0545, 0.0474, 0.0563, 0.0545, 0.0498, 0.0509,\n",
      "        0.0487], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0463, 0.0489, 0.0463, 0.0490, 0.0449, 0.0449, 0.0490, 0.0489, 0.0506,\n",
      "        0.0478], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.0239665900589898e-05\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0403, 0.0541, 0.0499, 0.0449], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0469, 0.0520, 0.0532, 0.0466, 0.0439, 0.0449, 0.0533, 0.0465, 0.0532,\n",
      "        0.0452], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0460, 0.0479, 0.0479, 0.0468, 0.0511, 0.0468, 0.0479, 0.0447, 0.0479,\n",
      "        0.0480], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.703775706118904e-05\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0398, 0.0547, 0.0490, 0.0450], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0457, 0.0494, 0.0478, 0.0433, 0.0522, 0.0567, 0.0522, 0.0432, 0.0521,\n",
      "        0.0524], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0527, 0.0459, 0.0510, 0.0510, 0.0470, 0.0468, 0.0470, 0.0459, 0.0470,\n",
      "        0.0473], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.43067295034416e-05\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0546, 0.0454, 0.0474, 0.0499, 0.0465, 0.0512, 0.0483, 0.0479, 0.0454,\n",
      "        0.0512], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0467, 0.0455, 0.0485, 0.0474, 0.0467, 0.0469, 0.0492, 0.0455, 0.0455,\n",
      "        0.0469], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.129093379859114e-05\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0519, 0.0477, 0.0459, 0.0493, 0.0409, 0.0493, 0.0476, 0.0495, 0.0516,\n",
      "        0.0491], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0445, 0.0465, 0.0436, 0.0486, 0.0485, 0.0486, 0.0476, 0.0442, 0.0436,\n",
      "        0.0452], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.2674652427667752e-05\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([0.0485, 0.0485, 0.0477, 0.0472, 0.0497, 0.0552, 0.0476, 0.0407, 0.0548,\n",
      "        0.0485], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0497, 0.0497, 0.0497, 0.0440, 0.0458, 0.0497, 0.0460, 0.0460, 0.0460,\n",
      "        0.0497], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.7228576325578615e-05\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([0.0507, 0.0458, 0.0418, 0.0484], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0421, 0.0478, 0.0475, 0.0470, 0.0462, 0.0486, 0.0478, 0.0565, 0.0521,\n",
      "        0.0429], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0472, 0.0468, 0.0472, 0.0452, 0.0472, 0.0505, 0.0505, 0.0443, 0.0468,\n",
      "        0.0508], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.8044963983120397e-05\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([0.0516, 0.0447, 0.0422, 0.0481], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0412, 0.0481, 0.0429, 0.0481, 0.0557, 0.0445, 0.0523, 0.0468, 0.0523,\n",
      "        0.0481], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0467, 0.0471, 0.0469, 0.0450, 0.0501, 0.0465, 0.0471, 0.0474, 0.0471,\n",
      "        0.0471], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.475469889555825e-05\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([0.0520, 0.0441, 0.0429, 0.0469], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0519, 0.0499, 0.0545, 0.0489, 0.0545, 0.0496, 0.0498, 0.0498, 0.0498,\n",
      "        0.0519], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0467, 0.0449, 0.0490, 0.0467, 0.0490, 0.0447, 0.0490, 0.0490, 0.0490,\n",
      "        0.0467], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.695766331977211e-05\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([0.0521, 0.0439, 0.0434, 0.0457], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0402, 0.0533, 0.0419, 0.0460, 0.0493, 0.0515, 0.0429, 0.0503, 0.0525,\n",
      "        0.0429], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0473, 0.0480, 0.0443, 0.0469, 0.0463, 0.0447, 0.0463, 0.0480, 0.0480,\n",
      "        0.0463], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.87660007213708e-05\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([0.0525, 0.0444, 0.0436, 0.0436], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0476, 0.0444, 0.0502, 0.0506, 0.0475, 0.0466, 0.0517, 0.0435, 0.0476,\n",
      "        0.0427], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0452, 0.0444, 0.0452, 0.0465, 0.0450, 0.0451, 0.0465, 0.0452, 0.0452,\n",
      "        0.0438], grad_fn=<AddBackward0>)\n",
      "LOSS: 9.323971426056232e-06\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([0.0523, 0.0449, 0.0436, 0.0420], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0488, 0.0440, 0.0488, 0.0440, 0.0498, 0.0388, 0.0502, 0.0466, 0.0449,\n",
      "        0.0505], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0455, 0.0428, 0.0428, 0.0428, 0.0427, 0.0409, 0.0451, 0.0452, 0.0471,\n",
      "        0.0455], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.6056357708293945e-05\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([0.0456, 0.0514, 0.0500, 0.0472, 0.0495, 0.0477, 0.0492, 0.0494, 0.0390,\n",
      "        0.0477], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0446, 0.0438, 0.0450, 0.0413, 0.0454, 0.0424, 0.0443, 0.0454, 0.0401,\n",
      "        0.0454], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.0893048713332973e-05\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([0.0523, 0.0453, 0.0408, 0.0413], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0498, 0.0440, 0.0512, 0.0448, 0.0498, 0.0498, 0.0474, 0.0427, 0.0439,\n",
      "        0.0478], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0448, 0.0431, 0.0442, 0.0443, 0.0448, 0.0448, 0.0465, 0.0431, 0.0442,\n",
      "        0.0465], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.2705281733360607e-05\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([0.0504, 0.0402, 0.0434, 0.0472, 0.0380, 0.0477, 0.0529, 0.0454, 0.0496,\n",
      "        0.0504], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0454, 0.0446, 0.0439, 0.0477, 0.0406, 0.0439, 0.0476, 0.0477, 0.0406,\n",
      "        0.0454], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.0502993720583618e-05\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([0.0525, 0.0446, 0.0376, 0.0419], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0453, 0.0463, 0.0469, 0.0385, 0.0430, 0.0439, 0.0476, 0.0453, 0.0482,\n",
      "        0.0443], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0459, 0.0459, 0.0473, 0.0449, 0.0446, 0.0487, 0.0422, 0.0459, 0.0448,\n",
      "        0.0422], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.1121355782961473e-05\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([0.0523, 0.0446, 0.0367, 0.0416], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0453, 0.0408, 0.0543, 0.0453, 0.0397, 0.0343, 0.0502, 0.0483, 0.0428,\n",
      "        0.0431], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0459, 0.0448, 0.0489, 0.0459, 0.0448, 0.0428, 0.0450, 0.0444, 0.0448,\n",
      "        0.0489], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.235835199826397e-05\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0533, 0.0516, 0.0533, 0.0478, 0.0428, 0.0383, 0.0403, 0.0430, 0.0441,\n",
      "        0.0477], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0479, 0.0465, 0.0479, 0.0435, 0.0421, 0.0442, 0.0456, 0.0435, 0.0479,\n",
      "        0.0442], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.9189492377336137e-05\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([0.0497, 0.0427, 0.0404, 0.0450], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0439, 0.0475, 0.0430, 0.0427, 0.0484, 0.0475, 0.0497, 0.0384, 0.0523,\n",
      "        0.0523], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0471, 0.0433, 0.0427, 0.0438, 0.0443, 0.0427, 0.0448, 0.0394, 0.0471,\n",
      "        0.0471], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.4900911082804669e-05\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([0.0491, 0.0426, 0.0412, 0.0442], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0424, 0.0512, 0.0512, 0.0466, 0.0373, 0.0383, 0.0396, 0.0437, 0.0466,\n",
      "        0.0455], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0442, 0.0460, 0.0460, 0.0460, 0.0420, 0.0415, 0.0415, 0.0436, 0.0460,\n",
      "        0.0442], grad_fn=<AddBackward0>)\n",
      "LOSS: 9.36712785915006e-06\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0496, 0.0478, 0.0447, 0.0427, 0.0478, 0.0379, 0.0442, 0.0478, 0.0468,\n",
      "        0.0430], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0446, 0.0446, 0.0409, 0.0426, 0.0446, 0.0415, 0.0437, 0.0446, 0.0409,\n",
      "        0.0431], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.1764179362216964e-05\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([0.0433, 0.0475, 0.0426, 0.0427], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0441, 0.0477, 0.0472, 0.0477, 0.0405, 0.0475, 0.0475, 0.0497, 0.0476,\n",
      "        0.0477], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0434, 0.0447, 0.0425, 0.0447, 0.0411, 0.0447, 0.0447, 0.0447, 0.0429,\n",
      "        0.0447], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.1197783351235557e-05\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([0.0471, 0.0429, 0.0410, 0.0438], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0479, 0.0460, 0.0460, 0.0436, 0.0428, 0.0395, 0.0470, 0.0420, 0.0460,\n",
      "        0.0422], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0431, 0.0461, 0.0461, 0.0424, 0.0424, 0.0424, 0.0423, 0.0433, 0.0461,\n",
      "        0.0461], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.156714218581328e-06\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([0.0463, 0.0427, 0.0399, 0.0446], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0448, 0.0410, 0.0487, 0.0389, 0.0446, 0.0428, 0.0431, 0.0453, 0.0382,\n",
      "        0.0446], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0473, 0.0416, 0.0438, 0.0390, 0.0473, 0.0436, 0.0473, 0.0429, 0.0421,\n",
      "        0.0473], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.546057870262302e-06\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([0.0455, 0.0430, 0.0396, 0.0445], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0463, 0.0448, 0.0413, 0.0438, 0.0464, 0.0432, 0.0396, 0.0443, 0.0385,\n",
      "        0.0446], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0409, 0.0426, 0.0409, 0.0401, 0.0388, 0.0471, 0.0436, 0.0471, 0.0417,\n",
      "        0.0436], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.5476422049687244e-05\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([0.0456, 0.0433, 0.0395, 0.0438], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0440, 0.0437, 0.0396, 0.0424, 0.0440, 0.0416, 0.0453, 0.0475, 0.0440,\n",
      "        0.0390], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0461, 0.0427, 0.0428, 0.0406, 0.0461, 0.0388, 0.0407, 0.0428, 0.0461,\n",
      "        0.0413], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.404831532971002e-06\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([0.0459, 0.0438, 0.0399, 0.0425], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0444, 0.0435, 0.0493, 0.0432, 0.0399, 0.0395, 0.0451, 0.0429, 0.0421,\n",
      "        0.0493], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0443, 0.0430, 0.0443, 0.0402, 0.0413, 0.0374, 0.0406, 0.0413, 0.0431,\n",
      "        0.0443], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.792191692919005e-06\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0428, 0.0418, 0.0389, 0.0447, 0.0477, 0.0447, 0.0438, 0.0446, 0.0417,\n",
      "        0.0477], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0410, 0.0429, 0.0368, 0.0429, 0.0429, 0.0429, 0.0426, 0.0402, 0.0402,\n",
      "        0.0402], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.1711947081494145e-05\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([0.0448, 0.0445, 0.0403, 0.0414], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0438, 0.0385, 0.0400, 0.0446, 0.0447, 0.0472, 0.0385, 0.0472, 0.0385,\n",
      "        0.0437], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0430, 0.0401, 0.0394, 0.0401, 0.0425, 0.0425, 0.0401, 0.0425, 0.0401,\n",
      "        0.0430], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.86862256063614e-06\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([0.0445, 0.0447, 0.0402, 0.0412], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0443, 0.0430, 0.0399, 0.0441, 0.0412, 0.0433, 0.0398, 0.0443, 0.0375,\n",
      "        0.0365], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0420, 0.0395, 0.0403, 0.0397, 0.0399, 0.0390, 0.0413, 0.0420, 0.0399,\n",
      "        0.0410], grad_fn=<AddBackward0>)\n",
      "LOSS: 9.083109034691006e-06\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([0.0429, 0.0427, 0.0380, 0.0398], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0440, 0.0397, 0.0367, 0.0435, 0.0429, 0.0432, 0.0423, 0.0412, 0.0390,\n",
      "        0.0381], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0396, 0.0410, 0.0405, 0.0420, 0.0387, 0.0438, 0.0396, 0.0392, 0.0377,\n",
      "        0.0396], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.205234396678861e-06\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0353, 0.0365, 0.0455, 0.0422, 0.0427, 0.0404, 0.0427, 0.0468, 0.0491,\n",
      "        0.0384], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0359, 0.0381, 0.0394, 0.0394, 0.0421, 0.0387, 0.0421, 0.0421, 0.0395,\n",
      "        0.0394], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.6611096725682728e-05\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([0.0449, 0.0446, 0.0384, 0.0404], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0397, 0.0418, 0.0372, 0.0464, 0.0418, 0.0379, 0.0408, 0.0443, 0.0415,\n",
      "        0.0418], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0404, 0.0417, 0.0387, 0.0417, 0.0417, 0.0435, 0.0379, 0.0435, 0.0391,\n",
      "        0.0417], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.9814682319702115e-06\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([0.0449, 0.0446, 0.0381, 0.0398], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0430, 0.0377, 0.0413, 0.0414, 0.0386, 0.0432, 0.0420, 0.0376, 0.0420,\n",
      "        0.0428], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0426, 0.0354, 0.0414, 0.0426, 0.0388, 0.0388, 0.0402, 0.0394, 0.0402,\n",
      "        0.0386], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.395210791903082e-06\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([0.0451, 0.0448, 0.0381, 0.0388], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0406, 0.0397, 0.0448, 0.0421, 0.0411, 0.0397, 0.0451, 0.0383, 0.0390,\n",
      "        0.0411], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0383, 0.0406, 0.0386, 0.0386, 0.0406, 0.0406, 0.0406, 0.0371, 0.0382,\n",
      "        0.0406], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.055421858443879e-06\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([0.0458, 0.0441, 0.0378, 0.0382], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0380, 0.0415, 0.0427, 0.0407, 0.0387, 0.0407, 0.0406, 0.0445, 0.0438,\n",
      "        0.0420], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0405, 0.0386, 0.0403, 0.0400, 0.0378, 0.0400, 0.0405, 0.0400, 0.0395,\n",
      "        0.0378], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.914257366792299e-06\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([0.0463, 0.0433, 0.0376, 0.0378], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0367, 0.0402, 0.0381, 0.0441, 0.0441, 0.0378, 0.0403, 0.0421, 0.0404,\n",
      "        0.0394], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0406, 0.0375, 0.0417, 0.0397, 0.0397, 0.0363, 0.0366, 0.0397, 0.0397,\n",
      "        0.0372], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.0181234756601043e-05\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([0.0460, 0.0425, 0.0370, 0.0382], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0434, 0.0403, 0.0396, 0.0424, 0.0372, 0.0434, 0.0355, 0.0415, 0.0444,\n",
      "        0.0396], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0399, 0.0374, 0.0400, 0.0381, 0.0380, 0.0399, 0.0381, 0.0399, 0.0400,\n",
      "        0.0400], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.07630658528069e-06\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0400, 0.0455, 0.0412, 0.0393, 0.0363, 0.0347, 0.0365, 0.0452, 0.0432,\n",
      "        0.0391], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0365, 0.0409, 0.0371, 0.0407, 0.0371, 0.0376, 0.0353, 0.0407, 0.0389,\n",
      "        0.0394], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.0137225217476953e-05\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([0.0369, 0.0388, 0.0368, 0.0391], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0387, 0.0391, 0.0422, 0.0392, 0.0389, 0.0428, 0.0369, 0.0362, 0.0387,\n",
      "        0.0369], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0394, 0.0410, 0.0385, 0.0417, 0.0396, 0.0394, 0.0352, 0.0380, 0.0385,\n",
      "        0.0385], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.5016231524641626e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0379, 0.0361, 0.0371, 0.0440], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0394, 0.0418, 0.0394, 0.0452, 0.0387, 0.0387, 0.0372, 0.0393, 0.0394,\n",
      "        0.0407], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0407, 0.0376, 0.0407, 0.0407, 0.0417, 0.0417, 0.0391, 0.0353, 0.0407,\n",
      "        0.0417], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.214764420699794e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0373, 0.0362, 0.0377, 0.0432], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0430, 0.0344, 0.0399, 0.0373, 0.0437, 0.0413, 0.0400, 0.0374, 0.0380,\n",
      "        0.0392], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0387, 0.0360, 0.0393, 0.0388, 0.0393, 0.0404, 0.0393, 0.0360, 0.0369,\n",
      "        0.0404], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.897031431028154e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0371, 0.0363, 0.0380, 0.0423], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0397, 0.0422, 0.0404, 0.0380, 0.0371, 0.0376, 0.0423, 0.0375, 0.0425,\n",
      "        0.0404], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0391, 0.0380, 0.0380, 0.0365, 0.0380, 0.0348, 0.0380, 0.0347, 0.0383,\n",
      "        0.0360], grad_fn=<AddBackward0>)\n",
      "LOSS: 9.798156497708987e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0371, 0.0366, 0.0381, 0.0412], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0392, 0.0388, 0.0409, 0.0405, 0.0385, 0.0401, 0.0356, 0.0387, 0.0401,\n",
      "        0.0409], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0337, 0.0380, 0.0368, 0.0368, 0.0368, 0.0368, 0.0349, 0.0364, 0.0368,\n",
      "        0.0368], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.0950401701848023e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0361, 0.0363, 0.0369, 0.0398, 0.0398, 0.0419, 0.0408, 0.0391, 0.0316,\n",
      "        0.0398], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0352, 0.0349, 0.0333, 0.0367, 0.0367, 0.0377, 0.0367, 0.0333, 0.0352,\n",
      "        0.0367], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.2574122592923231e-05\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0403, 0.0351, 0.0383, 0.0365], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0361, 0.0392, 0.0360, 0.0383, 0.0416, 0.0396, 0.0389, 0.0360, 0.0363,\n",
      "        0.0416], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0362, 0.0375, 0.0340, 0.0378, 0.0375, 0.0374, 0.0382, 0.0343, 0.0374,\n",
      "        0.0374], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.108102868689457e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0371, 0.0345, 0.0356, 0.0420], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0363, 0.0367, 0.0370, 0.0361, 0.0403, 0.0361, 0.0415, 0.0375, 0.0349,\n",
      "        0.0431], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0363, 0.0376, 0.0363, 0.0356, 0.0378, 0.0363, 0.0374, 0.0388, 0.0330,\n",
      "        0.0388], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.9082473196904175e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0370, 0.0339, 0.0347, 0.0424], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0374, 0.0360, 0.0412, 0.0340, 0.0370, 0.0438, 0.0373, 0.0339, 0.0389,\n",
      "        0.0370], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0382, 0.0377, 0.0371, 0.0326, 0.0381, 0.0395, 0.0371, 0.0336, 0.0377,\n",
      "        0.0381], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.604138666763902e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0369, 0.0333, 0.0340, 0.0426], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0420, 0.0368, 0.0352, 0.0370, 0.0369, 0.0383, 0.0426, 0.0380, 0.0357,\n",
      "        0.0357], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0345, 0.0381, 0.0321, 0.0368, 0.0383, 0.0378, 0.0383, 0.0378, 0.0401,\n",
      "        0.0401], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.2638112821150571e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0370, 0.0332, 0.0340, 0.0417], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0436, 0.0358, 0.0357, 0.0351, 0.0369, 0.0408, 0.0332, 0.0417, 0.0325,\n",
      "        0.0370], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0392, 0.0392, 0.0364, 0.0392, 0.0332, 0.0367, 0.0333, 0.0375, 0.0323,\n",
      "        0.0375], grad_fn=<AddBackward0>)\n",
      "LOSS: 9.562963896314614e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0371, 0.0332, 0.0346, 0.0402], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0380, 0.0337, 0.0331, 0.0417, 0.0360, 0.0380, 0.0378, 0.0380, 0.0332,\n",
      "        0.0371], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0346, 0.0342, 0.0358, 0.0375, 0.0358, 0.0346, 0.0366, 0.0346, 0.0334,\n",
      "        0.0362], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.230992767086718e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0371, 0.0329, 0.0348, 0.0396], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0371, 0.0375, 0.0360, 0.0358, 0.0359, 0.0342, 0.0360, 0.0327, 0.0345,\n",
      "        0.0327], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0357, 0.0342, 0.0347, 0.0355, 0.0367, 0.0340, 0.0347, 0.0328, 0.0357,\n",
      "        0.0350], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.378791805313085e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0367, 0.0324, 0.0351, 0.0393], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0400, 0.0365, 0.0362, 0.0322, 0.0353, 0.0401, 0.0401, 0.0393, 0.0338,\n",
      "        0.0389], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0360, 0.0360, 0.0361, 0.0350, 0.0350, 0.0361, 0.0361, 0.0354, 0.0336,\n",
      "        0.0354], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.440865713055246e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0358, 0.0319, 0.0354, 0.0394], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0346, 0.0312, 0.0345, 0.0335, 0.0405, 0.0345, 0.0394, 0.0377, 0.0377,\n",
      "        0.0365], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0341, 0.0307, 0.0340, 0.0341, 0.0365, 0.0355, 0.0355, 0.0358, 0.0358,\n",
      "        0.0358], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.176143647782737e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0351, 0.0313, 0.0354, 0.0396], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0348, 0.0336, 0.0365, 0.0364, 0.0385, 0.0328, 0.0348, 0.0377, 0.0376,\n",
      "        0.0348], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0349, 0.0331, 0.0349, 0.0359, 0.0346, 0.0339, 0.0349, 0.0359, 0.0349,\n",
      "        0.0349], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.9096997877786634e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0346, 0.0307, 0.0350, 0.0401], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0373, 0.0334, 0.0328, 0.0373, 0.0369, 0.0369, 0.0307, 0.0375, 0.0370,\n",
      "        0.0404], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0364, 0.0341, 0.0319, 0.0364, 0.0357, 0.0341, 0.0319, 0.0364, 0.0357,\n",
      "        0.0364], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.3680821616144385e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0343, 0.0301, 0.0344, 0.0405], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0330, 0.0409, 0.0366, 0.0341, 0.0366, 0.0354, 0.0300, 0.0314, 0.0330,\n",
      "        0.0359], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0364, 0.0368, 0.0368, 0.0339, 0.0368, 0.0342, 0.0323, 0.0340, 0.0364,\n",
      "        0.0364], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.396954747993732e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0342, 0.0302, 0.0341, 0.0400], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0325, 0.0322, 0.0377, 0.0341, 0.0342, 0.0377, 0.0330, 0.0331, 0.0400,\n",
      "        0.0355], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0319, 0.0328, 0.0339, 0.0316, 0.0360, 0.0339, 0.0328, 0.0360, 0.0360,\n",
      "        0.0360], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.294062586675864e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0345, 0.0305, 0.0335, 0.0392], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0316, 0.0308, 0.0336, 0.0302, 0.0305, 0.0392, 0.0352, 0.0310, 0.0392,\n",
      "        0.0324], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0322, 0.0336, 0.0352, 0.0301, 0.0313, 0.0353, 0.0353, 0.0301, 0.0353,\n",
      "        0.0333], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.362471372587606e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0349, 0.0308, 0.0328, 0.0385], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0351, 0.0340, 0.0351, 0.0357, 0.0354, 0.0351, 0.0309, 0.0331, 0.0354,\n",
      "        0.0352], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0352, 0.0346, 0.0352, 0.0352, 0.0346, 0.0352, 0.0322, 0.0319, 0.0346,\n",
      "        0.0330], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.0099230394189362e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0350, 0.0310, 0.0323, 0.0379], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0346, 0.0310, 0.0379, 0.0350, 0.0346, 0.0350, 0.0350, 0.0346, 0.0304,\n",
      "        0.0333], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0348, 0.0316, 0.0341, 0.0341, 0.0348, 0.0341, 0.0341, 0.0348, 0.0312,\n",
      "        0.0348], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.010770003835205e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0349, 0.0312, 0.0320, 0.0374], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0329, 0.0349, 0.0344, 0.0325, 0.0330, 0.0330, 0.0320, 0.0344, 0.0329,\n",
      "        0.0335], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0316, 0.0337, 0.0343, 0.0316, 0.0337, 0.0343, 0.0316, 0.0343, 0.0316,\n",
      "        0.0350], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.0063206445920514e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0346, 0.0315, 0.0318, 0.0369], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0329, 0.0329, 0.0329, 0.0369, 0.0381, 0.0355, 0.0283, 0.0312, 0.0381,\n",
      "        0.0342], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0326, 0.0337, 0.0326, 0.0332, 0.0322, 0.0322, 0.0284, 0.0284, 0.0322,\n",
      "        0.0337], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.0089769602927845e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0343, 0.0306, 0.0316, 0.0376], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0381, 0.0343, 0.0376, 0.0312, 0.0373, 0.0333, 0.0345, 0.0332, 0.0376,\n",
      "        0.0306], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0343, 0.0338, 0.0338, 0.0295, 0.0330, 0.0300, 0.0336, 0.0338, 0.0338,\n",
      "        0.0312], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.720280336798169e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0339, 0.0299, 0.0314, 0.0380], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0325, 0.0327, 0.0331, 0.0314, 0.0339, 0.0336, 0.0339, 0.0384, 0.0339,\n",
      "        0.0374], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0295, 0.0321, 0.0309, 0.0309, 0.0342, 0.0309, 0.0342, 0.0346, 0.0342,\n",
      "        0.0336], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.087847966933623e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0341, 0.0292, 0.0311, 0.0379], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0308, 0.0330, 0.0311, 0.0348, 0.0308, 0.0323, 0.0379, 0.0376, 0.0292,\n",
      "        0.0383], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0304, 0.0338, 0.0313, 0.0341, 0.0304, 0.0345, 0.0341, 0.0338, 0.0305,\n",
      "        0.0345], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.103022431285353e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0340, 0.0285, 0.0311, 0.0378], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0340, 0.0311, 0.0333, 0.0378, 0.0378, 0.0342, 0.0279, 0.0311, 0.0333,\n",
      "        0.0378], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0340, 0.0314, 0.0342, 0.0340, 0.0340, 0.0342, 0.0305, 0.0314, 0.0342,\n",
      "        0.0340], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.121677077113418e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0339, 0.0282, 0.0313, 0.0373], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0373, 0.0335, 0.0373, 0.0335, 0.0306, 0.0282, 0.0373, 0.0337, 0.0373,\n",
      "        0.0376], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0335, 0.0335, 0.0336, 0.0335, 0.0302, 0.0302, 0.0336, 0.0335, 0.0336,\n",
      "        0.0339], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.392195584543515e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0337, 0.0281, 0.0314, 0.0366], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0337, 0.0364, 0.0337, 0.0336, 0.0310, 0.0310, 0.0314, 0.0366, 0.0330,\n",
      "        0.0303], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0329, 0.0327, 0.0327, 0.0319, 0.0313, 0.0313, 0.0313, 0.0329, 0.0329,\n",
      "        0.0293], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.244080971853691e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0334, 0.0279, 0.0315, 0.0360], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0311, 0.0315, 0.0325, 0.0334, 0.0337, 0.0311, 0.0311, 0.0370, 0.0307,\n",
      "        0.0325], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0311, 0.0311, 0.0345, 0.0324, 0.0321, 0.0330, 0.0345, 0.0333, 0.0311,\n",
      "        0.0345], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.089915819349699e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0329, 0.0282, 0.0314, 0.0354], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0375, 0.0350, 0.0342, 0.0342, 0.0298, 0.0302, 0.0281, 0.0314, 0.0304,\n",
      "        0.0354], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0295, 0.0315, 0.0319, 0.0319, 0.0288, 0.0295, 0.0301, 0.0308, 0.0308,\n",
      "        0.0319], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.0545758414082229e-05\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0320, 0.0290, 0.0311, 0.0347], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0333, 0.0282, 0.0311, 0.0347, 0.0289, 0.0320, 0.0295, 0.0303, 0.0285,\n",
      "        0.0347], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0306, 0.0317, 0.0299, 0.0312, 0.0291, 0.0312, 0.0305, 0.0285, 0.0273,\n",
      "        0.0312], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.103031980979722e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0315, 0.0293, 0.0302, 0.0346], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0328, 0.0323, 0.0323, 0.0293, 0.0351, 0.0293, 0.0295, 0.0329, 0.0323,\n",
      "        0.0351], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0311, 0.0304, 0.0304, 0.0295, 0.0316, 0.0294, 0.0309, 0.0314, 0.0304,\n",
      "        0.0316], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.211677605781006e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0273, 0.0307, 0.0352, 0.0307, 0.0269, 0.0346, 0.0306, 0.0316, 0.0308,\n",
      "        0.0278], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0277, 0.0311, 0.0317, 0.0311, 0.0303, 0.0311, 0.0311, 0.0311, 0.0317,\n",
      "        0.0294], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.0301533772435505e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0304, 0.0293, 0.0280, 0.0356], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0340, 0.0297, 0.0356, 0.0293, 0.0263, 0.0356, 0.0324, 0.0277, 0.0277,\n",
      "        0.0256], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0306, 0.0316, 0.0320, 0.0292, 0.0292, 0.0320, 0.0295, 0.0284, 0.0284,\n",
      "        0.0278], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.292976650001947e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0299, 0.0291, 0.0275, 0.0358], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0295, 0.0300, 0.0331, 0.0278, 0.0336, 0.0303, 0.0310, 0.0321, 0.0358,\n",
      "        0.0322], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0318, 0.0272, 0.0282, 0.0309, 0.0302, 0.0309, 0.0322, 0.0318, 0.0322,\n",
      "        0.0294], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.98421478975797e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0295, 0.0287, 0.0280, 0.0355], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0304, 0.0355, 0.0355, 0.0291, 0.0295, 0.0294, 0.0350, 0.0280, 0.0355,\n",
      "        0.0350], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0274, 0.0320, 0.0320, 0.0292, 0.0315, 0.0308, 0.0315, 0.0274, 0.0320,\n",
      "        0.0315], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.764438123558648e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0294, 0.0283, 0.0284, 0.0347], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0305, 0.0294, 0.0254, 0.0340, 0.0323, 0.0261, 0.0347, 0.0305, 0.0306,\n",
      "        0.0340], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0313, 0.0313, 0.0284, 0.0306, 0.0291, 0.0263, 0.0313, 0.0313, 0.0306,\n",
      "        0.0306], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.940394657955039e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0297, 0.0280, 0.0289, 0.0337], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0280, 0.0337, 0.0331, 0.0305, 0.0246, 0.0291, 0.0303, 0.0303, 0.0329,\n",
      "        0.0298], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0275, 0.0303, 0.0298, 0.0303, 0.0275, 0.0289, 0.0296, 0.0296, 0.0296,\n",
      "        0.0288], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.409193479659734e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0299, 0.0277, 0.0291, 0.0329], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0318, 0.0308, 0.0305, 0.0318, 0.0277, 0.0314, 0.0318, 0.0334, 0.0329,\n",
      "        0.0321], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0286, 0.0299, 0.0288, 0.0286, 0.0266, 0.0288, 0.0286, 0.0301, 0.0296,\n",
      "        0.0286], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.663760698051192e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0301, 0.0273, 0.0286, 0.0325], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0263, 0.0286, 0.0273, 0.0276, 0.0334, 0.0289, 0.0252, 0.0275, 0.0299,\n",
      "        0.0318], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0247, 0.0283, 0.0262, 0.0283, 0.0301, 0.0284, 0.0269, 0.0272, 0.0286,\n",
      "        0.0286], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.0707385576533852e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0303, 0.0268, 0.0278, 0.0324], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0316, 0.0275, 0.0279, 0.0303, 0.0336, 0.0303, 0.0290, 0.0303, 0.0275,\n",
      "        0.0317], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0285, 0.0261, 0.0285, 0.0292, 0.0303, 0.0292, 0.0287, 0.0292, 0.0261,\n",
      "        0.0292], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.591253744161804e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0296, 0.0262, 0.0271, 0.0330], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0285, 0.0303, 0.0309, 0.0268, 0.0258, 0.0309, 0.0330, 0.0310, 0.0310,\n",
      "        0.0296], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0299, 0.0283, 0.0298, 0.0267, 0.0278, 0.0298, 0.0297, 0.0279, 0.0279,\n",
      "        0.0297], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.258910394128179e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0336, 0.0258, 0.0289, 0.0308, 0.0254, 0.0300, 0.0293, 0.0303, 0.0324,\n",
      "        0.0275], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0303, 0.0273, 0.0299, 0.0277, 0.0270, 0.0273, 0.0277, 0.0303, 0.0291,\n",
      "        0.0299], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.2657096603070386e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0275, 0.0258, 0.0262, 0.0337], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0272, 0.0337, 0.0269, 0.0337, 0.0272, 0.0272, 0.0283, 0.0290, 0.0269,\n",
      "        0.0313], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0298, 0.0303, 0.0279, 0.0303, 0.0298, 0.0298, 0.0282, 0.0303, 0.0279,\n",
      "        0.0282], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.798061465611681e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0263, 0.0259, 0.0268, 0.0330], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0263, 0.0258, 0.0245, 0.0268, 0.0279, 0.0320, 0.0279, 0.0246, 0.0279,\n",
      "        0.0295], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0278, 0.0264, 0.0264, 0.0264, 0.0288, 0.0288, 0.0288, 0.0235, 0.0288,\n",
      "        0.0281], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.2007684492564294e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0255, 0.0261, 0.0272, 0.0322], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0264, 0.0322, 0.0252, 0.0264, 0.0286, 0.0308, 0.0299, 0.0285, 0.0246,\n",
      "        0.0308], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0268, 0.0290, 0.0300, 0.0280, 0.0277, 0.0277, 0.0283, 0.0280, 0.0280,\n",
      "        0.0277], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.0306582529156e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0256, 0.0257, 0.0274, 0.0316], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0299, 0.0271, 0.0298, 0.0256, 0.0298, 0.0299, 0.0258, 0.0298, 0.0253,\n",
      "        0.0258], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0268, 0.0260, 0.0268, 0.0285, 0.0268, 0.0268, 0.0269, 0.0268, 0.0237,\n",
      "        0.0269], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.076817044231575e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0261, 0.0252, 0.0266, 0.0315], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0311, 0.0297, 0.0297, 0.0236, 0.0278, 0.0252, 0.0236, 0.0285, 0.0315,\n",
      "        0.0261], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0280, 0.0268, 0.0268, 0.0247, 0.0271, 0.0247, 0.0247, 0.0284, 0.0284,\n",
      "        0.0284], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.570970304484945e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0271, 0.0249, 0.0258, 0.0311], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0311, 0.0245, 0.0277, 0.0235, 0.0263, 0.0274, 0.0277, 0.0277, 0.0278,\n",
      "        0.0305], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0280, 0.0232, 0.0264, 0.0232, 0.0280, 0.0288, 0.0264, 0.0264, 0.0267,\n",
      "        0.0275], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.199256752850488e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0278, 0.0245, 0.0248, 0.0310], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0258, 0.0270, 0.0268, 0.0265, 0.0268, 0.0239, 0.0284, 0.0306, 0.0284,\n",
      "        0.0310], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0266, 0.0268, 0.0265, 0.0266, 0.0265, 0.0225, 0.0264, 0.0275, 0.0264,\n",
      "        0.0279], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.99573662232433e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0282, 0.0242, 0.0238, 0.0309], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0282, 0.0242, 0.0260, 0.0240, 0.0320, 0.0242, 0.0260, 0.0320, 0.0309,\n",
      "        0.0285], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0278, 0.0252, 0.0267, 0.0258, 0.0288, 0.0252, 0.0267, 0.0288, 0.0278,\n",
      "        0.0288], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.615172772697406e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0284, 0.0242, 0.0233, 0.0304], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0264, 0.0321, 0.0247, 0.0237, 0.0284, 0.0300, 0.0294, 0.0304, 0.0233,\n",
      "        0.0304], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0268, 0.0289, 0.0248, 0.0255, 0.0274, 0.0274, 0.0264, 0.0274, 0.0270,\n",
      "        0.0274], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.2557446653954685e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0279, 0.0242, 0.0234, 0.0301], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0257, 0.0305, 0.0305, 0.0255, 0.0249, 0.0242, 0.0301, 0.0305, 0.0315,\n",
      "        0.0305], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0262, 0.0275, 0.0275, 0.0244, 0.0262, 0.0244, 0.0271, 0.0275, 0.0283,\n",
      "        0.0275], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.91767457081005e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0274, 0.0242, 0.0236, 0.0297], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0284, 0.0265, 0.0244, 0.0259, 0.0304, 0.0275, 0.0297, 0.0263, 0.0236,\n",
      "        0.0295], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0267, 0.0256, 0.0276, 0.0258, 0.0274, 0.0258, 0.0267, 0.0266, 0.0256,\n",
      "        0.0266], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.727297891804483e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0265, 0.0238, 0.0240, 0.0297], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0297, 0.0300, 0.0297, 0.0300, 0.0297, 0.0297, 0.0300, 0.0298, 0.0252,\n",
      "        0.0246], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0267, 0.0270, 0.0267, 0.0270, 0.0267, 0.0267, 0.0270, 0.0268, 0.0258,\n",
      "        0.0270], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.713912964391056e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0257, 0.0233, 0.0243, 0.0297], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0261, 0.0293, 0.0254, 0.0266, 0.0250, 0.0233, 0.0250, 0.0291, 0.0257,\n",
      "        0.0297], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0256, 0.0264, 0.0258, 0.0258, 0.0235, 0.0241, 0.0235, 0.0265, 0.0267,\n",
      "        0.0267], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.148511950712418e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0250, 0.0229, 0.0246, 0.0295], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0255, 0.0238, 0.0250, 0.0259, 0.0255, 0.0199, 0.0251, 0.0211, 0.0222,\n",
      "        0.0285], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0257, 0.0253, 0.0265, 0.0253, 0.0257, 0.0242, 0.0253, 0.0240, 0.0232,\n",
      "        0.0257], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.0338022699870635e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0250, 0.0226, 0.0248, 0.0290], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0279, 0.0226, 0.0226, 0.0221, 0.0226, 0.0246, 0.0279, 0.0282, 0.0282,\n",
      "        0.0226], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0251, 0.0234, 0.0234, 0.0234, 0.0234, 0.0267, 0.0251, 0.0253, 0.0253,\n",
      "        0.0234], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.024878762720618e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0253, 0.0226, 0.0247, 0.0284], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0284, 0.0253, 0.0284, 0.0267, 0.0246, 0.0279, 0.0254, 0.0279, 0.0241,\n",
      "        0.0284], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0255, 0.0255, 0.0255, 0.0248, 0.0265, 0.0251, 0.0245, 0.0251, 0.0229,\n",
      "        0.0255], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.9455888984084595e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0252, 0.0228, 0.0244, 0.0277], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0273, 0.0277, 0.0277, 0.0247, 0.0237, 0.0255, 0.0230, 0.0263, 0.0194,\n",
      "        0.0247], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0246, 0.0250, 0.0250, 0.0244, 0.0225, 0.0222, 0.0229, 0.0244, 0.0222,\n",
      "        0.0244], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.661430466512684e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0252, 0.0229, 0.0237, 0.0274], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0214, 0.0252, 0.0219, 0.0246, 0.0274, 0.0274, 0.0222, 0.0244, 0.0225,\n",
      "        0.0234], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0214, 0.0246, 0.0227, 0.0224, 0.0246, 0.0246, 0.0228, 0.0244, 0.0224,\n",
      "        0.0221], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.2919493858353235e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0251, 0.0231, 0.0230, 0.0269], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0269, 0.0271, 0.0235, 0.0246, 0.0199, 0.0237, 0.0246, 0.0288, 0.0271,\n",
      "        0.0269], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0242, 0.0244, 0.0227, 0.0242, 0.0227, 0.0242, 0.0242, 0.0259, 0.0244,\n",
      "        0.0242], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.630333478417015e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0250, 0.0232, 0.0221, 0.0267], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0233, 0.0267, 0.0250, 0.0267, 0.0236, 0.0267, 0.0220, 0.0268, 0.0271,\n",
      "        0.0236], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0218, 0.0241, 0.0241, 0.0241, 0.0244, 0.0241, 0.0224, 0.0241, 0.0244,\n",
      "        0.0244], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.061473191541154e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0247, 0.0233, 0.0214, 0.0264], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0269, 0.0264, 0.0228, 0.0228, 0.0269, 0.0213, 0.0264, 0.0223, 0.0225,\n",
      "        0.0231], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0242, 0.0238, 0.0242, 0.0215, 0.0242, 0.0229, 0.0238, 0.0229, 0.0238,\n",
      "        0.0222], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.7222137052594917e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0244, 0.0233, 0.0211, 0.0259], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0259, 0.0254, 0.0265, 0.0224, 0.0223, 0.0259, 0.0224, 0.0223, 0.0259,\n",
      "        0.0259], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0233, 0.0224, 0.0238, 0.0238, 0.0212, 0.0233, 0.0238, 0.0212, 0.0233,\n",
      "        0.0233], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.906050889985636e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0240, 0.0232, 0.0213, 0.0251], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0232, 0.0251, 0.0226, 0.0251, 0.0255, 0.0211, 0.0256, 0.0233, 0.0226,\n",
      "        0.0213], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0205, 0.0226, 0.0230, 0.0226, 0.0230, 0.0213, 0.0230, 0.0226, 0.0230,\n",
      "        0.0210], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.4287277230760083e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0233, 0.0232, 0.0215, 0.0243], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0246, 0.0229, 0.0229, 0.0206, 0.0279, 0.0243, 0.0221, 0.0229, 0.0255,\n",
      "        0.0229], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0222, 0.0222, 0.0222, 0.0204, 0.0221, 0.0219, 0.0222, 0.0243, 0.0256,\n",
      "        0.0222], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.876435014011804e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0232, 0.0226, 0.0215, 0.0238], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0228, 0.0228, 0.0224, 0.0228, 0.0240, 0.0246, 0.0208, 0.0270, 0.0240,\n",
      "        0.0236], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0214, 0.0216, 0.0243, 0.0216, 0.0216, 0.0224, 0.0243, 0.0222, 0.0216,\n",
      "        0.0224], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.1049963733239565e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0232, 0.0216, 0.0211, 0.0238], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0201, 0.0224, 0.0238, 0.0202, 0.0201, 0.0231, 0.0216, 0.0238, 0.0224,\n",
      "        0.0241], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0185, 0.0217, 0.0214, 0.0206, 0.0196, 0.0244, 0.0227, 0.0214, 0.0217,\n",
      "        0.0217], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.389394694546354e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0215, 0.0209, 0.0176, 0.0231, 0.0261, 0.0195, 0.0248, 0.0200, 0.0222,\n",
      "        0.0232], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0219, 0.0205, 0.0211, 0.0215, 0.0235, 0.0201, 0.0223, 0.0205, 0.0219,\n",
      "        0.0235], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.8780041247955523e-06\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([0.0246, 0.0211, 0.0226, 0.0203, 0.0226, 0.0240, 0.0207, 0.0255, 0.0166,\n",
      "        0.0255], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0221, 0.0221, 0.0216, 0.0202, 0.0216, 0.0216, 0.0202, 0.0229, 0.0182,\n",
      "        0.0229], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.0642816000181483e-06\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0243, 0.0227, 0.0249, 0.0162, 0.0231, 0.0210, 0.0233, 0.0189, 0.0181,\n",
      "        0.0158], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0219, 0.0224, 0.0224, 0.0179, 0.0208, 0.0219, 0.0211, 0.0198, 0.0179,\n",
      "        0.0201], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.5339384087128565e-06\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0212, 0.0163, 0.0205, 0.0206], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0222, 0.0246, 0.0228, 0.0204, 0.0250, 0.0198, 0.0246, 0.0190, 0.0234,\n",
      "        0.0192], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0225, 0.0221, 0.0195, 0.0187, 0.0225, 0.0187, 0.0221, 0.0186, 0.0187,\n",
      "        0.0195], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.578106538450811e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0200, 0.0186, 0.0193, 0.0249], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0244, 0.0200, 0.0171, 0.0193, 0.0204, 0.0171, 0.0243, 0.0206, 0.0171,\n",
      "        0.0194], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0228, 0.0227, 0.0187, 0.0187, 0.0227, 0.0187, 0.0210, 0.0179, 0.0187,\n",
      "        0.0217], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.642187377612572e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0185, 0.0166, 0.0164, 0.0191, 0.0200, 0.0194, 0.0204, 0.0207, 0.0194,\n",
      "        0.0243], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0182, 0.0184, 0.0173, 0.0212, 0.0219, 0.0219, 0.0219, 0.0219, 0.0213,\n",
      "        0.0219], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.1850672712607775e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0194, 0.0187, 0.0200, 0.0230], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0173, 0.0194, 0.0230, 0.0224, 0.0239, 0.0191, 0.0207, 0.0230, 0.0224,\n",
      "        0.0185], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0178, 0.0207, 0.0207, 0.0218, 0.0215, 0.0186, 0.0186, 0.0207, 0.0218,\n",
      "        0.0188], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.3575812519993633e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([0.0214, 0.0196, 0.0175, 0.0196, 0.0246, 0.0175, 0.0204, 0.0240, 0.0172,\n",
      "        0.0194], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0192, 0.0193, 0.0189, 0.0193, 0.0221, 0.0189, 0.0193, 0.0216, 0.0173,\n",
      "        0.0191], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.2245390027819667e-06\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0173, 0.0215, 0.0242, 0.0215, 0.0195, 0.0202, 0.0242, 0.0197, 0.0212,\n",
      "        0.0247], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0191, 0.0194, 0.0218, 0.0194, 0.0173, 0.0182, 0.0218, 0.0182, 0.0171,\n",
      "        0.0223], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.85366888117278e-06\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0173, 0.0183, 0.0190, 0.0132], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0188, 0.0210, 0.0215, 0.0176, 0.0188, 0.0188, 0.0166, 0.0211, 0.0177,\n",
      "        0.0188], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0182, 0.0182, 0.0194, 0.0194, 0.0182, 0.0182, 0.0172, 0.0209, 0.0192,\n",
      "        0.0182], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.9039400740439305e-06\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0179, 0.0219, 0.0192, 0.0161], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0208, 0.0206, 0.0184, 0.0175, 0.0175, 0.0189, 0.0245, 0.0175, 0.0224,\n",
      "        0.0248], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0216, 0.0210, 0.0210, 0.0184, 0.0184, 0.0172, 0.0221, 0.0184, 0.0223,\n",
      "        0.0224], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.5588547032384668e-06\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0200, 0.0212, 0.0251, 0.0134], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0214, 0.0167, 0.0226, 0.0167, 0.0209, 0.0155, 0.0245, 0.0167, 0.0155,\n",
      "        0.0216], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0222, 0.0187, 0.0220, 0.0187, 0.0214, 0.0196, 0.0220, 0.0187, 0.0196,\n",
      "        0.0196], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.7489478422212414e-06\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0203, 0.0210, 0.0241, 0.0139], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0204, 0.0200, 0.0174, 0.0206, 0.0195, 0.0195, 0.0174, 0.0185, 0.0186,\n",
      "        0.0179], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0185, 0.0224, 0.0183, 0.0185, 0.0176, 0.0176, 0.0183, 0.0224, 0.0180,\n",
      "        0.0194], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.93810751120327e-06\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0198, 0.0212, 0.0231, 0.0145], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0130, 0.0191, 0.0182, 0.0182, 0.0199, 0.0231, 0.0195, 0.0210, 0.0207,\n",
      "        0.0182], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0186, 0.0171, 0.0179, 0.0179, 0.0179, 0.0208, 0.0181, 0.0217, 0.0201,\n",
      "        0.0179], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.8327187869290356e-06\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0190, 0.0214, 0.0222, 0.0153], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0243, 0.0125, 0.0159, 0.0203, 0.0237, 0.0176, 0.0200, 0.0203, 0.0199,\n",
      "        0.0229], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0219, 0.0165, 0.0176, 0.0197, 0.0219, 0.0176, 0.0213, 0.0197, 0.0202,\n",
      "        0.0206], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.5869222756446106e-06\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0183, 0.0210, 0.0212, 0.0167], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0219, 0.0239, 0.0195, 0.0174, 0.0174, 0.0216, 0.0150, 0.0173, 0.0178,\n",
      "        0.0179], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0197, 0.0215, 0.0178, 0.0187, 0.0185, 0.0191, 0.0195, 0.0178, 0.0160,\n",
      "        0.0169], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.696616997534875e-06\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0163, 0.0171, 0.0220, 0.0206, 0.0185, 0.0220, 0.0220, 0.0220, 0.0184,\n",
      "        0.0206], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0162, 0.0186, 0.0198, 0.0186, 0.0209, 0.0198, 0.0198, 0.0198, 0.0196,\n",
      "        0.0186], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.678257371575455e-06\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0175, 0.0195, 0.0214, 0.0183], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0153, 0.0225, 0.0172, 0.0172, 0.0227, 0.0227, 0.0217, 0.0165, 0.0227,\n",
      "        0.0212], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0175, 0.0175, 0.0175, 0.0191, 0.0204, 0.0204, 0.0191, 0.0161, 0.0204,\n",
      "        0.0191], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.031884822732536e-06\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0173, 0.0189, 0.0218, 0.0184], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0148, 0.0219, 0.0228, 0.0176, 0.0219, 0.0228, 0.0200, 0.0228, 0.0228,\n",
      "        0.0200], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0150, 0.0213, 0.0206, 0.0192, 0.0213, 0.0206, 0.0180, 0.0206, 0.0206,\n",
      "        0.0180], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.2266118523693876e-06\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0180, 0.0188, 0.0171, 0.0183, 0.0179, 0.0228, 0.0180, 0.0183, 0.0218,\n",
      "        0.0228], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0191, 0.0174, 0.0196, 0.0174, 0.0214, 0.0205, 0.0191, 0.0165, 0.0191,\n",
      "        0.0205], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.422773599799257e-06\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0174, 0.0140, 0.0173, 0.0167], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0183, 0.0207, 0.0179, 0.0191, 0.0222, 0.0155, 0.0179, 0.0133, 0.0222,\n",
      "        0.0152], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0185, 0.0209, 0.0164, 0.0185, 0.0200, 0.0171, 0.0164, 0.0159, 0.0200,\n",
      "        0.0179], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.1029273941385327e-06\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([0.0155, 0.0156, 0.0155, 0.0201], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0224, 0.0196, 0.0186, 0.0214, 0.0214, 0.0214, 0.0172, 0.0186, 0.0173,\n",
      "        0.0224], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0171, 0.0201, 0.0167, 0.0192, 0.0192, 0.0192, 0.0161, 0.0167, 0.0170,\n",
      "        0.0171], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.71956820244668e-06\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([0.0152, 0.0163, 0.0173, 0.0175], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0202, 0.0195, 0.0177, 0.0206, 0.0163, 0.0206, 0.0148, 0.0163, 0.0191,\n",
      "        0.0179], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0194, 0.0176, 0.0182, 0.0185, 0.0176, 0.0185, 0.0165, 0.0176, 0.0176,\n",
      "        0.0185], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.1474504592333687e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0165, 0.0167, 0.0196, 0.0176], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0176, 0.0123, 0.0176, 0.0178, 0.0176, 0.0196, 0.0165, 0.0165, 0.0167,\n",
      "        0.0110], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0176, 0.0178, 0.0176, 0.0178, 0.0176, 0.0182, 0.0176, 0.0176, 0.0160,\n",
      "        0.0182], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.807583071757108e-06\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0191, 0.0188, 0.0159, 0.0196, 0.0153, 0.0184, 0.0160, 0.0171, 0.0161,\n",
      "        0.0173], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0171, 0.0167, 0.0155, 0.0185, 0.0155, 0.0185, 0.0185, 0.0169, 0.0196,\n",
      "        0.0168], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.842410140146967e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0174, 0.0171, 0.0178, 0.0174], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0160, 0.0178, 0.0149, 0.0199, 0.0165, 0.0165, 0.0163, 0.0174, 0.0178,\n",
      "        0.0157], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0160, 0.0167, 0.0214, 0.0200, 0.0178, 0.0178, 0.0156, 0.0160, 0.0167,\n",
      "        0.0160], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.969507244823035e-06\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0161, 0.0153, 0.0171, 0.0148], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0162, 0.0177, 0.0187, 0.0143, 0.0177, 0.0169, 0.0177, 0.0162, 0.0192,\n",
      "        0.0187], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0157, 0.0154, 0.0201, 0.0150, 0.0163, 0.0173, 0.0163, 0.0163, 0.0170,\n",
      "        0.0201], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.823922843868786e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0172, 0.0190, 0.0174, 0.0192], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0174, 0.0161, 0.0171, 0.0169, 0.0171, 0.0222, 0.0173, 0.0174, 0.0211,\n",
      "        0.0222], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0173, 0.0167, 0.0172, 0.0167, 0.0172, 0.0199, 0.0172, 0.0173, 0.0190,\n",
      "        0.0199], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.4803421208853251e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0167, 0.0189, 0.0166, 0.0205], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0169, 0.0153, 0.0199, 0.0151, 0.0157, 0.0157, 0.0220, 0.0144, 0.0156,\n",
      "        0.0166], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0162, 0.0147, 0.0179, 0.0164, 0.0184, 0.0184, 0.0198, 0.0151, 0.0162,\n",
      "        0.0184], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.029408844668069e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0162, 0.0188, 0.0166, 0.0206], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0188, 0.0162, 0.0166, 0.0193, 0.0201, 0.0201, 0.0166, 0.0163, 0.0166,\n",
      "        0.0178], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0174, 0.0153, 0.0185, 0.0181, 0.0181, 0.0181, 0.0185, 0.0181, 0.0185,\n",
      "        0.0180], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.645235099407728e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0158, 0.0187, 0.0174, 0.0198], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0178, 0.0207, 0.0187, 0.0167, 0.0176, 0.0198, 0.0216, 0.0147, 0.0180,\n",
      "        0.0196], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0176, 0.0186, 0.0194, 0.0160, 0.0180, 0.0178, 0.0194, 0.0163, 0.0198,\n",
      "        0.0176], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.3924862944113556e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0157, 0.0183, 0.0181, 0.0192], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0145, 0.0160, 0.0210, 0.0160, 0.0171, 0.0168, 0.0177, 0.0181, 0.0192,\n",
      "        0.0181], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0173, 0.0171, 0.0189, 0.0171, 0.0157, 0.0191, 0.0173, 0.0173, 0.0173,\n",
      "        0.0173], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.707184421524289e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0162, 0.0178, 0.0185, 0.0187], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0151, 0.0159, 0.0185, 0.0187, 0.0147, 0.0187, 0.0172, 0.0152, 0.0157,\n",
      "        0.0154], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0158, 0.0166, 0.0181, 0.0168, 0.0154, 0.0168, 0.0168, 0.0168, 0.0154,\n",
      "        0.0144], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.2606548125404515e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0167, 0.0173, 0.0190, 0.0181], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0169, 0.0166, 0.0190, 0.0181, 0.0175, 0.0175, 0.0190, 0.0164, 0.0190,\n",
      "        0.0190], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0163, 0.0163, 0.0171, 0.0163, 0.0163, 0.0163, 0.0171, 0.0149, 0.0171,\n",
      "        0.0171], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.3490661078540143e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0167, 0.0168, 0.0193, 0.0178], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0175, 0.0147, 0.0220, 0.0162, 0.0176, 0.0168, 0.0158, 0.0198, 0.0193,\n",
      "        0.0170], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0160, 0.0150, 0.0198, 0.0160, 0.0160, 0.0158, 0.0173, 0.0181, 0.0174,\n",
      "        0.0165], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.0189631868561264e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0162, 0.0167, 0.0194, 0.0176], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0176, 0.0176, 0.0163, 0.0192, 0.0167, 0.0195, 0.0161, 0.0177, 0.0176,\n",
      "        0.0175], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0175, 0.0175, 0.0150, 0.0173, 0.0175, 0.0181, 0.0160, 0.0159, 0.0175,\n",
      "        0.0192], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.4312836356111802e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0156, 0.0169, 0.0194, 0.0174], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0156, 0.0176, 0.0153, 0.0203, 0.0194, 0.0169, 0.0176, 0.0165, 0.0176,\n",
      "        0.0214], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0175, 0.0159, 0.0182, 0.0172, 0.0175, 0.0159, 0.0159, 0.0159, 0.0159,\n",
      "        0.0172], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.376866283768322e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0151, 0.0164, 0.0193, 0.0176], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0182, 0.0197, 0.0193, 0.0150, 0.0179, 0.0178, 0.0140, 0.0135, 0.0176,\n",
      "        0.0176], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0177, 0.0174, 0.0174, 0.0167, 0.0192, 0.0161, 0.0161, 0.0151, 0.0174,\n",
      "        0.0174], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.3969998892425792e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0148, 0.0162, 0.0190, 0.0177], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0133, 0.0148, 0.0151, 0.0190, 0.0163, 0.0173, 0.0213, 0.0177, 0.0180,\n",
      "        0.0190], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0148, 0.0173, 0.0142, 0.0171, 0.0173, 0.0162, 0.0191, 0.0171, 0.0162,\n",
      "        0.0171], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.6478562631382374e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0149, 0.0160, 0.0184, 0.0179], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0179, 0.0137, 0.0148, 0.0186, 0.0184, 0.0153, 0.0151, 0.0157, 0.0182,\n",
      "        0.0132], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0165, 0.0158, 0.0158, 0.0165, 0.0165, 0.0156, 0.0164, 0.0167, 0.0164,\n",
      "        0.0142], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.2032666038285242e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0152, 0.0161, 0.0179, 0.0175], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0180, 0.0179, 0.0156, 0.0173, 0.0152, 0.0147, 0.0179, 0.0179, 0.0179,\n",
      "        0.0195], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0161, 0.0161, 0.0161, 0.0180, 0.0162, 0.0137, 0.0161, 0.0161, 0.0161,\n",
      "        0.0170], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.58433146882453e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0155, 0.0163, 0.0173, 0.0172], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0162, 0.0153, 0.0156, 0.0173, 0.0175, 0.0185, 0.0187, 0.0141, 0.0158,\n",
      "        0.0175], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0159, 0.0141, 0.0151, 0.0155, 0.0173, 0.0167, 0.0167, 0.0136, 0.0158,\n",
      "        0.0158], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.5219025044643786e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0157, 0.0167, 0.0163, 0.0169], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0172, 0.0167, 0.0159, 0.0163, 0.0135, 0.0181, 0.0163, 0.0177, 0.0181,\n",
      "        0.0159], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0155, 0.0161, 0.0155, 0.0152, 0.0147, 0.0163, 0.0152, 0.0168, 0.0163,\n",
      "        0.0155], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.4881828747093095e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([0.0170, 0.0158, 0.0158, 0.0162, 0.0170, 0.0149, 0.0158, 0.0152, 0.0170,\n",
      "        0.0170], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0153, 0.0147, 0.0147, 0.0176, 0.0165, 0.0153, 0.0187, 0.0149, 0.0153,\n",
      "        0.0153], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.2417975742428098e-06\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0135, 0.0188, 0.0127, 0.0153], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0162, 0.0152, 0.0148, 0.0153, 0.0145, 0.0168, 0.0164, 0.0166, 0.0159,\n",
      "        0.0166], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0148, 0.0147, 0.0148, 0.0156, 0.0156, 0.0135, 0.0170, 0.0149, 0.0149,\n",
      "        0.0149], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.159639279852854e-06\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0152, 0.0170, 0.0155, 0.0140], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0137, 0.0157, 0.0137, 0.0167, 0.0205, 0.0205, 0.0157, 0.0195, 0.0159,\n",
      "        0.0152], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0159, 0.0159, 0.0159, 0.0176, 0.0185, 0.0185, 0.0159, 0.0185, 0.0143,\n",
      "        0.0143], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.3212658106785966e-06\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0152, 0.0187, 0.0140, 0.0189], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0152, 0.0159, 0.0143, 0.0176, 0.0152, 0.0151, 0.0135, 0.0133, 0.0135,\n",
      "        0.0152], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0157, 0.0145, 0.0156, 0.0174, 0.0154, 0.0147, 0.0157, 0.0151, 0.0157,\n",
      "        0.0157], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.7587420870768256e-06\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0159, 0.0165, 0.0154, 0.0132], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0154, 0.0149, 0.0149, 0.0138, 0.0200, 0.0153, 0.0152, 0.0138, 0.0149,\n",
      "        0.0149], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0149, 0.0151, 0.0151, 0.0149, 0.0180, 0.0153, 0.0149, 0.0149, 0.0151,\n",
      "        0.0151], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.915740300428297e-07\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0156, 0.0174, 0.0144, 0.0185], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0142, 0.0131, 0.0177, 0.0180, 0.0179, 0.0167, 0.0130, 0.0142, 0.0179,\n",
      "        0.0151], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0149, 0.0164, 0.0180, 0.0152, 0.0161, 0.0180, 0.0144, 0.0149, 0.0161,\n",
      "        0.0149], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.987743300764123e-06\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0158, 0.0151, 0.0164, 0.0135], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0154, 0.0149, 0.0149, 0.0139, 0.0187, 0.0151, 0.0117, 0.0149, 0.0151,\n",
      "        0.0187], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0146, 0.0151, 0.0151, 0.0151, 0.0178, 0.0146, 0.0156, 0.0151, 0.0168,\n",
      "        0.0178], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.1915550405537942e-06\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0158, 0.0142, 0.0174, 0.0139], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0150, 0.0150, 0.0130, 0.0178, 0.0122, 0.0148, 0.0161, 0.0162, 0.0130,\n",
      "        0.0150], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0150, 0.0150, 0.0145, 0.0179, 0.0156, 0.0158, 0.0158, 0.0146, 0.0145,\n",
      "        0.0150], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.010245680139633e-06\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0154, 0.0133, 0.0187, 0.0143], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0142, 0.0162, 0.0118, 0.0147, 0.0152, 0.0113, 0.0120, 0.0166, 0.0168,\n",
      "        0.0159], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0146, 0.0146, 0.0136, 0.0136, 0.0146, 0.0147, 0.0136, 0.0149, 0.0151,\n",
      "        0.0159], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.7238752409175504e-06\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([0.0153, 0.0153, 0.0127, 0.0159, 0.0162, 0.0182, 0.0158, 0.0157, 0.0145,\n",
      "        0.0145], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0140, 0.0140, 0.0133, 0.0170, 0.0133, 0.0192, 0.0142, 0.0142, 0.0153,\n",
      "        0.0153], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.971919800780597e-06\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0130, 0.0138, 0.0192, 0.0185], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0093, 0.0155, 0.0152, 0.0201, 0.0160, 0.0166, 0.0151, 0.0143, 0.0151,\n",
      "        0.0108], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0165, 0.0155, 0.0138, 0.0181, 0.0181, 0.0161, 0.0150, 0.0150, 0.0150,\n",
      "        0.0155], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.401297236559913e-06\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0143, 0.0198, 0.0164, 0.0146, 0.0189, 0.0143, 0.0130, 0.0164, 0.0151,\n",
      "        0.0135], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0148, 0.0179, 0.0179, 0.0138, 0.0200, 0.0147, 0.0159, 0.0179, 0.0148,\n",
      "        0.0136], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.882978494904819e-06\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0136, 0.0150, 0.0157, 0.0091], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0198, 0.0215, 0.0126, 0.0114, 0.0145, 0.0154, 0.0172, 0.0150, 0.0154,\n",
      "        0.0140], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0178, 0.0194, 0.0153, 0.0153, 0.0170, 0.0141, 0.0172, 0.0151, 0.0141,\n",
      "        0.0140], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.03484182243119e-06\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0136, 0.0161, 0.0146, 0.0091], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0156, 0.0137, 0.0147, 0.0151, 0.0171, 0.0138, 0.0148, 0.0151, 0.0156,\n",
      "        0.0136], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0169, 0.0147, 0.0141, 0.0136, 0.0167, 0.0148, 0.0141, 0.0136, 0.0169,\n",
      "        0.0135], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.0785122412926285e-06\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0170, 0.0201, 0.0196, 0.0189], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0147, 0.0147, 0.0176, 0.0175, 0.0125, 0.0145, 0.0182, 0.0141, 0.0169,\n",
      "        0.0148], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0138, 0.0146, 0.0158, 0.0158, 0.0132, 0.0158, 0.0158, 0.0158, 0.0158,\n",
      "        0.0144], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.9521512513165362e-06\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0171, 0.0210, 0.0189, 0.0187], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0176, 0.0170, 0.0176, 0.0118, 0.0161, 0.0143, 0.0135, 0.0135, 0.0161,\n",
      "        0.0184], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0158, 0.0142, 0.0158, 0.0147, 0.0145, 0.0153, 0.0145, 0.0153, 0.0139,\n",
      "        0.0165], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.873611149174394e-06\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0178, 0.0213, 0.0183, 0.0187], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0153, 0.0153, 0.0122, 0.0130, 0.0137, 0.0213, 0.0189, 0.0152, 0.0141,\n",
      "        0.0177], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0159, 0.0159, 0.0170, 0.0159, 0.0166, 0.0191, 0.0166, 0.0168, 0.0154,\n",
      "        0.0159], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.765138666902203e-06\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0187, 0.0208, 0.0185, 0.0186], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0136, 0.0153, 0.0155, 0.0139, 0.0155, 0.0138, 0.0158, 0.0165, 0.0141,\n",
      "        0.0139], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0158, 0.0148, 0.0148, 0.0140, 0.0157, 0.0140, 0.0147, 0.0140, 0.0157,\n",
      "        0.0148], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.6538464251425467e-06\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0196, 0.0201, 0.0187, 0.0188], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0141, 0.0141, 0.0156, 0.0156, 0.0150, 0.0175, 0.0201, 0.0123, 0.0141,\n",
      "        0.0150], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0146, 0.0156, 0.0158, 0.0158, 0.0136, 0.0158, 0.0181, 0.0156, 0.0146,\n",
      "        0.0136], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.5234987788280705e-06\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0201, 0.0192, 0.0191, 0.0192], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0147, 0.0182, 0.0115, 0.0145, 0.0163, 0.0178, 0.0178, 0.0161, 0.0145,\n",
      "        0.0161], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0137, 0.0164, 0.0150, 0.0150, 0.0164, 0.0161, 0.0161, 0.0161, 0.0150,\n",
      "        0.0161], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.354712478336296e-06\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0157, 0.0157, 0.0159, 0.0168], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0135, 0.0159, 0.0202, 0.0184, 0.0148, 0.0172, 0.0202, 0.0148, 0.0161,\n",
      "        0.0168], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0137, 0.0182, 0.0152, 0.0166, 0.0150, 0.0149, 0.0152, 0.0150, 0.0151,\n",
      "        0.0162], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.585472874576226e-06\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0160, 0.0135, 0.0181, 0.0177], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0155, 0.0189, 0.0160, 0.0135, 0.0121, 0.0189, 0.0143, 0.0181, 0.0162,\n",
      "        0.0158], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0170, 0.0170, 0.0148, 0.0165, 0.0148, 0.0170, 0.0148, 0.0163, 0.0142,\n",
      "        0.0142], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.671799731819192e-06\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0147, 0.0133, 0.0190, 0.0180], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0125, 0.0190, 0.0118, 0.0150, 0.0190, 0.0164, 0.0161, 0.0171, 0.0165,\n",
      "        0.0156], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0152, 0.0171, 0.0149, 0.0149, 0.0171, 0.0152, 0.0145, 0.0195, 0.0170,\n",
      "        0.0149], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.5429318359092576e-06\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0139, 0.0136, 0.0196, 0.0176], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0192, 0.0157, 0.0133, 0.0116, 0.0174, 0.0146, 0.0157, 0.0140, 0.0125,\n",
      "        0.0192], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0173, 0.0147, 0.0143, 0.0148, 0.0201, 0.0168, 0.0147, 0.0147, 0.0148,\n",
      "        0.0173], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.905602170561906e-06\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0137, 0.0144, 0.0194, 0.0171], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0182, 0.0154, 0.0190, 0.0155, 0.0139, 0.0176, 0.0190, 0.0152, 0.0190,\n",
      "        0.0155], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0198, 0.0147, 0.0171, 0.0156, 0.0156, 0.0171, 0.0171, 0.0146, 0.0171,\n",
      "        0.0156], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.7162301446660422e-06\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0137, 0.0152, 0.0190, 0.0165], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0150, 0.0190, 0.0152, 0.0146, 0.0187, 0.0190, 0.0146, 0.0187, 0.0190,\n",
      "        0.0155], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0153, 0.0171, 0.0163, 0.0144, 0.0168, 0.0171, 0.0144, 0.0168, 0.0171,\n",
      "        0.0132], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.4686396500328556e-06\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0137, 0.0161, 0.0185, 0.0159], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0130, 0.0161, 0.0140, 0.0182, 0.0182, 0.0159, 0.0201, 0.0136, 0.0185,\n",
      "        0.0134], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0159, 0.0138, 0.0140, 0.0163, 0.0163, 0.0138, 0.0188, 0.0140, 0.0167,\n",
      "        0.0147], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.184914703524555e-06\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0141, 0.0165, 0.0182, 0.0150], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0136, 0.0133, 0.0128, 0.0144, 0.0151, 0.0205, 0.0139, 0.0133, 0.0135,\n",
      "        0.0133], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0145, 0.0138, 0.0145, 0.0148, 0.0145, 0.0184, 0.0148, 0.0138, 0.0145,\n",
      "        0.0138], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.108265905713779e-06\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0148, 0.0167, 0.0175, 0.0146], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0131, 0.0146, 0.0144, 0.0131, 0.0207, 0.0134, 0.0138, 0.0167, 0.0172,\n",
      "        0.0131], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0137, 0.0156, 0.0146, 0.0137, 0.0186, 0.0146, 0.0148, 0.0150, 0.0154,\n",
      "        0.0137], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.4840799167359364e-06\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0156, 0.0167, 0.0169, 0.0144], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0135, 0.0156, 0.0146, 0.0165, 0.0146, 0.0165, 0.0207, 0.0167, 0.0157,\n",
      "        0.0130], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0132, 0.0149, 0.0148, 0.0148, 0.0148, 0.0148, 0.0186, 0.0159, 0.0141,\n",
      "        0.0149], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.7047166238626232e-06\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([0.0161, 0.0135, 0.0164, 0.0132, 0.0154, 0.0132, 0.0132, 0.0183, 0.0159,\n",
      "        0.0206], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0152, 0.0153, 0.0147, 0.0147, 0.0185, 0.0147, 0.0147, 0.0165, 0.0150,\n",
      "        0.0185], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.17882290801208e-06\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0160, 0.0170, 0.0155, 0.0166], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0148, 0.0167, 0.0148, 0.0128, 0.0148, 0.0164, 0.0135, 0.0160, 0.0150,\n",
      "        0.0138], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0151, 0.0154, 0.0151, 0.0139, 0.0151, 0.0147, 0.0139, 0.0139, 0.0153,\n",
      "        0.0147], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.1514741800056072e-06\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0157, 0.0164, 0.0156, 0.0170], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0157, 0.0157, 0.0134, 0.0153, 0.0147, 0.0153, 0.0161, 0.0146, 0.0165,\n",
      "        0.0153], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0148, 0.0148, 0.0132, 0.0153, 0.0146, 0.0153, 0.0145, 0.0144, 0.0149,\n",
      "        0.0153], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.910738079568546e-07\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0159, 0.0149, 0.0161, 0.0154], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0143, 0.0165, 0.0164, 0.0154, 0.0164, 0.0159, 0.0153, 0.0180, 0.0148,\n",
      "        0.0134], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0144, 0.0148, 0.0153, 0.0148, 0.0153, 0.0152, 0.0143, 0.0162, 0.0170,\n",
      "        0.0153], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.9100493773294147e-06\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0156, 0.0146, 0.0160, 0.0156], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0169, 0.0144, 0.0126, 0.0153, 0.0156, 0.0173, 0.0153, 0.0157, 0.0178,\n",
      "        0.0157], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0142, 0.0146, 0.0142, 0.0146, 0.0159, 0.0165, 0.0146, 0.0142, 0.0161,\n",
      "        0.0142], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.962192982318811e-06\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0150, 0.0144, 0.0158, 0.0161], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0164, 0.0172, 0.0150, 0.0173, 0.0173, 0.0164, 0.0166, 0.0164, 0.0137,\n",
      "        0.0121], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0147, 0.0155, 0.0147, 0.0147, 0.0147, 0.0147, 0.0149, 0.0147, 0.0163,\n",
      "        0.0156], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.556225121632451e-06\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0148, 0.0141, 0.0130, 0.0169], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0154, 0.0147, 0.0167, 0.0147, 0.0171, 0.0156, 0.0125, 0.0181, 0.0141,\n",
      "        0.0167], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0154, 0.0148, 0.0151, 0.0148, 0.0134, 0.0144, 0.0134, 0.0163, 0.0148,\n",
      "        0.0151], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.5505139547021827e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0152, 0.0126, 0.0145, 0.0164], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0167, 0.0166, 0.0168, 0.0145, 0.0135, 0.0167, 0.0130, 0.0134, 0.0145,\n",
      "        0.0127], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0150, 0.0149, 0.0134, 0.0147, 0.0157, 0.0150, 0.0151, 0.0141, 0.0147,\n",
      "        0.0127], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.0268643058661837e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0150, 0.0133, 0.0146, 0.0155], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0140, 0.0142, 0.0138, 0.0146, 0.0146, 0.0146, 0.0146, 0.0160, 0.0141,\n",
      "        0.0129], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0139, 0.0144, 0.0141, 0.0140, 0.0140, 0.0140, 0.0140, 0.0144, 0.0140,\n",
      "        0.0143], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.27340398295928e-07\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0149, 0.0138, 0.0143, 0.0151], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0143, 0.0132, 0.0139, 0.0155, 0.0162, 0.0155, 0.0155, 0.0119, 0.0155,\n",
      "        0.0190], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0136, 0.0146, 0.0140, 0.0140, 0.0146, 0.0140, 0.0140, 0.0124, 0.0140,\n",
      "        0.0171], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.8717507828114321e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0146, 0.0143, 0.0141, 0.0148], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0138, 0.0169, 0.0138, 0.0169, 0.0141, 0.0141, 0.0150, 0.0141, 0.0146,\n",
      "        0.0150], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0148, 0.0175, 0.0148, 0.0175, 0.0133, 0.0133, 0.0135, 0.0133, 0.0146,\n",
      "        0.0135], grad_fn=<AddBackward0>)\n",
      "LOSS: 9.200849717672099e-07\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0147, 0.0147, 0.0148, 0.0148, 0.0146, 0.0131, 0.0149, 0.0137, 0.0131,\n",
      "        0.0143], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0133, 0.0133, 0.0178, 0.0178, 0.0139, 0.0134, 0.0134, 0.0134, 0.0134,\n",
      "        0.0134], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.645713720994536e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0141, 0.0147, 0.0137, 0.0148], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0147, 0.0133, 0.0130, 0.0135, 0.0128, 0.0137, 0.0137, 0.0138, 0.0148,\n",
      "        0.0146], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0147, 0.0147, 0.0138, 0.0131, 0.0135, 0.0133, 0.0133, 0.0131, 0.0133,\n",
      "        0.0135], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.400179811156704e-07\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0139, 0.0146, 0.0137, 0.0148], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0190, 0.0146, 0.0144, 0.0137, 0.0137, 0.0128, 0.0150, 0.0190, 0.0135,\n",
      "        0.0167], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0171, 0.0150, 0.0135, 0.0134, 0.0134, 0.0150, 0.0171, 0.0171, 0.0131,\n",
      "        0.0159], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.8345181160839275e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0137, 0.0142, 0.0141, 0.0149], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0182, 0.0141, 0.0149, 0.0151, 0.0131, 0.0149, 0.0141, 0.0142, 0.0131,\n",
      "        0.0136], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0164, 0.0134, 0.0134, 0.0146, 0.0129, 0.0134, 0.0134, 0.0128, 0.0129,\n",
      "        0.0159], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.6600070011918433e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0140, 0.0156, 0.0123, 0.0142, 0.0142, 0.0173, 0.0123, 0.0170, 0.0142,\n",
      "        0.0130], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0130, 0.0140, 0.0124, 0.0136, 0.0140, 0.0157, 0.0130, 0.0153, 0.0136,\n",
      "        0.0126], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.068112169377855e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0140, 0.0131, 0.0141, 0.0154], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0125, 0.0134, 0.0117, 0.0140, 0.0140, 0.0127, 0.0122, 0.0139, 0.0122,\n",
      "        0.0172], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0125, 0.0123, 0.0139, 0.0127, 0.0131, 0.0131, 0.0127, 0.0119, 0.0127,\n",
      "        0.0150], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.7767004010238452e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0140, 0.0126, 0.0139, 0.0155], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0138, 0.0146, 0.0114, 0.0163, 0.0145, 0.0152, 0.0146, 0.0141, 0.0141,\n",
      "        0.0135], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0127, 0.0131, 0.0130, 0.0146, 0.0140, 0.0137, 0.0155, 0.0130, 0.0121,\n",
      "        0.0131], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.7133667142843478e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0141, 0.0124, 0.0134, 0.0157], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0141, 0.0116, 0.0153, 0.0129, 0.0161, 0.0141, 0.0137, 0.0134, 0.0131,\n",
      "        0.0137], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0141, 0.0141, 0.0145, 0.0123, 0.0152, 0.0129, 0.0123, 0.0141, 0.0123,\n",
      "        0.0123], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.4797374205954839e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0138, 0.0123, 0.0130, 0.0156], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0160, 0.0128, 0.0135, 0.0130, 0.0135, 0.0130, 0.0156, 0.0107, 0.0135,\n",
      "        0.0131], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0144, 0.0121, 0.0123, 0.0141, 0.0125, 0.0141, 0.0141, 0.0120, 0.0123,\n",
      "        0.0118], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.5187681583483936e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0132, 0.0124, 0.0131, 0.0150], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0132, 0.0151, 0.0160, 0.0120, 0.0139, 0.0160, 0.0109, 0.0150, 0.0145,\n",
      "        0.0138], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0129, 0.0144, 0.0144, 0.0125, 0.0122, 0.0144, 0.0122, 0.0135, 0.0132,\n",
      "        0.0148], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.5693198065491742e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0130, 0.0123, 0.0131, 0.0146], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0131, 0.0128, 0.0114, 0.0146, 0.0131, 0.0115, 0.0117, 0.0131, 0.0121,\n",
      "        0.0129], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0131, 0.0121, 0.0125, 0.0131, 0.0131, 0.0131, 0.0152, 0.0131, 0.0122,\n",
      "        0.0127], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.891111423901748e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0131, 0.0108, 0.0146, 0.0121, 0.0138, 0.0143, 0.0143, 0.0108, 0.0116,\n",
      "        0.0135], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0129, 0.0123, 0.0131, 0.0123, 0.0124, 0.0129, 0.0129, 0.0123, 0.0123,\n",
      "        0.0124], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.3712988220504485e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0130, 0.0113, 0.0130, 0.0144], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0110, 0.0136, 0.0144, 0.0145, 0.0122, 0.0144, 0.0122, 0.0117, 0.0122,\n",
      "        0.0124], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0126, 0.0126, 0.0130, 0.0131, 0.0122, 0.0130, 0.0122, 0.0122, 0.0122,\n",
      "        0.0119], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.0362576858824468e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0130, 0.0107, 0.0127, 0.0147], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0126, 0.0120, 0.0117, 0.0108, 0.0147, 0.0116, 0.0147, 0.0127, 0.0122,\n",
      "        0.0116], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0115, 0.0119, 0.0112, 0.0113, 0.0132, 0.0119, 0.0132, 0.0132, 0.0112,\n",
      "        0.0114], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.714444905104756e-07\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0147, 0.0140, 0.0116, 0.0119, 0.0138, 0.0122, 0.0137, 0.0125, 0.0116,\n",
      "        0.0127], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0132, 0.0126, 0.0117, 0.0114, 0.0147, 0.0109, 0.0111, 0.0117, 0.0117,\n",
      "        0.0147], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.8431198895996204e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0129, 0.0105, 0.0121, 0.0142], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0132, 0.0121, 0.0129, 0.0121, 0.0102, 0.0126, 0.0116, 0.0140, 0.0142,\n",
      "        0.0142], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0128, 0.0113, 0.0111, 0.0128, 0.0112, 0.0114, 0.0128, 0.0128, 0.0128,\n",
      "        0.0128], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.3904360685046413e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0128, 0.0109, 0.0117, 0.0136], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0116, 0.0124, 0.0135, 0.0118, 0.0099, 0.0116, 0.0143, 0.0116, 0.0109,\n",
      "        0.0128], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0113, 0.0116, 0.0132, 0.0112, 0.0112, 0.0113, 0.0134, 0.0113, 0.0115,\n",
      "        0.0115], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.491905312737799e-07\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0123, 0.0115, 0.0112, 0.0132], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0112, 0.0139, 0.0112, 0.0132, 0.0120, 0.0108, 0.0128, 0.0112, 0.0112,\n",
      "        0.0124], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0118, 0.0139, 0.0118, 0.0127, 0.0124, 0.0113, 0.0127, 0.0118, 0.0118,\n",
      "        0.0118], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.459953236666479e-07\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0118, 0.0119, 0.0109, 0.0128], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0109, 0.0128, 0.0109, 0.0156, 0.0128, 0.0109, 0.0134, 0.0137, 0.0118,\n",
      "        0.0111], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0115, 0.0115, 0.0115, 0.0140, 0.0115, 0.0121, 0.0109, 0.0143, 0.0115,\n",
      "        0.0121], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.599128495399782e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0116, 0.0120, 0.0107, 0.0124], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0110, 0.0110, 0.0157, 0.0107, 0.0112, 0.0090, 0.0107, 0.0110, 0.0110,\n",
      "        0.0090], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0120, 0.0120, 0.0141, 0.0112, 0.0119, 0.0121, 0.0112, 0.0129, 0.0120,\n",
      "        0.0121], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.0233866255002795e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0118, 0.0112, 0.0114, 0.0122], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0144, 0.0112, 0.0109, 0.0122, 0.0111, 0.0102, 0.0122, 0.0127, 0.0112,\n",
      "        0.0116], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0135, 0.0112, 0.0110, 0.0109, 0.0110, 0.0110, 0.0104, 0.0125, 0.0110,\n",
      "        0.0110], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.785434152334346e-07\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0120, 0.0104, 0.0120, 0.0120], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0106, 0.0111, 0.0120, 0.0104, 0.0114, 0.0143, 0.0113, 0.0132, 0.0120,\n",
      "        0.0120], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0128, 0.0108, 0.0117, 0.0104, 0.0119, 0.0129, 0.0107, 0.0129, 0.0108,\n",
      "        0.0108], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.044459509103035e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0121, 0.0095, 0.0124, 0.0122], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0116, 0.0116, 0.0124, 0.0124, 0.0122, 0.0122, 0.0103, 0.0116, 0.0122,\n",
      "        0.0101], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0110, 0.0111, 0.0112, 0.0112, 0.0112, 0.0112, 0.0122, 0.0111, 0.0136,\n",
      "        0.0110], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.240219603459991e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0120, 0.0090, 0.0129, 0.0119], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0129, 0.0119, 0.0119, 0.0116, 0.0120, 0.0129, 0.0115, 0.0129, 0.0105,\n",
      "        0.0090], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0116, 0.0116, 0.0116, 0.0122, 0.0122, 0.0116, 0.0110, 0.0116, 0.0108,\n",
      "        0.0106], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.24813639610511e-07\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0118, 0.0089, 0.0133, 0.0115], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0133, 0.0075, 0.0115, 0.0114, 0.0118, 0.0104, 0.0130, 0.0130, 0.0117,\n",
      "        0.0094], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0120, 0.0116, 0.0120, 0.0116, 0.0125, 0.0109, 0.0131, 0.0115, 0.0141,\n",
      "        0.0128], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.9252981878235005e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0119, 0.0097, 0.0126, 0.0114], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0123, 0.0126, 0.0114, 0.0114, 0.0126, 0.0136, 0.0137, 0.0116, 0.0117,\n",
      "        0.0125], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0109, 0.0113, 0.0109, 0.0109, 0.0113, 0.0132, 0.0132, 0.0109, 0.0118,\n",
      "        0.0132], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.256086860252253e-07\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0118, 0.0104, 0.0121, 0.0111], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0116, 0.0142, 0.0113, 0.0143, 0.0118, 0.0113, 0.0112, 0.0134, 0.0096,\n",
      "        0.0113], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0114, 0.0128, 0.0104, 0.0132, 0.0106, 0.0104, 0.0105, 0.0126, 0.0105,\n",
      "        0.0104], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.864167284627911e-07\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0135, 0.0124, 0.0125, 0.0141, 0.0119, 0.0147, 0.0107, 0.0105, 0.0107,\n",
      "        0.0107], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0103, 0.0127, 0.0108, 0.0122, 0.0103, 0.0122, 0.0103, 0.0108, 0.0103,\n",
      "        0.0103], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.5435360839765053e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0117, 0.0111, 0.0123, 0.0100], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0146, 0.0103, 0.0083, 0.0103, 0.0123, 0.0109, 0.0102, 0.0100, 0.0117,\n",
      "        0.0102], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0124, 0.0119, 0.0127, 0.0105, 0.0110, 0.0113, 0.0127, 0.0110, 0.0106,\n",
      "        0.0105], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.69948475054116e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0112, 0.0107, 0.0124, 0.0107], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0107, 0.0112, 0.0124, 0.0109, 0.0104, 0.0099, 0.0116, 0.0095, 0.0106,\n",
      "        0.0124], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0103, 0.0101, 0.0112, 0.0106, 0.0101, 0.0113, 0.0114, 0.0104, 0.0106,\n",
      "        0.0121], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.234326406229229e-07\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0107, 0.0103, 0.0125, 0.0113], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0123, 0.0089, 0.0104, 0.0125, 0.0135, 0.0111, 0.0111, 0.0135, 0.0103,\n",
      "        0.0122], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0108, 0.0104, 0.0108, 0.0112, 0.0122, 0.0107, 0.0107, 0.0122, 0.0108,\n",
      "        0.0114], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.1063800684496528e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0103, 0.0102, 0.0124, 0.0118], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0118, 0.0107, 0.0114, 0.0141, 0.0131, 0.0116, 0.0141, 0.0115, 0.0131,\n",
      "        0.0121], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0112, 0.0113, 0.0106, 0.0118, 0.0118, 0.0105, 0.0118, 0.0102, 0.0118,\n",
      "        0.0113], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.8532282410888001e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0100, 0.0104, 0.0119, 0.0120], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0120, 0.0120, 0.0126, 0.0091, 0.0117, 0.0114, 0.0114, 0.0134, 0.0091,\n",
      "        0.0114], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0108, 0.0108, 0.0121, 0.0103, 0.0105, 0.0103, 0.0103, 0.0120, 0.0103,\n",
      "        0.0115], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.1895712077603093e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0099, 0.0107, 0.0114, 0.0118], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0103, 0.0114, 0.0117, 0.0109, 0.0099, 0.0110, 0.0138, 0.0114, 0.0131,\n",
      "        0.0107], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0108, 0.0106, 0.0106, 0.0114, 0.0114, 0.0115, 0.0124, 0.0106, 0.0114,\n",
      "        0.0115], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.0747464784799377e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0102, 0.0112, 0.0108, 0.0117], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0117, 0.0106, 0.0133, 0.0092, 0.0117, 0.0123, 0.0106, 0.0106, 0.0117,\n",
      "        0.0103], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0105, 0.0096, 0.0120, 0.0096, 0.0105, 0.0111, 0.0096, 0.0096, 0.0111,\n",
      "        0.0113], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.0798062248795759e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0105, 0.0117, 0.0103, 0.0113], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0103, 0.0111, 0.0098, 0.0120, 0.0106, 0.0118, 0.0100, 0.0098, 0.0118,\n",
      "        0.0100], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0105, 0.0102, 0.0102, 0.0120, 0.0119, 0.0106, 0.0090, 0.0102, 0.0122,\n",
      "        0.0090], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.596636694666813e-07\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0109, 0.0123, 0.0086, 0.0113], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0090, 0.0109, 0.0101, 0.0101, 0.0112, 0.0092, 0.0101, 0.0121, 0.0106,\n",
      "        0.0106], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0091, 0.0135, 0.0108, 0.0108, 0.0107, 0.0123, 0.0108, 0.0110, 0.0108,\n",
      "        0.0108], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.9062911178480135e-06\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0101, 0.0131, 0.0098, 0.0095], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0103, 0.0081, 0.0107, 0.0106, 0.0102, 0.0145, 0.0106, 0.0116, 0.0103,\n",
      "        0.0102], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0100, 0.0095, 0.0104, 0.0103, 0.0095, 0.0131, 0.0117, 0.0105, 0.0108,\n",
      "        0.0104], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.34516845568578e-07\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0100, 0.0129, 0.0120, 0.0109], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0114, 0.0113, 0.0089, 0.0119, 0.0103, 0.0076, 0.0116, 0.0099, 0.0109,\n",
      "        0.0129], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0112, 0.0104, 0.0112, 0.0126, 0.0112, 0.0100, 0.0112, 0.0104, 0.0112,\n",
      "        0.0116], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.5291141153284116e-06\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0101, 0.0122, 0.0123, 0.0112], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0115, 0.0115, 0.0077, 0.0115, 0.0096, 0.0114, 0.0114, 0.0099, 0.0077,\n",
      "        0.0115], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0105, 0.0105, 0.0103, 0.0105, 0.0108, 0.0103, 0.0103, 0.0103, 0.0103,\n",
      "        0.0105], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.1333980839699507e-06\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0128, 0.0127, 0.0116, 0.0120], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0099, 0.0116, 0.0120, 0.0119, 0.0116, 0.0087, 0.0109, 0.0120, 0.0107,\n",
      "        0.0109], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0097, 0.0115, 0.0102, 0.0098, 0.0115, 0.0098, 0.0115, 0.0108, 0.0112,\n",
      "        0.0098], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.2060745575581677e-06\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0108, 0.0110, 0.0110, 0.0129], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0110, 0.0075, 0.0111, 0.0115, 0.0130, 0.0099, 0.0099, 0.0084, 0.0099,\n",
      "        0.0089], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0117, 0.0103, 0.0109, 0.0101, 0.0116, 0.0115, 0.0115, 0.0110, 0.0115,\n",
      "        0.0118], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.4487027278373716e-06\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0100, 0.0103, 0.0094, 0.0115], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0108, 0.0080, 0.0102, 0.0110, 0.0115, 0.0102, 0.0102, 0.0110, 0.0101,\n",
      "        0.0080], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0099, 0.0104, 0.0094, 0.0099, 0.0094, 0.0094, 0.0094, 0.0099, 0.0114,\n",
      "        0.0104], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.2060189621697646e-06\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0106, 0.0110, 0.0090, 0.0114], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0120, 0.0120, 0.0131, 0.0113, 0.0106, 0.0095, 0.0095, 0.0099, 0.0106,\n",
      "        0.0110], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0108, 0.0108, 0.0118, 0.0118, 0.0094, 0.0094, 0.0094, 0.0103, 0.0094,\n",
      "        0.0100], grad_fn=<AddBackward0>)\n",
      "LOSS: 9.162557716990705e-07\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0110, 0.0106, 0.0108, 0.0114], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0126, 0.0103, 0.0090, 0.0090, 0.0102, 0.0111, 0.0107, 0.0102, 0.0114,\n",
      "        0.0080], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0113, 0.0096, 0.0096, 0.0096, 0.0100, 0.0100, 0.0096, 0.0100, 0.0103,\n",
      "        0.0100], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.0887534926951048e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0107, 0.0109, 0.0110, 0.0111], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0119, 0.0088, 0.0108, 0.0104, 0.0114, 0.0096, 0.0094, 0.0099, 0.0088,\n",
      "        0.0119], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0125, 0.0098, 0.0100, 0.0103, 0.0103, 0.0098, 0.0109, 0.0098, 0.0098,\n",
      "        0.0106], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.112806426652241e-07\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0106, 0.0110, 0.0109, 0.0112], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0119, 0.0099, 0.0109, 0.0109, 0.0089, 0.0106, 0.0105, 0.0106, 0.0118,\n",
      "        0.0106], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0107, 0.0099, 0.0101, 0.0101, 0.0100, 0.0099, 0.0101, 0.0099, 0.0126,\n",
      "        0.0099], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.206488478710526e-07\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0094, 0.0139, 0.0077, 0.0091, 0.0110, 0.0128, 0.0091, 0.0108, 0.0094,\n",
      "        0.0094], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0102, 0.0125, 0.0102, 0.0102, 0.0101, 0.0120, 0.0102, 0.0110, 0.0102,\n",
      "        0.0102], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.3991350442665862e-06\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0092, 0.0107, 0.0101, 0.0108], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0098, 0.0107, 0.0091, 0.0105, 0.0104, 0.0112, 0.0110, 0.0096, 0.0105,\n",
      "        0.0097], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0129, 0.0100, 0.0105, 0.0098, 0.0113, 0.0104, 0.0103, 0.0104, 0.0098,\n",
      "        0.0108], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.7308132100879448e-06\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0098, 0.0102, 0.0101, 0.0110], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0101, 0.0128, 0.0105, 0.0127, 0.0110, 0.0127, 0.0116, 0.0098, 0.0117,\n",
      "        0.0107], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0115, 0.0115, 0.0115, 0.0124, 0.0130, 0.0099, 0.0115, 0.0105, 0.0105,\n",
      "        0.0099], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.95607253772323e-06\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0104, 0.0099, 0.0104, 0.0110], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0102, 0.0080, 0.0124, 0.0109, 0.0124, 0.0114, 0.0110, 0.0104, 0.0124,\n",
      "        0.0104], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0105, 0.0111, 0.0111, 0.0104, 0.0111, 0.0090, 0.0099, 0.0111, 0.0111,\n",
      "        0.0111], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.157388280465966e-06\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0102, 0.0098, 0.0110, 0.0113], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0111, 0.0110, 0.0115, 0.0118, 0.0112, 0.0115, 0.0095, 0.0102, 0.0120,\n",
      "        0.0116], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0109, 0.0108, 0.0109, 0.0092, 0.0108, 0.0092, 0.0089, 0.0109, 0.0108,\n",
      "        0.0104], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.6756819150032243e-06\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0097, 0.0102, 0.0114, 0.0112], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0115, 0.0114, 0.0094, 0.0109, 0.0109, 0.0109, 0.0113, 0.0109, 0.0111,\n",
      "        0.0109], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0104, 0.0104, 0.0099, 0.0104, 0.0118, 0.0104, 0.0097, 0.0104, 0.0102,\n",
      "        0.0104], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.496262810491316e-07\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0090, 0.0101, 0.0119, 0.0101], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0119, 0.0101, 0.0101, 0.0100, 0.0095, 0.0100, 0.0071, 0.0100, 0.0103,\n",
      "        0.0119], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0107, 0.0107, 0.0106, 0.0102, 0.0102, 0.0102, 0.0102, 0.0115, 0.0107,\n",
      "        0.0107], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.6366377622034634e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0089, 0.0106, 0.0115, 0.0096], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0107, 0.0096, 0.0095, 0.0111, 0.0096, 0.0090, 0.0124, 0.0096, 0.0098,\n",
      "        0.0101], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0113, 0.0103, 0.0111, 0.0117, 0.0103, 0.0100, 0.0105, 0.0103, 0.0103,\n",
      "        0.0110], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.0128599114977987e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0090, 0.0107, 0.0108, 0.0097], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0096, 0.0099, 0.0120, 0.0076, 0.0108, 0.0091, 0.0106, 0.0112, 0.0115,\n",
      "        0.0121], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0117, 0.0112, 0.0108, 0.0106, 0.0117, 0.0113, 0.0119, 0.0119, 0.0124,\n",
      "        0.0124], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.5011920570250368e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0093, 0.0101, 0.0104, 0.0105], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0104, 0.0091, 0.0101, 0.0105, 0.0109, 0.0105, 0.0109, 0.0109, 0.0131,\n",
      "        0.0115], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0094, 0.0104, 0.0112, 0.0094, 0.0108, 0.0114, 0.0095, 0.0108, 0.0118,\n",
      "        0.0107], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.0059989108412992e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0097, 0.0093, 0.0097, 0.0115], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0115, 0.0097, 0.0120, 0.0097, 0.0120, 0.0115, 0.0110, 0.0120, 0.0120,\n",
      "        0.0120], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0116, 0.0104, 0.0112, 0.0116, 0.0112, 0.0104, 0.0104, 0.0112, 0.0112,\n",
      "        0.0112], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.434045639660326e-07\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0107, 0.0086, 0.0092, 0.0119], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0120, 0.0133, 0.0135, 0.0092, 0.0099, 0.0135, 0.0100, 0.0098, 0.0110,\n",
      "        0.0123], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0108, 0.0120, 0.0121, 0.0107, 0.0106, 0.0121, 0.0117, 0.0107, 0.0112,\n",
      "        0.0121], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.3301066701387754e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0115, 0.0084, 0.0091, 0.0116], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0143, 0.0106, 0.0091, 0.0143, 0.0083, 0.0143, 0.0091, 0.0091, 0.0126,\n",
      "        0.0091], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0129, 0.0129, 0.0104, 0.0129, 0.0118, 0.0129, 0.0104, 0.0104, 0.0114,\n",
      "        0.0104], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.272999720138614e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0100, 0.0103, 0.0103, 0.0121, 0.0149, 0.0108, 0.0088, 0.0109, 0.0108,\n",
      "        0.0108], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0108, 0.0108, 0.0108, 0.0109, 0.0134, 0.0129, 0.0118, 0.0108, 0.0134,\n",
      "        0.0134], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.242002321712789e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0111, 0.0084, 0.0118, 0.0094], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0094, 0.0118, 0.0114, 0.0118, 0.0118, 0.0096, 0.0091, 0.0118, 0.0109,\n",
      "        0.0094], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0106, 0.0106, 0.0108, 0.0106, 0.0106, 0.0110, 0.0104, 0.0103, 0.0107,\n",
      "        0.0106], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.3578070365838357e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0105, 0.0085, 0.0125, 0.0093], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0125, 0.0115, 0.0133, 0.0113, 0.0125, 0.0125, 0.0110, 0.0093, 0.0115,\n",
      "        0.0085], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0113, 0.0116, 0.0120, 0.0102, 0.0113, 0.0113, 0.0123, 0.0113, 0.0116,\n",
      "        0.0099], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.516971906312392e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0098, 0.0089, 0.0128, 0.0095], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0098, 0.0085, 0.0094, 0.0108, 0.0128, 0.0107, 0.0128, 0.0111, 0.0094,\n",
      "        0.0113], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0125, 0.0098, 0.0097, 0.0097, 0.0115, 0.0093, 0.0115, 0.0113, 0.0118,\n",
      "        0.0125], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.33048331210739e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0093, 0.0097, 0.0123, 0.0100], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0123, 0.0126, 0.0116, 0.0123, 0.0112, 0.0119, 0.0112, 0.0114, 0.0101,\n",
      "        0.0119], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0111, 0.0120, 0.0107, 0.0111, 0.0107, 0.0107, 0.0113, 0.0107, 0.0105,\n",
      "        0.0114], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.948563964215282e-07\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0089, 0.0104, 0.0118, 0.0103], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0108, 0.0099, 0.0108, 0.0102, 0.0118, 0.0103, 0.0118, 0.0120, 0.0099,\n",
      "        0.0099], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0108, 0.0107, 0.0097, 0.0108, 0.0106, 0.0106, 0.0108, 0.0108, 0.0108,\n",
      "        0.0107], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.512354045502434e-07\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0086, 0.0109, 0.0113, 0.0106], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0106, 0.0087, 0.0086, 0.0112, 0.0106, 0.0131, 0.0119, 0.0106, 0.0121,\n",
      "        0.0106], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0102, 0.0111, 0.0111, 0.0112, 0.0102, 0.0120, 0.0111, 0.0102, 0.0120,\n",
      "        0.0111], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.4233664842322469e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0091, 0.0109, 0.0110, 0.0109], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0115, 0.0120, 0.0109, 0.0102, 0.0115, 0.0110, 0.0120, 0.0120, 0.0102,\n",
      "        0.0109], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0113, 0.0113, 0.0099, 0.0113, 0.0108, 0.0110, 0.0113, 0.0110, 0.0113,\n",
      "        0.0099], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.843127948741312e-07\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0094, 0.0112, 0.0113, 0.0105], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0105, 0.0105, 0.0113, 0.0107, 0.0089, 0.0112, 0.0114, 0.0118, 0.0112,\n",
      "        0.0109], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0101, 0.0101, 0.0101, 0.0117, 0.0103, 0.0104, 0.0112, 0.0121, 0.0117,\n",
      "        0.0112], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.583950724030728e-07\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0095, 0.0114, 0.0118, 0.0100], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0109, 0.0120, 0.0100, 0.0114, 0.0119, 0.0104, 0.0118, 0.0108, 0.0116,\n",
      "        0.0120], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0112, 0.0108, 0.0110, 0.0103, 0.0119, 0.0110, 0.0106, 0.0117, 0.0106,\n",
      "        0.0108], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.647336358080793e-07\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0099, 0.0114, 0.0123, 0.0095], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0116, 0.0110, 0.0123, 0.0112, 0.0114, 0.0123, 0.0113, 0.0112, 0.0116,\n",
      "        0.0116], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0108, 0.0103, 0.0110, 0.0110, 0.0101, 0.0110, 0.0108, 0.0101, 0.0108,\n",
      "        0.0108], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.638407962280326e-07\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0104, 0.0115, 0.0126, 0.0088], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0099, 0.0110, 0.0110, 0.0127, 0.0126, 0.0117, 0.0096, 0.0119, 0.0124,\n",
      "        0.0126], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0105, 0.0113, 0.0113, 0.0116, 0.0113, 0.0096, 0.0111, 0.0098, 0.0116,\n",
      "        0.0113], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.677419277257286e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0107, 0.0116, 0.0125, 0.0084], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0084, 0.0106, 0.0107, 0.0121, 0.0125, 0.0098, 0.0125, 0.0111, 0.0092,\n",
      "        0.0126], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0112, 0.0115, 0.0110, 0.0114, 0.0112, 0.0114, 0.0112, 0.0100, 0.0110,\n",
      "        0.0117], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.0332299754954875e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0110, 0.0114, 0.0119, 0.0089], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0104, 0.0119, 0.0131, 0.0130, 0.0108, 0.0119, 0.0109, 0.0104, 0.0098,\n",
      "        0.0127], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0107, 0.0107, 0.0118, 0.0117, 0.0109, 0.0107, 0.0108, 0.0107, 0.0108,\n",
      "        0.0115], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.741662895772606e-07\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0113, 0.0110, 0.0113, 0.0095], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0116, 0.0105, 0.0116, 0.0113, 0.0113, 0.0133, 0.0113, 0.0095, 0.0095,\n",
      "        0.0113], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0115, 0.0102, 0.0119, 0.0102, 0.0102, 0.0120, 0.0102, 0.0102, 0.0102,\n",
      "        0.0102], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.957763727972633e-07\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0125, 0.0106, 0.0094, 0.0112], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0126, 0.0111, 0.0111, 0.0104, 0.0119, 0.0104, 0.0138, 0.0112, 0.0076,\n",
      "        0.0116], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0113, 0.0113, 0.0113, 0.0109, 0.0123, 0.0109, 0.0124, 0.0113, 0.0106,\n",
      "        0.0113], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.3000602621104917e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0119, 0.0103, 0.0121, 0.0126, 0.0128, 0.0128, 0.0095, 0.0099, 0.0095,\n",
      "        0.0128], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0113, 0.0114, 0.0110, 0.0111, 0.0124, 0.0124, 0.0111, 0.0105, 0.0122,\n",
      "        0.0124], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.54054782797175e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0137, 0.0098, 0.0079, 0.0128], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0117, 0.0095, 0.0115, 0.0092, 0.0092, 0.0092, 0.0079, 0.0119, 0.0092,\n",
      "        0.0128], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0122, 0.0128, 0.0128, 0.0110, 0.0110, 0.0110, 0.0111, 0.0113, 0.0110,\n",
      "        0.0123], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.72526005776308e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0124, 0.0104, 0.0093, 0.0124], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0112, 0.0124, 0.0109, 0.0122, 0.0124, 0.0081, 0.0121, 0.0124, 0.0117,\n",
      "        0.0107], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0116, 0.0112, 0.0112, 0.0105, 0.0112, 0.0110, 0.0121, 0.0112, 0.0105,\n",
      "        0.0105], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.7665040559222689e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([0.0114, 0.0116, 0.0122, 0.0079, 0.0103, 0.0079, 0.0122, 0.0114, 0.0115,\n",
      "        0.0096], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0104, 0.0104, 0.0109, 0.0102, 0.0109, 0.0102, 0.0109, 0.0104, 0.0104,\n",
      "        0.0109], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.062534349533962e-06\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([0.0090, 0.0105, 0.0106, 0.0115], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0123, 0.0127, 0.0091, 0.0111, 0.0111, 0.0111, 0.0110, 0.0092, 0.0121,\n",
      "        0.0091], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0112, 0.0114, 0.0108, 0.0108, 0.0100, 0.0108, 0.0108, 0.0112, 0.0110,\n",
      "        0.0108], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.5547059319942491e-06\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0113, 0.0107, 0.0127, 0.0106], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0113, 0.0102, 0.0095, 0.0098, 0.0082, 0.0119, 0.0095, 0.0107, 0.0098,\n",
      "        0.0095], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0114, 0.0118, 0.0105, 0.0107, 0.0105, 0.0118, 0.0127, 0.0102, 0.0107,\n",
      "        0.0127], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.161825588904321e-06\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0104, 0.0095, 0.0122, 0.0111], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0124, 0.0134, 0.0111, 0.0091, 0.0104, 0.0109, 0.0134, 0.0111, 0.0131,\n",
      "        0.0110], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0111, 0.0120, 0.0110, 0.0101, 0.0120, 0.0101, 0.0120, 0.0115, 0.0127,\n",
      "        0.0111], grad_fn=<AddBackward0>)\n",
      "LOSS: 9.730608780955663e-07\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0105, 0.0100, 0.0124, 0.0114], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0114, 0.0117, 0.0118, 0.0114, 0.0113, 0.0115, 0.0116, 0.0124, 0.0120,\n",
      "        0.0100], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0112, 0.0115, 0.0105, 0.0120, 0.0115, 0.0120, 0.0120, 0.0112, 0.0112,\n",
      "        0.0106], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.17546254741319e-07\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0112, 0.0097, 0.0117, 0.0123], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0125, 0.0119, 0.0113, 0.0122, 0.0117, 0.0125, 0.0108, 0.0113, 0.0110,\n",
      "        0.0122], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0112, 0.0126, 0.0125, 0.0126, 0.0111, 0.0112, 0.0111, 0.0125, 0.0121,\n",
      "        0.0126], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.513664511156094e-07\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0113, 0.0093, 0.0112, 0.0134], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0100, 0.0131, 0.0134, 0.0087, 0.0112, 0.0125, 0.0096, 0.0118, 0.0125,\n",
      "        0.0093], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0118, 0.0111, 0.0121, 0.0106, 0.0121, 0.0128, 0.0118, 0.0106, 0.0128,\n",
      "        0.0117], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.6045861432066886e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0113, 0.0099, 0.0111, 0.0135], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0114, 0.0124, 0.0124, 0.0114, 0.0111, 0.0119, 0.0124, 0.0135, 0.0114,\n",
      "        0.0111], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0122, 0.0127, 0.0127, 0.0122, 0.0121, 0.0116, 0.0127, 0.0121, 0.0122,\n",
      "        0.0121], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.028930101820151e-07\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0110, 0.0107, 0.0114, 0.0132], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0132, 0.0116, 0.0108, 0.0120, 0.0114, 0.0122, 0.0122, 0.0095, 0.0126,\n",
      "        0.0108], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0119, 0.0120, 0.0111, 0.0111, 0.0119, 0.0124, 0.0124, 0.0104, 0.0114,\n",
      "        0.0111], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.664472837452195e-07\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0106, 0.0114, 0.0117, 0.0129], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0108, 0.0117, 0.0120, 0.0105, 0.0129, 0.0119, 0.0115, 0.0112, 0.0117,\n",
      "        0.0123], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0116, 0.0116, 0.0104, 0.0110, 0.0118, 0.0120, 0.0113, 0.0108, 0.0116,\n",
      "        0.0111], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.385224082805507e-07\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0101, 0.0122, 0.0121, 0.0124], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0108, 0.0121, 0.0118, 0.0110, 0.0118, 0.0129, 0.0107, 0.0122, 0.0114,\n",
      "        0.0120], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0125, 0.0112, 0.0107, 0.0099, 0.0107, 0.0116, 0.0108, 0.0119, 0.0116,\n",
      "        0.0108], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.0197625215369044e-06\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0098, 0.0127, 0.0123, 0.0121], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0111, 0.0104, 0.0094, 0.0113, 0.0117, 0.0111, 0.0142, 0.0118, 0.0123,\n",
      "        0.0141], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0115, 0.0123, 0.0120, 0.0120, 0.0112, 0.0109, 0.0127, 0.0115, 0.0115,\n",
      "        0.0127], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.6939209217525786e-06\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0101, 0.0136, 0.0109, 0.0105], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0120, 0.0124, 0.0143, 0.0109, 0.0126, 0.0119, 0.0097, 0.0129, 0.0119,\n",
      "        0.0136], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0116, 0.0116, 0.0129, 0.0116, 0.0116, 0.0112, 0.0120, 0.0117, 0.0112,\n",
      "        0.0112], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.7904955029734992e-06\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0118, 0.0127, 0.0118, 0.0101], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0127, 0.0102, 0.0125, 0.0110, 0.0117, 0.0124, 0.0124, 0.0156, 0.0107,\n",
      "        0.0119], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0140, 0.0112, 0.0116, 0.0110, 0.0118, 0.0116, 0.0116, 0.0140, 0.0117,\n",
      "        0.0127], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.358050536116934e-07\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0105, 0.0153, 0.0139, 0.0109], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0127, 0.0131, 0.0112, 0.0144, 0.0108, 0.0093, 0.0124, 0.0118, 0.0108,\n",
      "        0.0124], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0109, 0.0115, 0.0115, 0.0129, 0.0121, 0.0115, 0.0116, 0.0115, 0.0121,\n",
      "        0.0116], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.7033351014106302e-06\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0105, 0.0152, 0.0137, 0.0112], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0109, 0.0110, 0.0121, 0.0120, 0.0137, 0.0121, 0.0113, 0.0131, 0.0109,\n",
      "        0.0117], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0115, 0.0119, 0.0116, 0.0115, 0.0130, 0.0116, 0.0119, 0.0137, 0.0115,\n",
      "        0.0137], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.782899160702073e-07\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0112, 0.0121, 0.0121, 0.0093, 0.0128, 0.0121, 0.0121, 0.0128, 0.0128,\n",
      "        0.0112], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0120, 0.0120, 0.0120, 0.0106, 0.0112, 0.0109, 0.0119, 0.0116, 0.0116,\n",
      "        0.0120], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.0074214742417098e-06\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0119, 0.0141, 0.0121, 0.0127], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0107, 0.0121, 0.0127, 0.0127, 0.0105, 0.0121, 0.0105, 0.0130, 0.0104,\n",
      "        0.0107], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0121, 0.0127, 0.0117, 0.0117, 0.0110, 0.0127, 0.0110, 0.0117, 0.0109,\n",
      "        0.0121], grad_fn=<AddBackward0>)\n",
      "LOSS: 9.030703154166986e-07\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0120, 0.0138, 0.0127, 0.0129], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0119, 0.0119, 0.0107, 0.0119, 0.0128, 0.0128, 0.0128, 0.0116, 0.0098,\n",
      "        0.0120], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0119, 0.0119, 0.0108, 0.0119, 0.0116, 0.0116, 0.0116, 0.0117, 0.0106,\n",
      "        0.0116], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.114977739140159e-07\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0123, 0.0136, 0.0133, 0.0127], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0123, 0.0120, 0.0124, 0.0133, 0.0123, 0.0136, 0.0123, 0.0111, 0.0120,\n",
      "        0.0123], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0127, 0.0119, 0.0119, 0.0120, 0.0115, 0.0123, 0.0115, 0.0108, 0.0119,\n",
      "        0.0107], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.933587653496943e-07\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0127, 0.0135, 0.0136, 0.0126], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0135, 0.0116, 0.0120, 0.0109, 0.0103, 0.0120, 0.0107, 0.0124, 0.0124,\n",
      "        0.0114], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0122, 0.0098, 0.0122, 0.0110, 0.0112, 0.0122, 0.0110, 0.0112, 0.0112,\n",
      "        0.0104], grad_fn=<AddBackward0>)\n",
      "LOSS: 9.72473344518221e-07\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0129, 0.0133, 0.0138, 0.0125], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0100, 0.0122, 0.0123, 0.0133, 0.0123, 0.0119, 0.0117, 0.0123, 0.0100,\n",
      "        0.0116], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0105, 0.0111, 0.0111, 0.0124, 0.0111, 0.0110, 0.0124, 0.0111, 0.0111,\n",
      "        0.0124], grad_fn=<AddBackward0>)\n",
      "LOSS: 9.909932714435854e-07\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0131, 0.0121, 0.0126, 0.0120, 0.0121, 0.0103, 0.0120, 0.0116, 0.0121,\n",
      "        0.0103], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0125, 0.0109, 0.0113, 0.0109, 0.0109, 0.0110, 0.0109, 0.0125, 0.0105,\n",
      "        0.0110], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.1401956498957588e-06\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0122, 0.0112, 0.0109, 0.0052], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0128, 0.0122, 0.0114, 0.0139, 0.0128, 0.0119, 0.0119, 0.0130, 0.0121,\n",
      "        0.0121], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0128, 0.0110, 0.0111, 0.0125, 0.0126, 0.0127, 0.0111, 0.0127, 0.0109,\n",
      "        0.0109], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.86092982707487e-07\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0121, 0.0110, 0.0109, 0.0051], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0109, 0.0106, 0.0120, 0.0115, 0.0120, 0.0142, 0.0085, 0.0101, 0.0115,\n",
      "        0.0120], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0097, 0.0117, 0.0108, 0.0124, 0.0108, 0.0127, 0.0127, 0.0113, 0.0124,\n",
      "        0.0108], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.900729441535077e-06\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0112, 0.0107, 0.0109, 0.0056], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0101, 0.0135, 0.0121, 0.0106, 0.0099, 0.0121, 0.0120, 0.0106, 0.0107,\n",
      "        0.0118], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0121, 0.0117, 0.0116, 0.0117, 0.0111, 0.0116, 0.0113, 0.0107, 0.0129,\n",
      "        0.0111], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.6432644542874186e-06\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0103, 0.0112, 0.0108, 0.0058], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0119, 0.0101, 0.0117, 0.0127, 0.0123, 0.0094, 0.0107, 0.0109, 0.0110,\n",
      "        0.0119], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0107, 0.0113, 0.0107, 0.0119, 0.0111, 0.0113, 0.0098, 0.0099, 0.0107,\n",
      "        0.0113], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.1782910860347329e-06\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0112, 0.0128, 0.0137, 0.0140], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0110, 0.0122, 0.0122, 0.0089, 0.0122, 0.0128, 0.0128, 0.0126, 0.0120,\n",
      "        0.0126], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0110, 0.0110, 0.0110, 0.0108, 0.0110, 0.0113, 0.0113, 0.0113, 0.0110,\n",
      "        0.0113], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.650817694098805e-06\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0096, 0.0121, 0.0095, 0.0062], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0128, 0.0094, 0.0104, 0.0103, 0.0101, 0.0095, 0.0120, 0.0120, 0.0123,\n",
      "        0.0115], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0115, 0.0115, 0.0099, 0.0111, 0.0115, 0.0099, 0.0103, 0.0103, 0.0111,\n",
      "        0.0103], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.7574568573763827e-06\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0110, 0.0134, 0.0125, 0.0137], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0115, 0.0119, 0.0114, 0.0107, 0.0110, 0.0106, 0.0096, 0.0097, 0.0106,\n",
      "        0.0128], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0108, 0.0108, 0.0102, 0.0102, 0.0115, 0.0099, 0.0112, 0.0108, 0.0105,\n",
      "        0.0115], grad_fn=<AddBackward0>)\n",
      "LOSS: 9.125933502218686e-07\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0099, 0.0130, 0.0086, 0.0052], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0130, 0.0131, 0.0108, 0.0108, 0.0093, 0.0092, 0.0118, 0.0093, 0.0108,\n",
      "        0.0112], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0117, 0.0118, 0.0103, 0.0103, 0.0112, 0.0100, 0.0108, 0.0112, 0.0103,\n",
      "        0.0101], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.3787081343252794e-06\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0113, 0.0139, 0.0122, 0.0121], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0106, 0.0094, 0.0102, 0.0101, 0.0109, 0.0103, 0.0122, 0.0105, 0.0114,\n",
      "        0.0109], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0114, 0.0102, 0.0118, 0.0121, 0.0112, 0.0114, 0.0126, 0.0097, 0.0112,\n",
      "        0.0112], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.055567963703652e-06\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0117, 0.0137, 0.0124, 0.0117], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0120, 0.0137, 0.0111, 0.0100, 0.0101, 0.0124, 0.0111, 0.0137, 0.0103,\n",
      "        0.0108], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0123, 0.0123, 0.0104, 0.0111, 0.0103, 0.0123, 0.0104, 0.0123, 0.0099,\n",
      "        0.0116], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.704744350827241e-07\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0120, 0.0133, 0.0125, 0.0114], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([0.0103, 0.0133, 0.0086, 0.0101, 0.0110, 0.0133, 0.0103, 0.0099, 0.0103,\n",
      "        0.0099], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0108, 0.0119, 0.0100, 0.0100, 0.0117, 0.0119, 0.0108, 0.0106, 0.0108,\n",
      "        0.0106], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.975688163242012e-07\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0121, 0.0127, 0.0128, 0.0114], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0112, 0.0121, 0.0103, 0.0102, 0.0102, 0.0097, 0.0102, 0.0125, 0.0100,\n",
      "        0.0102], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0113, 0.0101, 0.0100, 0.0100, 0.0100, 0.0108, 0.0100, 0.0115, 0.0107,\n",
      "        0.0100], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.583164804396802e-07\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0103, 0.0107, 0.0118, 0.0107, 0.0104, 0.0096, 0.0107, 0.0104, 0.0111,\n",
      "        0.0108], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0106, 0.0096, 0.0096, 0.0108, 0.0105, 0.0103, 0.0096, 0.0096, 0.0100,\n",
      "        0.0101], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.0048690910480218e-06\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0115, 0.0111, 0.0104, 0.0045], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0134, 0.0120, 0.0112, 0.0134, 0.0113, 0.0093, 0.0103, 0.0121, 0.0110,\n",
      "        0.0093], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0123, 0.0118, 0.0100, 0.0123, 0.0123, 0.0113, 0.0093, 0.0100, 0.0099,\n",
      "        0.0113], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.9422184323047986e-06\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0110, 0.0106, 0.0110, 0.0047], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0117, 0.0103, 0.0103, 0.0112, 0.0089, 0.0103, 0.0100, 0.0109, 0.0139,\n",
      "        0.0097], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0099, 0.0114, 0.0088, 0.0105, 0.0099, 0.0088, 0.0105, 0.0128, 0.0128,\n",
      "        0.0105], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.5609563206453458e-06\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0104, 0.0102, 0.0111, 0.0052], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0120, 0.0076, 0.0113, 0.0125, 0.0098, 0.0091, 0.0098, 0.0116, 0.0113,\n",
      "        0.0093], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0108, 0.0106, 0.0108, 0.0108, 0.0109, 0.0112, 0.0109, 0.0112, 0.0108,\n",
      "        0.0106], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.2220006030693185e-06\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0106, 0.0101, 0.0105, 0.0054], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0139, 0.0116, 0.0093, 0.0116, 0.0116, 0.0116, 0.0108, 0.0101, 0.0116,\n",
      "        0.0106], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0125, 0.0105, 0.0100, 0.0105, 0.0105, 0.0105, 0.0125, 0.0125, 0.0105,\n",
      "        0.0095], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.7881236544781132e-06\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0109, 0.0106, 0.0090, 0.0061], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0118, 0.0126, 0.0117, 0.0113, 0.0123, 0.0101, 0.0101, 0.0118, 0.0132,\n",
      "        0.0105], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0109, 0.0113, 0.0118, 0.0118, 0.0113, 0.0113, 0.0113, 0.0109, 0.0098,\n",
      "        0.0112], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.9074059309787117e-06\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0116, 0.0111, 0.0080, 0.0061], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0110, 0.0128, 0.0108, 0.0125, 0.0127, 0.0129, 0.0095, 0.0092, 0.0128,\n",
      "        0.0110], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0115, 0.0115, 0.0113, 0.0113, 0.0105, 0.0110, 0.0116, 0.0115, 0.0115,\n",
      "        0.0115], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.3838208562665386e-06\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0126, 0.0114, 0.0077, 0.0058], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0104, 0.0114, 0.0100, 0.0117, 0.0124, 0.0082, 0.0104, 0.0104, 0.0125,\n",
      "        0.0089], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0112, 0.0110, 0.0102, 0.0114, 0.0112, 0.0109, 0.0109, 0.0109, 0.0110,\n",
      "        0.0112], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.7258176967516192e-06\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0132, 0.0117, 0.0081, 0.0052], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([0.0095, 0.0115, 0.0095, 0.0095, 0.0089, 0.0112, 0.0095, 0.0104, 0.0111,\n",
      "        0.0095], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0114, 0.0104, 0.0105, 0.0105, 0.0094, 0.0114, 0.0105, 0.0094, 0.0105,\n",
      "        0.0105], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.0288902103638975e-06\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([0.0101, 0.0107, 0.0123, 0.0090, 0.0104, 0.0124, 0.0103, 0.0119, 0.0107,\n",
      "        0.0099], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0103, 0.0103, 0.0101, 0.0089, 0.0107, 0.0112, 0.0106, 0.0112, 0.0103,\n",
      "        0.0089], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.088180720733362e-07\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.0088, -0.0020,  0.0159,  0.0015], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0093, 0.0078, 0.0093, 0.0078, 0.0119, 0.0126, 0.0088, 0.0115, 0.0119,\n",
      "        0.0141], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0106, 0.0106, 0.0106, 0.0106, 0.0107, 0.0113, 0.0114, 0.0111, 0.0107,\n",
      "        0.0127], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.291222583357012e-06\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([0.0103, 0.0095, 0.0095, 0.0087, 0.0069, 0.0087, 0.0121, 0.0115, 0.0091,\n",
      "        0.0111], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0100, 0.0103, 0.0103, 0.0117, 0.0119, 0.0117, 0.0115, 0.0113, 0.0101,\n",
      "        0.0117], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.652836651075631e-06\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.0058, -0.0025,  0.0171,  0.0036], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0105, 0.0124, 0.0131, 0.0126, 0.0112, 0.0102, 0.0124, 0.0085, 0.0110,\n",
      "        0.0116], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0118, 0.0116, 0.0118, 0.0115, 0.0104, 0.0104, 0.0116, 0.0104, 0.0115,\n",
      "        0.0100], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.2721964139927877e-06\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.0044, -0.0030,  0.0172,  0.0054], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0090, 0.0122, 0.0122, 0.0099, 0.0110, 0.0122, 0.0102, 0.0127, 0.0087,\n",
      "        0.0112], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0111, 0.0119, 0.0119, 0.0101, 0.0101, 0.0119, 0.0101, 0.0114, 0.0116,\n",
      "        0.0117], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.5623506897100015e-06\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.0031, -0.0027,  0.0175,  0.0062], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0134, 0.0134, 0.0134, 0.0106, 0.0128, 0.0134, 0.0055, 0.0090, 0.0043,\n",
      "        0.0099], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0121, 0.0121, 0.0121, 0.0103, 0.0121, 0.0121, 0.0122, 0.0103, 0.0116,\n",
      "        0.0117], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.0987291716446634e-05\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.0041, -0.0018,  0.0176,  0.0048], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0118, 0.0137, 0.0118, 0.0121, 0.0130, 0.0069, 0.0097, 0.0061, 0.0136,\n",
      "        0.0122], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0123, 0.0121, 0.0125, 0.0110, 0.0121, 0.0108, 0.0109, 0.0103, 0.0123,\n",
      "        0.0110], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.252405688021099e-06\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.0061, -0.0011,  0.0173,  0.0035], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0107, 0.0133, 0.0081, 0.0091, 0.0100, 0.0089, 0.0094, 0.0133, 0.0111,\n",
      "        0.0106], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0120, 0.0120, 0.0104, 0.0103, 0.0104, 0.0100, 0.0100, 0.0120, 0.0103,\n",
      "        0.0095], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.5167844367169891e-06\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.0085, -0.0007,  0.0166,  0.0026], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0138, 0.0086, 0.0086, 0.0103, 0.0097, 0.0104, 0.0103, 0.0091, 0.0125,\n",
      "        0.0106], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0124, 0.0099, 0.0123, 0.0101, 0.0114, 0.0101, 0.0101, 0.0113, 0.0116,\n",
      "        0.0101], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.597163302198169e-06\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.0106, -0.0005,  0.0155,  0.0026], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0101, 0.0121, 0.0116, 0.0133, 0.0098, 0.0120, 0.0079, 0.0120, 0.0102,\n",
      "        0.0133], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0109, 0.0109, 0.0122, 0.0120, 0.0112, 0.0134, 0.0115, 0.0134, 0.0122,\n",
      "        0.0120], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.862681412807433e-06\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.0110, -0.0004,  0.0151,  0.0033], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0109, 0.0089, 0.0115, 0.0122, 0.0118, 0.0112, 0.0118, 0.0088, 0.0112,\n",
      "        0.0089], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0126, 0.0126, 0.0111, 0.0113, 0.0126, 0.0126, 0.0126, 0.0129, 0.0126,\n",
      "        0.0126], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.274483555695042e-06\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.0094, -0.0006,  0.0168,  0.0039], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0135, 0.0123, 0.0115, 0.0111, 0.0110, 0.0126, 0.0111, 0.0113, 0.0101,\n",
      "        0.0096], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0124, 0.0111, 0.0114, 0.0102, 0.0102, 0.0114, 0.0102, 0.0105, 0.0118,\n",
      "        0.0105], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.12978636934713e-06\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.0079, -0.0005,  0.0184,  0.0040], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0111, 0.0117, 0.0151, 0.0151, 0.0117, 0.0114, 0.0140, 0.0088, 0.0111,\n",
      "        0.0151], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0110, 0.0126, 0.0136, 0.0136, 0.0126, 0.0112, 0.0126, 0.0112, 0.0110,\n",
      "        0.0136], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.6268422768916935e-06\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.0066, -0.0004,  0.0194,  0.0046], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0090, 0.0124, 0.0149, 0.0149, 0.0124, 0.0132, 0.0124, 0.0084, 0.0121,\n",
      "        0.0149], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0119, 0.0134, 0.0134, 0.0134, 0.0134, 0.0134, 0.0134, 0.0118, 0.0128,\n",
      "        0.0134], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.9666302907571662e-06\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.0064, -0.0003,  0.0191,  0.0058], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0159, 0.0076, 0.0126, 0.0124, 0.0119, 0.0116, 0.0145, 0.0139, 0.0137,\n",
      "        0.0114], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0143, 0.0131, 0.0143, 0.0115, 0.0113, 0.0113, 0.0130, 0.0115, 0.0130,\n",
      "        0.0117], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.490988885663683e-06\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.8663e-03, 2.2635e-05, 1.8983e-02, 5.7264e-03],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0123, 0.0139, 0.0159, 0.0160, 0.0123, 0.0124, 0.0144, 0.0127, 0.0119,\n",
      "        0.0139], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0115, 0.0129, 0.0143, 0.0144, 0.0115, 0.0143, 0.0129, 0.0117, 0.0129,\n",
      "        0.0129], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.6218388054767274e-06\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0072, 0.0004, 0.0192, 0.0053], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0082, 0.0145, 0.0137, 0.0148, 0.0098, 0.0091, 0.0160, 0.0117, 0.0145,\n",
      "        0.0127], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0112, 0.0130, 0.0130, 0.0147, 0.0121, 0.0117, 0.0144, 0.0130, 0.0130,\n",
      "        0.0113], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.209803935533273e-06\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0079, 0.0005, 0.0189, 0.0054], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0149, 0.0149, 0.0138, 0.0117, 0.0138, 0.0118, 0.0138, 0.0138, 0.0118,\n",
      "        0.0095], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0146, 0.0146, 0.0128, 0.0112, 0.0128, 0.0116, 0.0128, 0.0128, 0.0116,\n",
      "        0.0118], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.0620952934914385e-06\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0089, 0.0007, 0.0190, 0.0048], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0143, 0.0112, 0.0150, 0.0150, 0.0143, 0.0112, 0.0107, 0.0132, 0.0107,\n",
      "        0.0143], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0128, 0.0118, 0.0147, 0.0147, 0.0128, 0.0118, 0.0118, 0.0128, 0.0119,\n",
      "        0.0128], grad_fn=<AddBackward0>)\n",
      "LOSS: 9.266036613553297e-07\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0098, 0.0005, 0.0190, 0.0046], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0130, 0.0115, 0.0132, 0.0118, 0.0132, 0.0142, 0.0142, 0.0142, 0.0114,\n",
      "        0.0130], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0128, 0.0122, 0.0119, 0.0122, 0.0110, 0.0128, 0.0128, 0.0128, 0.0110,\n",
      "        0.0128], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.3408637187239947e-06\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0109, 0.0003, 0.0187, 0.0044], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0139, 0.0139, 0.0146, 0.0128, 0.0099, 0.0146, 0.0139, 0.0139, 0.0109,\n",
      "        0.0107], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0125, 0.0125, 0.0146, 0.0127, 0.0127, 0.0146, 0.0125, 0.0125, 0.0115,\n",
      "        0.0115], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.6842113836901262e-06\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 1.1608e-02, -1.5534e-05,  1.8269e-02,  4.8209e-03],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0121, 0.0128, 0.0126, 0.0144, 0.0116, 0.0113, 0.0117, 0.0120, 0.0123,\n",
      "        0.0128], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0125, 0.0122, 0.0121, 0.0129, 0.0122, 0.0121, 0.0125, 0.0114, 0.0131,\n",
      "        0.0122], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.034416060174408e-07\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.0120, -0.0001,  0.0180,  0.0051], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0117, 0.0152, 0.0142, 0.0132, 0.0117, 0.0162, 0.0116, 0.0148, 0.0117,\n",
      "        0.0124], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0146, 0.0131, 0.0114, 0.0119, 0.0116, 0.0146, 0.0124, 0.0169, 0.0146,\n",
      "        0.0146], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.19027765019564e-06\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0113, 0.0006, 0.0180, 0.0051], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0147, 0.0117, 0.0181, 0.0133, 0.0129, 0.0181, 0.0133, 0.0133, 0.0121,\n",
      "        0.0117], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0142, 0.0111, 0.0119, 0.0120, 0.0127, 0.0119, 0.0120, 0.0119, 0.0120,\n",
      "        0.0111], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.355650606972631e-06\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0090, 0.0027, 0.0183, 0.0045], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0121, 0.0119, 0.0152, 0.0160, 0.0119, 0.0114, 0.0121, 0.0135, 0.0135,\n",
      "        0.0121], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0126, 0.0116, 0.0141, 0.0146, 0.0116, 0.0116, 0.0126, 0.0126, 0.0126,\n",
      "        0.0126], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.697933715964609e-07\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0069, 0.0049, 0.0180, 0.0041], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0147, 0.0119, 0.0119, 0.0124, 0.0144, 0.0080, 0.0115, 0.0133, 0.0133,\n",
      "        0.0155], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0138, 0.0142, 0.0142, 0.0163, 0.0150, 0.0132, 0.0146, 0.0142, 0.0142,\n",
      "        0.0140], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.858962478872854e-06\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0063, 0.0048, 0.0182, 0.0049], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0120, 0.0079, 0.0148, 0.0108, 0.0123, 0.0114, 0.0117, 0.0124, 0.0105,\n",
      "        0.0077], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0140, 0.0131, 0.0160, 0.0160, 0.0160, 0.0139, 0.0128, 0.0140, 0.0148,\n",
      "        0.0128], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.2950406016898341e-05\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0081, 0.0015, 0.0192, 0.0072], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0140, 0.0130, 0.0099, 0.0140, 0.0134, 0.0192, 0.0138, 0.0141, 0.0100,\n",
      "        0.0118], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0126, 0.0125, 0.0124, 0.0126, 0.0139, 0.0172, 0.0140, 0.0135, 0.0126,\n",
      "        0.0125], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.246711574116489e-06\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.0102, -0.0023,  0.0190,  0.0089], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0152, 0.0152, 0.0147, 0.0133, 0.0134, 0.0190, 0.0124, 0.0127, 0.0152,\n",
      "        0.0081], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0136, 0.0136, 0.0150, 0.0150, 0.0140, 0.0171, 0.0139, 0.0163, 0.0136,\n",
      "        0.0140], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.388031124515692e-06\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.0121, -0.0049,  0.0187,  0.0088], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0156, 0.0163, 0.0158, 0.0083, 0.0092, 0.0182, 0.0163, 0.0144, 0.0152,\n",
      "        0.0148], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0149, 0.0147, 0.0142, 0.0135, 0.0140, 0.0163, 0.0147, 0.0133, 0.0137,\n",
      "        0.0133], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.793748525524279e-06\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.0132, -0.0061,  0.0184,  0.0083], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0163, 0.0117, 0.0164, 0.0184, 0.0160, 0.0160, 0.0155, 0.0190, 0.0164,\n",
      "        0.0173], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0147, 0.0171, 0.0148, 0.0166, 0.0148, 0.0148, 0.0139, 0.0149, 0.0148,\n",
      "        0.0156], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.611897788388887e-06\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.0133, -0.0069,  0.0180,  0.0073], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0150, 0.0176, 0.0131, 0.0140, 0.0175, 0.0150, 0.0108, 0.0175, 0.0089,\n",
      "        0.0180], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0157, 0.0159, 0.0142, 0.0147, 0.0157, 0.0157, 0.0152, 0.0157, 0.0144,\n",
      "        0.0162], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.535026386700338e-06\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.0127, -0.0069,  0.0163,  0.0070], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0155, 0.0155, 0.0105, 0.0165, 0.0155, 0.0155, 0.0155, 0.0086, 0.0145,\n",
      "        0.0108], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0143, 0.0143, 0.0144, 0.0160, 0.0143, 0.0143, 0.0143, 0.0153, 0.0160,\n",
      "        0.0164], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.019375031319214e-05\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.0100, -0.0050,  0.0152,  0.0065], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0145, 0.0164, 0.0129, 0.0140, 0.0140, 0.0129, 0.0204, 0.0165, 0.0138,\n",
      "        0.0157], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0149, 0.0154, 0.0154, 0.0165, 0.0165, 0.0161, 0.0183, 0.0142, 0.0146,\n",
      "        0.0146], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.243221610522596e-06\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.0082, -0.0034,  0.0128,  0.0073], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0141, 0.0166, 0.0121, 0.0104, 0.0136, 0.0191, 0.0126, 0.0133, 0.0147,\n",
      "        0.0133], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0147, 0.0151, 0.0143, 0.0147, 0.0147, 0.0176, 0.0143, 0.0147, 0.0158,\n",
      "        0.0147], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.768863734876504e-06\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.0071, -0.0028,  0.0097,  0.0097], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([0.0122, 0.0169, 0.0097, 0.0150, 0.0107, 0.0151, 0.0186, 0.0161, 0.0151,\n",
      "        0.0169], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0140, 0.0164, 0.0087, 0.0159, 0.0154, 0.0148, 0.0168, 0.0164, 0.0140,\n",
      "        0.0164], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.220715825591469e-06\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.0067, -0.0031,  0.0066,  0.0121], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([0.0148, 0.0157, 0.0148, 0.0189, 0.0066, 0.0148, 0.0169, 0.0125, 0.0107,\n",
      "        0.0148], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([0.0154, 0.0175, 0.0174, 0.0174, 0.0109, 0.0174, 0.0193, 0.0175, 0.0175,\n",
      "        0.0175], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.2195655472169165e-05\n",
      "---\n",
      "end game\n"
     ]
    }
   ],
   "source": [
    "import gym \n",
    "import gym_examples\n",
    "from dqn import Agent\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('gym_examples/GridWorld-v0', size=4)\n",
    "agent = Agent(gamma=0.9, epsilon=0.5, batch_size=10, n_actions=4, eps_end=0.01, input_dims=[16], lr=0.003)\n",
    "scores, eps_hist = [], []\n",
    "n_games = 5\n",
    "\n",
    "for i in range(n_games):\n",
    "    score = 0\n",
    "    done=False\n",
    "    observation = env.reset()[0]['agent']\n",
    "    counter = 0\n",
    "\n",
    "    while not done:\n",
    "        agent_location = env.get_agent_location()\n",
    "        print(f\"Agent's Location: {agent_location}\")\n",
    "        # choose an action based on current state of env\n",
    "        action = agent.choose_action(observation)\n",
    "        observation_, reward, done, truncated, info = env.step(action) #observation_ is just next_state\n",
    "        observation_ = observation_['agent']\n",
    "        score += reward\n",
    "        # print(f\"this is the observation:{observation_}\")\n",
    "        agent.store_transitions(observation, action, reward, observation_, done)\n",
    "        agent.learn()\n",
    "        observation = observation_\n",
    "        # end of an episode\n",
    "        counter += 1\n",
    "        print('---')\n",
    "\n",
    "    scores.append(score)\n",
    "    eps_hist.append(agent.epsilon)\n",
    "    print('end game')\n",
    "\n",
    "    avg_score = np.mean(scores[-100:])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TESTTTTT\n",
    "import gym \n",
    "import gym_examples\n",
    "from dqn import Agent\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialization\n",
    "env = gym.make('gym_examples/GridWorld-v0', size=4)\n",
    "n_games = 5\n",
    "n_runs = 10 # Number of runs for averaging\n",
    "scores_all_runs = []\n",
    "\n",
    "for run in range(n_runs):\n",
    "    agent = Agent(gamma=0.9, epsilon=0.5, batch_size=10, n_actions=4, eps_end=0.01, input_dims=[16], lr=0.003)\n",
    "    scores = []\n",
    "    for i in range(n_games):\n",
    "        score = 0\n",
    "        done = False\n",
    "        observation = env.reset()[0]['agent']\n",
    "        while not done:\n",
    "            action = agent.choose_action(observation)\n",
    "            observation_, reward, done, truncated, info = env.step(action)\n",
    "            observation_ = observation_['agent']\n",
    "            score += reward\n",
    "            agent.store_transitions(observation, action, reward, observation_, done)\n",
    "            agent.learn()\n",
    "            observation = observation_\n",
    "        scores.append(score)\n",
    "    scores_all_runs.append(scores)\n",
    "\n",
    "# Calculate the average scores\n",
    "average_scores = np.mean(scores_all_runs, axis=0)\n",
    "\n",
    "# Plotting\n",
    "plt.plot(average_scores)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Score')\n",
    "plt.title('Learning Curve Averaged Over 10 Runs')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch as T\n",
    "T.tensor(observation, dtype=T.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9873709528509469"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.choice([1,2,3], 2, replace=False)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(2, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = np.array([1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, lr, input_dims, fc1_dims, fc2_dims, n_actions):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.n_actions = n_actions\n",
    "        self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)\n",
    "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "        self.fc3 = nn.Linear(self.fc2_dims, self.n_actions)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.loss = F.mse_loss\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        actions = self.fc3(x)\n",
    "\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = DeepQNetwork(lr =0.003, n_actions=4, input_dims=[16], fc1_dims=128, fc2_dims=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = net.forward(T.tensor([0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0], dtype=T.float32))\n",
    "T.argmax(w).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "memo = T.tensor([\n",
    "    [0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0]], dtype=T.float32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = net.forward(memo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0018,  0.0231,  0.0456, -0.0359],\n",
       "        [-0.0359,  0.0097,  0.0867, -0.0367],\n",
       "        [-0.0359,  0.0097,  0.0867, -0.0367],\n",
       "        [ 0.0365,  0.0507,  0.0973, -0.0360],\n",
       "        [-0.0475,  0.0321,  0.0967, -0.0469]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0456, 0.0867, 0.0867, 0.0973, 0.0967], grad_fn=<MaxBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.max(actions, dim=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
