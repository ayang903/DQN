{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent's INITIAL Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [1 3], Target Location: [0 0]\n",
      "Agent's Location: [0 3], Target Location: [0 0]\n",
      "Agent's Location: [0 3], Target Location: [0 0]\n",
      "Agent's Location: [0 3], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 3], Target Location: [0 0]\n",
      "Agent's Location: [0 3], Target Location: [0 0]\n",
      "Agent's Location: [1 3], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [1 3], Target Location: [0 0]\n",
      "Agent's Location: [0 3], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 3], Target Location: [0 0]\n",
      "Agent's Location: [0 3], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 3], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 1], Target Location: [0 0]\n",
      "Agent's Location: [0 0], Target Location: [0 0]\n",
      "Episode finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andyyang/Desktop/DS598 Reinforcement Learning/gym-examples/.env/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "# Complete random movements\n",
    "\n",
    "import gym\n",
    "import gym_examples\n",
    "# import numpy as np\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make('gym_examples/GridWorld-v0', size=4)\n",
    "\n",
    "# Reset the environment to start a new episode\n",
    "state = env.reset()\n",
    "agent_location = env.get_agent_location()\n",
    "target_location = env.get_target_location()\n",
    "print(f\"Agent's INITIAL Location: {agent_location}, Target Location: {target_location}\")\n",
    "# Initialize done to False\n",
    "done = False\n",
    "\n",
    "# Loop until the episode ends\n",
    "while not done:\n",
    "    # Select an action randomly\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # Apply the action to the environment\n",
    "    next_state, reward, done, truncated, info = env.step(action)\n",
    "    \n",
    "    # Optionally, render the environment to visualize it\n",
    "    # env.render()\n",
    "    agent_location = env.get_agent_location()\n",
    "    target_location = env.get_target_location()\n",
    "    print(f\"Agent's Location: {agent_location}, Target Location: {target_location}\")\n",
    "\n",
    "    # Update state\n",
    "    state = next_state\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "\n",
    "print(\"Episode finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0623, 0.0256, 0.0519, 0.0224], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0623, 0.0256, 0.0519, 0.0224], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([ 0.0818, -0.0340,  0.0311,  0.0175], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([ 0.0631, -0.0082,  0.0678,  0.0325], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([ 0.0256,  0.0623, -0.0340,  0.0818,  0.0678,  0.0623, -0.0340,  0.0647,\n",
      "         0.0325,  0.0325], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9264, -0.9440, -0.9389, -0.9264, -0.9417, -0.9440, -0.9389, -0.9389,\n",
      "        -0.9264, -0.9264], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.945106029510498\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([ 0.1743, -0.1002,  0.0572, -0.0025], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([ 0.1017, -0.0220, -0.0491,  0.1743,  0.1258, -0.1002,  0.1743, -0.1002,\n",
      "         0.1128, -0.0220], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9084, -0.8432, -0.8432, -0.8432, -0.8985, -0.8868, -0.8432, -0.8868,\n",
      "        -0.8868, -0.8432], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.8355841636657715\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([ 0.2561, -0.1606,  0.0954, -0.0368], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([ 0.2561, -0.0865,  0.1428, -0.1212, -0.1606, -0.0865,  0.1863, -0.1606,\n",
      "         0.2561,  0.1587], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.7695, -0.7695, -0.8715, -0.7695, -0.8323, -0.7695, -0.8571, -0.8323,\n",
      "        -0.7695, -0.8323], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.7459270358085632\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([ 0.3355, -0.2291,  0.1378, -0.0736], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([ 0.2021,  0.3355, -0.2291,  0.1831, -0.2291,  0.1831,  0.3355, -0.1929,\n",
      "         0.3355,  0.3355], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.7762, -0.6981, -0.7762, -0.8352, -0.7762, -0.8352, -0.6981, -0.6981,\n",
      "        -0.6981, -0.6981], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.8157483339309692\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([ 0.3810,  0.2139,  0.3308,  0.1984, -0.3114, -0.3114,  0.1907, -0.1981,\n",
      "         0.3810, -0.2663], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6571, -0.7023, -0.7936, -0.8137, -0.7023, -0.7023, -0.7347, -0.6571,\n",
      "        -0.6571, -0.6571], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.680895209312439\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.3345, -0.3370,  0.3384, -0.1501], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.4197, -0.2475, -0.4007,  0.2019,  0.4197,  0.2175,  0.2019, -0.3345,\n",
      "         0.4197,  0.3384], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6222, -0.6222, -0.6254, -0.7602, -0.6222, -0.6254, -0.7602, -0.6222,\n",
      "        -0.6222, -0.7116], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.7195102572441101\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.2915, -0.4055,  0.4087, -0.2217], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.3964, -0.4882, -0.2981,  0.1961,  0.1816,  0.4340,  0.1816, -0.2981,\n",
      "         0.4340,  0.4087], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6094, -0.5424, -0.6094, -0.5424, -0.7017, -0.6094, -0.7017, -0.6094,\n",
      "        -0.6094, -0.6321], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5608681440353394\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.2785, -0.4733,  0.4987, -0.2660], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.1469,  0.3793, -0.4545,  0.4715, -0.5729,  0.4987,  0.6090,  0.4374,\n",
      "         0.4374,  0.4374], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6360, -0.5756, -0.6063, -0.5511, -0.4519, -0.5511, -0.5942, -0.6063,\n",
      "        -0.6063, -0.6063], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.8426305055618286\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.2532, -0.5363,  0.5868, -0.3109], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.6478, -0.5087, -0.6478, -0.4144,  0.1173,  0.5868,  0.1052,  0.4245,\n",
      "         0.5868,  0.1052], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3798, -0.6067, -0.3798, -0.6067, -0.3798, -0.4719, -0.5733, -0.6067,\n",
      "        -0.4719, -0.5733], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4663065969944\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([ 0.7734,  0.3847, -0.4711,  0.3847,  0.3847,  0.5992,  0.6762, -0.4711,\n",
      "        -0.7031,  0.2080], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4530, -0.5388, -0.5388, -0.5388, -0.5388, -0.3914, -0.3914, -0.5388,\n",
      "        -0.3040, -0.4607], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6798995733261108\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.2299, -0.6047,  0.6696, -0.3575], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.0065, -0.7425, -0.5246,  0.3199,  0.7745,  0.3199,  0.3199,  0.7745,\n",
      "        -0.0012, -0.7425], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.2320, -0.2320, -0.4558, -0.4558, -0.3029, -0.4558, -0.4558, -0.3029,\n",
      "        -0.4347, -0.2320], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.48918724060058594\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([ 0.8642,  0.0590, -0.0807,  0.7402, -0.5710,  0.7402, -0.7573, -0.5710,\n",
      "         0.8642, -0.7573], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.2222, -0.3339, -0.1667, -0.2222, -0.3662, -0.2222, -0.1667, -0.3662,\n",
      "        -0.2222, -0.1667], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5156086683273315\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.0229, -0.6358,  0.9317, -0.4678], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.9317,  0.9317,  0.1568, -0.1316,  0.9317,  0.7668, -0.5912,  0.1568,\n",
      "        -0.7413, -0.1316], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.1614, -0.1614, -0.3102, -0.3167, -0.1614, -0.1614, -0.3102, -0.3102,\n",
      "        -0.1345, -0.3167], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5398551225662231\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-0.5871, -0.1988,  0.0758,  0.9902,  0.9902, -0.7176, -0.5993,  0.0758,\n",
      "        -0.7176, -0.5993], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.2620, -0.2745, -0.2620, -0.1089, -0.1089, -0.1135, -0.2620, -0.2620,\n",
      "        -0.1135, -0.2620], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.37126195430755615\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([ 1.0112, -0.6737,  0.8418, -0.2921,  1.0112, -0.5618, -0.0020,  0.7667,\n",
      "        -0.4717,  0.7667], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.0899, -0.1241, -0.3100, -0.1241, -0.0899, -0.2424, -0.2424, -0.0899,\n",
      "        -0.1656, -0.0899], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5802872776985168\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.2148, -0.5562,  0.9120, -0.3895], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.3183,  1.0225,  0.9120, -0.3183, -0.6231, -0.6231, -0.5274, -0.0697,\n",
      "        -0.0697,  0.7375], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.2514, -0.0798, -0.1792, -0.2514, -0.1495, -0.1495, -0.2451, -0.2451,\n",
      "        -0.2451, -0.0798], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.36723434925079346\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.2637, -0.5052,  0.8819, -0.3679], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.9030,  1.0220, -0.3099,  0.8259, -0.1258, -0.4840,  0.6938, -0.5349,\n",
      "        -0.1258,  0.8819], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.2021, -0.0802, -0.3755, -0.3755, -0.2567, -0.2567, -0.0802, -0.2567,\n",
      "        -0.2567, -0.2063], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5830472707748413\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-0.1725, -0.4536, -0.3659, -0.1725,  0.8516, -0.4464,  1.0217,  1.0217,\n",
      "        -0.5089, -0.4588], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.2771, -0.2279, -0.4108, -0.2771, -0.2336, -0.2771, -0.0805, -0.0805,\n",
      "        -0.2279, -0.0805], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.39327576756477356\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4120, -0.4457,  1.0008, -0.3950], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.4796,  0.7731,  1.0008,  1.0008, -0.2115, -0.4537, -0.2115,  0.6113,\n",
      "        -0.4931, -0.3950], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3042, -0.4498, -0.0993, -0.0993, -0.3042, -0.2741, -0.3042, -0.0993,\n",
      "        -0.2741, -0.2728], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.45639222860336304\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4507, -0.4087,  0.9792, -0.3703], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.5723,  0.9792,  0.7624, -0.2433, -0.4507, -0.5235,  0.7624, -0.2433,\n",
      "         0.7353, -0.4676], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.1187, -0.1187, -0.3138, -0.3383, -0.4850, -0.3208, -0.3138, -0.3383,\n",
      "        -0.4850, -0.3104], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5573387742042542\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4820, -0.3765,  0.9614, -0.3503], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.9614, -0.3582,  0.7181, -0.2682,  0.9614,  0.9614, -0.2682, -0.3224,\n",
      "        -0.4234, -0.4234], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.1347, -0.3641, -0.3537, -0.3750, -0.1347, -0.1347, -0.3750, -0.1347,\n",
      "        -0.3750, -0.3750], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.48161521553993225\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.5080, -0.3429,  0.9330, -0.3314], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.2893,  0.9330,  0.9330,  0.6552, -0.3314,  0.9330, -0.3201, -0.2893,\n",
      "         0.6598, -0.5057], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4103, -0.1603, -0.1603, -0.5421, -0.3958, -0.1603, -0.4062, -0.4103,\n",
      "        -0.3088, -0.3584], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6020435094833374\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([ 0.6155, -0.2544,  0.9020,  0.4876, -0.3169,  0.9020, -0.5309,  0.9020,\n",
      "        -0.3791,  0.9020], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3309, -0.1882, -0.1882, -0.1882, -0.4330, -0.1882, -0.5612, -0.1882,\n",
      "        -0.4440, -0.1882], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6129432916641235\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-0.5330, -0.3637,  0.8721, -0.5330,  0.8721, -0.3212, -0.3212, -0.5515,\n",
      "         0.5923, -0.3212], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4041, -0.4741, -0.2151, -0.4041, -0.2151, -0.4741, -0.4741, -0.5854,\n",
      "        -0.4669, -0.4741], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.36026254296302795\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([ 0.8415, -0.3371,  0.8415, -0.2322,  0.8415,  0.5396, -0.5708, -0.3508,\n",
      "        -0.2322,  0.8415], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.2426, -0.4987, -0.2426, -0.5143, -0.2426, -0.3647, -0.6076, -0.4987,\n",
      "        -0.5143, -0.2426], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5727834105491638\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-0.3538, -0.2156,  0.5367, -0.1995], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.4156, -0.5895,  0.8127,  0.5367, -0.1881,  0.5304, -0.3418,  0.8127,\n",
      "         0.5367,  0.8127], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.2686, -0.6259, -0.2686, -0.6259, -0.2686, -0.5226, -0.5170, -0.2686,\n",
      "        -0.6259, -0.2686], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.7826136946678162\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3011, -0.1721,  0.4074, -0.1765], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.4862, -0.5641,  0.7932, -0.3363,  0.7932, -0.3093,  0.5108,  0.5088,\n",
      "         0.4074, -0.3664], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3822, -0.4557, -0.2861, -0.5402, -0.2861, -0.5421, -0.6333, -0.5421,\n",
      "        -0.2861, -0.5402], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6116222143173218\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.6280, -0.2185,  0.7827, -0.3205], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.7827,  0.7827,  0.4939,  0.7827, -0.3378, -0.5736, -0.3781,  0.7827,\n",
      "        -0.3781,  0.7827], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.2956, -0.2956, -0.5555, -0.2956, -0.5602, -0.4665, -0.5602, -0.2956,\n",
      "        -0.5602, -0.2956], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.7041900753974915\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.6456, -0.2068,  0.7639, -0.3346], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.7639,  0.7639, -0.3425,  0.7639, -0.3910,  0.7639,  0.4766, -0.6456,\n",
      "        -0.1774, -0.1839], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3125, -0.3125, -0.5774, -0.3125, -0.5774, -0.3125, -0.5710, -0.6418,\n",
      "        -0.6016, -0.5774], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6156800389289856\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.6626, -0.2034,  0.7459, -0.3514], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.4576, -0.3484,  0.7459, -0.3506, -0.6626,  0.7459, -0.1547,  0.4628,\n",
      "         0.7459,  0.3942], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6452, -0.5882, -0.3287, -0.5882, -0.6452, -0.3287, -0.3287, -0.5835,\n",
      "        -0.3287, -0.3287], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.644217312335968\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([ 0.7373, -0.7478,  0.7373,  0.7373,  0.4462,  0.7373,  0.7373,  0.4157,\n",
      "        -0.3614, -0.1805], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3364, -0.6259, -0.3364, -0.3364, -0.6446, -0.3364, -0.3364, -0.3935,\n",
      "        -0.5984, -0.5984], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.7854764461517334\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([ 0.7248,  0.7248, -0.1725,  0.7248, -0.3741,  0.4023, -0.6984, -0.3741,\n",
      "         0.7248, -0.4311], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3477, -0.3477, -0.6379, -0.3477, -0.6084, -0.3921, -0.6407, -0.6084,\n",
      "        -0.3477, -0.6084], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5593177080154419\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.7124, -0.2139,  0.7135, -0.4160], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.6310,  0.4304,  0.4049,  0.7135,  0.7135,  0.7135,  0.7135, -0.1916,\n",
      "        -0.1632, -0.3925], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4980, -0.6356, -0.3578, -0.3578, -0.3578, -0.3578, -0.3578, -0.6126,\n",
      "        -0.3578, -0.6126], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6590584516525269\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.7268, -0.2266,  0.7101, -0.4425], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.8157, -0.6418, -0.4125,  0.7101, -0.4252, -0.2033,  0.7101,  0.4367,\n",
      "         0.7101, -0.7268], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6533, -0.4966, -0.6164, -0.3609, -0.6164, -0.6164, -0.3609, -0.6069,\n",
      "        -0.3609, -0.6281], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.48364201188087463\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-0.1943, -0.4687,  0.4234, -0.7348,  0.4189, -0.6486,  0.7010,  0.3764,\n",
      "         0.7010,  0.7010], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6612, -0.6109, -0.6230, -0.6230, -0.3691, -0.4973, -0.3691, -0.3677,\n",
      "        -0.3691, -0.3691], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5978642702102661\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4538, -0.1923,  0.4256, -0.2836], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.6985,  0.6985,  0.3694,  0.4256,  0.6985, -0.4966, -0.1990, -0.4538,\n",
      "         0.4320, -0.4715], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3714, -0.3714, -0.3564, -0.3714, -0.3714, -0.6112, -0.3714, -0.6227,\n",
      "        -0.6112, -0.6227], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5778213739395142\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.7506, -0.2776,  0.7044, -0.5292], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.7044, -0.2186,  0.7044,  0.7044,  0.7044, -0.4796,  0.4300, -0.4774,\n",
      "         0.7044,  0.4300], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3660, -0.6724, -0.3660, -0.3660, -0.3660, -0.6228, -0.3660, -0.6228,\n",
      "        -0.3660, -0.3660], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.7244256734848022\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.7661, -0.2996,  0.7189, -0.5623], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.4213,  0.4389, -0.6641,  0.7189,  0.7189,  0.7189,  0.7189,  0.7189,\n",
      "         0.7189, -0.2667], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6153, -0.6050, -0.4871, -0.3530, -0.3530, -0.3530, -0.3530, -0.3530,\n",
      "        -0.3530, -0.6209], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.9214661717414856\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([ 0.4362, -0.2461, -0.7744,  0.7198, -0.4958, -0.6667,  0.7198,  0.7198,\n",
      "         0.7198,  0.4166], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6075, -0.3522, -0.6183, -0.3522, -0.6250, -0.4882, -0.3522, -0.3522,\n",
      "        -0.3522, -0.6183], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6840904951095581\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.6780, -0.2951,  0.6012, -0.4521], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.7110, -0.5436,  0.7110, -0.5436, -0.9034,  0.4285, -0.7742, -0.7742,\n",
      "        -0.6178, -0.4976], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3601, -0.6350, -0.3601, -0.6350, -0.6894, -0.6144, -0.6219, -0.6219,\n",
      "        -0.6144, -0.6350], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.35098546743392944\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([ 0.4154,  0.4154, -0.6555,  0.6928, -0.7621,  0.6928, -0.7621,  0.3303,\n",
      "         0.6928, -0.5629], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6262, -0.6262, -0.5061, -0.3765, -0.6314, -0.3765, -0.6314, -0.3054,\n",
      "        -0.3765, -0.6466], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.606725811958313\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-0.2847,  0.3962, -0.6416,  0.6655, -0.6416,  0.6655, -0.3686, -0.7407,\n",
      "        -0.4747, -0.2847], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.7212, -0.6434, -0.5208, -0.4011, -0.5208, -0.4011, -0.4855, -0.6472,\n",
      "        -0.6611, -0.7212], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.38231003284454346\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-0.3395,  0.6424, -0.6269,  0.6424, -0.7204, -0.6269, -0.5966, -0.7204,\n",
      "        -0.3014,  0.6424], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5306, -0.4218, -0.5333, -0.4218, -0.6596, -0.5333, -0.6714, -0.6596,\n",
      "        -0.7333, -0.4218], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.36513781547546387\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.6342, -0.3513,  0.5168, -0.5516], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.6181,  0.6181, -0.6101,  0.3660, -0.6975,  0.3642,  0.6181,  0.3660,\n",
      "         0.6181, -0.6975], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4437, -0.4437, -0.5463, -0.6706, -0.6722, -0.4437, -0.4437, -0.6706,\n",
      "        -0.4437, -0.6722], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.7316827774047852\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.5626, -0.3572,  0.4393, -0.5361], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.5918,  0.5918, -0.4270,  0.5918,  0.2701,  0.4393, -0.4736,  0.5918,\n",
      "         0.3419,  0.5918], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4674, -0.4674, -0.5105, -0.4674, -0.3254, -0.6047, -0.6932, -0.4674,\n",
      "        -0.4674, -0.4674], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.7763829827308655\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([ 0.5684,  0.3281, -0.3365, -0.4465,  0.5684,  0.5684, -0.6404,  0.5351,\n",
      "        -0.5796, -0.6578], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4885, -0.7151, -0.4885, -0.5184, -0.4885, -0.4885, -0.7047, -0.5184,\n",
      "        -0.5769, -0.7151], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.558468759059906\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-0.3506, -0.4661,  0.5454,  0.5224, -0.5685,  0.2983,  0.5454,  0.5454,\n",
      "         0.4831,  0.5454], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5091, -0.5299, -0.5091, -0.5299, -0.5906, -0.5091, -0.5091, -0.5091,\n",
      "        -0.6419, -0.5091], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.7502623200416565\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([ 0.4564, -0.4878,  0.5227,  0.5227,  0.2750, -0.6339, -0.3885,  0.5227,\n",
      "        -0.5588, -0.8447], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6508, -0.5472, -0.5296, -0.5296, -0.5296, -0.7525, -0.8010, -0.5296,\n",
      "        -0.6058, -0.8010], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5386976003646851\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([ 0.4890, -0.6299,  0.5051, -0.3864, -0.6299, -0.8428, -0.4631,  0.2514,\n",
      "        -0.5523,  0.5051], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5599, -0.7738, -0.5454, -0.6764, -0.7738, -0.8129, -0.7469, -0.5454,\n",
      "        -0.6188, -0.5454], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.41536492109298706\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([ 0.4988,  0.4988, -0.4709,  0.4793, -0.8524,  0.2729, -0.4282, -0.5422,\n",
      "        -0.6943,  0.4988], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5511, -0.5511, -0.7544, -0.5687, -0.8201, -0.7909, -0.8201, -0.5687,\n",
      "        -0.7544, -0.5511], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5775776505470276\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4804, -0.4568,  0.3335, -0.5917], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.5594,  0.1956, -0.4812,  0.4708, -0.7099,  0.4929,  0.2569,  0.2651,\n",
      "        -0.4361, -0.6541], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6329, -0.3547, -0.7614, -0.5763, -0.7614, -0.5564, -0.7688, -0.8012,\n",
      "        -0.7614, -0.8012], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.49033671617507935\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4983, -0.4810,  0.3329, -0.6079], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.4344,  0.3832,  0.3996,  0.4946, -0.6777, -0.6777, -0.4980,  0.3687,\n",
      "         0.4946,  0.4946], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.7004, -0.6403, -0.6403, -0.5549, -0.8021, -0.8021, -0.7645, -0.6681,\n",
      "        -0.5549, -0.5549], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6681060791015625\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([ 0.4939,  0.2600,  0.3404, -0.4681,  0.2600, -0.5879,  0.4939,  0.4939,\n",
      "        -0.7481, -0.6639], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5555, -0.8017, -0.6937, -0.5555, -0.8017, -0.6349, -0.5555, -0.5555,\n",
      "        -0.7660, -0.6646], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6637369394302368\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.5472, -0.5360,  0.3414, -0.6546], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.5200, -0.6141, -0.8807, -0.6672,  0.4907,  0.4907,  0.4174,  0.4907,\n",
      "        -0.6033, -0.5239], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.8299, -0.6813, -0.7761, -0.5948, -0.5584, -0.5584, -0.6243, -0.5584,\n",
      "        -0.6382, -0.7743], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4567525386810303\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([ 0.2448,  0.4839,  0.4416,  0.4839,  0.4839, -0.5549, -0.5549, -0.6198,\n",
      "         0.4839, -0.7611], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.7797, -0.5645, -0.6026, -0.5645, -0.5645, -0.7816, -0.7816, -0.6411,\n",
      "        -0.5645, -0.7925], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6640455722808838\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-0.6386, -0.5717,  0.3943, -0.5513], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.5375, -0.5706,  0.2317,  0.4707,  0.4707,  0.4707, -0.5779,  0.2381,\n",
      "         0.4707, -0.5717], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5763, -0.8371, -0.5763, -0.5763, -0.5763, -0.5763, -0.7883, -0.7857,\n",
      "        -0.5763, -0.7883], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.62502121925354\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.6625, -0.6489,  0.3882, -0.7139], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.3882, -0.8181,  0.3369,  0.2320,  0.4189,  0.2320,  0.3369,  0.3369,\n",
      "        -0.7667,  0.4648], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.7912, -0.7925, -0.6968, -0.7912, -0.6230, -0.7912, -0.6968, -0.6968,\n",
      "        -0.6230, -0.5817], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.8892424702644348\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-0.6268, -0.6275, -0.7925,  0.4402,  0.3804,  0.4402,  0.3164, -0.6275,\n",
      "         0.4402,  0.4128], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.8045, -0.8495, -0.6450, -0.6038, -0.6757, -0.6038, -0.7152, -0.8495,\n",
      "        -0.6038, -0.6285], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6685896515846252\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-0.6252,  0.2938, -0.6520, -0.9753,  0.4155,  0.4155,  0.4155,  0.3684,\n",
      "         0.4155, -0.8467], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.8157, -0.7356, -0.8157, -0.8070, -0.6260, -0.6260, -0.6260, -0.6684,\n",
      "        -0.6260, -0.8157], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6566344499588013\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-0.6789,  0.3805, -0.6844, -0.6236,  0.3830,  0.2663,  0.3830,  0.2003,\n",
      "         0.3830, -0.6380], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.8291, -0.6575, -0.8715, -0.7076, -0.6553, -0.7603, -0.6553, -0.8197,\n",
      "        -0.6553, -0.6553], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6471145153045654\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9140, -0.8667,  0.3481, -0.9888], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.3481,  0.2376, -0.7079,  0.1741,  0.2246, -0.6626, -1.1471,  0.1840,\n",
      "        -0.6494, -0.8667], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6867, -0.7862, -0.8433, -0.8430, -0.6766, -0.6867, -0.8860, -0.8344,\n",
      "        -0.7233, -0.7233], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5115886926651001\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9308, -0.8835,  0.3108, -0.9937], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.1565, -0.8243,  0.2722, -0.8039, -0.7398,  0.3108,  0.2074, -0.6979,\n",
      "         0.3108,  0.3108], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.7202, -0.7389, -0.8523, -0.8259, -0.9041, -0.7202, -0.8134, -0.8626,\n",
      "        -0.7202, -0.7202], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6326435804367065\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9500, -0.9028,  0.2750, -0.9959], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.2750,  0.2750, -1.1859,  0.1477,  0.2750, -0.7667, -0.6945,  0.2750,\n",
      "        -0.6284,  0.1350], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.7525, -0.7525, -0.9225, -0.8671, -0.7525, -0.9225, -0.7575, -0.7525,\n",
      "        -0.8408, -0.7525], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6183093786239624\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9647, -0.9229,  0.2405, -1.0042], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.7941,  0.1279,  0.2405,  0.2405, -0.7643, -0.8830,  0.2029,  0.2405,\n",
      "        -0.7400, -0.7787], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9423, -0.8849, -0.7835, -0.7835, 10.0000, -0.9061, -0.8174, -0.7835,\n",
      "        -0.9061, -0.9061], grad_fn=<AddBackward0>)\n",
      "LOSS: 12.114992141723633\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9508, -0.9200,  0.2108, -0.9260], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.1729,  0.0458,  0.2108,  0.1729,  0.2108, -0.8273,  0.2108, -0.8055,\n",
      "         0.2729,  0.2108], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.8444, -0.5529, -0.8103, -0.8444, -0.8103, -0.9223, -0.8103, -0.9588,\n",
      "        -0.7544, -0.8103], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.7686395049095154\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-1.1709, -0.7805, -0.7356,  0.2034,  0.1786,  0.1786,  0.1786, -0.9193,\n",
      "        -0.7416, -0.7364], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9778, -0.9397, -0.8393, -0.8644, -0.8393, -0.8393, -0.8393, -0.8729,\n",
      "        -0.9397, -0.9397], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.44044384360313416\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([ 0.1512,  0.0967,  0.0967, -0.7372,  0.0061,  0.1833, -0.7418, -0.6727,\n",
      "         0.1512, -0.9222], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.8639, -0.9130, -0.9130, -0.9531, -0.5960, -0.8776, -0.9531, -0.8215,\n",
      "        -0.8639, -0.8987], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5701966881752014\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.6997, -0.7550,  0.0728, -0.5079], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.1286,  0.0314,  0.1286,  0.1286,  0.1286, -0.7914,  0.0865,  0.1286,\n",
      "         0.1286,  0.1286], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.8842, -0.8842, -0.8842, -0.8842, -0.8842, -0.9622, -0.9222, -0.8842,\n",
      "        -0.8842, -0.8842], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.906620979309082\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([ 0.1030, -0.4822,  0.1030,  0.1030,  0.1030, -0.4742, -0.7140, -0.9213,\n",
      "         0.1030, -1.1471], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9073, -0.9476, -0.9073, -0.9073, -0.9073, -0.9340, -0.9476, -0.9907,\n",
      "        -0.9073, -1.0272], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5605400800704956\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.7161, -0.7780,  0.0490, -0.4696], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.0799,  0.0799,  0.0799,  0.0799, -0.6906, -0.9238,  0.0799,  0.0799,\n",
      "         0.0799, -0.7388], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9281, -0.9281, -0.9281, -0.9281, -0.9559, -1.0063, -0.9281, -0.9281,\n",
      "        -0.9281, -0.8537], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.7202714681625366\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-0.0051, -0.7604,  0.0531, -0.6581, -0.7286, -0.8214, -0.0263,  0.0543,\n",
      "         0.1798, -0.9322], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.8382, -0.9046, -0.9522, -0.9958, -0.9034, -0.9958, -0.9522, -0.9511,\n",
      "        -0.8382, -1.0236], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.48133817315101624\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-0.7500, -0.8402,  0.0335,  0.0364,  0.0364, -0.9496, -0.0422,  0.1064,\n",
      "        -0.0785, -0.4571], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9042, -1.0011, -0.9698, -0.9673, -0.9673, -1.0380, -0.9673, -0.9698,\n",
      "        -0.6530, -0.9698], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5685731172561646\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.8198, -0.8530, -0.0552, -0.4950], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.0552,  0.0281, -0.6527, -0.0133,  0.0281,  0.0281, -0.8649, -0.9729,\n",
      "         0.0281,  0.0367], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9747, -0.9747, -1.0026, -1.0120, -0.9747, -0.9747, -1.0026, -1.0497,\n",
      "        -0.9747, -0.9670], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.7019935250282288\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9989, -0.9867,  0.0213, -0.6623], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.9989,  0.0213,  0.0213, -0.0359, -0.4673, -0.0715,  0.0213,  0.0377,\n",
      "         0.0213, -0.8922], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.0644, -0.9808, -0.9808, -0.8387, -0.9603, -0.9808, -0.9808, -0.9660,\n",
      "        -0.9808, -1.0042], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6755884289741516\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.0282, -1.0030,  0.0161, -0.6673], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.0161, -0.1115, -0.0898, -0.0399, -0.6673,  0.0161, -1.0030, -0.6673,\n",
      "        -0.3280, -1.2155], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9855, -0.6361, -0.9855, -1.0360, -1.0065, -0.9855, -1.0360, -0.9662,\n",
      "        10.0000, -1.1003], grad_fn=<AddBackward0>)\n",
      "LOSS: 11.096170425415039\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.0372, -1.0126,  0.0217, -0.6334], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.0217,  0.0217,  0.0217, -0.8894, -0.8352,  0.0217,  0.0510,  0.0449,\n",
      "        -0.0464,  0.0217], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9805, -0.9805, -0.9805, -0.9805, -1.0022, -0.9805, -0.9541, -0.9596,\n",
      "        -1.0417, -0.9805], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.8067812919616699\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.0476, -1.0263,  0.0183, -0.6043], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.5239, -0.9901, -0.8769, -0.9482,  0.0183, -0.1180, -0.9901, -0.6043,\n",
      "        -0.4446,  0.0183], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.8604, -1.1148, -0.8987, -1.0026, -0.9836, -0.9836, -1.1148, -0.9589,\n",
      "        -0.9505, -0.9836], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3286004662513733\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-0.0652, -1.0646,  0.0560, -0.5166,  0.0277,  0.2206, -0.9695,  0.0277,\n",
      "         0.2206,  0.0277], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.0586, -1.1154, -0.9496, -0.8475, -0.9751, -0.8015, -1.0561, -0.9751,\n",
      "        -0.8015, -0.9751], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.7223635911941528\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.0035, -0.9948, -0.0752, -0.5075], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.0743,  0.0304, -0.9502,  0.0304,  0.0611, -0.1823,  0.0304, -1.0845,\n",
      "        -0.4438,  0.0304], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9331, -0.9727, -0.8359, -0.9727, -0.9450, 10.0000, -0.9727, -1.1279,\n",
      "        -0.9331, -0.9727], grad_fn=<AddBackward0>)\n",
      "LOSS: 10.998529434204102\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-0.9742,  0.0319, -0.8815,  0.0646, -1.2399,  0.0319, -0.9970,  0.0656,\n",
      "         0.0085,  0.0319], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9713, -0.9713, -0.9419, -0.8781, -1.1232, -0.9713, -0.9924, -0.9410,\n",
      "        -1.1384, -0.9713], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.625363290309906\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9374, -0.9350, -0.0817, -0.4475], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.4723,  0.0012,  0.0266, -0.5727,  0.0953, -0.5727, -1.0688,  0.0266,\n",
      "         0.0266,  0.0548], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.8249, -1.1496, -0.9761, -0.9989, -0.9142, -0.9989, -1.1307, -0.9761,\n",
      "        -0.9761, -0.8730], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6711794137954712\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.1384, -1.1537,  0.2407, -0.4651], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.0685, -0.9483,  0.0278, -0.0855,  0.0278,  0.0278, -1.0086,  0.0278,\n",
      "         0.0278, -1.0154], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9384, -0.8583, -0.9750, -0.7834, -0.9750, -0.9750, -0.9750, -0.9750,\n",
      "        -0.9750, -1.0009], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6538093090057373\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.1511, -1.1739,  0.2434, -0.4581], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.0650, -1.0235, -1.0638, -0.0106, -1.2311,  0.0183, -0.5606, -1.1111,\n",
      "        -0.9825,  0.0325], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9415, -1.0095, -1.0906, -1.1610, -1.1448, -0.9835, -1.0095, -1.1610,\n",
      "        -1.0095, -0.8514], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4334465563297272\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.1630, -1.1938,  0.2474, -0.4547], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.0110, -0.5618, -0.9589, -1.1178, -0.9819, -0.1721,  0.0110,  0.0110,\n",
      "        -0.9045,  0.0110], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9901, -1.0187, -0.8402, -1.1654, -1.1654, -0.5355, -0.9901, -0.9901,\n",
      "        -0.9862, -0.9901], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.44062671065330505\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.1764, -1.2179,  0.2532, -0.4555], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.0032, -0.4556, -1.1343,  0.0032,  0.0032,  0.0032, -0.1255,  0.0032,\n",
      "        -0.9951, -1.0419], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9971, -0.9425, -1.1664, -0.9971, -0.9971, -0.9971, -0.7721, -0.9971,\n",
      "        -1.0241, -1.0241], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5660544037818909\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.1896, -1.2420,  0.2638, -0.4612], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.4592,  0.1283,  0.2029,  0.2318, -1.0616, -0.4592, -0.1911, -0.1911,\n",
      "        -0.0081, -0.9625], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9385, -0.8845, -0.9385, -0.7914, -1.0073, -0.9385, -1.0073, -1.0073,\n",
      "        -1.0073, -1.0303], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.617323100566864\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2036, -1.2653,  0.2811, -0.4775], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.0890, -0.6025, -0.1778, -0.9352, -1.1570, -0.0058, -0.0058, -0.0191,\n",
      "        -0.4783, -0.0362], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9199, -1.0326, -1.1600, -1.0172, -1.1779, -1.0052, -1.0052, -0.8077,\n",
      "        -0.9199, -1.1779], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6292800307273865\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2205, -1.2888,  0.3016, -0.5014], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.0779e+00, -9.2474e-04, -1.9705e-01, -9.2474e-04, -1.8872e-01,\n",
      "        -1.0779e+00, -1.0753e+00,  1.1392e-01, -9.2476e-04,  1.1392e-01],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.0343, -1.0008, -1.0008, -1.0008, -1.1698, -1.0343, -0.7670, -0.8975,\n",
      "        -1.0008, -0.8975], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6752815246582031\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-0.0064, -0.0064, -0.2098, -0.0064, -0.0064,  0.2578,  0.3082, -0.2098,\n",
      "        -0.5358, -0.0064], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.0058, -1.0058, -1.0058, -1.0058, -1.0058, -0.7680, -0.7226, -1.0058,\n",
      "        -0.8883, -1.0058], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.8499820828437805\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2311, -1.3237,  0.3093, -0.5505], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.0906, -0.0124, -0.0650, -1.1994, -1.0906, -0.0124, -0.0124, -0.2353,\n",
      "         0.0563, -0.0655], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.0585, -1.0111, -1.2118, -1.2118, -1.0585, -1.0111, -1.0111, -1.0111,\n",
      "        10.0000, -0.7814], grad_fn=<AddBackward0>)\n",
      "LOSS: 10.430058479309082\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2325, -1.3391,  0.3095, -0.5403], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.0872, -0.0164, -0.4387,  0.3095, -0.0164, -0.2586, -0.6971, -0.0164,\n",
      "         0.1526, -0.2586], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.2327, -1.0148, -0.8627, -0.7215, -1.0148, -1.0148, -1.0785, -1.0148,\n",
      "        -0.8627, -1.0148], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.7864786386489868\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2374, -1.3552,  0.3086, -0.5414], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.0138, -0.7138, -0.0138, -0.0138, -0.1133, -1.2973, -0.7138, -1.2335,\n",
      "        -0.0138, -0.4413], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.0124, -1.1023, -1.0124, -1.0124, -0.7686, -1.2731, -1.1023, -1.2156,\n",
      "        -1.0124, -0.8601], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4896584153175354\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2452, -1.3713,  0.3137, -0.5565], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.1375, -0.9600, -1.0581, -0.0090, -0.0090,  0.3137,  0.1628,  0.3137,\n",
      "        -0.3295, -0.0090], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.7526, -0.7526, -1.1168, -1.0081, -1.0081, -0.7176, -0.8535, -0.7176,\n",
      "        -1.2966, -1.0081], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.7514902949333191\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2493, -1.3867,  0.3035, -0.5732], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.0188, -0.3319,  0.3035, -0.3696, -0.0188, -0.0188,  0.3035, -0.3319,\n",
      "         0.3035,  0.3035], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.0169, -1.0169, -0.7269, -1.3327, -1.0169, -1.0169, -0.7269, -1.0169,\n",
      "        -0.7269, -0.7269], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.9101211428642273\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2559, -1.4010,  0.2776, -0.5929], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.2776, -1.1199, -0.9473, -0.0336, -0.0336, -1.2781, -0.2775, -0.0336,\n",
      "        -0.0336, -0.0336], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.7502, -1.1709, -0.7403, -1.0302, -1.0302, -1.3193, -0.8791, -1.0302,\n",
      "        -1.0302, -1.0302], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6431840658187866\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2607, -1.4150,  0.2464, -0.6196], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.2464,  0.1541, -0.4750, -0.2863, -0.6350, -0.0573, -1.1383,  0.2464,\n",
      "        -0.0573, -0.0573], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.7782, -0.8297, -1.4275, -0.8885, -0.8885, -1.0516, -1.3828, -0.7782,\n",
      "        -1.0516, -1.0516], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.7427345514297485\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-0.0878, -0.0878, -0.6664,  0.2091, -0.2641, -0.0878, -0.4689, -1.1213,\n",
      "         0.2091, -0.0878], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.0790, -1.0790, -0.8959, -0.8118, -1.4202, -1.0790, -1.0790, -1.2376,\n",
      "        -0.8118, -1.0790], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.7789825201034546\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([ 0.1511,  0.0992, -0.1358,  0.1511, -1.3477, -0.1358, -1.3293,  0.0936,\n",
      "        -0.1358, -0.9093], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.8640, -0.9107, -1.1222, -0.8640, -1.3957, -1.1222, -1.4258, -0.9158,\n",
      "        -1.1222, -1.2907], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.7175778746604919\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-0.1904, -0.5797, -0.1904, -0.1904, -1.3560, -1.1773, -1.3845, -0.6057,\n",
      "        -0.5797, -0.9461], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.1714, -1.1714, -1.1714, -1.1714, -1.4712, -1.4410, -1.4410, -0.8028,\n",
      "        -1.1714, -1.3424], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3868853449821472\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3224, -1.4401, -0.7376, -0.7150], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.0353, -1.2213, -0.6456, -0.4251, -0.4550,  0.0487,  0.0487, -1.2230,\n",
      "        -0.5636, -0.2243], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9682, -1.4748, -0.7969, -1.4637, -0.8222, -0.9562, -0.9562, -1.2019,\n",
      "        -0.5760, -1.2019], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5283488035202026\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-0.2754, -0.8154, -0.6178, -0.2754, -1.2772, -0.2754, -1.1887, -1.2583,\n",
      "        -0.8154, -0.0314], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.2479, -1.6742, -0.6116, -1.2479, -1.5192, -1.2479, -1.0512, -1.2479,\n",
      "        -1.6742, -1.0283], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5383481383323669\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-0.3555, -0.1232, -1.1446, -0.1232, -0.6371, -1.2948, -0.6442, -0.1232,\n",
      "        -0.3555, -0.6442], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3200, -1.1109, -1.3937, -1.1109, -1.6622, -1.3200, -1.1109, -1.1109,\n",
      "        -1.3200, -1.1109], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6336086988449097\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-0.2016, -0.7280, -0.4370, -0.4370, -0.4370,  0.0737, -0.4370, -1.6314,\n",
      "        -0.4370, -0.7996], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.1815, -1.1815, -1.3933, -1.3933, -1.3933, -1.0773, -1.3933, -1.6321,\n",
      "        -1.3933, -1.0773], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.714007556438446\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3882, -1.6588, -1.0791, -0.6806], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.0791e+00, -5.3147e-01, -2.8082e-01, -5.3147e-01, -8.3253e-01,\n",
      "        -9.3074e-01, -2.8082e-01, -5.3147e-01,  5.9605e-08, -8.2175e-01],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.6125, -1.4783, -1.2527, -1.4783, -1.2735, -1.4783, -1.2527, -1.4783,\n",
      "        -0.7669, -1.2527], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6131519675254822\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-0.6271, -0.6271, -1.5528, -1.2037, -1.7442, -1.0197, -1.7094, -0.6271,\n",
      "        -0.3628, -1.1738], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.5644, -1.5644, -1.6360, -1.4119, -1.5921, -1.5644, -1.6360, -1.5644,\n",
      "        -1.3266, -1.5921], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.41147422790527344\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-1.7549, -1.1184, -1.5042, -1.2764, -1.1184, -1.1184, -0.4520, -0.7240,\n",
      "        -0.7240, -1.3122], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.6545, -1.6516, -1.4476, -1.5718, -1.6516, -1.6516, -1.4068, -1.6516,\n",
      "        -1.6516, -1.4073], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3594861626625061\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-0.5351, -1.7264, -0.7983, -1.3805, -1.2633, -1.2317, -0.7983, -0.7983,\n",
      "        -0.7983, -0.7983], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4816, -2.0081, -1.7184, -1.5519, -1.4159, -1.7184, -1.7184, -1.7184,\n",
      "        -1.7184, -1.7184], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5498401522636414\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-0.8777, -0.4378, -1.4855, -1.1094, -0.8777, -1.4804, -0.8777, -1.4901,\n",
      "        -0.6255, -0.8777], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7691, -1.2761, -1.7691, -1.4642, -1.7691, -1.4642, -1.7691, -1.5331,\n",
      "        -1.5629, -1.7691], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4968850612640381\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.1382, -1.6458, -0.4166, -0.4255], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.0248, -1.0248, -1.0248, -0.7663, -1.4919, -1.9104, -0.7937, -1.0248,\n",
      "        -1.6646, -1.6202], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7144, -1.7144, -1.7144, -1.6897, -1.7144, -1.6720, -1.3506, -1.7144,\n",
      "        -1.3991, -1.4935], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3257297873497009\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.1014, -1.5681, -0.5229, -0.3455], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.3669, -1.2173, -1.2173, -0.7312, -1.6967, -1.2173, -0.9426, -1.8990,\n",
      "        -0.7084, -0.3915], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3668, -1.6376, -1.6376, -0.7264, -1.3770, -1.6376, -1.7677, -1.9681,\n",
      "        -1.3109, -1.3523], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2604083716869354\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.1278, -1.5898, -0.6442, -0.2862], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.8139, -1.9061, -1.0170, -1.5481, -1.9061, -1.1452, -1.3903, -0.3486,\n",
      "        -1.1452, -1.4337], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.5568, -1.4181, -1.3528, -1.4039, -1.4181, -1.7069, -1.3328, -1.3138,\n",
      "        -1.7069, -1.5568], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.22568586468696594\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-1.4070, -1.6323, -2.0206, -1.5479, -1.6323, -1.5564, -1.6323, -1.9603,\n",
      "        -1.6323, -1.3483], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3040, -1.4905, -1.3872, -1.2777, -1.4905, -1.3298, -1.4905, -1.4905,\n",
      "        -1.4905, -1.6418], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09235242754220963\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.1560, -1.6119, -0.8427, -0.1914], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.5094, -0.1914, -2.0570, -1.5094, -2.0368, -1.7770, -0.5004, -1.5665,\n",
      "        -1.7770, -1.5094], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.5996, -1.1723, -1.4503, -1.5996, -1.5996, -1.4503, -1.1723, -1.3198,\n",
      "        -1.4503, -1.5996], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.22714188694953918\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.1433, -1.6015, -0.8926, -0.1640], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.3638, -0.1640, -1.8771, -1.9547, -1.8771, -1.8771, -1.8771, -1.0760,\n",
      "        -1.6394, -0.8926], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.5171, -1.1476, -1.4441, -1.5590, -1.4441, -1.4441, -1.4441, -1.1476,\n",
      "        -1.5861, -1.1476], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1970440149307251\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.1295, -1.5774, -0.9249, -0.1512], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.9214, -1.9214, -2.1034, -1.9818, -1.1295, -1.7279, -0.9295, -1.7279,\n",
      "        -2.0112, -0.9249], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4549, -1.4549, -1.3699, -1.3699, -1.2716, -1.5893, -1.3373, -1.5893,\n",
      "        -1.4030, -1.1361], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.19868913292884827\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-2.1148, -0.1550, -1.9064, -1.7595, -0.4323, -0.9320, -1.1139, -0.4850,\n",
      "        -1.9064, -2.1148], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4797, -1.1395, -1.4797, -1.6109, -1.4797, -1.1395, -1.2951, -0.7082,\n",
      "        -1.4797, -1.4797], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3384935259819031\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-1.5279, -1.5654, -1.8508, -1.8607, -2.0538, -0.7128, -1.5654, -1.2739,\n",
      "        -0.9754, -1.7082], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3890, -1.4240, -1.8779, -1.5128, -1.5128, -1.4214, -1.4240, -1.3206,\n",
      "        -1.3890, -1.6415], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11534900963306427\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.0838, -1.4386, -0.9115, -0.1858], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.7402, -1.8074, -1.7402, -0.4965, -1.8074, -1.1311, -1.7402, -1.9282,\n",
      "        -1.8074, -1.4476], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.6781, -1.5531, -1.6781, -1.5531, -1.5531, -1.1673, -1.6781, -1.4468,\n",
      "        -1.5531, -1.4208], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15557558834552765\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.0617, -1.3905, -0.8854, -0.2048], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.7405, -1.7372, -0.8854, -1.7372, -1.7372, -0.2048, -0.8854, -1.6221,\n",
      "        -0.2048, -1.7372], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7421, -1.5876, -1.1843, -1.5876, -1.5876, -1.1843, -1.1843, -1.4784,\n",
      "        -1.1843, -1.5876], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.22077438235282898\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-0.4706, -1.6698, -1.6698, -1.6700, -1.5369, -1.4008, -1.3905, -1.0830,\n",
      "        -1.6700, -0.5653], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4236, -1.7529, -1.7529, -1.6196, -1.5088, -1.4947, -1.9877, -1.2034,\n",
      "        -1.6196, -1.6196], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.24193808436393738\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.0528, -1.3176, -0.8666, -0.2394], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.6098, -1.6467, -0.2394, -1.7804, -0.7076, -1.6098, -1.6098, -1.6098,\n",
      "        -1.2577, -1.4100], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.6369, -1.7793, -1.2155, -1.6369, -1.2155, -1.6369, -1.6369, -1.6369,\n",
      "        -1.5096, -2.0070], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1671583205461502\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.0724, -1.2935, -0.8603, -0.2437], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.3680, -0.8603, -1.2935, -1.7121, -0.2437, -1.1260, -1.2586, -1.0724,\n",
      "         0.2457, -1.7334], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.5262, -1.2193, -1.6528, -1.5670, -1.2193, -1.5257, -1.5257, -1.4739,\n",
      "        10.0000, -1.6528], grad_fn=<AddBackward0>)\n",
      "LOSS: 9.680137634277344\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-1.9056, -1.5152, -1.5152, -1.6091, -1.5152, -1.5152, -0.2196, -1.6705,\n",
      "        -1.2743, -0.5049], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7840, -1.6379, -1.6379, -1.7840, -1.6379, -1.6379, -1.1976, -1.5645,\n",
      "        -1.6771, -1.4544], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.21371273696422577\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-1.3418, -1.5868, -1.4818, -1.6036, -1.4818, -0.1970, -0.7335, -0.4845,\n",
      "        -0.3460, -1.6489], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.5611, -1.9940, -1.6215, -1.7691, -1.6215, -1.1773, -0.7277, -1.4361,\n",
      "        -0.7277, -1.6215], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.22932367026805878\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4258, -1.3299, -1.4610, -0.6698], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.6232, -1.6232, -1.6232, -1.6172, -1.4610, -1.6090, -1.0844, -0.9410,\n",
      "        -1.6236, -0.1747], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.6028, -1.6028, -1.6028, -1.5533, -1.6028, -1.7508, -1.4611, -1.5533,\n",
      "        -1.5983, -1.1572], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15284590423107147\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-1.6172, -0.6960, -0.3312, -1.0759, -1.3209, -1.6011, -1.4451, -1.4451,\n",
      "        -1.0549, -1.4848], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7372, -0.6879, -0.6879, -1.4397, -1.5432, -1.5879, -1.5879, -1.5879,\n",
      "        -1.1417, -1.6264], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.039195891469717026\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4850, -1.3251, -1.4388, -0.6340], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.2891, -1.0702, -0.9072, -0.1362, -1.8857, -1.4388, -1.3643, -0.1362,\n",
      "        -1.6332, -0.1362], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3906, -1.4136, -1.1226, -1.1226, -1.8517, -1.5706, -1.5706, -1.1226,\n",
      "        -1.7195, -1.1226], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3161938786506653\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-0.1201, -1.6544, -0.1201, -1.3985, -0.1201, -1.4377, -1.6544, -1.5624,\n",
      "        -0.6131, -1.4377], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.1081, -1.7013, -1.1081, -1.5518, -1.1081, -1.5518, -1.7013, -1.5172,\n",
      "        -1.1081, -1.5518], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3229372203350067\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-0.1041, -1.1100, -1.4377, -1.4434, -0.5969, -1.5074, -1.3524, -1.4112,\n",
      "        -1.4434, -1.4112], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.0937, -1.0937, -1.5372, -1.5372, -1.0937, -1.9637, -1.5070, -1.3604,\n",
      "        -1.5372, -1.3604], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1491093635559082\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-1.7051, -1.4546, -1.6614, -1.7051, -1.3415, -1.4546, -1.7051, -1.4546,\n",
      "        -0.3439, -0.5497], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.6665, -1.5234, -1.5533, -1.6665, -1.3451, -1.5234, -1.6665, -1.5234,\n",
      "        -0.6143, -1.5234], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10516055673360825\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4983, -1.5367, -1.0110, -0.0560], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.9485, -0.3650, -0.5592, -1.6025, -0.0560, -0.0560, -0.5592, -1.4629,\n",
      "        -0.0560, -1.4629], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.6468, -1.3285, -1.0504, -1.3156, -1.0504, -1.0504, -1.0504, -1.5033,\n",
      "        -1.0504, -1.5033], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4553914964199066\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-1.4755, -1.4755, -1.4507, -1.4755, -1.4755, -0.0345, -1.4755, -1.5760,\n",
      "        -1.7177, -1.5241], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4998, -1.4998, -1.4953, -1.4998, -1.4998, -1.0310, -1.4998, -1.4998,\n",
      "        -1.5280, -1.3226], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10805020481348038\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.6373, -1.6064, -1.1922, -0.3604], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.4884, -0.3637, -1.7246, -1.0342, -1.4760, -0.0181, -1.7742, -0.0181,\n",
      "         0.4746, -1.5696], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4997, -0.5728, -1.5269, -1.0163, -1.5001, -1.0163, -1.6459, -1.0163,\n",
      "        10.0000, -1.5001], grad_fn=<AddBackward0>)\n",
      "LOSS: 9.283048629760742\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([ 0.0206, -1.6471, -1.0377, -1.3364, -1.4948, -1.4948, -1.7868, -1.6520,\n",
      "        -1.6176, -1.6374], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9815, -1.9437, -0.9815, -1.8441, -1.4820, -1.4820, -1.6272, -1.3585,\n",
      "        -1.2666, -1.4820], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16121898591518402\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.5350, -1.6713, -1.0367,  0.0538], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.2015, -1.6014, -1.7921, -0.3181, -1.4964, -1.7921, -1.4964, -1.2078,\n",
      "         0.0538, -1.4964], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.9328, -1.2432, -1.6102, -1.2863, -1.4658, -1.6102, -1.4658, -0.9516,\n",
      "        -0.9516, -1.4658], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.22832706570625305\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-1.3587, -1.1973, -2.1708, -0.3076, -1.6135, -0.6738, -0.3076, -1.5633,\n",
      "        -1.3496, -1.4892], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.2306, -0.9350, -1.9331, -1.2768, -1.3347, -1.4876, -1.2768, -1.4843,\n",
      "        -1.2924, -1.4592], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.27711087465286255\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-1.7818, -1.9237, -0.5468, -1.5635,  0.0758, -1.5715, -1.3543, -1.0237,\n",
      "        -0.3401, -0.5221], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.6294, -1.6294, -1.4613, -1.4921, -0.9318, -1.4921, -1.4699, -0.9318,\n",
      "        -0.4391, -0.4391], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.20112740993499756\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.5311, -1.6964, -1.1610, -0.3129], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.0750, -1.7106, -1.7786, -1.7786, -1.7786, -2.0215, -2.0664, -1.4588,\n",
      "        -0.3129, -1.4453], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9325, -1.4593, -1.6586, -1.6586, -1.6586, -1.8258, -1.9572, -1.4593,\n",
      "        -1.2816, -1.2816], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.21368511021137238\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4826, -1.6821, -1.1424, -0.3337], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.4387,  0.0661, -0.5944, -1.4878,  0.0661, -1.6953, -0.5205, -1.4387,\n",
      "        -1.4387, -1.7622], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4684, -0.9405, -1.4684, -1.4464, -0.9405, -1.4684, -0.9405, -1.4684,\n",
      "        -1.4684, -1.6945], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.30273550748825073\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4376, -1.6630, -1.1260, -0.3559], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.4215, -1.4215,  0.0586, -0.5325, -1.4215, -1.4215, -1.7474, -1.4215,\n",
      "        -1.7474, -1.5679], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4792, -1.4792, -0.9473, -0.9473, -1.4792, -1.4792, -1.7311, -1.4792,\n",
      "        -1.7311, -1.5692], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12010294198989868\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3984, -1.6469, -1.1136, -0.3763], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.7374, -1.1136, -1.7374, -1.5770, -1.5106,  0.0557, -1.4121, -1.4121,\n",
      "        -1.4121, -1.7374], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7656, -0.9498, -1.7656, -1.6009, -1.4927, -0.9498, -1.4927, -1.4927,\n",
      "        -1.4927, -1.7656], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10607800632715225\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3609, -1.6305, -1.1002, -0.3917], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.5369, -1.1295,  0.0494, -1.7313, -1.4063, -1.4063, -0.3917,  0.0494,\n",
      "        -1.4063, -0.3026], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.6278, -1.3323, -0.9555, -1.7961, -1.5040, -1.5040, -1.3526, -0.9555,\n",
      "        -1.5040, -0.3676], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.30294641852378845\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-1.7325, -1.4071,  0.0408, -1.5389, -1.3072, -1.4071, -1.4692, -1.2825,\n",
      "         0.0408, -0.7227], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.8229, -1.5124, -0.9632, -1.6505, -1.4424, -1.5124, -1.9025, -1.2895,\n",
      "        -0.9632, -1.5124], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.28888359665870667\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2196, -1.6261, -0.9492,  0.0355], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.1740, -1.7393, -0.9492, -1.7393, -1.5082, -1.4092, -1.1863, -1.5587,\n",
      "         0.0355, -1.7393], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.2935, -1.8378, -0.9681, -1.8378, -1.9008, -1.5063, -1.2935, -1.6768,\n",
      "        -0.9681, -1.8378], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12399061024188995\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2092, -1.6430, -0.9577,  0.0342], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.5497, -0.3072,  0.0342, -0.4125, -1.7539, -0.3072, -1.7539, -1.4172,\n",
      "        -1.2092, -1.4172], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9692, -0.3320, -0.9692, -1.3712, -1.8418, -0.3320, -1.8418, -1.4948,\n",
      "        -1.3712, -1.4948], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.21569831669330597\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2051, -1.6631, -0.9681,  0.0377], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.6975, -0.9698, -1.7743, -1.6975, -0.4078, -1.4316, -1.7743, -1.4316,\n",
      "        -1.4316, -1.4316], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7196, -1.7196, -1.8439, -1.7196, -1.3670, -1.4887, -1.8439, -1.4887,\n",
      "        -1.4887, -1.4887], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15059544146060944\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-1.2473, -1.5502, -1.4496, -0.5374, -1.7280,  0.0390, -1.4496, -1.4496,\n",
      "        -1.4496, -0.9823], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.2829, -1.4837, -1.4837, -0.9649, -1.7325, -0.9649, -1.4837, -1.4837,\n",
      "        -1.4837, -0.9649], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12012074887752533\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2932, -1.7129, -1.1357, -0.4148], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.8261, -0.9928, -1.4711, -1.8261, -1.8222,  0.7823, -1.6965, -0.5389,\n",
      "        -1.8261, -1.4711], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.8535, -0.9604, -1.4850, -1.8535, -1.8535, 10.0000, -1.7459, -0.9604,\n",
      "        -1.8535, -1.4850], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.515159606933594\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-1.4944, -1.2031, -1.4944, -1.2105, -1.8523, -1.7325, -1.7474, -1.7325,\n",
      "        -0.9972, -1.2514], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4662, -1.3511, -1.4662, -1.2572, -1.8360, -1.4662, -1.8717, -1.4662,\n",
      "        -0.9222, -1.2572], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.018883123993873596\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-0.9577, -1.5177,  0.1230, -0.3420, -1.8530, -0.3659, -1.5177, -1.5177,\n",
      "        -0.3518, -1.1051], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7548, -1.4574, -0.8893, -0.2012, -1.8237, -1.3293, -1.4574, -1.4574,\n",
      "        -0.2012, -1.3086], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2683985233306885\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-1.7471, -1.5363, -0.9099, -1.5363,  0.1500, -1.8923, -1.5363, -1.2007,\n",
      "        -1.7150, -1.5363], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4535, -1.4535, -1.2908, -1.4535, -0.8650, -1.8189, -1.4535, -1.3194,\n",
      "        -1.4586, -1.4535], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1374066025018692\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-1.8735, -0.3520, -1.4807, -1.5490, -1.9076, -1.9640, -1.1651, -0.3520,\n",
      "        -1.9076, -1.5490], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.1071, -1.3168, -1.3844, -1.4628, -1.8358, -1.8944, -0.8523, -1.3168,\n",
      "        -1.8358, -1.4628], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2053401917219162\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3898, -1.8411, -1.5542, -0.5303], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.5542, -1.5408, -1.5542,  0.1666,  0.1666, -0.3561, -0.9748,  0.1666,\n",
      "        -0.3385, -1.8411], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4773, -1.2140, -1.4773, -0.8500, -0.8500, -1.3205, -0.8500, -0.8500,\n",
      "        -0.0999, -1.7667], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.42275047302246094\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-1.8480,  0.1519, -1.5560,  0.1519, -1.7196, -0.5573,  0.1519, -1.6053,\n",
      "        -1.9760, -1.3993], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.9493, -0.8633, -1.5015, -0.8633, -1.5015, -0.8633, -0.8633, -1.5015,\n",
      "        -1.9493, -1.4003], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.32577189803123474\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.1935, -1.6976, -0.9572,  0.1321], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.1366, -0.9572, -1.5625, -1.9172, -1.9172, -1.5625, -1.8823, -1.9730,\n",
      "        -1.7636, -0.5979], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.8811, -0.8811, -1.5381, -1.9324, -1.9324, -1.5381, -2.1538, -1.9815,\n",
      "        -1.4173, -0.8811], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03466619923710823\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-1.8795, -1.8809, -1.8795, -1.5680, -1.8418, -1.1869, -1.1245, -1.5680,\n",
      "        -0.4183, -1.4207], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.9718, -2.1687, -1.9718, -1.5791, -1.8201, -1.3765, -1.8201, -1.5791,\n",
      "        -1.3765, -1.4408], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1538933962583542\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.1893, -1.6619, -0.9392,  0.0923], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.1710, -0.2944, -1.5758,  0.0923, -0.9392,  0.0923, -1.9152, -1.6619,\n",
      "        -1.5758, -1.3170], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.8292, -0.0372, -1.6138, -0.9169, -0.9169, -0.9169, -2.0003, -1.6138,\n",
      "        -1.6138, -1.2919], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.25499433279037476\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.1922, -1.6509, -0.9368,  0.0651], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.2309, -1.5849, -0.4511, -0.9368, -1.3209, -1.6014, -0.4511, -1.7916,\n",
      "        -1.6509,  0.0651], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.8338, -1.6508, -1.4060, -0.9414, -1.3160, -1.6508, -1.4060, -2.1888,\n",
      "        -1.6508, -0.9414], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3364812433719635\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2078, -1.6457, -0.9405,  0.0340], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.0340,  0.0340, -0.7603, -1.9266, -1.5031, -1.6457, -1.5956, -1.9266,\n",
      "        -1.5956, -1.5956], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9694, -0.9694, -0.9694, -2.0654, -1.3399, -1.6843, -1.6843, -2.0654,\n",
      "        -1.6843, -1.6843], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.21475854516029358\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-1.6140, -0.9507, -1.2797, -1.9452, -1.1502, -1.9452, -1.6140, -1.6140,\n",
      "        -1.3813, -1.6140], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7154, -0.9955, -1.3661, -2.0921, -1.3515, -2.0921, -1.7154, -1.7154,\n",
      "        -1.3661, -1.7154], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.013447629287838936\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-0.2660, -1.8970, -2.0131, -1.6399, -0.0137, -0.0137, -1.9739, -1.6399,\n",
      "        -0.0137, -0.8151], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.0039, -2.2009, -2.1027, -1.7336, -1.0123, -1.0123, -2.1027, -1.7336,\n",
      "        -1.0123, -1.0123], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3233966529369354\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2761, -1.6498, -0.9869, -0.0339], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.8315, -0.0339, -0.8338, -1.1146, -1.1146, -1.1146, -0.0339, -1.8911,\n",
      "        -2.0587, -1.5758], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.8190, -1.0305, -1.0305, -1.0305, -1.0305, -1.0305, -1.0305, -1.8190,\n",
      "        -2.1071, -1.5311], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.20560555160045624\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3046, -1.6539, -1.0081, -0.0626], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.6989, -1.6776, -0.0626, -1.6989, -1.6539, -1.6742, -0.9084, -0.0626,\n",
      "        -1.4848, -0.0626], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7731, -1.5434, -1.0564, -1.7731, -1.7731, -1.7731, -1.7731, -1.0564,\n",
      "        -1.4133, -1.0564], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.37683385610580444\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-0.5582, -1.9139, -1.7217, -0.8677, -1.3353, -2.0629, -1.7217, -1.7217,\n",
      "        -0.8677, -1.2481], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.5024, -1.8279, -1.7809, -1.0865, -1.5024, -2.1233, -1.7809, -1.7809,\n",
      "        -1.0865, -1.2508], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1036660447716713\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-2.0947, -2.0947, -1.0610, -1.8887, -2.0947, -1.7494, -1.9239, -1.7494,\n",
      "        -2.1578, -1.1378], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.1296, -2.1296, -1.1058, -1.8394, -2.1296, -1.7934, -1.8394, -1.7934,\n",
      "        -2.3843, -1.1058], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.007143767084926367\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3964, -1.6861, -1.0862, -0.1350], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.6861, -2.1273, -1.6861, -1.0862, -1.7766, -1.9069, -1.4557, -1.7511,\n",
      "        -0.2570, -1.7766], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.8011, -2.1304, -1.8011, -1.1215, -1.8011, -2.4053, -1.4595, -1.8011,\n",
      "         0.0248, -1.8011], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03592492640018463\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-1.8005, -1.7009, -1.8005, -1.7009, -0.5541, -1.1542, -0.1458, -0.1458,\n",
      "        -2.1578, -0.5541], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7974, -1.5898, -1.7974, -1.5898, -1.4987, -1.1313, -1.1313, -1.1313,\n",
      "        -2.1207, -1.4987], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.37531954050064087\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4620, -1.7522, -1.1387, -0.1678], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.5618, -0.1678, -1.8213, -2.1840, -1.8024, -1.1387, -1.7522, -1.7635,\n",
      "        -1.4620, -1.8213], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.5056, -1.1511, -1.8012, -2.1217, -1.8012, -1.1511, -1.8012, -1.6025,\n",
      "        -1.5056, -1.8012], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.18925681710243225\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4919, -1.7850, -1.1661, -0.1945], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.0346, -1.1753, -0.1945, -1.8384, -1.8384, -2.3166, -0.1945, -1.8384,\n",
      "        -1.3196, -2.2610], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.4260, -1.1751, -1.1751, -1.8075, -1.8075, -2.1279, -1.1751, -1.8075,\n",
      "        -1.3684, -2.1636], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.21264442801475525\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.5194, -1.8253, -1.1934, -0.2264], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.5890, -1.7496, -2.0311, -1.7996, -0.2264, -1.8532, -1.5194, -0.5890,\n",
      "        -1.6729, -1.8532], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.5301, -1.6353, -1.8945, -1.6353, -1.2037, -1.8172, -1.5301, -1.5301,\n",
      "        -1.4935, -1.8172], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2820214629173279\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.5366, -1.8611, -1.2184, -0.2664], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.2466, -1.5366, -1.8655, -1.1926, -1.8613, -1.2184, -1.8613, -1.7615,\n",
      "        -2.1077, -1.8002], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.1665, -1.5565, -1.8326, -1.2398, -1.8326, -1.2398, -1.8326, -1.6606,\n",
      "        -1.9158, -1.6606], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.007871326990425587\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.5463, -1.8837, -1.2387, -0.3056], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.8632, -0.3056, -0.3056, -1.8632, -1.5463, -1.5014, -0.3056, -1.6158,\n",
      "        -1.8632, -0.6477], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.8496, -1.2751, -1.2751, -1.8496, -1.5829, -1.2774, -1.2751, -1.9408,\n",
      "        -1.8496, -1.5829], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.38517680764198303\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.5557, -1.9077, -1.2652, -0.3581], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.9077, -0.3581, -1.8672, -1.8672, -0.3581, -1.8672, -1.8672, -1.2652,\n",
      "        -0.9716, -1.8672], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.8745, -1.3223, -1.8745, -1.8745, -1.3223, -1.8745, -1.8745, -1.3223,\n",
      "        -1.3223, -1.8745], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.19869130849838257\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-1.8773, -1.2207, -1.8773, -2.3602, -1.6596, -0.7292, -1.7707, -0.4057,\n",
      "        -0.7292, -1.9284], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.9056, -1.3651, -1.9056, -2.2720, -1.5949, -1.6563, -1.7564, -1.3651,\n",
      "        -1.6563, -1.9056], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2674619257450104\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-1.1348, -1.4757, -0.2284, -1.6850, -1.8896, -2.1235, -1.7630, -1.0430,\n",
      "        -1.3137, -1.8896], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.9387, -1.3644, -0.0082, -1.6271, -1.9387, -2.0213, -1.7904, -1.4081,\n",
      "        -1.4081, -1.9387], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08687973022460938\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-1.9494, -1.8982, -2.3272, -1.5682, -1.3165, -1.5682, -1.2645, -1.7526,\n",
      "        -0.2235, -0.4878], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.9653, -1.9653, -2.3595, -1.7502, -1.3965, -1.7502, -1.4390, -1.8243,\n",
      "        -0.0177, -1.4390], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10611865669488907\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.3044, -2.5609, -1.6459, -1.1983], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.5161, -1.4112, -1.5864, -1.9108, -0.5161, -0.5161, -0.5161, -0.8620,\n",
      "        -1.2137, -1.2831], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4645, -1.7758, -1.7758, -1.9843, -1.4645, -1.4645, -1.4645, -1.7758,\n",
      "        -1.9843, -1.4645], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5233768224716187\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.6111, -2.1759, -1.9954, -0.9746], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.3814, -2.2235, -2.2864, -0.2208, -2.3814, -1.6172, -2.3814, -1.3490,\n",
      "        -2.0144, -1.9261], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.4141, -2.1330, -2.2962, -0.0334, -2.4141, -1.7853, -2.4141, -1.4401,\n",
      "        -1.9935, -1.9935], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.008816367015242577\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.5376, -1.8482, -1.3173, -0.8720], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.9417, -1.4184, -1.7869, -1.3173, -1.9417, -1.9417, -0.5758, -2.0469,\n",
      "        -2.4130, -1.6363], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.9994, -1.5182, -1.8993, -1.5182, -1.9994, -1.9994, -1.5182, -1.9994,\n",
      "        -2.4320, -1.7225], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09711552411317825\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.5638, -1.8669, -1.3432, -0.8736], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.3114, -1.9642, -1.9642, -1.6915, -2.4652, -1.4540, -0.5941, -0.5941,\n",
      "        -2.4853, -0.8736], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.1972, -2.0016, -2.0016, -1.7863, -2.6440, -1.5347, -1.5347, -1.5347,\n",
      "        -2.4432, -1.7863], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.26673609018325806\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.5923, -1.8917, -1.3692, -0.8772], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.7360, -0.6186, -1.9874, -1.8422, -1.7360, -0.6186, -1.4671, -0.6186,\n",
      "        -1.9874, -1.7505], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7895, -1.5567, -2.0053, -1.9316, -1.7895, -1.5567, -1.7445, -1.5567,\n",
      "        -2.0053, -1.7445], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.27316194772720337\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-1.2742, -2.1111, -2.0121, -2.0121, -2.5275, -2.5275, -0.6516, -0.5434,\n",
      "        -1.6360, -1.7898], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.9474, -2.0115, -2.0115, -2.0115, -2.4724, -2.4724, -1.5865, -0.0592,\n",
      "        -1.4890, -1.7918], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1599019467830658\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-0.6900, -0.6900, -0.8890, -1.5910, -1.9664, -2.4386, -1.4133, -2.3011,\n",
      "        -2.3011, -0.8890], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.6210, -1.6210, -1.8001, -1.4840, -2.1986, -2.2807, -1.6210, -2.2807,\n",
      "        -2.2807, -1.8001], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.35279494524002075\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.7967, -2.3284, -2.1404, -1.0660], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.6285, -1.1434, -2.5811, -0.9161, -2.3225, -1.1434, -0.7294, -1.8686,\n",
      "        -0.7294, -1.1434], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.6564, -1.6564, -2.5008, -1.8245, -2.3076, -1.6564, -1.6564, -1.7728,\n",
      "        -1.6564, -1.6564], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.335022509098053\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-2.0894, -0.9528, -2.2578, -0.9528, -2.0751, -0.7430, -2.6709, -1.9136,\n",
      "        -2.6096, -0.7430], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.0668, -1.8575, -2.0668, -1.8575, -2.2847, -1.6687, -2.7073, -1.8575,\n",
      "        -2.5298, -1.6687], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.34426349401474\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8507, -2.3536, -2.1746, -1.1102], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.1746, -2.4411, -1.6650, -2.1253, -1.6011, -0.7650, -1.6650, -0.7650,\n",
      "        -2.1253, -1.1102], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.1105, -2.4410, -1.6885, -2.1105, -1.8214, -1.6885, -1.6885, -1.6885,\n",
      "        -2.1105, -1.9036], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.23894107341766357\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-1.5440, -2.2785, -2.1602, -0.7916, -1.9189, -0.7916, -0.7916, -1.9728,\n",
      "        -2.2785, -1.6846], grad_fn=<IndexBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andyyang/Desktop/DS598 Reinforcement Learning/gym-examples/.env/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_TARGET: tensor([-1.7125, -2.1566, -2.1566, -1.7125, -1.8451, -1.7125, -1.7125, -1.9171,\n",
      "        -2.1566, -1.7125], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2611270248889923\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9914, -2.2748, -1.7045, -0.8246], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.1984, -2.2282, -1.0482, -2.6886, -2.1984, -1.0482, -0.8246, -2.1984,\n",
      "        -1.6459, -1.6012], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2091, -2.2091, -1.9434, -2.6462, -2.2091, -1.9434, -1.7421, -2.2091,\n",
      "        -2.2091, -1.7067], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.27753958106040955\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.0147, -2.2752, -1.7266, -0.8618], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.2752, -2.2308, -2.0550, -2.7164, -0.3181, -1.0883, -0.8618, -2.2308,\n",
      "        -1.7266, -1.5998], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2559, -2.2559, -2.1183, -2.6923, -0.1755, -1.9795, -1.7756, -2.2559,\n",
      "        -1.7756, -1.7756], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1689145714044571\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-0.8959, -2.2656, -2.0371, -1.7731, -0.8959, -0.8959, -2.7484, -1.7505,\n",
      "        -2.2840, -2.0371], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.8063, -2.3000, -2.0225, -2.3000, -1.8063, -1.8063, -2.7365, -1.8063,\n",
      "        -2.3000, -2.0225], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.27693021297454834\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9070, -2.0659, -1.6830, -1.1871], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.0619, -0.9387, -0.9387, -0.3460, -0.9387, -2.7827, -2.3149, -2.4647,\n",
      "        -2.7897, -1.1871], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.0683, -1.8448, -1.8448, -0.2170, -1.8448, -2.7831, -2.3394, -2.6532,\n",
      "        -2.9204, -2.0683], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.33098527789115906\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9436, -2.0995, -1.7287, -1.2418], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.8456, -1.5315, -0.9896, -0.9896, -2.8601, -2.3287, -0.9896, -1.7390,\n",
      "        -0.9896, -2.0430], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.7234, -1.8907, -1.8907, -1.8907, -2.6923, -2.3784, -1.8907, -1.6661,\n",
      "        -1.8907, -2.0171], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.34280189871788025\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9668, -2.1300, -1.7722, -1.3045], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.3307, -2.5235, -2.3636, -2.3636, -2.1099, -2.1723, -1.3045, -2.1099,\n",
      "        -2.5002, -2.1099], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.4310, -2.7433, -2.4310, -2.4310, -2.1741, -2.2886, -2.1741, -2.1741,\n",
      "        -2.7938, -2.1741], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09356091171503067\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9985, -2.1856, -1.8151, -1.3464], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.9498, -2.4043, -2.8938, -1.9012, -1.0968, -1.4676, -2.4043, -2.4043,\n",
      "        -1.0968, -2.2171], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.8414, -2.4678, -2.7741, -1.9872, -1.9872, -2.2117, -2.4678, -2.4678,\n",
      "        -1.9872, -2.3209], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2195306122303009\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-2.9379, -2.9379, -1.5180, -2.2667, -2.4607, -1.8386, -1.1461, -3.0030,\n",
      "        -1.3553, -1.1461], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.9637, -2.9637, -2.2198, -2.3662, -2.5053, -2.0315, -2.0315, -2.8910,\n",
      "        -2.2198, -2.0315], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2870589792728424\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.7754, -2.1106, -1.8767, -1.0480], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.8030, -2.3222, -2.4970, -1.1940, -2.9873, -2.9873, -0.4318, -2.9873,\n",
      "        -2.4970, -1.3488], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7882, -2.4226, -2.5437, -2.0746, -3.0002, -3.0002, -0.3240, -3.0002,\n",
      "        -2.5437, -2.2139], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15506257116794586\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.7948, -2.1519, -1.9115, -1.0780], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.8824, -1.2433, -1.7560, -2.3007, -1.2433, -1.3504, -2.5469, -2.3818,\n",
      "        -2.5469, -1.2433], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.1190, -2.1190, -2.1190, -2.4729, -2.1190, -2.2154, -2.5804, -2.4729,\n",
      "        -2.5804, -2.1190], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.32764607667922974\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-2.6255, -2.4508, -3.1031, -2.1150, -2.6255, -2.2686, -2.1167, -1.3692,\n",
      "        -2.2686, -3.1031], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.6217, -2.5179, -3.0729, -2.2837, -2.6217, -2.2322, -2.1556, -2.2322,\n",
      "        -2.2322, -3.0729], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0783873051404953\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-2.1578, -2.1690, -3.1088, -2.6664, -1.9820, -1.3967, -3.1619, -2.6813,\n",
      "        -2.1690, -3.1619], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.3263, -2.1913, -3.1113, -2.6627, -2.1913, -2.2570, -3.1113, -2.6627,\n",
      "        -2.1913, -3.1113], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0818837583065033\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-2.3005, -2.9975, -2.5695, -1.5124], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.3551, -2.7224, -1.4329, -2.3122, -2.9446, -1.3551, -1.3551, -1.8522,\n",
      "        -2.7224, -3.2183], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2196, -2.7016, -2.2896, -2.2896, -3.1315, -2.2196, -2.2196, -2.0500,\n",
      "        -2.7016, -3.1487], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3056291341781616\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-1.4773, -1.0395, -2.3439, -3.2714, -1.3977, -1.3977, -3.2714, -2.7595,\n",
      "        -1.4773, -2.1003], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.3296, -0.4260, -2.3296, -3.1900, -2.2579, -2.2579, -3.1900, -2.8850,\n",
      "        -2.3296, -2.2579], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.33630818128585815\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-2.8284, -3.1345, -2.8527, -3.3162, -3.3162, -2.8284, -2.3804, -2.5496,\n",
      "        -1.5292, -3.2598], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.7823, -3.0696, -2.7823, -3.2261, -3.2261, -2.7823, -2.4341, -3.2130,\n",
      "        -2.3763, -3.2261], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11913721263408661\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.9372, -2.4244, -2.1250, -1.2516], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.3113, -2.8982, -2.8982, -1.4858, -2.8670, -2.3516, -1.9149, -1.2516,\n",
      "        -1.4858, -2.3516], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2746, -2.8280, -2.8280, -2.3372, -2.8280, -2.3372, -1.9291, -2.1265,\n",
      "        -2.3372, -2.3372], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2228509485721588\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.9705, -2.4646, -2.1587, -1.2985], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.6633, -2.3847, -2.3847, -2.4454, -2.9311, -1.5417, -1.5417, -1.6865,\n",
      "        -1.5417, -1.5417], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.4970, -2.3875, -2.3875, -2.5179, -2.8821, -2.3875, -2.3875, -2.1686,\n",
      "        -2.3875, -2.3875], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3796795606613159\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.9919, -2.4845, -2.1727, -1.3296], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.9452, -2.9452, -2.0194, -2.4285, -3.7522, -2.9572, -1.7465, -3.4145,\n",
      "        -1.7465, -2.1632], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.9469, -2.9469, -2.5718, -2.4562, -3.4917, -2.9469, -2.5718, -3.4052,\n",
      "        -2.5718, -2.4562], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.18222030997276306\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-2.2465, -3.4498, -2.3712, -2.4893, -1.6892, -1.6892, -1.6892, -2.9902,\n",
      "        -3.2642, -2.9902], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.5203, -3.4831, -2.5203, -2.6670, -2.5203, -2.5203, -2.5203, -3.0219,\n",
      "        -3.3913, -3.0219], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.22200767695903778\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-3.0490, -1.4068, -3.4941, -2.5781, -1.7500, -1.7500, -3.0490, -3.0490,\n",
      "        -1.7500, -1.9612], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.0942, -2.2661, -3.5525, -2.7304, -2.5750, -2.5750, -3.0942, -3.0942,\n",
      "        -2.5750, -2.0066], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.28151148557662964\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.0867, -2.5571, -2.2537, -1.4510], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.4022, -3.0669, -3.1086, -3.0669, -3.0484, -3.5479, -0.7741, -1.8155,\n",
      "        -3.1143, -3.1143], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.6339, -3.1620, -3.1620, -3.1620, -3.2239, -3.6176, -0.6723, -2.6339,\n",
      "        -3.1620, -3.1620], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0795077532529831\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1303, -2.6024, -2.2972, -1.4869], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.1772, -3.1772, -0.8096, -2.6154, -2.4636, -3.1879, -3.1879, -1.8669,\n",
      "        -2.9898, -3.4704], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2173, -3.2173, -0.7011, -2.6802, -2.6802, -3.2173, -3.2173, -2.6802,\n",
      "        -3.0700, -3.5194], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07381151616573334\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-2.6974, -2.5280, -2.6771, -2.1714, -3.1972, -3.1934, -3.4581, -1.9078,\n",
      "        -3.2502, -2.6104], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.8437, -3.1157, -2.7170, -2.3648, -3.2752, -3.2686, -3.8401, -2.7170,\n",
      "        -3.2686, -2.7170], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12299790233373642\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-3.2848, -3.5954, -1.9390, -1.5313, -3.3544, -3.3544, -2.2504, -2.7532,\n",
      "        -2.2504, -1.9390], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3189, -3.5750, -2.7451, -2.3782, -3.2996, -3.2996, -2.3782, -2.7451,\n",
      "        -2.3782, -2.7451], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.20570893585681915\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3418, -2.7789, -2.4531, -1.5516], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.5376, -2.8286, -0.9260, -3.4321, -2.9610, -1.5516, -2.3418, -3.3825,\n",
      "        -2.1138, -3.8782], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6148, -2.7824, -0.7711, -3.3349, -2.9282, -2.3965, -2.3965, -3.3349,\n",
      "        -2.9024, -3.7923], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1390938013792038\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-2.0309, -3.4905, -3.8213, -2.8811, -2.1617, -3.0674, -2.4283, -3.4905,\n",
      "        -2.1617, -3.4262], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.8278, -3.3802, -3.5930, -3.3802, -2.9455, -2.9664, -2.9455, -3.3802,\n",
      "        -2.9455, -3.3802], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.24693194031715393\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.9875, -3.3273, -2.8449, -2.2121], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.5230, -3.5230, -3.8557, -2.8449, -2.5181, -2.1027, -2.1027, -3.5230,\n",
      "        -3.5230, -2.9300], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4403, -3.4403, -3.6566, -2.8924, -2.4710, -2.8924, -2.8924, -3.4403,\n",
      "        -3.4403, -2.8924], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13202551007270813\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.0634, -3.3519, -2.8655, -2.2771], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.5903, -2.1897, -3.5353, -3.3519, -3.2402, -2.2771, -0.9922, -2.9639,\n",
      "        -3.5353, -2.1897], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.5252, -2.9707, -3.5085, -3.3236, -3.0958, -3.0494, -0.8718, -2.9707,\n",
      "        -3.5085, -2.9707], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1858300417661667\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.1227, -3.3724, -2.8863, -2.3554], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.8863, -1.0062, -2.8863, -2.2930, -3.3724, -2.8721, -3.0019, -3.8206,\n",
      "        -3.5468, -3.5468], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.0637, -0.9147, -3.0637, -3.0637, -3.4050, -3.0637, -3.0637, -3.8856,\n",
      "        -3.5849, -3.5849], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07139725983142853\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-4.1035, -3.3625, -3.3625, -3.5791, -3.9389, -2.4324, -2.4324, -1.8185,\n",
      "        -3.0350, -1.3952], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.1931, -3.2448, -3.2448, -3.6520, -3.8954, -3.1892, -3.1892, -2.6366,\n",
      "        -3.1231, -0.9520], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.20619109272956848\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-2.5136, -2.7396, -3.9418, -1.8782, -3.0738, -2.5136, -2.6506, -2.4237,\n",
      "        -3.0738, -2.4237], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2622, -2.6903, -3.9645, -2.6903, -3.1813, -3.2622, -3.2622, -3.1813,\n",
      "        -3.1813, -3.1813], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3328687846660614\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.2940, -3.5115, -3.0576, -2.5632], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.5115, -3.1418, -4.2349, -4.2349, -3.6654, -3.4411, -3.6804, -3.6650,\n",
      "        -4.2349, -2.4856], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5930, -3.2370, -4.3326, -4.3326, -3.7712, -3.3726, -3.7712, -3.7712,\n",
      "        -4.3326, -3.2370], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06444397568702698\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3367, -3.5764, -3.1407, -2.5982], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.5982, -3.4834, -3.2530, -4.3400, -3.1407, -0.0875, -3.7574, -3.7574,\n",
      "        -3.7574, -3.1146], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3384, -3.3384, -3.6330, -4.3740, -3.2823, 10.0000, -3.8031, -3.8031,\n",
      "        -3.8031, -3.2823], grad_fn=<AddBackward0>)\n",
      "LOSS: 10.252683639526367\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-4.3611, -3.1497, -2.4006, -2.4743, -3.7585, -2.4006, -3.4897, -3.2824,\n",
      "        -4.3611, -3.4403], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.2537, -3.1605, -3.1605, -3.2268, -3.6922, -3.1605, -3.2965, -3.6922,\n",
      "        -4.2537, -3.2268], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.19997891783714294\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3690, -3.6978, -3.2070, -2.2964], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.3690, -3.7398, -3.9746, -3.2070, -3.5875, -2.2964, -3.2672, -3.2070,\n",
      "        -4.3650, -3.3690], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.1587, -3.6042, -3.8874, -3.0668, -3.4471, -3.0668, -2.6956, -3.0668,\n",
      "        -4.1709, -3.1587], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11312936246395111\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.2267, -3.6259, -3.1496, -2.2279], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.1496, -3.6259, -3.6934, -4.3311, -3.6259, -4.3311, -2.2062, -3.2323,\n",
      "        -2.3581, -2.2279], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.0051, -3.5523, -3.5523, -4.1313, -3.5523, -4.1313, -1.9246, -3.1780,\n",
      "        -3.1223, -3.0051], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14017751812934875\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-3.5401, -2.0928, -3.6300, -2.1902, -1.8954, -2.1902, -1.8954, -2.1902,\n",
      "        -1.8954, -2.3481], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5296, -1.8680, -3.5296, -2.9712, -2.7059, -2.9712, -2.7059, -2.9712,\n",
      "        -2.7059, -3.1133], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.44465503096580505\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-3.4719, -3.2334, -3.5759, -3.0195, -4.2098, -3.1623, -2.3617, -4.2098,\n",
      "        -2.6498, -3.4719], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5270, -3.1717, -3.5270, -2.9671, -4.1322, -3.1717, -3.1255, -4.1322,\n",
      "        -3.1255, -3.5270], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08368317037820816\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-2.1874, -3.7232, -3.6047, -2.1874, -3.5616, -2.9154, -2.5783, -3.2102,\n",
      "        -2.1874, -2.9888], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.9687, -3.8891, -3.7831, -2.9687, -3.5279, -3.1844, -2.7817, -3.5279,\n",
      "        -2.9687, -2.9687], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2106703519821167\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.7941, -3.4109, -2.9346, -2.1902], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.4769, -3.5363, -0.5594, -2.7941, -2.7941, -2.1902, -2.7478, -2.1902,\n",
      "        -2.7997, -2.8770], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5197, -3.5197, -0.3232, -3.1119, -3.1119, -2.9712, -2.8097, -2.9712,\n",
      "        -2.9712, -3.1859], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.160848930478096\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.8102, -3.4229, -2.9209, -2.1903], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.5233, -2.1903, -2.2921, -4.0674, -2.1903, -3.4439, -2.8901, -2.7310,\n",
      "        -2.1903, -2.2921], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5032, -2.9713, -3.0629, -4.1421, -2.9713, -3.5032, -2.9713, -2.8227,\n",
      "        -2.9713, -3.0629], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.30424875020980835\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.8374, -3.4433, -2.9249, -2.2055], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.0481, -2.2055, -2.0399, -3.2487, -2.2055, -3.4252, -2.9249, -3.3442,\n",
      "        -2.0399, -2.2055], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.1399, -2.9850, -2.8359, -3.4943, -2.9850, -3.4943, -2.9850, -3.4237,\n",
      "        -2.8359, -2.9850], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.31733259558677673\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.8729, -3.4791, -2.9480, -2.2361], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.2361, -2.8539, -2.2361, -2.7620, -2.2361, -2.7479, -2.8729, -2.0681,\n",
      "        -2.8539, -2.2361], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.0125, -3.0125, -3.0125, -3.0125, -3.0125, -2.8613, -3.0286, -2.8613,\n",
      "        -3.0125, -3.0125], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3190411925315857\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-2.0957, -2.7641, -2.9237, -2.7068, -4.2146, -4.0199, -0.4065, -2.2546,\n",
      "        -3.4257, -2.0957], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.8862, -3.0359, -3.0291, -2.8862, -4.0927, -4.1531, -0.1268, -3.0291,\n",
      "        -3.4877, -2.8862], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.20813798904418945\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-3.4095, -2.2526, -2.2767, -2.7795, -3.0090, -2.8904, -3.0096, -2.7426,\n",
      "        -3.4481, -0.3830], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4683, -3.0274, -3.0490, -2.9140, -3.0490, -3.0490, -3.0274, -3.0274,\n",
      "        -3.4965, -0.0946], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1411929726600647\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-3.5823, -3.7676, -2.9141, -3.0397, -2.2851, -3.4769, -3.0548, -2.1517,\n",
      "        -4.1158, -4.1158], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.0246, -4.0246, -3.0566, -3.0566, -3.0566, -3.5039, -3.0205, -2.9365,\n",
      "        -4.0246, -4.0246], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1511995494365692\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-2.2514, -2.2933, -2.7959, -3.5101, -3.5101, -2.2514, -3.6268, -3.5101,\n",
      "        -2.2514, -4.1326], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.0263, -3.0640, -3.0640, -3.5163, -3.5163, -3.0263, -3.5163, -3.5163,\n",
      "        -3.0263, -4.0262], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.24906587600708008\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.9070, -4.2780, -3.4665, -3.0910], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.1461, -4.1453, -3.6793, -3.1661, -2.3070, -3.5510, -3.0910, -2.2193,\n",
      "        -2.3070, -2.3070], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.0580, -4.1602, -4.0429, -3.0580, -3.0763, -3.5536, -3.5325, -2.9974,\n",
      "        -3.0763, -3.0763], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.27276045083999634\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-2.8962, -0.7362, -2.4046, -4.1548, -3.0442, -3.5922, -2.3579, -2.3519,\n",
      "        -3.2131, -3.1629], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.1167, -0.0151, -3.0474, -4.0614, -3.1167, -3.6066, -3.1222, -3.1167,\n",
      "        -3.1222, -3.1641], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.21733012795448303\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0404, -4.3858, -3.5763, -3.2676], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.2201, -2.8788, -4.1578, -3.7145, -2.9603, -2.3879, -2.3879, -3.9604,\n",
      "        -4.1973, -3.6575], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2296, -3.1939, -4.0798, -3.6643, -3.1491, -3.1491, -3.1491, -4.0798,\n",
      "        -4.2449, -3.5909], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1323530524969101\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3627, -4.0030, -3.7515, -2.9290], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.3496, -4.2217, -3.6839, -2.4245, -2.4946, -3.6839, -2.4245, -3.6839,\n",
      "        -0.2972, -3.1159], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6361, -4.2895, -3.7326, -3.1820, -3.2451, -3.7326, -3.1820, -3.7326,\n",
      "        -0.0176, -3.0845], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.18840329349040985\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-3.2295, -3.1892, -3.3268, -3.3975, -2.4761, -3.3063, -3.7429, -2.5642,\n",
      "        -3.1428, -2.4761], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3078, -3.2285, -3.3639, -3.3639, -3.2285, -3.3078, -3.6756, -3.3078,\n",
      "        -3.1117, -3.2285], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1700761765241623\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4309, -4.0587, -3.8127, -3.0217], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.1145, -3.7942, -3.1913, -3.2193, -4.2948, -1.5472, -2.5371, -4.4130,\n",
      "        -3.7880, -3.2971], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.1537, -3.8721, -3.2834, -3.2834, -4.3949, -1.5886, -3.2834, -4.7133,\n",
      "        -3.8721, -3.1471], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07086833566427231\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3246, -3.8418, -3.2900, -2.6854], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.4220, -2.5848, -3.8517, -3.3683, -2.6854, -2.4220, -2.6854, -2.6854,\n",
      "        -2.5848, -4.2223], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.1798, -3.3263, -3.9288, -3.4169, -3.4169, -3.1798, -3.4169, -3.4169,\n",
      "        -3.3263, -4.1802], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.386341392993927\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-4.2625, -2.4777, -3.3524, -4.2946, -3.9232, -4.3957, -3.3276, -3.9814,\n",
      "        -3.9232, -3.7763], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.2194, -3.2300, -3.3910, -4.2194, -3.9944, -4.4839, -3.3910, -4.3987,\n",
      "        -3.9944, -3.7958], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07713063806295395\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-3.4547, -2.7050, -3.7551, -4.0048, -1.5470, -3.4341, -4.0064, -3.9920,\n",
      "        -3.4043, -3.5118], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5331, -3.4345, -3.8254, -4.0391, -1.5689, -3.4345, -4.0391, -3.8254,\n",
      "        -3.4345, -3.6109], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05844707414507866\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.3305, -3.9163, -3.4868, -2.5622], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.7535, -3.5751, -4.0689, -4.0379, -3.5154, -3.3305, -3.5006, -4.5483,\n",
      "        -3.5724, -4.3318], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4782, -3.6548, -4.0787, -3.8540, -3.5655, -3.3060, -3.4782, -4.5406,\n",
      "        -3.6548, -4.2728], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05793130397796631\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.3896, -3.9548, -3.5494, -2.6008], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.1181,  1.0194, -2.6008, -4.1610, -1.5639, -2.8053, -2.8053, -2.6008,\n",
      "        -4.1610, -2.8730], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.1166, 10.0000, -3.3407, -4.1166, -1.5531, -3.5247, -3.5247, -3.3407,\n",
      "        -4.1166, -3.5857], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.329309463500977\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.4203, -3.9655, -3.5662, -2.5610], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.6624, -2.5610, -4.6215, -4.6215, -2.7678, -3.8968, -3.4203, -3.6258,\n",
      "        -3.5859, -4.9186], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6366, -3.3049, -4.5071, -4.5071, -3.4910, -1.4556, -3.3049, -3.5193,\n",
      "        -3.4910, -4.5071], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.7265959978103638\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.4202, -3.9590, -3.5441, -2.5034], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.1307, -3.3222, -3.3222,  1.3255, -4.1307, -2.6925, -4.1236, -2.6925,\n",
      "        -4.4983, -4.4116], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9900, -3.4233, -3.4233, 10.0000, -3.9900, -3.4233, -3.9900, -3.4233,\n",
      "        -4.3337, -4.1895], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.646925926208496\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-2.4059, -3.3774, -3.4650, -4.4240, -4.0479, -4.0479, -4.5752, -3.3774,\n",
      "        -4.0479, -2.4059], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.1653, -3.3126, -3.3126, -4.1043, -3.8922, -3.8922, -4.7060, -3.3126,\n",
      "        -3.8922, -3.1653], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.137711301445961\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.3899, -3.8820, -3.4003, -2.3479], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.3479, -4.3469, -2.4860, -4.2239, -3.7207, -3.5875, -2.3479, -3.5694,\n",
      "        -3.9390, -3.8820], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.1131, -4.0628, -3.2374, -4.0628, -3.6469, -3.3501, -3.1131, -3.2043,\n",
      "        -3.8204, -3.3501], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.23343691229820251\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.3181, -3.7573, -3.3054, -2.3260], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.3181, -2.4526, -3.7456, -3.8213, -2.4526, -3.4199, -3.8213, -3.5268,\n",
      "        -2.6107, -2.4187], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.0934, -3.2073, -3.6287, -3.7909, -3.2073, -3.6287, -3.7909, -3.3496,\n",
      "        -3.0934, -3.1769], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.20880500972270966\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.2163, -3.6196, -3.1992, -2.3005], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.7195, -3.6196, -3.7195, -3.7195, -2.4611, -3.4132, -3.4407, -2.3005,\n",
      "        -3.7195, -3.7195], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7896, -3.4068, -3.7896, -3.7896, -3.2150, -3.1806, -3.6302, -3.0705,\n",
      "        -3.7896, -3.7896], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13210493326187134\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-3.6476, -3.1104, -3.6427, -3.6266, -3.0247, -2.4808, -3.6427, -3.6427,\n",
      "        -3.0247, -0.3233], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7951, -3.0573, -3.7951, -3.7951, -3.2327, -3.2327, -3.7951, -3.7951,\n",
      "        -3.2327,  0.7564], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.19402852654457092\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.1129, -3.4356, -3.0162, -2.4630], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.4429, -3.0162, -2.4630, -2.4429, -3.0736, -3.4356, -3.5225, -3.0162,\n",
      "        -3.5452, -3.0736], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.1986, -3.1986, -3.2167, -3.1986, -3.1986, -3.5998, -4.1483, -3.1986,\n",
      "        -3.7388, -3.1986], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.22641384601593018\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.1191, -3.4199, -3.0489, -2.4631], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.6007, -3.6007, -3.4929, -2.3659, -2.4631,  2.1011, -3.0489, -3.2038,\n",
      "        -3.2925, -3.6007], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6260, -3.6260, -3.6501, -3.1293, -3.2167, 10.0000, -3.1293, -3.2167,\n",
      "        -3.5029, -3.6260], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.362094402313232\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-3.0291, -4.0156, -3.5453, -3.5035, -2.7989, -3.0836, -3.0291, -2.4228,\n",
      "        -2.4228, -3.5035], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.0258, -4.0891, -3.5190, -3.5190, -3.0258, -3.0258, -3.0258, -3.1806,\n",
      "        -3.1806, -3.5190], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12096939235925674\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.9247, -3.2610, -2.9677, -1.9729], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.9729, -2.6786, -3.3673, -2.1405, -2.1405, -2.1405, -2.6671, -3.4729,\n",
      "        -2.1405, -4.0404], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.7757, -2.9264, -3.3890, -2.9264, -2.9264, -2.9264, -2.7757, -3.4107,\n",
      "        -2.9264, -4.0678], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3193516135215759\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.9130, -3.2515, -2.9533, -1.9062], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.3974, -3.4613, -3.1668, -3.5197, -2.0633, -2.0633, -3.5015, -2.9979,\n",
      "        -1.9062, -3.7017], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.1577, -3.3425, -2.8570, -3.3425, -2.8570, -2.8570, -3.3425, -2.8570,\n",
      "        -2.7155, -4.0616], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.28091132640838623\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.9072, -3.2578, -2.9361, -1.8767], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.8767, -3.4306, -2.5838, -2.9072, -3.1883, -2.9361, -2.9921, -4.0579,\n",
      "        -1.8767, -3.1883], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.6891, -3.1820, -2.8417, -2.6891, -3.1694, -3.1694, -2.8417, -4.3389,\n",
      "        -2.6891, -3.1694], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16523173451423645\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([ 0.2781, -2.4184, -2.9074, -3.4541, -3.3868, -3.5507, -2.4184, -3.1600,\n",
      "        -3.2123, -2.0288], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.5027, -3.1765, -3.2535, -3.3402, -3.1394, -3.3402, -3.1765, -3.1765,\n",
      "        -2.7169, -2.8259], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3768743872642517\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.1555, -3.5994, -3.1552, -2.4749], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.9676, -3.0995, -3.3926, -2.0630, -3.3296, -4.0774, -3.0306, -2.0630,\n",
      "        -2.9328, -2.6830], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2274, -3.2274, -3.4147, -2.8567, -3.1524, -3.1524, -3.4670, -2.8567,\n",
      "        -2.8567, -2.8567], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.24578924477100372\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-2.1155, -2.5005, -3.0749, -3.8205, -2.5071, -3.2890, -3.2890, -3.3299,\n",
      "        -4.0096, -3.4372], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.9040, -3.2564, -3.2564, -4.1420, -3.2564, -3.2324, -3.2324, -3.5083,\n",
      "        -4.1420, -3.5083], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.19516676664352417\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-3.5637, -3.4291, -2.8732, -2.2557, -2.2557, -3.4291, -3.4291, -3.3198,\n",
      "        -2.1733, -2.2557], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5858, -3.5858, -2.9560, -3.0301, -3.0301, -3.5858, -3.5858, -3.3237,\n",
      "        -2.9560, -3.0301], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.24928399920463562\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-2.9462, -2.2274, -3.9909, -2.2274, -3.4443, -3.2557, -3.4387, -3.6067,\n",
      "        -2.3764, -3.8035], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.0046, -3.0046, -4.0179, -3.0046, -3.8550, -3.3707, -3.6516, -3.6516,\n",
      "        -3.1387, -3.3707], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2210206538438797\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.0542, -3.6510, -2.8454, -2.2850], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.2850,  0.5898, -3.9997, -3.2918, -3.9997, -3.5870, -3.0097, -2.3968,\n",
      "        -3.0174, -3.4474], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.0565,  1.7702, -4.1225, -3.5963, -4.1225, -3.4470, -3.0565, -3.1571,\n",
      "        -3.5963, -3.7088], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3114638328552246\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-3.2668, -3.4572, -3.4572, -2.5794, -2.5794, -3.9715, -3.0506, -2.7829,\n",
      "        -3.4572, -3.4572], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5041, -3.7455, -3.7455, -3.3215, -3.3215, -4.1684, -3.0904, -3.1138,\n",
      "        -3.7455, -3.7455], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16399160027503967\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.9736, -3.6095, -2.7683, -2.2509], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.5624, -3.5089, -3.8752, -3.4779, -2.3264, -2.8749, -2.3264, -2.6423,\n",
      "        -2.3264, -3.6095], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7876, -3.7360, -3.4906, -3.5532, -3.0938, -3.0938, -3.0938, -3.3781,\n",
      "        -3.0938, -3.5532], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.26148074865341187\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-2.3408, -2.3408, -1.6688, -3.1892, -3.2739, -2.6990, -3.3710, -2.3408,\n",
      "        -3.2739, -4.0999], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.1067, -3.1067, -0.5908, -2.9414, -2.9414, -3.4291, -3.5257, -3.1067,\n",
      "        -2.9414, -4.2650], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3788643479347229\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-3.8643, -2.4247, -3.7860, -2.2067, -2.2067, -2.2067, -2.7616, -3.0084,\n",
      "        -2.4247, -2.2067], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7541, -3.1822, -3.4868, -2.9861, -2.9861, -2.9861, -3.4854, -3.6634,\n",
      "        -3.1822, -2.9861], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.46317973732948303\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.1130, -3.7922, -3.3835, -2.8516], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.5616, -2.5616, -2.3294, -2.5616, -3.0985, -2.5616, -3.7122, -3.7122,\n",
      "        -2.8673, -3.3777], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3055, -3.3055, -3.0964, -3.3055, -3.3055, -3.3055, -3.8340, -3.8340,\n",
      "        -3.3055, -3.5712], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3103439509868622\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.1265, -3.8064, -3.4592, -2.9245], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.4592, -3.9070, -3.6133, -3.2145, -2.6634, -3.0857, -3.2027, -2.6634,\n",
      "         0.8932, -3.9070], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2160, -3.8931, -3.7772, -3.3970, -3.3970, -0.8008, -3.3970, -3.3970,\n",
      "         1.8687, -3.8931], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.7406319975852966\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.1218, -3.7976, -3.5049, -2.9428], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.9428, -3.2726, -3.9491, -2.5595, -3.8871, -2.7011, -3.6151, -2.9383,\n",
      "        -2.7011, -3.7976], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6485, -3.4310, -3.9959, -3.3035, -3.9146, -3.4310, -3.7977, -3.6485,\n",
      "        -3.4310, -3.6444], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2706405520439148\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.1109, -3.7618, -3.5269, -2.9266], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.9805, -3.9805, -4.4184, -2.9266, -3.6318, -3.9392, -3.6318, -3.2604,\n",
      "        -3.2062, -2.7517], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9540, -3.9540, -4.3801, -3.6340, -3.6757, -3.9540, -3.6757, -3.4006,\n",
      "        -3.4766, -3.4766], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11253523826599121\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.1140, -3.7326, -3.5588, -2.9134], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.4338, -2.7809, -4.0731, -4.0731, -2.7809, -4.4336, -4.4336, -3.7285,\n",
      "        -3.1012, -3.1012], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5028, -3.5028, -3.9819, -3.9819, -3.5028, -4.4226, -4.4226, -3.8219,\n",
      "        -3.6220, -3.6220], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1615244299173355\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-3.6808, -1.7849, -3.1535, -4.6616, -3.5115, -3.4539, -2.7989, -3.3224,\n",
      "        -3.5115, -2.8440], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5870, -1.2858, -3.5478, -4.5130, -3.5190, -3.5190, -3.5190, -3.5190,\n",
      "        -3.5190, -3.5596], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15091027319431305\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.7422, -4.5519, -4.2097, -3.2487], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.2423, -3.3701, -4.2423, -2.8243, -4.2423, -3.5871, -2.7554, -3.6488,\n",
      "        -3.9917, -2.9568], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.0331, -3.5418, -4.0331, -3.5418, -4.0331, -3.5418, -3.4799, -3.9238,\n",
      "        -4.1415, -3.6611], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17968013882637024\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.1816, -3.6382, -3.4931, -2.7110], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.5101,  1.1120, -3.0602, -3.0602, -3.9358, -3.7080, -2.7110, -2.7110,\n",
      "        -4.0976, -3.7080], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.5130,  1.7568, -3.7542, -3.7542, -3.8414, -3.5614, -3.4399, -3.4399,\n",
      "        -4.0937, -3.5614], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.24934658408164978\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-2.6988, -3.7001, -2.6988, -3.7657, -3.6251, -3.2049, -3.9194, -2.6988,\n",
      "        -4.1224, -3.5825], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4290, -3.5787, -3.4290, -3.6152, -4.0720, -3.8637, -3.5787, -3.4290,\n",
      "        -4.1755, -3.8844], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.24803991615772247\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-2.9712, -4.0954, -2.9712, -3.6094, -3.7931, -4.8334, -2.9712, -2.9712,\n",
      "        -4.3840, -2.7226], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6741, -4.2485, -3.6741, -3.6741, -3.6741, -4.7179, -3.6741, -3.6741,\n",
      "        -4.2485, -3.4503], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2579260468482971\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-3.7049, -4.3577, -3.6760, -3.0770, -3.6760, -3.7414, -2.7649, -3.0770,\n",
      "         2.9745, -3.0770], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7693, -4.4058, -3.6684, -3.7693, -3.6684, -3.4884, -3.4884, -3.7693,\n",
      "        10.0000, -3.7693], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.1389641761779785\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-3.8518, -4.2782, -3.7761, -3.3907, -4.4121, -4.8276, -4.1877, -4.2147,\n",
      "        -3.4365, -2.8015], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.0516, -4.3963, -4.2532, -4.0516, -4.3963, -4.8470, -4.3963, -4.0929,\n",
      "        -4.0516, -3.5214], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1673867404460907\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-3.9044, -4.3817, -4.1846, -3.9659], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-3.7782, -3.6288, -4.4199, -3.3663, -4.4167, -4.3189, -4.2441, -3.7964,\n",
      "        -3.7964, -3.2216], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.8994, -3.6738, -4.4168, -4.0297, -4.4168, -4.4168, -4.4168, -3.8994,\n",
      "        -3.8994, -3.8994], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09768792986869812\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-2.8647, -3.2618, -4.3518, -4.4338, -3.6216, -3.6216, -3.8142, -3.8028,\n",
      "        -3.8142, -3.9043], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5782, -3.9356, -4.2452, -4.4328, -3.6644, -3.6644, -3.9356, -3.9356,\n",
      "        -3.9356, -4.2452], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11414536088705063\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-4.0195, -4.5528, -4.2165, -4.0050], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.2956, -4.3556, -3.8555, -2.9046, -4.4688, -3.8379, -3.6772, -3.6233,\n",
      "        -3.6233, -3.6233], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9661, -4.3095, -3.9661, -3.6142, -4.4699, -3.9661, -4.0149, -3.6643,\n",
      "        -3.6643, -3.6643], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11028128862380981\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-3.7641, -3.3305, -4.9991, -2.9476, -3.5870, -3.3250, -4.2021, -2.9476,\n",
      "        -4.2021, -3.6623], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9975, -3.9975, -5.0250, -3.6528, -3.9975, -3.9925, -3.9975, -3.6528,\n",
      "        -3.9975, -3.6528], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.21926066279411316\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.7261, -4.3035, -3.6664, -3.0249], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.9191, -3.9191, -5.3005, -3.3164, -3.3164, -4.3035, -3.0249, -4.5794,\n",
      "         1.6239, -3.3164], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.0196, -4.0196, -4.6565, -3.9848, -3.9848, -4.2633, -3.7224, -4.5666,\n",
      "         2.1493, -3.9848], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.25393059849739075\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.7834, -4.3484, -3.7476, -3.1243], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.6447, -4.6447, -3.4103, -4.6551, -3.6483, -3.3472, -4.8587, -3.6862,\n",
      "        -3.4103, -3.1243], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6443, -4.6443, -4.0693, -4.6443, -3.8119, -4.0125, -4.6443, -3.7153,\n",
      "        -4.0693, -3.8119], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1857522428035736\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.8359, -4.3723, -3.8250, -3.2306], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.4282, -4.2267, -3.4033, -4.7225, -4.7225, -4.1201, -3.2306, -4.7505,\n",
      "        -4.8915, -4.8457], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7081, -4.8041, -4.0630, -4.7499, -4.7499, -4.0630, -3.9076, -4.7499,\n",
      "        -4.8922, -4.7499], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1319008767604828\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.8926, -4.3919, -3.9145, -3.3354], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.7690, -3.5556, -4.4616, -3.5556, -3.8817, -4.5297, -4.8163, -4.3919,\n",
      "        -4.4834, -4.8117], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.8259, -4.2000, -4.8029, -4.2000, -4.2000, -4.8029, -4.8537, -4.3976,\n",
      "        -4.8029, -4.8537], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12316224724054337\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.9516, -4.4384, -4.0251, -3.4187], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.4470, -3.4187, -3.5909, -4.9323, -3.5356, -3.4187, -3.4187, -5.4483,\n",
      "        -4.0251, -4.9191], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.3226, -4.0768, -4.2318, -4.9277, -4.1820, -4.0768, -4.0768, -5.3896,\n",
      "        -4.1820, -4.9277], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2171711027622223\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.0174, -4.4910, -4.1547, -3.5303], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.3014, -3.9260, -5.0494, -3.5303, -3.6103, -3.6103, -3.6448, -3.6448,\n",
      "        -4.0846, -4.3541], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.2493, -3.9138, -5.0094, -4.1773, -4.2493, -4.2493, -4.2803, -4.2803,\n",
      "        -4.2803, -4.2493], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.20966120064258575\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.0777, -4.5446, -4.2885, -3.6518], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.1697, -4.0136, -5.1697, -5.2777, -4.0777, -4.0136, -3.7242, -5.6312,\n",
      "        -3.7242, -3.7039], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1004, -3.9757, -5.1004, -5.0453, -4.2866, -3.9757, -4.3518, -5.5459,\n",
      "        -4.3518, -4.3335], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13015015423297882\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.1608, -4.5761, -4.4012, -3.7694], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.8511, -3.7694, -4.4137, -4.1608, -3.8511, -3.7694, -3.8511, -4.6641,\n",
      "        -4.5977, -4.4012], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4660, -4.3925, -4.4660, -4.3925, -4.4660, -4.3925, -4.4660, -4.3978,\n",
      "        -5.1379, -4.4660], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.23340407013893127\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-3.8853, -5.3539, -3.9783, -5.2698, -3.8673, -5.4025, -4.1586, -4.5194,\n",
      "        -5.5074, -5.0144], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4968, -5.2722, -4.5804, -5.6143, -4.4805, -5.2549, -4.4968, -4.5804,\n",
      "        -5.7165, -4.7427], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14953938126564026\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5132, -4.7697, -4.5460, -4.0990], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.9214, -3.9656, -4.6057, -5.4289, -4.3962, -4.8835, -3.9656, -5.0525,\n",
      "        -4.5492, -4.8010], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.8049, -4.5690, -4.6891, -5.3534, -4.5812, -5.3534, -4.5690, -5.0877,\n",
      "        -4.5690, -4.8534], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10139278322458267\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6554, -4.8737, -4.6286, -4.1858], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.2680, -4.0443, -4.0443, -5.4664, -4.7637, -4.6819, -5.9721, -5.4664,\n",
      "        -4.0687, -4.1858], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.2208, -4.6399, -4.6399, -5.3796, -4.7673, -4.7673, -4.6781, -5.3796,\n",
      "        -4.6618, -4.7673], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.30982130765914917\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7803, -4.8431, -4.6991, -4.3195], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.4905, -5.4905, -4.3195, -4.3195, -4.3078, -4.3195, -5.2139, -5.4905,\n",
      "        -4.6991, -4.1585], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4541, -5.4541, -4.8875, -4.8875, -4.3061, -4.8875, -5.1916, -5.4541,\n",
      "        -4.7963, -4.7427], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.132319837808609\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-5.6854, -5.5995, -5.5157, -5.5157, -5.5578, -5.0361, -4.2801,  1.9527,\n",
      "        -4.7883, -4.3506], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.7998, -5.2704, -5.5380, -5.5380, -5.2704, -5.0283, -4.8521,  2.3941,\n",
      "        -4.8521, -4.4007], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07335829734802246\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-4.6753, -5.5221, -5.5573, -2.1598, -4.8752, -4.6204, -5.2912, -4.8505,\n",
      "        -5.0165, -4.5010], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4848, -5.6068, -5.7943,  2.3885, -5.0509, -5.1584, -5.2980, -5.0509,\n",
      "        -4.9581, -5.0509], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.1453640460968018\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.8842, -5.3154, -5.2349, -5.5141], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.3384, -4.3384, -4.3686, -4.3686, -5.4682, -5.3308, -4.4562, -4.8829,\n",
      "        -4.5827, -4.5827], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4149, -4.4149, -4.9317, -4.9317, -5.5128, -5.3378, -5.0106, -5.5128,\n",
      "        -5.1244, -5.1244], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.19390329718589783\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.6810, -5.4067, -5.8281, -5.3361], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.5426, -4.5426, -5.4009, -4.4318, -4.9061, -5.4970, -5.4009, -5.4009,\n",
      "        -4.3371, -4.5426], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0884, -5.0884, -5.3816, -4.9886, -5.0884, -5.5932, -5.3816, -5.3816,\n",
      "        -4.9034, -5.0884], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15677788853645325\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-5.7981, -5.6764, -4.7221, -4.7527, -4.4290, -4.9045, -5.2503, -4.7393,\n",
      "        -4.5199, -4.5199], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.7253, -5.6081, -5.0679, -4.9861, -4.9861, -4.9861, -5.2774, -5.2499,\n",
      "        -5.0679, -5.0679], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13631074130535126\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-4.6567, -4.4967, -4.3211, -4.4967, -5.3257, -5.8497, -4.3211, -4.9523,\n",
      "        -5.3257, -4.3211], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9599, -5.0470, -4.8890, -5.0470, -5.1911, -5.6442, -4.8890, -4.9599,\n",
      "        -5.1911, -4.8890], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17437052726745605\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-5.4505, -5.2802, -5.4042, -4.6162], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.4505, -4.6180, -4.5067, -4.5067, -4.3760, -4.3760, -4.3760, -4.3429,\n",
      "        -5.3115, -4.6180], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1546, -4.9384, -5.0560, -5.0560, -4.9384, -4.9384, -4.9384, -4.9086,\n",
      "        -5.1562, -4.9384], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.21894025802612305\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.1763, -4.8734, -5.0845, -4.4004], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.0906, -5.0093, -4.2903, -4.5597, -4.4004, -4.4004, -4.3621, -4.2903,\n",
      "        -4.3621, -5.0093], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1964, -5.2078, -4.2010, -5.1037, -4.9604, -4.9604, -4.9259, -4.2010,\n",
      "        -4.9259, -5.2078], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16647931933403015\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-4.3846, -4.8464, -5.0859, -5.1287, -5.2343, -4.2870, -4.7179, -4.3846,\n",
      "        -5.0314, -4.9530], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9462, -4.2252, -4.9462, -5.1737, -5.4982, -4.2252, -4.9462, -4.9462,\n",
      "        -5.2202, -5.2342], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12783464789390564\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.2631, -5.0916, -5.1401, -4.5627], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.4194, -4.2842, -5.4129, -4.4194, -5.6718, -5.6481, -4.4194, -5.3242,\n",
      "         1.9903, -4.5627], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9775, -4.2958, -5.6328, -4.9775, -5.2689, -5.6328, -4.9775, -5.4359,\n",
      "         2.0327, -5.1064], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14553089439868927\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.3132, -5.2487, -5.1506, -4.6454], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.7361, -4.4689, -5.0722, -5.5813, -5.4401, -5.1506, -4.7874, -5.3666,\n",
      "        -4.6454, -5.3666], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.8732, -5.0220, -5.0220, -5.6401, -5.3703, -5.3086, -5.3086, -5.3860,\n",
      "        -5.1809, -5.3860], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.43715110421180725\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-5.6050, -5.0559, -5.5170, -5.3106, -5.3106, -4.4698, -0.9743, -5.4091,\n",
      "        -5.3106, -5.6622], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1899, -5.0228, -5.3132, -5.4047, -5.4047, -5.0228,  1.9654, -5.3178,\n",
      "        -5.4047, -5.6116], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.9200178980827332\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-4.7396, -4.0725, -5.1722, -5.4226, -5.3377, -4.6747, -4.7604, -5.6853,\n",
      "        -4.0725, -4.4139], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2656, -4.1653, -5.2656, -5.5001, -5.2073, -5.2073, -5.2073, -5.5001,\n",
      "        -4.1653, -4.9725], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11553561687469482\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-5.5483, -5.3060, -5.0047, -5.5227, -4.6993, -4.6566, -4.7535, -4.3834,\n",
      "        -4.6566, -5.3735], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3221, -5.2294, -4.9450, -5.2782, -5.2294, -5.1909, -5.1909, -4.9450,\n",
      "        -5.1909, -5.2782], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14883047342300415\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.1864, -5.5029, -5.2285, -4.5706], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.6511, -5.2618, -5.2618, -4.8623, -4.6511, -4.3850, -4.8123, -5.2056,\n",
      "        -4.3850, -5.3120], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1860, -5.3310, -5.3310, -4.9465, -5.1860, -4.9465, -4.9465, -5.1860,\n",
      "        -4.9465, -5.3779], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12422256171703339\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.1531, -5.4449, -5.0181, -4.7076], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.7076, -5.2889, -4.7076, -4.6576, -3.8832, -4.7076, -4.6576, -4.3909,\n",
      "        -5.5281, -5.1971], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2369, -5.3505, -5.2369, -5.1919, -3.9566, -5.2369, -5.1919, -4.9518,\n",
      "        -5.3505, -5.1919], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17664560675621033\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.1171, -5.4391, -5.0524, -4.7443], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.4150, -5.3259, -4.5648, -5.3347, -4.4150, -4.7443, -5.1321, -5.0524,\n",
      "        -4.6881, -5.0524], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9735, -5.4630, -5.2699, -5.3957, -4.9735, -5.2699, -5.2699, -4.9735,\n",
      "        -5.2193, -4.9735], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17332926392555237\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0788, -5.4094, -5.0442, -4.7328], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.7255, -4.9389, -5.3399, -3.8393, -5.5909, -5.1324, -4.7255, -5.0442,\n",
      "        -4.7327, -4.7255], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2530, -5.0209, -5.0321, -3.9143, -5.1541, -5.2595, -5.2530, -5.0209,\n",
      "        -5.2595, -5.2530], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1426638662815094\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0739, -5.3602, -5.0188, -4.7316], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.5352, -5.1712, -3.8086, -4.9690, -5.4049, -4.7799, -5.3074, -4.7316,\n",
      "        -4.5352, -5.3074], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0817, -5.2584, -3.9068, -5.2085, -5.5006, -5.3019, -5.7766, -5.2584,\n",
      "        -5.0817, -5.7766], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16714897751808167\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-5.1083, -4.8382, -5.3299, -4.6138, -5.1433, -5.4348, -4.8382, -5.0138,\n",
      "        -4.9535, -5.4348], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3544, -5.3544, -5.2612, -5.1524, -5.3544, -5.5569, -5.3544, -5.1524,\n",
      "        -5.2849, -5.5569], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10916604101657867\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3637, -5.5283, -5.0803, -4.6882], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.8686, -5.4792, -4.7527, -0.3908, -4.6882, -4.8686, -5.4840, -5.2221,\n",
      "        -4.8686, -4.6882], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3817, -5.6822, -5.2774,  1.5618, -5.2194, -5.3817, -5.6042, -5.3357,\n",
      "        -5.3817, -5.2194], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5510936975479126\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-5.5539, -4.7317, -4.7317, -4.7293, -4.7317, -4.8777, -4.7277, -4.7277,\n",
      "        -5.0270, -5.4955], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6095, -5.2585, -5.2585, -3.8689, -5.2585, -5.3899, -5.2549, -5.2549,\n",
      "        -5.2549, -5.6877], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.24833567440509796\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-4.9164, -4.9164, -4.7863, -5.1809, -4.7863, -5.5346, -4.7863, -5.0311,\n",
      "        -5.5346, -4.7863], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4247, -5.4247, -5.3077, -5.3077, -5.3077, -5.6320, -5.3077, -5.3583,\n",
      "        -5.6320, -5.3077], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17462587356567383\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-4.9686, -4.9686, -5.3719, -3.8169,  1.7939, -5.3719, -5.2572, -4.7949,\n",
      "        -5.6433, -4.7949], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4717, -5.4717, -5.4717, -3.9235,  1.4359, -5.4717, -5.4717, -5.3154,\n",
      "        -5.3154, -5.3154], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13611429929733276\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.8241, -5.7670, -5.6129, -5.2184], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.0120, -5.4499, -4.9425, -5.2255, -5.6129, -5.6129, -4.9425, -5.7123,\n",
      "        -5.5625, -5.4228], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5108, -5.7784, -5.4482, -5.3181, -5.6965, -5.6965, -5.4482, -5.3904,\n",
      "        -5.7784, -5.4054], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10413838922977448\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.7560, -5.7341, -5.3892, -5.0089], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.0591, -3.9165, -5.0089, -5.5760, -3.9165, -5.6876, -5.7147, -5.7390,\n",
      "        -5.0591, -5.6691], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5532, -4.0090, -5.5080, -5.7348, -4.0090, -5.8028, -5.9641, -5.5532,\n",
      "        -5.5532, -5.7348], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08940146118402481\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-5.0640, -5.4768, -5.7375, -5.8941, -5.1039, -5.7375, -5.1039, -5.8941,\n",
      "        -3.9864, -5.7787], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5576, -5.5576, -5.7552, -5.8117, -5.5935, -5.7552, -5.5935, -5.8117,\n",
      "        -4.0411, -5.7552], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07473719120025635\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-5.1427, -5.4445, -5.1597, -4.8413, -5.1427, -5.1282, -5.1282, -5.1427,\n",
      "        -5.1282, -5.7459], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6284, -5.5135, -5.6437, -5.7838, -5.6284, -5.6154, -5.6154, -5.6284,\n",
      "        -5.6154, -5.5934], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.25702714920043945\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.9223, -5.8642, -5.8717, -5.2358], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.8608, -5.2508, -5.2358, -5.2508, -4.1392, -6.0222, -5.2168, -5.9482,\n",
      "        -5.2168, -5.2358], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.7930, -5.7257, -5.7122, -5.7257, -4.1271, -5.5606, -5.6951, -5.7074,\n",
      "        -5.6951, -5.7122], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16384567320346832\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.9866, -5.9093, -5.9433, -5.3412], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.3316, -5.3879, -5.8925, -5.8861, -5.3316, -5.6771, -5.3412, -5.8861,\n",
      "        -5.8925, -5.8925], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.7984, -5.8491, -5.8330, -5.8330, -5.7984, -5.7984, -5.8071, -5.8330,\n",
      "        -5.8330, -5.8330], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08966455608606339\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-5.1624, -5.4373, -5.9932, -5.9537, -5.5379, -5.5379, -5.9537, -5.8997,\n",
      "        -5.7968, -5.4569], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.8951, -5.8936, -5.9841, -5.8951, -5.9841, -5.9841, -5.8951, -5.8951,\n",
      "        -5.7501, -5.9112], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1358851194381714\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-6.1152, -5.9658, -6.0807, -5.5839], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.6943, -5.8539, -5.4907, -5.9189, -4.2901, -5.5839, -5.6943, -5.5497,\n",
      "        -5.5497,  1.5355], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.1249, -5.7966, -5.9947, -5.9416, -4.3553, -6.0255, -6.1249, -5.9947,\n",
      "        -5.9947,  1.2108], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13294371962547302\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-6.1842, -5.9935, -6.1551, -5.7186], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.8569, -5.6416, -5.9512, -5.8569, -4.3448, -5.8931, -5.9536, -5.8825,\n",
      "        -5.8570, -5.8931], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.2713, -6.0775, -6.2545, -6.2713, -4.4408, -6.0215, -6.0215, -5.9617,\n",
      "        -6.2713, -6.0215], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0849958062171936\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-5.7371, -6.1360, -5.6404, -5.7371, -6.0131, -5.7371, -6.1314, -5.9726,\n",
      "        -6.1082, -5.7459], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.1634, -6.0763, -6.1634, -6.1634, -6.3773, -6.1634, -6.0509, -6.1634,\n",
      "        -6.3773, -6.2518], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13261310756206512\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-6.1907, -6.2087, -6.0596, -6.2987, -4.4772, -6.1907, -6.1312, -6.0390,\n",
      "        -6.0596, -5.8507], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.3950, -6.1637, -6.1848, -6.3950, -4.6128, -6.3950, -6.2399, -6.2657,\n",
      "        -6.1848, -6.2657], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03799647092819214\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-6.2040, -5.9555, -6.3178, -6.0003], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.3671, -6.2040, -6.1662, -5.8250, -6.0000, -6.0600, -5.9619, -6.0003,\n",
      "        -5.9571, -6.0600], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.3614, -6.3599, -6.2425, -6.1048, -6.3599, -6.4000, -6.2942, -6.4000,\n",
      "        -6.2720, -6.4000], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08385458588600159\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-6.2839, -5.5953, -6.3246, -6.4425], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.1371, -6.2430, -6.5311, -6.2430, -6.6091, -6.6091,  1.3641, -6.5207,\n",
      "        -5.7564, -6.2430], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.1808, -6.2859, -6.2859, -6.2859, -6.2655, -6.2655,  1.1020, -6.2655,\n",
      "        -6.1415, -6.2859], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05857842043042183\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.0873, -3.7394, -3.8585, -3.6455], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-6.2813, -5.7551, -6.1143, -5.7551, -6.3247, -6.3679, -6.1646, -6.2918,\n",
      "        -6.5919, -6.3679], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.1796, -6.1177, -6.2759, -6.1177, -6.2497, -6.1177, -6.1796, -6.1846,\n",
      "        -6.2759, -6.1177], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05418292433023453\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-6.3622, -5.5869, -6.3858, -6.6223], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.4412, -6.4675, -6.4374, -5.7086, -5.8562, -6.9334, -6.4157, -6.9334,\n",
      "        -6.8194, -6.4374], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.0523, -6.2952, -6.2706, -6.0523, -6.2952, -6.3663, -6.1972, -6.3663,\n",
      "        -6.2706, -6.2706], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.153956800699234\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-6.1793, -6.4776, -6.4514, -6.5555, -6.1793, -6.3648, -6.3257, -6.4930,\n",
      "        -6.5870, -6.1793], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.3751, -6.5004, -6.4633, -6.4633, -6.3751, -6.4584, -6.3751, -6.5004,\n",
      "        -6.5614, -6.3751], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.013606746681034565\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.0504, -4.0215, -3.8795, -3.7218], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-6.2684, -6.5310, -6.8931, -4.7877, -6.5187, -6.3285, -6.1755, -6.5487,\n",
      "        -6.4118, -6.3901], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.6073, -6.7144, -6.7706, -4.9962, -6.7144, -6.6706, -6.7144, -6.7706,\n",
      "        -6.5201, -6.3500], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07153820991516113\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-6.6894, -6.3190, -6.2291, -6.3190, -6.9145, -6.5583, -6.3190, -6.3111,\n",
      "        -6.9145, -6.3190], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.8483, -6.5460, -6.9025, -6.5460, -6.9025, -6.6023, -6.5460, -6.4374,\n",
      "        -6.9025, -6.5460], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07031217962503433\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-6.6659, -6.5774, -6.0959, -6.6180, -6.5774, -6.4269, -6.3035, -6.6180,\n",
      "        -6.2404, -5.2207], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.8681, -6.5615, -4.4136, -6.8039, -6.5615, -6.5615, -6.8681, -6.8039,\n",
      "        -6.7715, -5.0593], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3585779666900635\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-6.0887, -5.8881, -6.4275, -6.7592], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.9373, -6.8253, -6.3462, -6.5752, -4.9373, -6.8048, -6.5170, -6.8048,\n",
      "        -6.4998, -6.5752], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1880, -6.7445, -6.5294, -6.5386, -5.1880, -6.7116, -6.7445, -6.7116,\n",
      "        -6.3265, -6.5386], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.026767458766698837\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.1980, -4.0952, -4.2972, -4.3329], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-7.4550, -6.9362, -6.9362, -6.8509, -6.9362, -6.8509, -6.8934, -6.5777,\n",
      "        -6.6850, -6.8509], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.4838, -6.6369, -6.6369, -6.6369, -6.6369, -6.6369, -6.6816, -6.6481,\n",
      "        -6.6816, -6.6369], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13989779353141785\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.2658, -4.1788, -4.3847, -4.4699], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.9216, -6.5085, -6.7704, -6.3150, -6.1102, -6.9757, -6.6422, -6.9757,\n",
      "        -5.2161, -6.6451], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.6652, -6.2661, -6.6652, -6.4121, -6.2661, -6.6835, -6.6835, -6.6835,\n",
      "        -5.2939, -6.4992], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.036916252225637436\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.3245, -4.2947, -4.4408, -4.5495], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-7.0412, -6.8539, -6.9239, -6.9239, -6.5756, -6.6877, -6.3256, -6.7979,\n",
      "        -6.4480, -6.3275], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.7385, -6.7423, -6.7623, -6.7623, -6.4198, -6.7423, -6.6948, -6.7385,\n",
      "        -6.3209, -6.5088], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0372423455119133\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-4.9221, -4.9221, -6.5016, -6.7753, -6.7439, -6.4615, -6.9007, -6.8235,\n",
      "        -6.7439, -6.8235], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1952, -5.1952, -6.4309, -6.8067, -6.8153, -6.8067, -6.8067, -6.8514,\n",
      "        -6.8153, -6.8514], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02949506603181362\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-6.6590, -6.7487, -6.6590, -6.3659, -4.9162, -6.6590, -6.5711,  1.0833,\n",
      "        -6.7295, -6.7245], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.8827, -6.8393, -6.8827, -6.9173, -5.1293, -6.8827, -6.8343,  0.9984,\n",
      "        -6.8393, -6.9173], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06336073577404022\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-6.6776, -6.7227, -6.5854, -6.4238, -6.6867, -6.6862, -6.0745, -6.6862,\n",
      "        -6.5253, -6.6862], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.8909, -6.8687, -6.4670, -6.8888, -6.7814, -6.8888, -6.4670, -6.8888,\n",
      "        -6.8909, -6.8888], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07169057428836823\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-4.9844, -6.3095, -6.5907, -6.5907, -6.6024, -6.5432, -6.6726, -6.7108,\n",
      "        -6.7360, -6.7587], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1504, -6.7431, -6.8002, -6.8002, -6.3481, -6.8530, -6.7431, -6.6785,\n",
      "        -6.8002, -6.8002], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.047581546008586884\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.4541, -4.7149, -4.8192, -4.8627], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-6.7847, -5.3334, -6.9239, -5.0675, -6.6576, -6.9239, -6.7774, -6.7847,\n",
      "        -5.8692, -6.9239], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.6656, -5.8001, -6.6634, -5.1882, -6.5571, -6.6634, -6.5783, -6.6656,\n",
      "        -6.2823, -6.6634], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06846748292446136\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-6.8484, -5.7942, -6.3560, -4.9311, -6.7985, -6.9505, -6.9505, -6.5259,\n",
      "        -5.8735, -6.7985], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.6005, -6.1982, -5.7578, -5.7578, -6.6475, -6.6551, -6.6551, -6.7389,\n",
      "        -6.2861, -6.6475], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1701793372631073\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-5.2780, -5.5210, -5.8557, -6.0797], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-5.0950, -6.6645, -5.9925, -6.5734, -6.8086, -6.5734, -5.8557, -5.0950,\n",
      "        -6.0797, -6.5734], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2205, -6.7877, -4.8851, -6.4714, -6.6766, -6.4714, -6.2028, -5.2205,\n",
      "        -6.3166, -6.4714], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14983904361724854\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-5.3299, -5.4600, -5.8532, -6.0790], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-6.7936, -6.9341, -6.9341, -4.7125, -6.4144, -5.4600, -6.3651, -6.5736,\n",
      "        -6.0901, -6.5146], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.7286, -6.7593, -6.7593, -5.2412, -6.8566, -5.2412, -6.7593, -6.4811,\n",
      "        -6.4811, -6.4811], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09062032401561737\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-5.3875, -5.3863, -5.8383, -6.0615], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.8383, -6.1781, -5.3875, -6.8687, -6.9368, -6.8687, -6.9327, -6.9327,\n",
      "        -4.8976, -5.1194], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.1206, -5.8477, -5.8477, -6.8175, -6.8535, -6.8175, -6.8506, -6.8506,\n",
      "        -5.4078, -5.2143], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06957029551267624\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-5.0322, -4.8670, -5.1940, -5.3384], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.9722, -6.0667, -6.9002, -6.9002, -5.0322, -5.1277, -6.0667, -5.1316,\n",
      "        -6.4127, -6.9326], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3507, -6.4633, -6.8444, -6.8444, -5.3803, -5.3507, -6.4633, -5.2056,\n",
      "        -6.4600, -6.8551], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08916788548231125\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-5.2210, -4.8533, -5.2613, -5.4143], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.9301, -6.8167, -6.8167, -6.5198, -5.4143, -4.8533, -5.5848, -6.4041,\n",
      "         0.9134, -6.9004], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.7412, -6.3858, -6.3858, -6.2830, -5.7412, -5.3680, -6.2830, -6.3858,\n",
      "         0.9595, -6.7723], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1341162621974945\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-6.9099, -5.5322, -6.4597, -6.8995, -6.8844, -6.9099, -6.9769, -6.9099,\n",
      "        -6.4597, -6.4597], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.6259, -5.6863, -6.3518, -6.6805, -6.7079, -6.6259, -6.7079, -6.6259,\n",
      "        -6.3518, -6.3518], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.045204129070043564\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-6.9471, -6.9849, -6.8362, -6.5379, -5.9727, -6.3558, -6.2340, -6.3147,\n",
      "        -1.1356, -6.4626], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.6559, -6.6106, -6.6559, -6.3754, -6.1730, -6.6106, -6.1375, -6.1375,\n",
      "        -1.3044, -6.3754], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.046567849814891815\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3350, -4.8998, -5.0876, -5.0572], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.3218, -6.7527, -6.6017, -5.7381, -6.8473, -6.8257, -6.1966, -6.7527,\n",
      "        -6.4390, -0.3093], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.7381, -6.7873, -6.4452, -5.7145, -6.6896, -6.6424, -6.1643, -6.7873,\n",
      "        -6.4452,  0.9663], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.18873736262321472\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3866, -4.9811, -5.0361, -5.0458], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.3865, -6.6932, -6.1257, -6.7754, -5.7317, -5.8199, -5.6720, -6.5278,\n",
      "        -6.5278, -6.5278], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.5131, -6.7455, -6.1585, -6.8560, -5.7506, -5.7506, -5.7506, -6.6806,\n",
      "        -6.6806, -6.6806], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.010773435235023499\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-5.3800, -5.8040, -6.3604, -6.4415, -6.5778, -6.6076, -5.7760, -5.0137,\n",
      "        -6.6527, -6.6076], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5459, -5.7603, -6.5525, -6.6943, -6.7616, -6.7616, -5.7603, -5.5459,\n",
      "        -6.8892, -6.7616], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.055102743208408356\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.5838, -5.0249, -5.0900, -5.7910], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.9726, -6.6878, -6.6001, -6.6752, -6.2968, -6.6001, -6.9367, -4.7941,\n",
      "        -6.5338, -6.3856], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.5500, -6.5500, -6.7044, -6.8549, -6.0792, -6.7044, -6.6220, -5.0291,\n",
      "        -6.6671, -6.5500], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06528319418430328\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-6.6406, -6.8742, -6.7507, -6.6406, -5.2084, -6.4260, -6.4260, -6.2993,\n",
      "        -6.3563, -6.4260], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.6456, -6.5145, -6.8198, -6.6456, -5.6876, -6.5145, -6.5145, -6.6066,\n",
      "        -6.6066, -6.5145], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05443720892071724\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.1805, -5.5492, -5.7152, -6.4197], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.8453, -5.7152, -6.6872, -6.6872, -6.2323, -7.0237, -6.6872, -5.7422,\n",
      "        -4.7376, -6.0737], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.8000, -5.9943, -6.5926, -6.5926, -6.5307, -6.5926, -6.5926, -5.6531,\n",
      "        -5.0971, -5.9779], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.052802931517362595\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.4508, -4.8831, -5.0532, -5.9365], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.6585, -6.2600, -6.7037, -6.7037, -6.7037, -6.7037, -6.0415, -6.7037,\n",
      "        -6.4509, -5.1713], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5953, -6.7709, -6.5811, -6.5811, -6.5811, -6.5811, -5.9221, -6.5811,\n",
      "        -6.4488, -5.6542], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05876212567090988\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3825, -4.8415, -5.1157, -5.9398], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.8356, -4.8349, -6.8217, -6.6359, -6.5584, -5.8312, -4.8349, -6.8418,\n",
      "        -6.5584, -6.8418], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.8814, -5.1027, -6.5521, -6.3992, -6.3992, -5.9099, -5.1027, -6.7066,\n",
      "        -6.3992, -6.7066], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0367656871676445\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.2909, -4.8337, -5.1884, -5.9026], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.6341, -6.7448, -6.5204, -6.7448, -4.8337, -6.6341, -4.8991, -6.1784,\n",
      "        -6.3625, -5.6836], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.5605, -6.6708, -6.4017, -6.6708, -5.3503, -6.5605, -5.0549, -6.3328,\n",
      "        -6.3527, -5.6609], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03515031188726425\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-6.4614, -6.5577, -5.9903, -6.3288, -5.6362, -6.4614, -6.4614, -6.2175,\n",
      "        -5.6507, -5.9903], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.4218, -6.5957, -6.4218, -6.3238, -5.7615, -6.4218, -6.4218, -6.3238,\n",
      "        -5.8842, -6.4218], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04600255936384201\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.5330, -5.2998, -5.7341, -5.6129], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.2218, -6.5745, -5.1569, -5.0312, -5.9761, -6.0674, -6.5332, -6.4814,\n",
      "        -6.1057, -6.4814], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6996, -6.6135, -5.7698, -4.9688, -5.8962, -5.8971, -6.5987, -6.3049,\n",
      "        -5.5210, -6.3049], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10530861467123032\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-5.6380, -5.8637, -6.0626, -6.3852, -6.4958, -6.3394, -6.3394, -6.3394,\n",
      "        -6.3394, -6.4958], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.9189, -5.8863, -6.2515, -6.6147, -6.6147, -6.3486, -6.3486, -6.3486,\n",
      "        -6.3486, -6.6147], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.019642503932118416\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0557, -4.6939, -5.0370, -5.5883], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.1545, -6.4785, -5.4676, -6.3795, -6.4785, -6.5003, -6.2951, -6.5273,\n",
      "        -5.8191, -6.3795], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.1937, -6.5918, -5.5254, -6.5918, -6.5918, -6.2263, -6.2954, -6.2263,\n",
      "        -5.6869, -6.5918], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.030382823199033737\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-6.4510, -5.4713, -6.4211, -6.4018, -5.9143, -6.1218, -4.8590, -6.1576,\n",
      "        -5.2712, -6.2644], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.5070, -5.5127, -6.5550, -6.5070, -5.9241, -6.2478, -5.3731, -6.2478,\n",
      "        -5.6509, -6.2478], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04666627198457718\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.5188, -5.0834, -5.7415, -5.4874], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.4652, -5.0834, -6.7550, -6.5350, -6.7791,  0.1572, -5.2988, -6.4982,\n",
      "        -6.4644, -6.4652], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.4958, -5.5750, -6.4578, -6.1321, -6.4578,  0.8849, -5.3687, -6.4958,\n",
      "        -6.4578, -6.4958], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11319776624441147\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.5364, -5.0360, -5.6729, -5.4544], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.5662, -6.0533, -6.4789,  0.7208, -6.4789, -5.7160, -5.8237, -6.4677,\n",
      "        -6.4789, -6.4662], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.4575, -6.1057, -6.1057,  0.8765, -6.1057, -5.9217, -5.1012, -6.4479,\n",
      "        -6.1057, -6.4575], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10214260965585709\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.5384, -5.0414, -5.5371, -5.4095], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.6910, -6.5679, -6.1745, -6.1888, -5.6910, -6.4504, -6.3259, -6.0903,\n",
      "        -6.3288, -6.2483], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.7721, -6.4894, -6.0148, -6.1652, -5.7721, -6.4894, -6.1219, -6.0148,\n",
      "        -6.4812, -6.1652], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.012433836236596107\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.5384, -5.0549, -5.4035, -5.3639], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.4327, -6.1795, -6.1215, -6.1541, -4.8825,  0.7499, -6.1643, -6.0759,\n",
      "        -6.4327, -6.6487], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.5387, -6.1501, -6.0088, -6.1501, -4.8185,  0.8693, -6.2008, -6.0088,\n",
      "        -6.5387, -6.4998], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.008240913972258568\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-6.3148, -6.1537, -5.9361, -6.0328, -5.9361, -4.7817, -6.1537, -5.8154,\n",
      "        -4.7817, -5.7349], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.5216, -6.2338, -6.1756, -5.8186, -6.1756, -4.7986, -6.2338, -6.0500,\n",
      "        -4.7986, -5.8186], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.027884233742952347\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-6.6649, -6.1651, -6.3012, -5.8369, -6.1651, -4.7110, -6.3012, -6.3012,\n",
      "        -6.1768, -5.4991], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.5199, -6.2532, -6.5199, -6.0622, -6.2532, -4.7875, -6.5199, -6.5199,\n",
      "        -6.5948, -5.4891], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.041146524250507355\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.4824, -5.0202, -5.1318, -5.3291], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.8607, -5.4785, -6.3319, -6.4521, -6.3068, -6.3607, -6.4521, -6.3607,\n",
      "        -5.8123, -6.6416], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.1280, -5.4554, -6.2491, -6.5557, -6.2491, -6.5022, -6.5557, -6.5022,\n",
      "        -6.1280, -6.5022], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.026277834549546242\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-6.4464, -5.2461, -5.7987, -6.0939, -6.2961, -5.8136, -6.3555, -5.8136,\n",
      "        -5.9816, -5.1019], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.4665, -5.4673, -5.9932, -6.4665, -6.2242, -6.0635, -6.2242, -6.0635,\n",
      "        -5.1587, -5.5284], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1232602447271347\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.2469, -4.6788, -4.7198, -5.7162], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.8838, -4.8923, -5.6277, -6.3771, -5.1593, -5.9938, -4.6914, -6.5301,\n",
      "        -6.0616, -5.9938], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.9660, -5.4030, -5.6874, -6.1628, -5.4468, -5.9660, -4.8810, -6.3790,\n",
      "        -6.3790, -5.9660], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05608897656202316\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3869, -4.8469, -5.2681, -5.5071], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.4203, -5.9046, -4.7543, -6.5736, -5.9046, -6.4422,  0.7744, -6.1745,\n",
      "        -6.5804, -6.2873], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.1100, -5.8723, -4.9054, -6.2825, -5.8723, -6.1100,  0.8024, -6.2825,\n",
      "        -6.2971, -6.2971], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0409122072160244\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3693, -4.8350, -5.3644, -5.5253], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.5379, -4.8081, -6.3710, -5.8220, -6.5379, -6.5379, -6.2625, -5.8121,\n",
      "        -4.6316, -6.4496], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.2398, -4.8790, -6.2250, -5.8191, -6.2398, -6.2398, -6.2398, -5.8750,\n",
      "        -5.3515, -6.1037], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09351271390914917\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.2933, -4.8175, -5.3962, -5.4368], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 2.0055, -6.4111, -6.4111, -6.4582, -6.1623, -6.0008, -6.1974, -5.7985,\n",
      "        -4.8175, -5.4325], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([10.0000, -6.2186, -6.2186, -6.2219, -6.2186, -5.8071, -6.1336, -5.8071,\n",
      "        -5.3357, -5.2324], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.439566135406494\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.2016, -4.8086, -5.3854, -5.2760], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.4462, -5.7436, -5.6533, -5.9982, -5.3379, -6.4114, -6.1916, -6.2846,\n",
      "        -6.1916, -6.2846], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3069, -5.4982, -5.7226, -5.8041, -5.4982, -6.2024, -6.2024, -6.2264,\n",
      "        -6.2024, -6.2264], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.019852127879858017\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.1215, -4.8154, -5.3517, -5.1256], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.9708, -6.1215, -5.9708, -5.9062, -5.7521, -5.9952, -5.7964, -4.7115,\n",
      "        -4.8154, -5.9952], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.8188, -6.2151, -5.8188, -6.2151, -5.4861, -6.2101, -5.7203, -5.4415,\n",
      "        -5.3338, -6.2101], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11212091147899628\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0511, -4.8198, -5.3182, -5.0162], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.0192, -5.6549, -4.7935, -6.2431, -5.7924, -4.7395, -5.8540, -6.0192,\n",
      "        -4.7395, -6.0192], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.2451, -5.9385, -5.2838, -6.2132, -5.8250, -4.3447, -6.2132, -6.2451,\n",
      "        -4.3447, -6.2451], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09165774285793304\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0222, -4.7810, -5.2734, -4.9760], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.8888, -5.3273, -6.0053, -5.8658, -4.7508, -1.3451, -5.6711, -5.6593,\n",
      "        -4.8703, -5.8888], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.7945, -5.4357, -6.2284, -5.9007, -5.3833, -0.2491, -5.7945, -6.1732,\n",
      "        -5.3833, -5.7945], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2224212884902954\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9832, -4.7274, -5.2310, -4.9760], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.2289, -6.1189, -5.1492, -5.7217, -6.0057, -4.9778, -4.8719, -4.7815,\n",
      "        -5.5515, -5.8648], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.1031, -6.1031, -5.3034, -5.8486, -6.1971, -5.2546, -5.1488, -5.3034,\n",
      "        -5.3847, -5.7640], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.055610764771699905\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-5.7744, -4.6199, -5.7567, -4.5411, -5.7567, -5.4515, -6.0783, -5.2624,\n",
      "        -6.0783, -6.0783], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.0994, -4.3017, -6.0436, -5.0869, -6.0436, -5.2846, -6.1694, -5.3471,\n",
      "        -6.1694, -6.1694], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07294102013111115\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9390, -4.5837, -5.1000, -4.9830], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.2446, -5.9875, -6.3324, -5.6607, -5.7992, -5.4364, -4.7466, -4.7690,\n",
      "        -5.7930, -5.7992], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4773, -5.9684, -6.0193, -5.7402, -5.9684, -5.5512, -5.1464, -5.0128,\n",
      "        -5.9684, -5.9684], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.047930993139743805\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9409, -4.5020, -5.0366, -5.0245], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.5372, -5.8644, -5.9256, -4.9277, -4.9886, -5.0821, -5.8112, -4.4944,\n",
      "        -5.9256, -1.2339], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3996, -5.8858, -5.8858, -5.0518, -5.2640, -5.0518, -5.6453, -5.0450,\n",
      "        -5.8858, -0.1692], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15790587663650513\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9262, -4.4299, -4.9686, -5.0637], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.9295, -6.0742, -5.9295, -6.0742, -6.1173, -5.4248, -5.4895, -4.4034,\n",
      "        -5.9987, -5.1266], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.8192, -5.8823, -5.8192, -5.8823, -6.0052, -5.6261, -5.2542, -4.9631,\n",
      "        -5.8823, -5.2136], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05407456308603287\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9057, -4.3920, -4.9026, -5.0638], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.9438, -4.7541, -4.3920, -6.3806, -6.0908, -5.9438, -5.1280, -4.6670,\n",
      "        -4.3920, -5.9438], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.7883, -4.9079, -4.9528, -5.9881, -5.8666, -5.7883, -5.2003, -4.8307,\n",
      "        -4.9528, -5.7883], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09615789353847504\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8938, -4.3943, -4.8474, -5.0348], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.6823, -6.0551, -6.3614, -6.0551, -6.3614, -5.9022, -5.7286, -5.6823,\n",
      "        -4.2446, -4.4215], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6492, -5.8843, -6.0024, -5.8843, -6.0024, -5.7890, -5.7890, -5.6492,\n",
      "        -4.8202, -4.4907], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06708304584026337\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8793, -4.4231, -4.7929, -4.9653], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.1562, -5.8394, -5.6885, -6.0368, -6.2729, -5.9865, -4.9026, -4.9653,\n",
      "        -5.9633, -5.0023], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5129, -5.6406, -5.7085, -5.8192, -6.0397, -5.5021, -4.8788, -5.5129,\n",
      "        -5.9292, -5.4124], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09733929485082626\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9047, -4.5005, -4.7965, -4.9624], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.7214,  1.4270, -5.0894, -4.7965, -4.3466, -5.8612, -5.7214, -6.1667,\n",
      "        -5.8495, -5.8612], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.8671,  1.7419, -5.3092, -4.8618, -4.3957, -5.9792, -5.8671, -6.0873,\n",
      "        -5.7062, -5.9792], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02512728050351143\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-5.8458, -6.0825, -5.5905, -5.7923, -5.9617, -5.5905, -5.0707, -5.5760,\n",
      "        -5.5760, -5.6679], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.1162, -6.1162, -5.8034, -6.0105, -6.0105, -5.8034, -5.5015, -5.8988,\n",
      "        -5.8988, -5.8988], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06622599065303802\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-5.8029, -4.7953, -5.6967, -6.0406, -4.5739, -4.5739, -5.6799, -4.8061,\n",
      "        -5.6799, -5.7903], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6541, -4.8668, -5.7908, -6.0848, -5.1165, -5.1165, -5.8804, -4.8415,\n",
      "        -5.8804, -5.9930], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07496577501296997\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7681, -4.2286, -4.5654, -5.1645], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.6520, -5.6520, -5.6520, -5.6520, -4.5836, -5.7214, -6.0261, -5.9773,\n",
      "        -5.9773, -5.1041], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.7677, -5.7677, -5.7677, -5.7677, -5.1252, -5.8586, -6.0471, -5.9665,\n",
      "        -5.9665, -5.4900], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05153170973062515\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-6.0352, -6.0352, -6.0476, -5.8297, -5.8691,  1.5159, -4.4179, -4.6013,\n",
      "        -5.7067, -5.8691], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.9942, -5.9942, -5.9942, -5.8221, -5.9226,  1.7989, -4.4406, -5.1412,\n",
      "        -5.7310, -5.9226], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0384683720767498\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-6.0056, -5.8823, -5.9194, -5.7811, -6.0422, -6.1047, -4.1321, -5.7573,\n",
      "        -5.8667,  0.9008], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.8846, -5.7912, -5.8846, -5.7912, -5.9479, -5.9479, -4.7189, -5.6998,\n",
      "        -5.8846,  1.8021], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12181184440851212\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-5.8777, -5.9325, -4.0998, -6.0166, -0.1019, -5.9046, -5.4253, -6.0233,\n",
      "        -5.8232, -5.7875], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.7745, -5.8572, -4.6899, -5.8572, -0.1698, -5.6911, -5.3081, -5.9174,\n",
      "        -5.7745, -5.6782], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.047931231558322906\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-5.4048, -4.7744, -5.1553, -5.9834, -5.9834, -4.8027, -4.1508, -5.1553,\n",
      "        -6.0079, -4.4769], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2964, -5.2211, -5.2964, -5.9103, -5.9103, -4.7358, -4.7358, -5.2964,\n",
      "        -5.9103, -4.4826], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06179099157452583\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-5.4626, -5.8757, -5.9533, -5.4201, -6.0549, -4.7578, -5.8083, -5.9144,\n",
      "        -5.8827, -4.7203], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6725, -5.7997, -5.6817, -5.7187, -5.8781, -4.6878, -5.6817, -5.8781,\n",
      "        -5.7997, -5.2482], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05518703535199165\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3097, -4.7703, -5.3599, -5.5250], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.8749, -5.8723, -5.8577, -5.0859, -5.8723, -5.9282, -5.8723, -4.7703,\n",
      "        -5.4178, -5.9275], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.1034, -5.8384, -5.9312, -5.2763, -5.8384, -5.9430, -5.8384, -5.2933,\n",
      "        -5.3360, -5.7163], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09653525054454803\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3101, -4.8214, -5.3657, -5.5461], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.8695, -5.8545, -4.8214, -3.8281, -5.9947, -5.8545, -4.8214, -5.9148,\n",
      "        -5.8545, -5.7867], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.8780, -5.9695, -5.3393, -0.0980, -5.9809, -5.9695, -5.3393, -5.9809,\n",
      "        -5.9695, -5.7548], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.4495174884796143\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.2681, -4.8260, -5.3276, -5.4663], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.8002, -5.6877, -4.5204, -5.8002, -5.3276, -5.8170, -5.7309, -4.8260,\n",
      "        -4.8260, -5.7309], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.9787, -5.7522, -4.7413, -5.9787, -4.7413, -5.8660, -5.9570, -5.3434,\n",
      "        -5.3434, -5.9570], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11005978286266327\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.2103, -4.7895, -5.2439, -5.3828], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.7895, -5.7368, -5.6683, -5.6563, -0.8234, -5.6683, -5.6609, -5.6609,\n",
      "        -4.7357, -5.8396], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3105, -5.9673, -5.8416, -5.7286, -0.1733, -5.8416, -5.9374, -5.9374,\n",
      "        -4.7560, -5.8416], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09659420698881149\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-5.6499, -5.7238, -5.6041, -4.4943, -3.8728, -5.6499, -5.6055, -5.6499,\n",
      "        -5.6041, -5.0502], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.7968, -5.7968, -5.6880, -3.8224, -3.8224, -5.7968, -5.6958, -5.7968,\n",
      "        -5.6880, -5.2091], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0571642741560936\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.5574, -4.0706, -4.4439, -4.8587], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.7646, -5.4213, -5.5500, -5.1500, -5.1498, -3.7476, -5.0137, -5.0922,\n",
      "        -5.5500, -5.7604], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.8792, -5.5681, -5.6348, -5.1454, -5.1616, -3.7532, -5.1454, -5.1616,\n",
      "        -5.6348, -5.8425], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.007813027128577232\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-5.5136, -5.7125, -4.6024, -5.5136, -5.7125,  1.6748, -5.7093, -5.7212,\n",
      "        -5.5136, -5.6214], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5790, -5.8302, -5.1422, -5.5790, -5.8302,  1.7290, -5.6786, -5.6786,\n",
      "        -5.5790, -5.8302], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03811495378613472\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-4.9780, -4.4622, -4.9799, -5.3049], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.5809, -0.1420, -5.0228, -5.0933, -4.5358, -5.6388, -5.5809, -5.8475,\n",
      "        -4.9780, -5.8161], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5269, -0.4082, -5.0489, -5.1213, -5.0822, -5.7628, -5.5269, -5.7628,\n",
      "        -5.0160, -5.6149], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.044115908443927765\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.5272, -3.9003, -4.3569, -4.9000], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-3.4916, -4.4484,  2.9904, -4.4858, -5.5768, -5.4794, -5.4794, -5.7806,\n",
      "        -5.7189, -5.4456], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6072, -4.4912, 10.0000, -5.0372, -5.4800, -5.4698, -5.4698, -5.5696,\n",
      "        -5.6806, -5.4698], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.950932025909424\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-4.3357, -5.7326, -5.5271, -4.3613, -5.4589, -5.4589, -3.8657, -5.0049,\n",
      "        -5.7720, -3.8657], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0065, -5.6510, -5.5382, -4.4791, -5.4359, -5.4359, -4.4791, -4.4514,\n",
      "        -5.5382, -4.4791], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1585371196269989\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-4.9144, -4.3345, -4.9019, -5.2488], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.5071, -5.5071, -5.7825, -4.4664, -3.3101, -3.3101, -5.7825, -4.3748,\n",
      "        -5.8077, -4.3748], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5127, -5.5127, -5.6330, -4.9374, -3.3822, -3.3822, -5.6330, -4.9374,\n",
      "        -5.6407, -4.9374], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09376543760299683\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.5236, -3.8800, -4.3571, -4.8783], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.7063, -5.6903, -3.2348, -5.7505, -5.3865, -5.5930, -5.4190, -5.4190,\n",
      "        -4.3084, -5.1444], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5055, -5.6300, -3.2670, -5.6300, -5.4121, -5.5055, -5.4121, -5.4121,\n",
      "        -4.8776, -5.4093], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04620002955198288\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.5249, -3.9038, -4.3619, -4.8557], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.6628, -5.7158, -5.7158, -5.7158, -5.3976, -5.6628, -3.8299, -5.6628,\n",
      "        -5.1607, -5.3976], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6446, -5.6455, -5.6455, -5.6455, -5.4220, -5.6446, -4.4469, -5.6446,\n",
      "        -5.3080, -5.4220], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.041940636932849884\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.5264, -3.9321, -4.3676, -4.8282], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.6007, -5.4637, -5.5745, -4.8979, -5.6748, -5.5980, -5.4637, -3.8478,\n",
      "        -5.5745, -3.9321], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5245, -5.5245, -5.6660, -4.9090, -5.6652, -5.5245, -5.5245, -4.4630,\n",
      "        -5.6660, -4.5389], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07822678983211517\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-5.3691, -5.5524, -4.8097, -5.5133, -5.6345, -5.6693,  1.9408, -4.1966,\n",
      "        -4.3859, -5.5133], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4527, -5.5417, -4.9201, -5.6864, -5.6845, -5.6845,  2.2578, -4.7770,\n",
      "        -4.4820, -5.6864], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05284925550222397\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.5491, -3.9842, -4.4072, -4.8212], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.5296, -5.5080, -5.4145, -5.3686, -5.3686, -4.1651, -5.6805, -5.6235,\n",
      "        -3.0085, -5.6235], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5479, -5.6930, -5.4115, -5.4563, -5.4563, -4.7486, -5.6935, -5.6935,\n",
      "        -2.8861, -5.6935], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04153218865394592\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.5677, -3.9973, -4.4378, -4.8481], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.1325, -4.5677, -5.2153, -5.0494, -0.1265, -5.3793, -5.4672, -5.6859,\n",
      "        -4.8296, -5.3263], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7192, -4.5976, -5.4013, -5.4513, -0.2877, -5.4513, -5.5445, -5.6887,\n",
      "        -5.1390, -5.4513], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06897898018360138\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.5917, -4.0028, -4.4768, -4.8905], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.3292, -5.3890, -4.8101, -2.9601, -5.3890, -4.1164, -5.5028, -5.3975,\n",
      "        -5.3020, -5.3890], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6025, -5.4289, -4.8963, -2.8201, -5.4289, -4.7047, -5.5461, -5.3821,\n",
      "        -5.3821, -5.4289], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04611659422516823\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.6120, -3.9964, -4.5073, -4.9248], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.4050, -3.9964, -4.8723, -5.4050, -5.7029, -4.1020, -5.6354, -4.9248,\n",
      "        -2.9486, -4.8449], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4063, -4.5968, -5.1111, -5.4063, -5.6905, -4.6918, -5.6639, -4.8905,\n",
      "        -2.8047, -4.8905], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07901963591575623\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-5.4756, -5.0146, -4.5355, -4.9643, -3.4806, -4.3697, -5.5784, -5.6960,\n",
      "        -3.8564, -4.9643], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6500, -5.0888, -4.6927, -4.8846, -4.4707, -4.4707, -5.5436, -5.6500,\n",
      "        -4.4707, -4.8846], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14446860551834106\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-4.9089, -4.3014, -4.9412, -5.1331], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.7104, -5.4968, -5.7915, -5.8222, -5.7503, -5.7104, -5.0313, -4.9775,\n",
      "        -5.5975, -5.7915], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5282, -5.2556, -5.6697, -5.6180, -5.6180, -5.5282, -5.3539, -5.0516,\n",
      "        -5.5282, -5.6697], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03278182074427605\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-2.9770, -4.0952, -5.7199, -5.7840, -5.6896, -5.7199, -2.0459, -5.5419,\n",
      "        -5.7840, -5.8210], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.8413, -4.6856, -5.5484, -5.6692, -5.5484, -5.5484, -0.3252, -5.6211,\n",
      "        -5.6692, -5.6211], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.34791073203086853\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-5.7034, -5.6423, -5.2637, -5.6827, -5.6827, -5.5508, -5.4506, -3.7618,\n",
      "        -5.7034, -5.6827], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6672, -5.5732, -5.2881, -5.6304, -5.6304, -5.2881, -5.3349, -4.3857,\n",
      "        -5.6672, -5.6304], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0487758032977581\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-0.1891, -5.5870, -5.3222, -5.5885, -5.7872, -5.3970, -5.3970, -5.6135,\n",
      "        -5.3222, -5.5870], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3524, -5.6533, -5.3410, -5.6099, -5.6533, -5.6099, -5.6099, -5.6783,\n",
      "        -5.3410, -5.6533], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.014941307716071606\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-5.4734, -5.3014, -5.5566, -5.5566, -5.5566, -5.5566, -4.1405, -5.4734,\n",
      "        -4.0096, -4.8269], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6158, -5.3442, -5.6779, -5.6779, -5.6779, -5.6779, -4.7265, -5.6158,\n",
      "        -4.6086, -4.9664], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08229155093431473\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-5.5482, -5.1236, -4.9946, -4.6600, -5.5482, -5.4537, -4.8595, -5.4915,\n",
      "        -5.2963, -5.1883], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6695, -5.3394, -4.9212, -4.3569, -5.6695, -5.6112, -4.9212, -5.6112,\n",
      "        -5.3394, -5.2899], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.022832978516817093\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.1133, -4.4769, -4.6109, -5.1228], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.8342, -5.5422, -4.8790, -5.4730, -5.0112, -5.7793, -5.1228, -5.8342,\n",
      "        -3.7177, -4.7532], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6609, -5.6544, -4.9140, -5.6075, -5.2699, -5.6544, -5.3558, -5.6609,\n",
      "        -4.3460, -4.6047], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06455592811107635\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-5.1650, -5.2885, -5.4859, -5.3755, -5.6018, -2.5924, -3.9995, -5.4859,\n",
      "        -5.5197, -4.1121], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2424, -5.3057, -5.5977, -5.2424, -5.6485, -2.3984, -4.5995, -5.5977,\n",
      "        -5.5977, -4.7009], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08016030490398407\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.7253, -3.9959, -4.5144, -4.9671], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.9740, -5.4996, -5.4394, -4.5144, -4.1000, -4.1000, -5.6562, -4.7652,\n",
      "        -4.3702, -4.1000], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9332, -5.5899, -5.2887, -4.6900, -4.6900, -4.6900, -5.6392, -4.9332,\n",
      "        -4.3278, -4.6900], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11379875242710114\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.7253, -4.0061, -4.5429, -5.0205], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.6891, -4.0963, -5.6891, -5.1011, -5.5488, -4.9758, -5.3172, -5.5720,\n",
      "        -4.3497, -4.3497], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6365, -4.6867, -5.6365, -5.2892, -5.5910, -4.9147, -5.2892, -5.5910,\n",
      "        -4.6055, -4.6055], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.052695851773023605\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.7065, -4.0066, -4.5516, -5.0496], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.7623, -5.7623, -5.5546, -5.7231, -5.1641, -4.6465, -4.6465, -5.7231,\n",
      "        -4.1020, -5.7289], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6477, -5.6477, -5.6036, -5.6420, -5.3490, -4.3316, -4.3316, -5.6420,\n",
      "        -4.6918, -5.6477], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06288709491491318\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-5.7804, -4.0245, -2.5064, -0.3442, -5.7804, -5.7311, -5.7122, -5.7804,\n",
      "        -4.0245, -5.3286], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6844, -4.6221, -2.3693, -0.5790, -5.6844, -5.6652, -5.6844, -5.6844,\n",
      "        -4.6221, -5.3173], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08209967613220215\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.6616, -4.0557, -4.5316, -5.0578], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.6941, -5.6906, -5.7272, -5.6125, -5.1893, -5.6615, -5.6941, -5.3207,\n",
      "        -5.7272, -4.6679], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.7309, -5.6974, -5.6703, -5.6703, -5.3504, -5.7309, -5.7309, -5.3504,\n",
      "        -5.6703, -4.6502], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.004453395493328571\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.6481, -4.0832, -4.5263, -5.0604], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.4398, -5.7364, -5.3141, -5.3141, -5.7475, -5.7207, -4.8947, -5.7475,\n",
      "        -5.5621, -5.7364], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3772, -5.7078, -5.3772, -5.3772, -5.7687, -5.7252, -4.9550, -5.7687,\n",
      "        -5.7078, -5.7078], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.003927880898118019\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.6369, -4.1041, -4.5305, -5.0645], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.1086, -4.1086, -5.2732, -5.7470, -5.7406, -3.8647, -5.3219, -4.7114,\n",
      "        -4.8874, -5.6749], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6977, -4.6977, -5.2403, -5.8014, -5.7328, -4.4782, -5.3986, -5.1079,\n",
      "        -4.9655, -5.8014], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12598580121994019\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-5.3969, -5.3403, -4.1277, -5.7226, -5.7589, -5.7226, -5.6830, -5.1157,\n",
      "        -4.1180, -5.3403], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4635, -5.4237, -4.7150, -5.7669, -5.7629, -5.7669, -5.8314, -5.2652,\n",
      "        -4.7062, -5.4237], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0757460966706276\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.6613, -4.1453, -4.5647, -5.0964], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.4403, -5.6090, -5.4276, -4.6613, -4.7578, -5.7375, -2.4403, -4.1236,\n",
      "        -5.7198, -4.6077], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2913, -5.7870, -5.4740, -4.7308, -5.1470, -5.7793, -2.2913, -4.7112,\n",
      "        -5.8473, -4.7308], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.061302054673433304\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-4.2834, -5.8464, -5.5322, -5.4019, -5.7962, -5.7631, -5.3419, -5.7962,\n",
      "        -4.1279, -5.0610], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.3032, -5.8078, -5.7859, -5.4553, -5.8523, -5.7859, -5.4553, -5.8523,\n",
      "        -4.7151, -5.1496], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4361952245235443\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.7279, -4.1492, -4.5592, -5.1882], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.7998, -4.6489, -5.9590, -1.1137, -5.8435, -4.1252, -5.6599, -2.5102,\n",
      "        -5.8927, -4.7998], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1417, -4.7342, -5.8177, -0.7206, -5.8393, -4.7127, -5.8177, -2.4554,\n",
      "        -5.8393, -5.1417], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07915596663951874\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.7584, -4.1469, -4.5389, -5.2453], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.8696, -5.8758, -5.8479, -5.8758, -5.3592, -3.8407, -5.2482, -5.8758,\n",
      "        -4.1469, -4.1338], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9667, -5.7618, -5.7618, -5.7618, -5.4518, -4.4566, -5.4741, -5.7618,\n",
      "        -4.7322, -4.7205], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11815520375967026\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.7867, -4.1536, -4.5339, -5.2947], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.1536, -5.8878, -4.7867, -4.1536, -5.9117, -5.6063, -6.0397, -6.0397,\n",
      "        -5.9117, -5.3563], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7383, -5.7599, -4.7383, -4.7383, -5.7599, -5.8495, -5.8300, -5.8300,\n",
      "        -5.7599, -5.4588], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09058663994073868\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-6.1755, -5.3647, -5.3170, -4.7527, -5.3647, -5.3389, -5.3170, -5.5695,\n",
      "        -5.7839, -6.1755], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.8775, -5.4807, -5.1086, -4.7600, -5.4807, -5.4912, -5.1086, -5.4807,\n",
      "        -5.7745, -5.8775], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03226255252957344\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7738, -4.2161, -4.5300, -5.1717], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.3364, -4.5353, -4.1953, -6.1405,  4.0715, -6.0258, -5.4287, -6.0258,\n",
      "        -5.3848, -6.0258], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5263, -4.7945, -4.7758, -5.9148, 10.0000, -5.8859, -5.5150, -5.8859,\n",
      "        -5.5174, -5.8859], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.5722241401672363\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7814, -4.2512, -4.5410, -5.1148], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.8161, -5.9157, -5.9157, -5.8191, -5.5831, -4.2365, -6.0502, -5.5173,\n",
      "        -4.2365, -5.4458], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0193, -5.9410, -5.9410, -5.9410, -5.5954, -4.8129, -5.9656, -5.5675,\n",
      "        -4.8129, -5.5457], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07415787130594254\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7929, -4.2814, -4.5627, -5.0695], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.7526, -5.4544, -5.2861, -5.8250, -5.7487, -4.2758, -5.7487, -5.4262,\n",
      "        -5.6302, -5.4303], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.8872, -5.6091, -5.2052, -5.8872, -6.0096, -4.8482, -6.0096, -5.6581,\n",
      "        -6.0096, -5.6581], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07660195976495743\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-4.2889, -4.2856, -5.5703, -4.2889, -5.4237, -6.0431,  2.5115, -3.7951,\n",
      "        -5.4237, -4.2856], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8600, -4.8570, -5.6218, -4.8600, -5.6777, -6.0073,  3.0272, -4.4156,\n",
      "        -5.6777, -4.8570], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.20892992615699768\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.8848, -4.3053, -4.7170, -5.1452], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.6627, -4.2944, -5.9053, -4.2944, -4.3053, -5.6103, -4.3053, -5.1452,\n",
      "        -4.3053, -4.3053], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5704, -4.8650, -6.0172, -4.8650, -4.8748, -5.6322, -4.8748, -5.2528,\n",
      "        -4.8748, -4.8748], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.19813594222068787\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.9333, -4.3427, -4.7995, -5.1802], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.0215, -4.7630, -5.6847, -5.6847, -5.7678, -5.9086, -4.3175, -5.8745,\n",
      "        -2.9964, -4.3427], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0457, -4.9085, -5.6518, -5.6518, -5.9417, -6.0322, -4.8858, -6.0568,\n",
      "        -2.8227, -4.9085], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0775834321975708\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.9778, -4.3692, -4.8760, -5.2244], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.6346, -5.8144, -4.8760, -5.7564, -4.8300, -5.3614, -5.0532, -4.9022,\n",
      "        -4.3692, -4.7970], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5857, -6.0711, -4.9028, -5.6640, -4.9028, -5.3173, -5.0490, -4.9323,\n",
      "        -4.9323, -4.9323], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04210498929023743\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-4.3900, -4.3900, -4.3505, -5.2725, -5.4221, -5.5374, -3.1031, -4.3505,\n",
      "        -3.1031, -5.9862], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9510, -4.9510, -4.9154, -5.3483, -5.3483, -5.7027, -2.8987, -4.9154,\n",
      "        -2.8987, -6.0449], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13932518661022186\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-4.4224, -5.8763, -5.9329, -5.1409, -6.1228, -4.4224, -4.3742, -4.3742,\n",
      "        -5.9484, -6.0339], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9801, -5.6907, -6.0958, -5.0603, -6.0541, -4.9801, -4.9368, -4.9368,\n",
      "        -6.0126, -6.0541], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13318908214569092\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-6.0937, -6.0937, -6.0129, -5.5889, -5.9137, -5.6879, -3.1758, -6.1612,\n",
      "        -5.9156, -5.9137], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.0720, -6.0720, -6.1191, -5.3976, -5.7170, -5.6198, -3.0082, -6.1191,\n",
      "        -6.1191, -5.7170], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.020207479596138\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.1756, -4.5495, -5.1688, -5.6071], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.6587, -6.1078, -4.5107, -6.1078, -6.0932, -3.7995, -6.2173, -6.0932,\n",
      "        -6.1813,  4.9658], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0946, -6.0848, -5.0597, -6.0848, -6.1387, -4.4196, -6.1387, -6.1387,\n",
      "        -6.1387, 10.0000], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.623183250427246\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3844, -3.8288, -4.1574, -4.7222], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.8990, -6.1849, -5.4100, -6.1900, -6.1192, -5.1492, -5.6940, -5.5645,\n",
      "        -5.5645, -6.1900], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.7781, -6.1166, -5.4683, -6.1246, -6.1246, -5.0955, -5.7781, -5.4747,\n",
      "        -5.4747, -6.1246], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.005733089055866003\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.4071, -3.8643, -4.1602, -4.7687], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.2397, -6.1875, -6.2197, -6.1875, -5.8742, -5.0159, -5.7291, -5.8742,\n",
      "        -5.6534, -4.1602], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.2165, -6.2165, -6.1562, -6.2165, -5.8160, -5.0750, -5.7797, -5.8160,\n",
      "        -5.8160, -3.0481], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12822553515434265\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3804, -3.8560, -4.0794, -4.7720], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.3377, -5.6511, -4.5635, -3.8560, -5.8452, -5.2604, -5.0756, -5.8263,\n",
      "        -6.2916, -6.2630], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5506, -5.5506, -5.1072, -4.4704, -5.8039, -5.5530, -5.1188, -5.7344,\n",
      "        -6.1983, -6.2521], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08349715173244476\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3537, -3.8616, -4.0087, -4.7739], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.8277, -4.3537, -5.7474, -5.0341, -3.1189, -3.1189, -5.7885, -6.3493,\n",
      "        -5.7474, -6.3220], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.8767, -5.1502, -5.8767, -5.1502, -3.1769, -3.1769, -5.8767, -6.2344,\n",
      "        -5.8767, -6.2449], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07173582911491394\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3940, -3.8768, -3.9947, -4.8142], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.0033, -4.5981, -5.9098, -5.1100, -3.8768, -6.2920, -5.5599, -6.0033,\n",
      "        -5.7109, -6.3950], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.8129, -5.1383, -5.8129, -5.1383, -4.4892, -6.3188, -5.5990, -5.8129,\n",
      "        -5.8822, -6.2520], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08014572411775589\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-3.1526, -6.4268, -3.9899, -6.4272, -6.4272, -5.4274, -6.4268, -5.8131,\n",
      "        -5.8632, -5.3267], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2718, -6.3415, -3.2718, -6.2768, -6.2768, -5.1627, -6.3415, -5.8847,\n",
      "        -5.8831, -5.2828], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0667203962802887\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.4027, -3.9045, -3.9322, -4.8323], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.0219, -5.6192, -6.4517, -4.9948, -5.8227, -6.3920, -6.0075, -4.7918,\n",
      "        -4.6046, -5.8938], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1442, -5.8822, -6.3679, -5.1915, -5.6550, -6.3249, -6.3249, -5.3127,\n",
      "        -5.1442, -5.9095], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0825880914926529\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-6.1985, -6.4502, -6.3222, -5.8808, -6.4936, -6.4012, -3.2671, -5.8775,\n",
      "        -6.3276, -4.3949], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.9523, -6.3225, -6.3331, -5.8703, -6.3891, -6.3331, -3.4305, -5.8703,\n",
      "        -6.3891, -5.1381], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0675501897931099\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3011, -4.6999, -5.0409, -5.8883], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.4608, -4.8270, -5.8358, -4.4487, -4.5593, -6.4931, -4.8270, -4.8270,\n",
      "        -3.3299, -6.3781], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.3353, -5.3443, -5.6991, -5.1034, -5.1034, -6.4056, -5.3443, -5.3443,\n",
      "        -3.4750, -6.3376], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1592191904783249\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5517, -3.9659, -3.9733, -4.8615], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.4045, -6.4045, -6.3365,  6.2462, -6.2545, -4.8471, -4.8471, -6.3365,\n",
      "        -4.8471, -6.4045], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.3567, -6.3567, -6.3476, 10.0000, -6.3476, -5.3623, -5.3623, -6.3476,\n",
      "        -5.3623, -6.3567], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.4903124570846558\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-6.2651, -6.4035, -6.1944, -1.4562, -5.6404, -5.6155, -6.1151, -5.5391,\n",
      "        -6.3553, -5.9669], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.3702, -6.4691, -6.3702, -0.5396, -5.6142, -5.8728, -6.3702, -5.4003,\n",
      "        -6.3856, -5.8728], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10472993552684784\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5855, -4.9770, -3.4651, -2.7346], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-6.4197, -5.2800, -6.5347, -6.2079, -3.4651, -4.4210, -4.0449, -6.3420,\n",
      "        -6.3420, -6.2079], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.4778, -5.6361, -6.4778, -6.3447, -3.4611, -4.9789, -4.6404, -6.3881,\n",
      "        -6.3881, -6.3447], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08409657329320908\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4614, -2.3644, -0.4203,  0.5536], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-6.4752, -6.4752, -4.9037, -5.3012, -6.4752, -6.4752, -6.2433, -6.5872,\n",
      "        -4.3919, -5.1668], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.4788, -6.4788, -5.4133, -5.6227, -6.4788, -6.4788, -6.3077, -6.3853,\n",
      "        -4.9527, -4.9527], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07683835178613663\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.3561, -2.9764,  3.9017,  6.9971], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-6.3984, -6.2382, -6.5457, -4.3968, -6.4976, -6.4976, -5.3704, -6.6123,\n",
      "        -6.2382, -6.7304], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.3847, -6.2765, -6.4825, -4.9571, -6.4825, -6.4825, -5.5909, -6.3847,\n",
      "        -6.2765, -6.3847], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.054144419729709625\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-6.6019, -6.0050, -6.3161, -6.4365], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.9155, -4.9155, -6.2524, -6.1946, -6.2524, -6.4365, -6.5859, -4.4232,\n",
      "        -5.8899, -6.6880], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4239, -5.4239, -6.2578, -5.9554, -6.2578, -6.4045, -6.4858, -4.9809,\n",
      "        -6.1376, -6.4858], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09986026585102081\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-6.1585, -5.5277, -5.8566, -6.4232], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.5903, -4.4628, -6.4651, -4.4628, -4.8167, -6.3964, -5.9171, -5.8431,\n",
      "        -5.5277, -6.2614], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4514, -5.0165, -6.4371, -5.0165, -4.7454, -6.5116, -6.1574, -5.5606,\n",
      "        -5.8274, -6.2573], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08790434151887894\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-4.2058, -5.8532, -5.8326, -6.4987, -6.2891, -4.9826, -5.5662, -4.9826,\n",
      "        -6.2891, -6.6454], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7852, -5.9479, -5.5812, -6.4727, -6.2679, -5.4843, -5.4843, -5.4843,\n",
      "        -6.2679, -6.5399], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09308486431837082\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-6.1126, -5.6173, -5.8560, -6.4543], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.2642, -6.2955, -6.5175, -5.5470, -5.5413, -6.5175, -6.6463, -6.5358,\n",
      "        -4.2596, -5.8917], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8336, -6.3026, -6.5242, -5.3992, -5.5382, -6.5242, -6.5894, -6.5242,\n",
      "        -3.6362, -5.9923], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07482150942087173\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-5.9901, -5.4236, -5.7322, -6.6083], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.8724, -5.2388, -6.2886, -6.3239, -4.6415, -6.2050, -6.0096, -6.3025,\n",
      "        -6.6585, -6.7581], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.0364, -5.4506, -6.6475, -6.3502, -5.1774, -6.1123, -6.3502, -6.3502,\n",
      "        -6.6475, -6.6475], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06275902688503265\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-5.5314, -5.1415, -5.1121, -5.7949], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.7118, -3.7406, -6.1086, -3.7406, -4.3421, -6.4193, -5.7949, -6.2955,\n",
      "        -6.4193, -6.6963], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.8857, -3.8063, -6.1406, -3.8063, -4.7749, -6.3761, -5.8857, -6.2625,\n",
      "        -6.3761, -6.6659], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02412591129541397\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.2551, -4.7394, -5.0167, -5.9624], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.8650, -5.1111, -5.1867, -6.7154, -4.7394, -3.7812, -6.6103, -6.3426,\n",
      "        -6.4892, -4.1699], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.0803, -5.2654, -5.6000, -6.6528, -5.2654, -3.8796, -6.6471, -6.6528,\n",
      "        -6.4127, -3.8796], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.071901336312294\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3081, -4.7904, -5.0170, -6.0330], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.7904, -6.6393, -5.5538, -5.7895, -6.4006, -5.9620, -6.6051, -5.0819,\n",
      "        -5.8475, -6.7036], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3114, -6.6514, -5.6788, -6.3182, -6.6008, -5.9058, -6.6514, -5.3114,\n",
      "        -6.1145, -6.6008], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07464834302663803\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3661, -4.8121, -5.0437, -6.0963], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.3318, -4.9749, -5.1849, -6.4850, -5.4437, -5.6034, -6.6870, -6.6728,\n",
      "        -5.9007, -6.6519], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5459, -5.3309, -5.5459, -6.5798, -5.6112, -5.6982, -6.5798, -6.6635,\n",
      "        -5.6982, -6.4976], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.042536549270153046\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.4274, -4.7986, -5.0619, -6.1358], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.2142, -6.1358, -6.7234, -4.1364, -5.9379, -6.3109, -5.3082, -6.0740,\n",
      "        -6.6925, -4.7986], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5404, -5.7297, -6.5105, -4.1225, -5.8835, -6.1212, -5.6394, -6.1212,\n",
      "        -6.6883, -5.3188], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07383007556200027\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.4510, -4.7817, -5.0685, -6.0952], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.2473, -5.4510, -5.8745, -5.7068, -6.6902, -6.7293, -6.7293, -5.8035,\n",
      "        -5.2473, -6.5859], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5416, -5.6469, -5.8858, -5.5416, -6.7105, -6.5396, -6.5396, -5.6469,\n",
      "        -5.5416, -6.5615], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.033656712621450424\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.4833, -4.7711, -5.1028, -6.0562], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.5082, -6.5601, -4.7711, -6.3508, -6.4944, -6.0562, -6.2961, -5.3710,\n",
      "         0.5344, -6.6844], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.3504, -6.3504, -5.2940, -6.3504, -6.5736, -5.8547, -6.5610, -5.6559,\n",
      "         6.2101, -6.5610], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.2769603729248047\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-6.0947, -4.0435, -5.9864, -6.2351, -4.1822, -4.0435, -4.7154, -5.3629,\n",
      "        -5.3945, -6.4857], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.5492, -4.0417, -6.1006, -6.4852, -4.0417, -4.0417, -5.2439, -5.5676,\n",
      "        -5.5127, -6.6870], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06775953620672226\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.0060, -5.3715, -5.4119, -6.3465], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.1067, -6.1067, -6.1215, -5.3420, -5.9760, -6.1215, -6.3832, -4.4391,\n",
      "        -5.9760, -5.9760], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.4645, -6.4645, -6.0542, -5.4981, -6.0542, -6.0542, -6.6455, -4.7339,\n",
      "        -6.0542, -6.0542], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04635416716337204\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3031, -4.6014, -4.9665, -5.6565], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.3031, -6.3695, -5.2758, -5.3245, -5.9497, -6.2940, -5.9497, -5.3245,\n",
      "        -4.4135, -6.2940], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4094, -6.6066, -5.4094, -5.4094, -6.0175, -6.4674, -6.0175, -5.4094,\n",
      "        -4.6811, -6.4674], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.024070028215646744\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.2777, -4.5560, -4.9182, -5.6159], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.0165, -6.7139, -5.8650, -6.7139, -6.3982, -3.9616, -6.0108, -6.1736,\n",
      "        -6.1896, -6.2842], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.7112, -6.5706, -5.7112, -6.5706, -6.5706, -3.9374, -6.4264, -6.0003,\n",
      "        -6.2785, -6.4264], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04189705476164818\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.2228, -4.5064, -4.8865, -5.5893], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.0298, -5.3701, -6.2917, -5.2119, -6.2255, -6.2917, -6.1277, -5.9623,\n",
      "        -5.2666, -6.1417], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.3661, -5.3501, -6.3661, -5.3501, -6.4269, -6.3661, -5.9453, -5.9439,\n",
      "        -5.2070, -6.4269], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03026215359568596\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-5.4326, -4.3278, -6.6159, -6.3366, -5.2306, -6.4343, -6.4965, -5.4377,\n",
      "        -6.3366, -3.9532], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.7075, -4.5376, -6.4837, -6.2811, -5.0023, -6.1889, -6.4837, -5.3048,\n",
      "        -6.2811, -3.9983], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02754531241953373\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-5.3421, -5.1762, -4.5343, -4.7553], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-6.3486, -4.4148,  7.3128, -4.7553, -5.1401, -5.1309, -5.1762, -6.1011,\n",
      "        -6.5365, -5.5891], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.2205, -4.9733, 10.0000, -5.6261, -5.0809, -5.0809, -5.0809, -6.2205,\n",
      "        -6.4585, -5.6521], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.8347083330154419\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.1009, -4.3771, -4.8508, -5.6161], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.4289, -4.9015, -6.4289, -6.0953, -6.4289, -6.4289, -6.6369, -5.8800,\n",
      "        -6.3862, -6.6369], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.3609, -4.9394, -6.3609, -6.1541, -6.3609, -6.3609, -6.4188, -5.8154,\n",
      "        -6.3609, -6.4188], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.012340608984231949\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-4.3558, -6.0151, -4.3558, -5.3244, -5.8758, -6.0787, -6.4355, -6.3134,\n",
      "        -5.2138, -6.1306], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9202, -6.0659, -4.9202, -5.2387, -5.7920, -5.7920, -6.3368, -6.3368,\n",
      "        -5.2387, -6.1148], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07474568486213684\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0444, -4.3547, -4.8432, -5.5920], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.6324, -6.3271, -6.3271, -5.8743, -5.8622, -6.1831, -6.6216, -6.3271,\n",
      "        -3.9102, -4.3547], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.4256, -6.0960, -6.0960, -5.7898, -5.7898, -6.0457, -6.0960, -6.0960,\n",
      "        -3.9829, -4.9192], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08345294743776321\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0110, -4.3939, -4.8304, -5.5206], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.3880, -3.6988, -5.3454, -6.5657, -5.0110, -4.3939, -6.5010, -4.3207,\n",
      "        -6.5657, -6.0629], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5521, -3.8944, -5.2512, -6.4797, -5.2192, -4.9545, -6.1323, -4.3289,\n",
      "        -6.4797, -6.0724], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05826066806912422\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0100, -4.4526, -4.8309, -5.4510], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 2.4736, -6.5177, -3.8300, -5.2965, -5.2965, -6.1550, -5.7744, -6.4722,\n",
      "        -5.4510, -6.0597], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 5.8083, -6.5395, -3.7784, -5.2091, -5.2091, -6.1105, -5.8248, -6.5395,\n",
      "        -5.5801, -6.1875], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.11809504032135\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9890, -4.4717, -4.8193, -5.3675], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.1534, -6.0451, -5.7016, -5.7016, -4.9890, -5.7140, -4.6586, -4.4717,\n",
      "        -6.0451, -5.3023], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.1929, -6.4167, -5.7964, -5.7964, -5.1927, -5.8956, -5.0245, -5.0245,\n",
      "        -6.4167, -5.1927], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08216741681098938\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9850, -4.4743, -4.8140, -5.3283], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.7539, -4.4743, -6.2900, -5.2925, -6.3737, -5.8524, -6.0091, -5.6921,\n",
      "        -4.4743, -5.3201], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.2086, -5.0268, -6.5454, -5.2100, -6.4063, -6.1801, -6.4063, -6.0845,\n",
      "        -5.0268, -5.2100], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1412467658519745\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9901, -4.4516, -4.8302, -5.3542], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.9543, -4.7354, -6.3079, -4.4516, -4.3344, -6.3079, -6.3808, -6.1199,\n",
      "        -5.8469, -5.8836], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.8211, -5.0064, -6.5079, -5.0064, -4.2501, -6.5079, -6.3588, -6.0165,\n",
      "        -6.0165, -6.1335], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05885111168026924\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0005, -4.4025, -4.8527, -5.3998], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.4802, -5.8117, -5.9545, -6.1878, -5.9399, -6.3675, -6.2821, -6.1878,\n",
      "         4.0610, -5.6702], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3969, -5.9322, -6.0723, -6.2821, -5.7555, -6.4476, -6.4476, -6.2821,\n",
      "         5.6362, -5.7555], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.26096978783607483\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0251, -4.3447, -4.8732, -5.4553], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.3447, -6.0386, -4.3447, -5.4953, -6.4365, -6.0043, -4.2139, -4.3447,\n",
      "        -4.2139, -4.2139], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9102, -6.0033, -4.9102, -5.3342, -6.3726, -6.0033, -4.3596, -4.9102,\n",
      "        -4.3596, -4.3596], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10544705390930176\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-5.5603, -5.5141, -4.7764, -6.4900, -4.4722, -6.1704, -6.3825, -5.9997,\n",
      "        -4.9692, -5.0462], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4723, -5.4723, -5.4882, -6.3266, -4.3697, -5.9627, -6.3266, -6.1550,\n",
      "        -5.0250, -5.0250], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0627240389585495\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0659, -4.3154, -4.8776, -5.5479], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.2736, -5.5543, -6.5262, -5.4968, -6.1210, -4.3154, -6.5098, -4.1801,\n",
      "        -4.7904, -6.1640], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3114, -5.3114, -6.3089, -5.4457, -5.9516, -4.8839, -6.1281, -4.3504,\n",
      "        -5.4457, -6.1281], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10674037784337997\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0611, -4.3516, -4.8342, -5.5532], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.2935, -5.9292,  7.0683, -4.3516, -6.5162, -6.1564, -5.0611, -6.0175,\n",
      "        -4.7976, -5.4134], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3804, -5.7641, 10.0000, -4.9164, -6.3363, -6.1446, -5.3178, -5.9877,\n",
      "        -4.9164, -5.4570], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.9063946604728699\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0797, -4.4010, -4.8013, -5.5546], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.4010, -6.4589, -5.3273, -4.9409,  7.1629, -4.6273, -5.8619, -3.3859,\n",
      "        -6.1631, -5.9191], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9609, -6.1666, -5.4762, -5.1645, 10.0000, -4.9609, -6.0297, -3.5814,\n",
      "        -6.0297, -6.3588], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.8909298777580261\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0899, -4.4344, -4.7783, -5.5353], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.0142, -5.0899, -6.4265, -4.4344, -4.4344, -5.8298, -5.2043, -5.6305,\n",
      "        -4.4344, -6.3898], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.1881, -5.0425, -6.3631, -4.9910, -4.9910, -6.0675, -5.2025, -5.4941,\n",
      "        -4.9910, -6.3631], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10415644943714142\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.1052, -4.4687, -4.7731, -5.5333], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.3206, -6.3165, -6.3259, -4.4687, -5.8148, -4.4687, -5.1052, -5.6626,\n",
      "        -4.3687, -5.8148], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1410, -6.1915, -6.3639, -5.0218, -6.0963, -5.0218, -4.9483, -5.5127,\n",
      "        -4.0739, -6.0963], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09537696093320847\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-5.2919, -6.1120, -5.4762, -5.0620,  5.0525, -6.1120, -6.2555, -4.7388,\n",
      "        -5.6025, -6.2736], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1498, -6.0980, -5.7936, -4.9248,  5.9609, -6.0980, -6.1865, -5.0065,\n",
      "        -5.5150, -6.3424], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10540862381458282\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.1127, -4.4720, -4.8620, -5.5341], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-3.1374, -5.2962, -6.1177, -5.6487, -4.4720, -6.2080, -6.1177, -4.3799,\n",
      "        -6.1177, -4.3799], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2532, -5.5061, -6.0838, -5.5061, -5.0248, -6.1601, -6.0838, -4.1108,\n",
      "        -6.0838, -4.1108], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.053407222032547\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.1222, -4.4600, -4.9321, -5.5396], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.9889, -4.3983, -5.6990, -5.8534, -6.3020, -5.2173, -4.4600, -4.4600,\n",
      "        -6.0296, -4.4600], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9585, -4.2021, -5.7387, -5.9014, -6.2680, -4.9701, -5.0140, -5.0140,\n",
      "        -6.2680, -5.0140], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10831023752689362\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-5.7986, -4.3326, -3.1795, -6.1726, -4.6623, -6.1203, -5.1922, -6.1726,\n",
      "        -6.1726, -6.1456], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.0254, -4.3215, -3.2055, -6.2212, -5.0088, -6.0254, -5.0378, -6.2212,\n",
      "        -6.2212, -6.0813], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.021630581468343735\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8112, -4.3112, -3.8217, -4.6751], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-6.1685, -5.6158, -6.1685, -5.5945, -5.8144, -6.1685, -5.8410, -4.4322,\n",
      "        -6.1330, -6.1245], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.1783, -5.6723, -6.1783, -5.4608, -5.8938, -6.1783, -5.9880, -4.9890,\n",
      "        -6.0431, -5.9880], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.038600027561187744\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-6.0335, -4.4173, -6.1125, -6.0984, -6.1642, -6.1125, -6.1642, -6.0984,\n",
      "        -6.1727, -5.0001], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.9521, -4.9755, -5.9521, -6.0115, -6.1430, -5.9521, -6.1430, -6.0115,\n",
      "        -5.9521, -4.8652], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04525859281420708\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1992, -4.7479, -3.2692, -2.4282], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.4257, -6.1728, -6.0527, -5.7580, -6.0527, -4.6659, -6.0494, -6.0423,\n",
      "        -6.1585, -4.0164], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9831, -5.9341, -6.0086, -5.4785, -6.0086, -4.9831, -5.9341, -6.0086,\n",
      "        -6.1333, -3.1853], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12560084462165833\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.4016, -1.8094,  2.9346,  4.3504], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.9901, -6.0072, -4.8841, -5.7603, -5.9950, -5.9901, -5.7119, -5.9901,\n",
      "        -4.9979, -5.5803], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.0223, -5.9446, -5.1835, -5.5092, -5.9364, -6.0223, -5.9446, -6.0223,\n",
      "        -4.8608, -5.6559], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02417360059916973\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 1.0193, -3.2359,  6.0482,  8.4094], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-6.0813, -5.9563, -5.9390, -3.2823, -5.9389, -5.9390, -5.9291, -4.4653,\n",
      "        -6.1072, -5.9389], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.1721, -6.0539, -5.9536, -3.2572, -6.0539, -5.9536, -5.9536, -5.0188,\n",
      "        -6.1721, -6.0539], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03564001992344856\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-6.0638, -5.7690, -6.2571, -6.1266], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.8572, -4.9070, -5.9849, -5.9141, -5.0291, -5.9141, -6.1266, -5.7117,\n",
      "        -5.7690, -4.4831], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.9585, -4.8814, -5.9585, -5.9585, -5.0678, -5.9585, -6.1921, -5.5788,\n",
      "        -6.0123, -5.0348], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.040259040892124176\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-5.9931, -5.5927, -6.1659, -6.4514], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.2861, -5.9984, -5.7874, -5.1174, -4.9458, -6.1680, -6.4514, -5.9000,\n",
      "        -4.9458, -5.6901], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0193, -5.9624, -5.3751, -4.8883, -5.0193, -6.2216, -6.2216, -5.9624,\n",
      "        -5.0193, -5.6057], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.037242479622364044\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-5.7539, -5.2940, -5.7135, -6.3857], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.8785, -5.9727, -5.8481, -4.9760, -5.8785, -5.6392, -4.5317, -5.9777,\n",
      "        -4.5317, -5.8147], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.9740, -6.0499, -6.0499, -4.9730, -5.9740, -5.6282, -5.0785, -6.1102,\n",
      "        -5.0785, -6.1102], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07679713517427444\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-5.0065, -4.9925, -4.3799, -4.7213], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-6.2170, -6.2687, -4.5533, -5.8294, -5.1557, -6.0174, -6.2687, -5.1557,\n",
      "        -5.6821, -4.9925], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.1139, -6.2987, -5.0980, -5.9858, -4.9154, -6.1139, -6.2987, -4.9154,\n",
      "        -5.7462, -4.9419], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04650106281042099\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0592, -4.5684, -5.0392, -5.6961], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-3.2730, -4.9978, -4.3383, -4.5684, -4.9978, -4.3694, -5.9451, -5.0265,\n",
      "        -6.0879, -5.5525], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4952, -4.9237, -4.2020, -5.1115, -4.9237, -4.2020, -5.9890, -4.9237,\n",
      "        -6.1136, -5.6301], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.042117152363061905\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0813, -4.5751, -5.0588, -5.7407], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.5358, -6.1817, -5.6122, -5.5407, -4.3227, -5.1240, -5.5407, -6.0648,\n",
      "        -6.1817, -5.8197], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6116, -6.3272, -5.7384, -5.6116, -4.2210, -4.9337, -5.6116, -5.9822,\n",
      "        -6.3272, -5.9822], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.015385063365101814\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.1247, -4.5657, -5.0909, -5.7841], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.5392, -5.7917, -4.5657, -6.0470, -6.0131, -4.5657, -4.5657, -5.2233,\n",
      "        -6.0388, -5.2353], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5738, -5.9646, -5.1091, -6.3188, -5.9980, -5.1091, -5.1091, -4.9997,\n",
      "        -5.9646, -5.5163], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11256037652492523\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.1913, -4.5593, -5.1472, -5.8393], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.4417, -5.1762, -5.8025, -6.0856, -4.7645, -4.5404, -3.4307, -6.0604,\n",
      "        -4.9917, -6.5038], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.9656, -5.0864, -5.6586, -5.9387, -4.9174, -5.1034, -3.6806, -5.9656,\n",
      "        -5.6586, -6.2948], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11771663278341293\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.2231, -4.5312, -5.2064, -5.8681], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.9768, -5.4935, -6.3798, -5.8494, -6.1262, -6.1113, -3.8163, -5.4575,\n",
      "        -5.2781, -5.9849], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0780, -5.5775, -6.0362, -5.4994, -5.9117, -5.9117, -3.7646, -5.4994,\n",
      "        -5.5775, -5.9117], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04431042820215225\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.2443, -4.5217, -5.2358, -5.8686], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-3.6472,  8.6046, -5.6444, -6.5375, -6.3973, -4.9856, -6.5375, -5.6000,\n",
      "        -6.1977, -6.5375], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.8275, 10.0000, -5.4870, -6.2620, -6.0400, -4.9041, -6.2620, -5.6597,\n",
      "        -5.6597, -6.2620], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.26594632863998413\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-6.0669, -4.5499, -5.6256, -4.5499, -4.5499, -5.8782, -6.4204, -5.7004,\n",
      "        -6.0669, -6.0669], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.9578, -5.0949, -5.5108, -5.0949, -5.0949, -5.9710, -6.2904, -5.4922,\n",
      "        -5.9578, -5.9578], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10087975114583969\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-5.7040, -5.1301, -5.4462, -5.8375], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.2943, -6.2943, -5.9682, -5.9682, -4.6033, -5.9682, -6.2943, -3.6998,\n",
      "        -3.6998, -5.9350], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.3415, -6.3415, -6.0169, -6.0169, -5.1430, -6.0169, -6.3415, -3.7497,\n",
      "        -3.7497, -6.0134], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03161546587944031\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-5.7939, -5.2193, -5.5782, -5.9379], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.6511, -5.5782, -4.6511, -6.0745, -5.5855, -6.2032, -5.8969, -5.5855,\n",
      "        -5.2193, -6.1525], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1860, -5.1860, -5.1860, -6.1984, -5.5876, -6.3883, -6.0683, -5.5876,\n",
      "        -5.6974, -6.1984], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10357031971216202\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-5.8631, -5.2953, -5.6400, -5.9897], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.7164, -5.0044,  0.4259, -5.2962, -6.1078, -4.9901, -6.2757, -5.2962,\n",
      "        -5.6828, -5.5605], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2448, -5.7658,  3.1994, -5.6786, -6.0769, -5.0772, -6.4330, -5.7658,\n",
      "        -5.8586, -5.6193], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.8985413312911987\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-5.1971, -5.5738, -4.9568, -6.0486, -4.7724, -5.8617, -5.5738,  6.5688,\n",
      "        -5.7134, -6.1222], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2952, -5.6271, -5.2952, -6.1421, -5.2952, -6.1421, -5.6271,  7.0192,\n",
      "        -5.6271, -6.4413], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08025933802127838\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-6.1482, -5.4756, -6.1482, -6.0423, -5.1297, -4.7865, -5.9138, -6.0746,\n",
      "        -5.9023, -6.1342], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.4286, -5.7572, -6.4286, -6.0622, -5.0890, -5.3078, -5.6167, -5.5203,\n",
      "        -6.1380, -6.2530], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09756261855363846\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-5.2365, -5.0295, -6.1856, -6.2129, -5.9635, -6.1856, -4.8046, -5.4078,\n",
      "        -6.2129, -6.2129], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.7128, -5.7128, -6.2378, -6.4135, -6.1263, -6.2378, -5.3242, -5.6335,\n",
      "        -6.4135, -6.4135], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11674555391073227\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-5.8488, -5.0490, -5.5077, -5.9374], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.3132, -4.8031, -5.1559, -6.4948, -5.9221, -4.8031, -3.3133, -3.9824,\n",
      "        -4.8031, -6.0519], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5841, -5.3228, -5.6403, -6.2055, -6.0977, -5.3228,  2.9498, -3.9820,\n",
      "        -5.3228, -5.6403], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.062849998474121\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-5.9378, -5.0035, -5.5644, -5.8996], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.9327, -5.0035, -4.7272, -3.6073, -5.0035, -4.7272, -4.9594, -6.5893,\n",
      "        -5.8775, -5.6785], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6848, -5.5032, -5.2545, -3.5467, -5.5032, -5.2545, -5.2545, -6.2860,\n",
      "        -5.9890, -5.5032], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13427375257015228\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-5.8184, -4.8976, -5.4441, -5.6999], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 8.7018, -5.5002, -4.6666, -5.5941, -5.6999, -4.8640, -5.7962, -4.9513,\n",
      "        -4.6666, -5.8184], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([10.0000, -5.4230, -5.1999, -5.6153, -5.4562, -5.1999, -5.6153, -5.4079,\n",
      "        -5.1999, -5.4079], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.284251868724823\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-5.6500, -4.8082, -5.3059, -5.4692], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.2891, -5.7572, -5.2941, -6.4078, -4.3641, -5.4139, -2.9530, -4.8082,\n",
      "        -5.8913, -5.6500], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6169, -5.7647, -5.4712, -6.1815, -4.6169, -5.3842, -2.7908, -5.3273,\n",
      "        -5.8735, -5.3273], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09995663911104202\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-5.4645, -4.7424, -5.1631, -5.2559], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.3784, -4.8484, -5.7422, -5.7422, -4.7424, -3.9710, -5.3784, -4.5798,\n",
      "        -2.6502,  6.8276], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.8405, -4.9346, -6.1657, -6.1657, -5.2682, -2.4587, -5.8405, -5.1219,\n",
      "        -2.4587,  6.8861], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3690837025642395\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-5.3111, -4.6653, -4.9990, -5.1350], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.6653, -5.1985, -5.6696, -5.3167, -4.8111, -4.1445, -5.3521, -4.6653,\n",
      "        -5.7144, -5.1985], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1988, -5.3300, -5.7851, -5.7851, -4.9218, -4.4264, -5.3300, -5.1988,\n",
      "        -6.1381, -5.3300], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1108081191778183\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-5.1833, -4.5972, -4.8674, -5.0459], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.4765, -5.7221, -5.1116, -5.2418, -5.5728, -5.5715, -4.7471, -5.1669,\n",
      "        -5.2750, -4.4641], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.9288, -6.1133, -5.3028, -5.2371, -5.7475, -5.5028, -4.9053, -5.2371,\n",
      "        -5.7475, -5.0177], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09890959411859512\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-5.4994, -4.3998, -4.9829, -4.3998, -5.8888, -5.0619, -4.9938, -4.5174,\n",
      "        -5.4994, -4.3998], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.8897, -4.9598, -5.0657, -4.9598, -6.0719, -5.2606, -5.4846, -5.0657,\n",
      "        -5.8897, -4.9598], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.18668869137763977\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-5.5077, -4.9283, -5.2014, -5.5675], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.6988, -4.4373, -5.3137, -4.4373, -5.5718, -4.4373, -5.2384, -5.8700,\n",
      "        -5.9319, -5.8700], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8294, -4.9936, -5.6529, -4.9936, -5.5561, -4.9936, -5.2029, -6.0043,\n",
      "        -5.8393, -6.0043], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11065584421157837\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.9857, -4.3650, -4.7078, -5.1054], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.4430, -5.7346, -5.3910, -5.0243, -4.9857, -5.5276, -5.4327, -5.4854,\n",
      "        -4.7449, -1.9015], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7882, -5.7905, -5.5852, -5.1472, -4.9285, -5.5852, -5.5852, -5.4902,\n",
      "        -4.0922, -2.0165], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06441124528646469\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.9508, -4.2843, -4.6967, -5.1784], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.9313,  8.6193, -4.5366, -3.8303,  8.6193, -4.5366, -4.9058, -5.2000,\n",
      "        -5.5034, -5.7039], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.7335, 10.0000, -4.7275, -4.1223, 10.0000, -4.7275, -5.3275, -5.0830,\n",
      "        -5.4152, -5.8600], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4233645796775818\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-5.2337, -4.9182, -5.0252, -4.9547, -4.0826, -4.0826, -4.0826, -4.3553,\n",
      "        -4.0826, -5.2655], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2813, -4.8078, -5.0521, -5.0521, -4.6744, -4.6744, -4.6744, -4.6744,\n",
      "        -4.6744, -4.9198], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16466405987739563\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7490, -4.0184, -4.5585, -5.2398], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.8417, -4.8417, -4.2000, -4.6457, -5.8755, -5.6196, -4.7490, -5.6196,\n",
      "        -5.2778, -5.0227], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2548, -5.2548, -4.7800, -4.6165, -5.6964, -5.4330, -4.7800, -5.4330,\n",
      "        -5.2548, -5.0369], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07818262279033661\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-3.9782,  7.3200, -4.0195, -5.6426, -5.1857, -5.0122, -4.1877, -4.1877,\n",
      "        -5.6426, -6.1818], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5804,  7.0798, -4.0865, -5.4222, -5.1621, -5.0368, -4.7690, -4.7690,\n",
      "        -5.4222, -5.7095], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14218665659427643\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.8290, -4.2030, -4.5875, -5.2916], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.7969, -5.3071, -1.4829, -5.3071, -5.3228, -4.2030, -3.9665, -3.9665,\n",
      "        -6.2855, -5.6062], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.7430, -5.4325, -1.7782, -5.4325, -5.0587, -4.7827, -4.5698, -4.5698,\n",
      "        -5.7324, -5.4325], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15915460884571075\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.8021, -4.2394, -4.5723, -5.2506], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-3.8564, -3.9795, -5.2755, -5.4077, -5.0035, -6.2198, -6.2198, -5.5739,\n",
      "        -4.3745, -3.3911], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.0520, -4.5816, -5.4678, -5.7972, -5.1037, -5.7479, -5.7479, -5.4634,\n",
      "        -4.0520, -1.6790], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4092075228691101\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-3.9940, -5.4942, -6.1216, -5.4942, -5.5120, -4.5070, -3.9940, -6.1216,\n",
      "         3.3953, -5.4942], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5946, -5.5195, -5.7709, -5.5195, -5.5195, -4.5946, -4.5946, -5.7709,\n",
      "         7.3568, -5.5195], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.6670353412628174\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-4.0045, -5.9581, -5.9581, -5.3881, -4.3813, -5.4151, -5.1987, -5.0900,\n",
      "        -4.7608, -4.3251], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6041, -5.7821, -5.7821, -5.8921, -4.6548, -5.5696, -5.3011, -5.3011,\n",
      "        -4.8926, -4.8926], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11685869842767715\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.4421, -4.0193, -4.0691, -4.7338], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.4358, -4.3656, -4.3656, -4.7397, -5.3840, -5.2723, -4.0664, -4.8198,\n",
      "        -4.3656, -4.7876], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.8027, -4.9279, -4.9279, -4.9416, -5.5903, -5.8027, -3.6046, -5.2401,\n",
      "        -4.9279, -4.9416], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.18614597618579865\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.4458, -4.0556, -3.9406, -4.6593], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 7.4064, -4.4290, -5.3983, -5.3560, -4.0754, -4.5192, -4.4290, -4.0556,\n",
      "        -5.1567, -5.5191], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 7.2594, -4.7957, -5.5210, -5.5709, -4.6679, -4.8081, -4.7957, -4.5465,\n",
      "        -5.5210, -5.6411], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11748708784580231\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2052, -4.0086, -2.5651, -4.1146], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.4069, -4.5706, -4.1196, -4.8942, -4.1196, -5.4022, -4.1196, -5.8447,\n",
      "        -4.9269, -1.1398], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4314, -5.1136, -4.4079, -4.9850, -4.4079, -5.4117, -4.4079, -5.4613,\n",
      "        -4.6093, -1.5691], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0985170230269432\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6477, -4.3155, -1.0610, -0.5979], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.5069, -4.9846, -5.5069, -4.1937, -4.7822, -5.5070, -4.6277, -4.1937,\n",
      "        -4.1937, -4.1937], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3701, -4.4125, -5.3701, -4.2758, -5.3177, -5.3130, -4.4739, -4.2758,\n",
      "        -4.2758, -4.2758], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07396633923053741\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 1.1092, -1.6371,  2.9307,  3.7988], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.6848, -5.6848, -4.2470, -4.1333, -3.9635, -4.3659, -4.3659, -3.7241,\n",
      "        -5.2800, -4.2470], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2086, -5.2086, -4.1709, -3.0805, -4.3517, -4.9293, -4.9293, -4.3517,\n",
      "        -4.7440, -4.1709], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.30402231216430664\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 3.0494, -3.2972,  7.3059,  9.1128], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.5560, -4.2680, -4.6911, -4.2680, -4.6911, -4.6549, -4.1448, -5.6342,\n",
      "        -5.4788, -5.7603], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2442, -4.1254, -5.2220, -4.1254, -5.2220, -5.2220, -3.1379, -4.9228,\n",
      "        -5.2442, -5.1895], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2923870384693146\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.5480, -5.4918, -4.6995, -5.1649], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.5135, -2.5397, -4.4533, -4.4702,  7.2166, -5.5135, -4.7192, -5.5135,\n",
      "        -3.4914, -5.1649], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2894, -1.4780, -5.0080, -5.0080,  7.1639, -5.2894, -4.4462, -5.2894,\n",
      "        -3.2858, -5.2295], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.19986525177955627\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.8529, -5.7221, -4.8647, -5.4443], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.4443, -5.0766, -5.0677, -4.5291, -4.5291, -5.2701, -5.4756,  3.8351,\n",
      "        -4.6894, -5.0766], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3782, -5.2472, -5.2464, -5.0761, -5.0761, -5.1205, -5.2464,  7.1164,\n",
      "        -4.5215, -5.2472], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.1563349962234497\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.4997, -5.4087, -4.7314, -5.0020], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.6475, -4.5667, -4.1253, -5.2125, -4.7604, -0.6385, -5.3816, -4.1253,\n",
      "        -5.6554, -5.4087], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5871, -5.1100, -4.1515, -5.2078, -4.5871,  2.4876, -5.2543, -4.1515,\n",
      "        -5.5118, -5.1100], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.0228760242462158\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-4.8192, -4.1286, -1.2481, -5.2161, -4.7011, -4.8110, -4.5621, -4.0276,\n",
      "        -1.2481, -1.2481], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2310, -4.6097, -1.4027, -5.4660, -5.2310, -5.2394, -4.6097, -4.1187,\n",
      "        -1.4027, -1.4027], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10101146996021271\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.2117, -5.1724, -4.6566, -4.6955], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.9667, -4.9480, -3.8981, -5.4978, -5.4978, -4.4840, -5.0391, -1.1752,\n",
      "        -3.9667, -3.7808], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.0960, -5.1909, -4.5083, -5.2886, -5.2886, -4.5820, -5.2886, -1.2661,\n",
      "        -4.0960, -3.4782], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07239373028278351\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-3.9075, -4.9463, -3.9075, -3.9075, -4.4567, -4.5803, -5.0998, -4.5803,\n",
      "        -4.5772, -3.9789], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.0795, -5.1194, -4.0795, -4.0795, -4.5670, -5.1223, -5.4516, -5.1223,\n",
      "        -5.1194, -4.5670], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14821919798851013\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0534, -4.9932, -4.5788, -4.4818], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.8828,  6.5926, -4.3431, -3.6384, -5.4063, -3.8924, -4.6049, -3.8924,\n",
      "        -4.4036, -3.8924], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4257,  6.6237, -4.5177, -3.4999, -5.5570, -4.0651, -4.8688, -4.0651,\n",
      "        -4.5177, -4.0651], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05402175709605217\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-4.9418, -5.1568, -4.5920, -4.3717, -4.9511, -4.4357, -3.8924, -3.8225,\n",
      "        -3.8225, -4.8268], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0785, -4.9922, -4.8151, -4.4402, -5.5283, -4.9922, -4.0118, -4.0118,\n",
      "        -4.0118, -5.3441], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10965218394994736\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0339, -4.9201, -4.4263, -4.3497], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.8991, -4.2854, -4.7520, -4.7520, -3.5731, -4.3497, -5.0339, -4.5933,\n",
      "        -5.0075, -4.9581], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9242, -4.3728, -5.2754, -5.2754, -3.4456, -4.9147, -5.2754, -4.7500,\n",
      "        -4.9703, -5.4611], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12289591878652573\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0655, -4.9233, -4.3288, -4.3438], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.6967, -4.6316, -4.2872, -4.3905, -4.0693, -4.7283, -2.6599, -4.3402,\n",
      "        -3.6478, -4.2872], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3678, -4.8959, -4.2830, -4.2830, -4.0029, -5.1684, -0.8272, -3.8192,\n",
      "        -3.8192, -4.2830], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4047383666038513\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0895, -4.9018, -4.2197, -4.3759], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.4535, -4.4167, -4.7786, -4.7786, -3.9227, -4.6787, -3.9227,  0.1185,\n",
      "        -4.3759, -4.3759], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.2008, -4.2008, -5.0719, -5.0719, -3.7368, -4.3057, -3.7368,  2.4472,\n",
      "        -4.7977, -4.7977], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6269627809524536\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-4.3943, -5.1434, -3.5946, -4.6041, -3.9937, -4.2212, -4.5328, -3.8804,\n",
      "        -5.0907, -4.5328], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6824, -4.8437, -4.2352, -4.2352, -4.4449, -3.6391, -4.8437, -3.6391,\n",
      "        -4.9527, -4.8437], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15321794152259827\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0599, -4.7585, -3.9885, -4.4499], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.7585, -4.2993, -4.4499, -4.8668, -3.8340, -4.1546, -3.3689, -4.8668,\n",
      "        -4.1031, -4.5365], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6260, -4.9858, -4.5897, -4.8553, -3.5983, -4.0320, -3.5983, -4.8553,\n",
      "        -3.5983, -4.3240], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09318627417087555\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0379, -4.6659, -3.9039, -4.5206], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.1912, -3.9039, -4.9837, -4.0852, -5.3835, -4.8511, -3.9299,  0.1976,\n",
      "        -3.1947,  3.6558], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9908, -4.5135, -4.9627, -3.9908, -4.7055, -4.8715, -4.5369,  2.2902,\n",
      "        -2.9582,  6.1126], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.1720516681671143\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9462, -4.5405, -3.8175, -4.5179], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.6889, -4.9101, -5.1931, -3.6902, -2.8529, -3.6902, -3.9857, -3.9857,\n",
      "        -4.7670, -4.4291], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4357, -4.6726, -4.7566, -3.5676, -2.8730, -3.5676, -3.9474, -3.9474,\n",
      "        -4.7566, -4.1894], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.040209271013736725\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-4.8551, -4.5069, -3.8867, -4.2913, -4.8551, -4.5069, -3.7609, -4.0449,\n",
      "        -4.3165, -3.6012], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6002, -4.3930, -3.9285, -4.3848, -4.6002, -4.3930, -4.3848, -4.6002,\n",
      "        -4.1404, -3.5723], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0895933136343956\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-3.7972, -4.9608, -4.7513, -3.5231, -3.8345, -4.4699, -4.7558, -4.7513,\n",
      "        -4.3149, -2.5406], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9236, -4.6529, -4.6529, -3.5850, -3.9236, -4.3656, -4.5338, -4.6529,\n",
      "        -4.3399, -4.0961], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.26220792531967163\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.8920, -4.5617, -4.0828, -4.7493], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.2425, -4.7493, -3.6779, -3.8547, -3.6779, -4.6317, -4.4320, -4.3981,\n",
      "         0.0704, -4.3981], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3101, -4.6746, -4.3101, -3.9341, -4.3101, -4.4909, -4.3583, -4.8481,\n",
      "        -0.3709, -4.8481], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14408865571022034\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6854, -4.3326, -3.8545, -4.5956], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.8545, -2.6574, -3.4504, -4.7405, -4.1631, -4.5375, -3.6769, -3.8545,\n",
      "        -3.4315, -3.4315], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3438, -2.7178, -2.7178, -4.6887, -4.0891, -4.4691, -3.9379, -4.3438,\n",
      "        -3.6424, -3.6424], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11890468746423721\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.4259, -4.0973, -3.6784, -4.3088], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.5084, -4.7284, -3.4046, -3.6156, -4.3088, -3.6357, -3.6784,  7.2856,\n",
      "        -3.0390, -4.5686], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5116, -4.7035, -3.6541, -4.2541, -4.3106, -3.9376, -4.3106, 10.0000,\n",
      "        -3.6541, -4.4750], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.8716533780097961\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3407, -4.0481, -3.6293, -4.2245], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.2521, -4.7328, -4.2245, -2.9221, -3.6122, -3.7429, -4.0852, -0.9964,\n",
      "        -3.3782, -3.6122], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.0791, -4.7031, -4.2664, -2.7138, -3.9167, -3.9378, -4.0609, -2.7138,\n",
      "        -3.6299, -3.9167], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.33927422761917114\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3036, -4.0295, -3.5618, -4.1535], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.9822, -3.6708, -4.1535, -4.1535, -3.6132, -4.5145, -4.5675, -3.8182,\n",
      "        -4.4979, -3.6823], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5794, -4.0222, -4.2056, -4.2056, -3.8695, -4.4007, -4.6770, -4.2056,\n",
      "        -4.4364, -3.8695], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07650605589151382\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2969, -4.0120, -3.4775, -4.0845], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.6643, -4.0363, -3.3626, -3.9812, -3.6176, -4.1248, -3.4634, -4.0845,\n",
      "        -3.3626, -4.0845], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6326, -4.4043, -3.5103, -3.9676, -3.8080, -4.6247, -4.1170, -4.1298,\n",
      "        -3.5103, -4.1298], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08978848159313202\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3086, -4.0069, -3.3912, -4.0268], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.9138, -4.0268, -3.4126, -4.4084, -4.6633, -4.1736, -3.3656, -4.4084,\n",
      "        -4.4795, -4.6633], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9104, -4.0521, -4.0714, -4.2542, -4.5913, -3.9104, -3.4328, -4.2542,\n",
      "        -4.3564, -4.5913], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05815345048904419\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-3.3633, -3.7674, -3.3633, -4.4552, -3.6408, -2.5850,  7.5024, -4.2005,\n",
      "        -3.3255, -4.2683], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3712, -3.6714, -3.3712, -4.3223, -3.6907, -2.5191, 10.0000, -4.4762,\n",
      "        -3.9929, -4.5636], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6880583763122559\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3174, -3.9928, -3.2613, -3.9111], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.2613, -3.5956, -4.6264, -3.4826, -3.6521, -4.6929, -3.3481, -3.6482,\n",
      "        -3.3647, -3.6521], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9352, -3.4869, -4.5265, -4.4065, -3.6362, -4.2834, -4.0133, -3.9352,\n",
      "        -3.3078, -3.6362], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.20257584750652313\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3213, -3.9857, -3.2182, -3.8470], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.3655, -3.3655, -4.4048, -3.8470, -3.6431, -3.3521, -3.5484, -3.2182,\n",
      "        -3.2182, -3.2182], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2677, -3.2677, -4.2830, -3.8964, -4.0024, -3.4933, -3.2677, -3.8964,\n",
      "        -3.8964, -3.8964], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16440936923027039\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-3.3724, -4.0185, -3.3724, -4.3988, -4.0119, -3.3724, -4.3988, -3.3724,\n",
      "        -4.1027, -3.6674], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2584, -3.8058, -3.2584, -4.2878, -3.5848, -3.2584, -4.2878, -3.2584,\n",
      "        -3.8729, -3.5848], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.036381013691425323\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3065, -3.8043, -3.3253, -4.0605], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.3627, -4.3266, -4.5416, -4.3728, -4.5416, -2.8441, -3.6542, -3.6542,\n",
      "        -3.3253, -3.7652], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2698, -4.3078, -4.5110, -4.3078, -4.5110, -3.5597, -3.5828, -3.5828,\n",
      "        -3.9928, -3.8734], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09945906698703766\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-4.5096, -4.5096, -4.5096, -3.7299, -4.3497, -3.3344, -3.7030, -3.3344,\n",
      "        -4.6074, -3.6580], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5264, -4.5264, -4.5264, -3.8810, -4.3327, -4.0010, -3.8810, -4.0010,\n",
      "        -4.2160, -3.8323], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11277921497821808\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-2.8589, -3.7538, -4.3487, -3.7538, -4.3487, -3.6263, -4.2241, -3.6051,\n",
      "        -1.6182, -4.4793], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5730, -3.6076, -4.3671, -3.6076, -4.3671, -3.6076, -3.8481, -3.6076,\n",
      "         0.5230, -4.5375], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5283059477806091\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.5795, -3.1349, -2.8583, -3.5595], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.8583, -3.6104, -3.7210, -3.7210, -3.3055, -4.3237, -3.7210, -3.3055,\n",
      "        -3.5930, -3.3587], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5725, -3.6210, -3.8944, -3.8944, -3.3111, -4.5358, -3.8944, -3.3111,\n",
      "        -3.6210, -4.0229], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10872912406921387\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.5821, -3.1216, -2.8553, -3.5871], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.7188, -4.4348, -4.4987, -4.0461, -4.0461, -3.9263, -3.5870, -4.0622,\n",
      "        -3.2177, -2.4627], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.8653, -4.3994, -4.5230, -3.8653, -3.8653, -4.1151, -3.6282, -3.8959,\n",
      "        -3.8959, -2.3518], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0625988095998764\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.5756, -3.1079, -2.8553, -3.6134], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.4922, -3.2592, -2.8553, -3.5756, -3.8693, -3.7301, -3.2291, -3.3593,\n",
      "        -3.2291,  1.4367], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3263, -3.3033, -3.5698, -3.6435, -4.0233, -3.5698, -3.9062, -4.0233,\n",
      "        -3.9062,  2.4152], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.290934681892395\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.5664, -3.0929, -2.8575, -3.6284], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.2309, -3.8435,  1.4192, -3.2309, -3.3984, -2.3757, -3.2407, -4.4098,\n",
      "        -3.8375, -3.8435], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2963, -3.9166,  2.4063, -3.2963, -4.0151, -2.2681, -3.9166, -4.0586,\n",
      "        -3.5790, -3.9166], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.20326927304267883\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.5478, -3.0648, -2.8582, -3.6273], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.5415, -4.4926, -2.3196, -3.6765, -3.5415, -4.0098, -3.8607, -4.4926,\n",
      "         6.0572, -3.7299], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6543, -4.4211, -2.2285, -3.6543, -3.6543, -3.6543, -3.9275, -4.4211,\n",
      "         6.4361, -4.0521], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04226630553603172\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([ 0.3657, -4.4524, -3.2657, -3.8746, -3.1508, -3.1508, -4.8025, -4.2944,\n",
      "        -3.1508, -2.8204], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.3436, -4.4824, -3.9392, -3.9392, -3.2821, -3.2821, -4.4824, -4.4208,\n",
      "        -3.2821, -3.5749], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11984769254922867\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.9836, -3.3875, -2.8915, -4.0622], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.4778, -3.2761, -4.1284, -4.2155, -4.4414, -2.8379, -4.4414,  8.2732,\n",
      "        -4.8288, -3.2761], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4116, -3.9485, -4.0221, -4.3908, -4.4644, -3.5541, -4.4644, 10.0000,\n",
      "        -4.4644, -3.9485], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.45793694257736206\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-3.2685, -3.1057, -3.9405, -4.4387, -3.3536, -2.9000, -3.7816,  0.4320,\n",
      "        -3.5256, -3.1057], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9417, -3.2626, -3.9648, -4.4649, -3.9417, -3.5427, -4.0182,  0.4089,\n",
      "        -3.5427, -3.2626], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13194933533668518\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.8639, -2.2125, -1.2819, -2.8910], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.2475, -4.5962, -3.0982, -3.0982, -3.4509,  0.4622,  6.2393, -3.3171,\n",
      "        -3.0982, -3.0982], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9228, -4.4246, -3.2609, -3.2609, -3.6608,  0.4503,  6.5967, -3.9854,\n",
      "        -3.2609, -3.2609], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12098807096481323\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.1475, -2.6080,  0.5068,  1.6514], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.3125, -4.0067, -4.2383, -4.2878, -4.2383, -3.3320, -3.2241,  1.6514,\n",
      "        -3.1055, -3.3320], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6568, -3.9988, -4.3858, -4.4377, -4.3858, -3.9988, -3.9016,  2.3872,\n",
      "        -3.2497, -3.9988], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.20950031280517578\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.9658, -0.9475,  2.7146,  3.7669], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.8484, -2.9267, -3.4810,  8.5693, -3.1275, -4.1830, -2.9214, -3.8082,\n",
      "        -3.7067, -3.3416], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.8717, -3.2437, -3.6340, 10.0000, -3.2437, -3.8745, -3.4660, -3.8745,\n",
      "        -3.6543, -4.0075], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.30272114276885986\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 2.4045, -2.0037,  6.5279,  8.6951], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.3398, -4.4630, -3.1418, -3.4570, -3.1579, -3.3437, -3.4974, -2.2251,\n",
      "        -2.9140, -4.3871], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6518, -4.4421, -3.2251, -3.2251, -3.8421, -3.8421, -3.6226, -2.1041,\n",
      "        -3.2251, -4.3270], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10058216005563736\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.5787, -4.3202, -3.8144, -4.4794], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.2363, -4.1439, -3.3618, -3.8422, -3.1661, -3.5080, -4.2698, -4.3798,\n",
      "        -3.8435, -3.1661], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.0995, -4.0256, -4.0256, -4.0256, -3.2251, -3.6252, -4.3043, -4.3043,\n",
      "        -3.8681, -3.2251], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05352281779050827\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5387, -4.2353, -3.6464, -4.3702], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.5387, -4.4952, -3.1910, -3.7785, -3.4556, -2.2476, -3.1013, -4.3702,\n",
      "        -3.8048, -3.8244], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4243, -4.4243, -3.2240, -4.0183, -3.3564, -2.0968, -3.7912, -4.2818,\n",
      "        -4.2818, -3.6228], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08611893653869629\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2812, -3.9583, -3.3888, -4.2160], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.5038, -4.2812, -3.3888, -3.8256,  0.9721,  0.8442, -3.9583, -3.9010,\n",
      "        -3.9583, -3.2104], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6310, -4.2601, -4.0499, -4.2601,  2.4801,  0.8162, -3.7821, -3.8810,\n",
      "        -3.7821, -3.2328], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2980331778526306\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-3.4786, -2.8319, -3.0998, -4.3027, -4.5817, -4.5817, -2.5034, -1.2420,\n",
      "        -4.2454, -2.5909], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2530, -2.1178, -3.7898, -4.2401, -4.4798, -4.4798, -2.1178,  0.8461,\n",
      "        -4.0670, -3.3318], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6151277422904968\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2607, -3.9538, -3.4221, -4.2870], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.1065, -4.2919, -4.2457, -4.5847, -3.4566, -4.6404, -4.5847, -3.4004,\n",
      "        -3.2224, -4.1464], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7959, -4.2201, -4.4429, -4.5070, -3.2624, -4.5070, -4.5070, -3.7959,\n",
      "        -3.2624, -4.0571], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0752851814031601\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([ 9.0553, -4.9017, -3.4277, -4.6958,  1.8643, -4.2478, -2.1963, -3.5449,\n",
      "        -3.4373, -3.2190], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([10.0000, -4.5319, -3.2791, -4.5319,  2.3906, -4.2003, -2.0808, -3.6739,\n",
      "        -4.0936, -3.2791], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.18217197060585022\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-4.3349, -4.3363,  1.8511, -2.9769, -3.5109, -3.2118, -3.2118, -4.3363,\n",
      "        -3.2854, -3.4140], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4548, -4.1071,  2.3740, -3.2968, -3.6792, -3.2968, -3.2968, -4.1071,\n",
      "        -3.2992, -3.8019], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06885574758052826\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.6756, -3.3625, -3.0998, -3.8759], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.2833, -4.3127, -2.9794, -3.1974, -3.6756, -4.1950, -3.0998, -2.9800,\n",
      "        -4.0325, -4.7297], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2998, -4.1034, -3.2914, -3.2914, -4.0933, -4.1601, -3.7898, -3.6166,\n",
      "        -4.4433, -4.5835], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13974353671073914\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.6907, -3.3534, -3.0884, -3.8662], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.5628, -4.2792, -3.1799, -3.8516, -3.4959, -3.4959, -4.3803, -2.7342,\n",
      "        -4.1930, -2.5628], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3065, -4.0843, -3.2626, -3.9721, -3.6838, -3.6838, -4.4097, -2.0130,\n",
      "        -4.1228, -3.3065], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17620736360549927\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-3.0807, -4.6220, -3.4075, -3.7580, -3.4935, -2.9788, -4.2379, -4.7242,\n",
      "        -3.1633, -3.1633], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7726, -4.5922, -4.0668, -4.0828, -3.6810, -3.2389, -4.0668, -4.5922,\n",
      "        -3.2389, -3.2389], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1180669516324997\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.0534, -2.7226, -2.5583, -3.4741], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.1005, -4.1922, -4.1660, -3.3948, -3.1490, -3.4174, -3.1490, -4.7100,\n",
      "        -3.5033, -4.1660], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.0100, -4.0553, -4.6011, -4.0553, -3.2124, -3.7710, -3.2124, -4.6011,\n",
      "        -3.6889, -4.6011], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10211457312107086\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.0906, -2.7247, -2.5505, -3.4793], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.1517, -2.1031, -4.6621, -3.5545, -4.2250, -4.2250, -2.5505, -3.5215,\n",
      "        -4.4141, -4.1517], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.0393, -2.0061, -4.5786, -3.6859, -4.0685, -4.0685, -3.2954, -3.6859,\n",
      "        -4.3516, -4.0393], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06937722861766815\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.1160, -2.7300, -2.5506, -3.4782], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.1425, -1.1192, -4.6076, -3.0529, -4.6535,  3.6619, -3.7944, -3.4860,\n",
      "        -3.5388, -4.2578], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.1581,  0.7953, -4.5649, -3.7476, -4.5649,  7.4744, -4.0217, -3.6807,\n",
      "        -3.6807, -4.5649], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.889676809310913\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-2.3568, -3.3150, -3.9294, -3.9294,  0.8169, -4.3262, -3.5905, -4.0759,\n",
      "        -2.5265, -3.9140], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.9473, -3.9835, -3.6504, -3.6504,  0.6931, -4.5226, -3.6504, -4.0687,\n",
      "        -3.2738, -4.0687], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14101876318454742\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.1824, -2.7417, -2.5066, -3.5159], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.5499, -3.0082, -2.3081, -3.1419, -3.1419, -4.2620,  3.6753, -3.0082,\n",
      "        -2.0757, -4.0093], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4873, -3.7074, -1.9055, -3.0772, -3.0772, -4.0653,  7.2833, -3.7074,\n",
      "        -1.9055, -4.0311], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.4237927198410034\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.1966, -2.7391, -2.4901, -3.5235], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.7195, -2.0593, -3.5593, -3.4092, -3.1137, -3.1137, -3.8561, -3.8405,\n",
      "        -2.9950, -4.4832], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.5136, -1.8797, -3.6101, -3.9249, -3.0254, -3.0254, -4.0153, -3.9249,\n",
      "        -3.6955, -4.2520], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09353306889533997\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.2086, -2.7357, -2.4836, -3.5305], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.0844, -3.0844, -3.6176, -4.1793, -4.1301, -3.4628, -3.9444, -3.5546,\n",
      "        -4.4842, -4.3025], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.9848, -2.9848, -3.6031, -4.0914, -3.9644, -3.6031, -3.8957, -3.6031,\n",
      "        -4.4387, -4.0914], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.012630371376872063\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-3.3028, -3.0097, -3.5456,  6.9448, -3.8935,  0.6157, -4.5467, -4.0736,\n",
      "        -3.5456, -3.1954], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3173, -3.7088, -3.5996,  6.8382, -4.0004,  0.3675, -4.4307, -4.0004,\n",
      "        -3.5996, -3.8759], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1060916930437088\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-3.6173, -2.7277, -3.5409, -4.2120, -3.8378, -2.8896, -3.1420, -4.0945,\n",
      "        -4.4010, -3.8378], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6006, -1.8529, -3.6006, -4.1593, -3.8653, -2.9497, -2.9497, -4.0006,\n",
      "        -4.4328, -3.8653], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08239184319972992\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.6871, -3.1075, -2.5701, -3.9493], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.0085, -3.7945, -2.4828, -2.9039, -2.1694, -3.1075, -3.5541, -4.3689,\n",
      "        -3.9334, -4.3689], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.9525, -3.8649, -3.2345, -2.9525, -1.8753, -2.9525, -3.8649, -4.4415,\n",
      "        -4.0104, -4.4415], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07990316301584244\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.2027, -2.6640, -2.4802, -3.4680], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.5013, -3.8336, -4.2381, -4.3508, -4.3989, -3.5013, -2.9721, -3.6888,\n",
      "        -4.1752, -4.2381], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6267, -4.2408, -4.2408, -4.4502, -4.2508, -3.6267, -2.9463, -3.7733,\n",
      "        -4.0244, -4.2408], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02596713975071907\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.1913, -2.6410, -2.4773, -3.4489], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.1304, -4.3499, -4.3499, -3.9067, -4.3499, -2.9437, -4.0963, -2.9437,\n",
      "        -2.4773, -2.6410], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.2690, -4.4679, -4.4679, -4.0349, -4.4679, -2.9390, -4.0220, -2.9390,\n",
      "        -3.2295, -1.9262], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11597591638565063\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.1698, -2.5996, -2.4616, -3.4287], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.9833, -3.9157, -3.8735, -4.2619, -3.4708, -3.3793, -3.7032, -3.9100,\n",
      "        -4.1515,  0.3604], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.0268, -4.0414, -4.2902, -4.2902, -3.6432, -3.3035, -3.8490, -3.8490,\n",
      "        -4.2902,  0.1219], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03287113457918167\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.1532, -2.5669, -2.4436, -3.4158], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.6979, -2.9392, -3.4602, -4.4099, -1.1886, -3.1287, -3.6585, -2.8843,\n",
      "        -4.1195, -3.1287], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.8442, -2.9275, -3.6453, -4.5081, -1.9945, -3.8158, -3.8442, -2.9275,\n",
      "        -4.2927, -3.8158], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17255453765392303\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.1590, -2.5451, -2.4202, -3.4149], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.2391, -2.8734, -4.3785, -2.8734, -2.8734, -2.8734, -4.2973, -2.9848,\n",
      "        -3.1873, -2.4202], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.2661, -2.9146, -4.2661, -2.9146, -2.9146, -2.9146, -4.2893, -2.9146,\n",
      "        -3.2853, -3.1782], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.060928262770175934\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.1657, -2.5303, -2.4028, -3.4157], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.3194, -3.1418, -3.5061, -2.1207, -3.7236, -3.7236, -4.0679, -3.7236,\n",
      "        -3.1418, -3.4682], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.2866, -3.8276, -3.6322, -2.0032, -3.8307, -3.8307, -4.0232, -3.8307,\n",
      "        -3.8276, -3.6322], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10347355902194977\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.1733, -2.5210, -2.3845, -3.4214], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.6239, -3.1374, -3.6436, -4.3457, -2.1050, -4.5569, -4.0417, -2.8741,\n",
      "        -3.9380, -3.7489], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.0185, -3.8236, -3.8236, -4.2792, -2.0064, -4.5442, -3.8236, -2.8945,\n",
      "        -4.2792, -3.8236], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08433271199464798\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.1799, -2.5134, -2.3732, -3.4218], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.9669, -3.9885, -2.8752, -3.1531, -3.1393, -4.6097, -4.6097, -3.3810,\n",
      "        -2.8959, -3.1531], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5663, -4.0036, -2.8804, -3.8378, -3.8254, -4.5663, -4.5663, -3.1359,\n",
      "        -2.8804, -3.8378], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1633201241493225\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.1808, -2.5092, -2.3802, -3.4253], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.1745, -4.2610, -2.8733, -4.6482, -2.3802, -2.8733, -3.6260, -4.3697,\n",
      "        -3.1558, -2.3802], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.0184, -4.0126, -2.8792, -4.5968, -3.1422, -2.8792, -3.8534, -4.2888,\n",
      "        -3.8402, -3.1422], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17766781151294708\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([ 6.2623,  0.0639, -2.8767, -3.1847, -4.3724, -3.6758, -2.0948, -4.6783,\n",
      "        -3.9941, -2.8767], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 6.0252, -0.2359, -2.8853, -3.8662, -4.3082, -3.8662, -2.0498, -4.6323,\n",
      "        -4.0271, -2.8853], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06563860177993774\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1611, -3.3913, -3.2081, -4.0607], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.0794, -3.5135, -3.2081, -3.1730, -4.3716, -3.7361, -3.8307, -3.8307,\n",
      "        -3.2081, -2.8806], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3379, -3.6409, -3.8873, -3.2749, -4.3379, -4.0590, -3.8949, -3.8949,\n",
      "        -3.8873, -2.8960], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1129922866821289\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1642, -3.4016, -3.2319, -4.0616], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.2885, -3.9912, -4.2885, -4.3693,  7.7524, -4.1043, -3.5290, -3.4325,\n",
      "        -2.4415, -3.4050], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3605, -4.0645, -4.3605, -4.3605, 10.0000, -3.6567, -3.6567, -3.6567,\n",
      "        -3.1974, -3.2860], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5919734239578247\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1639, -3.4147, -3.2503, -4.0539], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.1223, -3.4145, -3.2729, -4.6795, -1.9856, -2.9102, -2.4619, -4.0539,\n",
      "        -3.8594, -1.9856], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.0843, -3.2966, -3.9456, -4.4349, -2.1058, -2.9147, -3.2157, -3.9456,\n",
      "        -3.9456, -2.1058], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1144026666879654\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1533, -3.4272, -3.2692, -4.0392], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.4859, -2.9540, -3.8757,  6.1572, -4.1533, -0.0850, -2.9339, -3.3058,\n",
      "        -3.4736, -2.4859], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2373, -2.9256, -3.9752,  6.1151, -4.0994, -0.2608, -2.9256, -3.9752,\n",
      "        -3.2373, -3.2373], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16795405745506287\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1428, -3.4401, -3.2945, -4.0204], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.5911, -4.7861, -4.0204, -3.8935, -3.5825, -2.0119, -2.5784, -4.7861,\n",
      "         3.9519, -3.2106], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8301, -4.8301, -4.0118, -4.0118, -3.6697, -2.1301, -2.1301, -4.8301,\n",
      "         6.1771, -3.3355], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5264726877212524\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-3.5950, -3.9337, -4.5000,  6.1219, -3.5950, -3.3084, -5.0949, -3.4466,\n",
      "        -3.3774, -3.4413], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6733, -4.1373, -4.5404,  6.1910, -3.6733, -3.9775, -4.8555, -3.2932,\n",
      "        -4.0396, -3.2932], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10492753982543945\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3157, -3.6869, -3.5020, -4.2204], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.6089, -1.2913, -4.0242, -3.5472, -3.4142, -3.2577, -4.8249, -4.3407,\n",
      "        -4.1548, -4.8249], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6799, -0.2441, -3.6799, -3.6799, -4.0728, -3.3603, -4.8861, -4.4446,\n",
      "        -4.1177, -4.8861], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17016039788722992\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0961, -3.4621, -3.3355, -4.0316], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.0961, -4.0149, -4.8668, -4.0316, -3.8257, -2.5963, -3.8983, -3.0295,\n",
      "        -3.3355, -4.0187], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.1588, -4.1588, -4.9074, -4.1018, -4.0019, -3.3367, -4.1255, -3.0166,\n",
      "        -4.0019, -4.1018], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11132438480854034\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0888, -3.4787, -3.3415, -4.0644], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.9201, -0.2606, -0.2606, -4.0903, -3.9373, -3.1871, -2.6126, -4.0644,\n",
      "        -4.0903, -3.4741], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9228, -0.3640, -0.3640, -4.1267, -4.1239, -3.3610, -3.3513, -4.1267,\n",
      "        -4.1267, -4.1267], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1064610704779625\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0920, -3.4984, -3.3490, -4.1061], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.9771, -4.9771, -3.0870, -3.6711, -4.1661, -3.4970, -3.4641, -3.4641,\n",
      "        -3.3490, -3.6711], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9365, -4.9365, -3.0577, -3.6923, -4.1496, -3.6923, -3.3539, -3.3539,\n",
      "        -4.0141, -3.6923], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05101293325424194\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1014, -3.5178, -3.3560, -4.1452], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.0244, -3.8479, -2.6510, -4.4570, -3.6937, -2.0649, -3.0451, -4.9148,\n",
      "        -2.6174, -3.1212], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9472, -4.1696, -3.3859, -4.4631, -3.6961, -2.1130, -3.0825, -4.9472,\n",
      "        -3.3859, -3.0825], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12464380264282227\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1121, -3.5377, -3.3685, -4.1823], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.2882, -3.5435, -3.3685, -4.2882, -4.0924, -3.7187, -4.2882, -1.2318,\n",
      "        -5.1108, -3.0857], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.1891, -4.1891, -4.0316, -4.1891, -4.1032, -3.7075, -4.1891, -0.5304,\n",
      "        -4.9630, -3.1130], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1400875300168991\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1223, -3.5597, -3.3862, -4.2234], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.3862, -3.0261, -3.1887, -5.1086, -3.5703,  7.9548, -3.1887, -3.5703,\n",
      "         7.9548, -2.0777], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.0476, -3.1439, -3.1439, -4.9820, -4.2133, 10.0000, -3.1439, -4.2133,\n",
      "        10.0000, -2.0908], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.9664425849914551\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1347, -3.5857, -3.4096, -4.2429], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.2222, -4.6971, -3.7823, -3.6017, -3.6017, -3.2554, -3.6017, -3.6191,\n",
      "        -3.7757, -5.1237], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.1655, -4.7653, -3.4535, -4.2415, -4.2415, -3.4535, -4.2415, -3.7344,\n",
      "        -3.7344, -5.0040], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1412719041109085\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1551, -3.6056, -3.4376, -4.2607], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.0871, -5.1111, -3.4376, -4.0623, -4.3924, -3.6276, -5.1366,  0.4349,\n",
      "        -4.8858, -3.5377], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.0511, -5.0319, -4.0938, -3.7496, -4.2796, -3.7496, -5.0319,  2.8831,\n",
      "        -4.7937, -4.0938], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.688584566116333\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1078, -3.5913, -3.4245, -4.1828], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.4245, -0.4500, -3.4245, -2.6772, -1.1330, -2.0631, -2.4478, -4.5097,\n",
      "        -4.3330, -2.6772], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.0821, -0.4544, -4.0821, -3.4095, -0.4544, -2.0197, -2.0197, -4.5701,\n",
      "        -4.2937, -3.4095], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.25880634784698486\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0738, -3.5830, -3.4149, -4.1308], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.2430, -3.2430, -5.0735, -2.6851, -2.0431, -3.5901, -3.2430, -3.4149,\n",
      "        -4.1309, -2.7646], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.1762, -3.1762, -5.0492, -3.4166, -1.9780, -3.7150, -3.1762, -4.0735,\n",
      "        -4.3037, -1.9780], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16510823369026184\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0510, -3.5654, -3.4244, -4.1050], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.7225,  4.4777, -4.5153, -4.4202, -3.6864, -4.2393, -3.6864, -3.7614,\n",
      "        -4.4202, -4.4202], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.9718,  6.7887, -4.5818, -4.5818, -4.3177, -4.8386, -4.3177, -3.7081,\n",
      "        -4.5818, -4.5818], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.7145804166793823\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0329, -3.5357, -3.4250, -4.1181], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.0228, -3.4250, -3.5533, -3.5250, -2.0049, -3.7244, -4.9776, -3.7244,\n",
      "        -5.0228, -3.4250], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0709, -4.0825, -3.6994, -3.6994, -1.9880, -3.6994, -5.0709, -3.6994,\n",
      "        -5.0709, -4.0825], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09312351793050766\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0307, -3.5149, -3.4268, -4.1394], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.5149, -3.4268, -3.6895, -4.2915, -3.6895, -3.6895, -2.6429, -3.5836,\n",
      "        -3.9825, -3.1535], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3438, -4.0841, -3.6779, -4.5843, -3.6779, -3.6779, -3.3438, -4.0841,\n",
      "        -4.3348, -3.1612], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1413496434688568\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0194, -3.4778, -3.4149, -4.1673], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.4149, -3.4149, -4.3029, -3.5333, -3.9496, -5.0290, -5.0290, -3.5688,\n",
      "        -4.5202, -3.4149], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.0734, -4.0734, -4.3305, -3.6603, -4.1637, -5.0781, -5.0781, -4.0734,\n",
      "        -4.5860, -4.0734], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16274051368236542\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0084, -3.4457, -3.3964, -4.2021], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.0256, -3.6880, -0.2808, -2.6482, -3.5326, -4.4242, -3.3964, -3.6968,\n",
      "        -3.6968, -4.3511], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0806, -4.0568, -0.1731, -3.3030, -3.6520, -4.5875, -4.0568, -4.3271,\n",
      "        -4.3271, -4.3271], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1851579248905182\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0063, -3.4085, -3.3740, -4.2403], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.7021, -3.3740, -3.1455, -3.0846, -3.3740, -5.1620, -3.1244, -5.1620,\n",
      "        -3.3740, -4.4178], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3319, -4.0366, -3.5988, -3.1758, -4.0366, -5.0863, -3.1758, -5.0863,\n",
      "        -4.0366, -4.3319], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.19490966200828552\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0237, -3.3845, -3.3639, -4.2785], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.7041, -3.5687,  4.6426, -4.7202, -3.2031, -3.7102, -5.2178, -3.5061,\n",
      "        -3.3639, -0.2464], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3013, -3.6367,  6.9252, -4.5872, -3.5988, -4.3392, -5.0802, -3.6367,\n",
      "        -4.0275, -0.2053], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6619868278503418\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0496, -3.3642, -3.3589, -4.3077], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.5459, -0.2337, -4.1504, -3.9887, -2.7532, -4.7627, -4.4055,  3.2433,\n",
      "        -4.6666, -1.9445], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6288, -0.2204, -4.1519, -4.3506, -3.3157, -4.5898, -4.2810,  3.2025,\n",
      "        -4.7354, -2.0708], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05221640318632126\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0736, -3.3453, -3.3644, -4.3271], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-3.3644, -3.9382, -3.0483, -4.7998, -3.8987, -3.0483, -4.5374,  8.7488,\n",
      "        -0.2295, -3.0575], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.0108, -3.5837, -3.2028, -4.6122, -4.0108, -3.2028, -4.3610, 10.0000,\n",
      "        -0.2292, -3.2028], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.22567689418792725\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4152, -2.6014, -2.8758, -4.1265], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-3.7601, -4.6179, -2.9243, -4.7918, -3.0302, -4.7720, -4.1097, -4.1095,\n",
      "        -2.9243, -3.3928], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3841, -4.6478, -3.1862, -4.6478, -3.1862, -4.6478, -4.1472, -4.1472,\n",
      "        -3.1862, -3.9965], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09551814943552017\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.9688, -1.9226, -1.2611, -3.3942], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.4546, -3.4897, -4.1758, -4.6747, -3.0382, -5.2648, -2.9496, -2.9496,\n",
      "        -3.0382, -2.9320], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9812, -3.6870, -3.9812, -4.7047, -3.1516, -5.0231, -3.3499, -3.3499,\n",
      "        -3.1516, -3.3499], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09343289583921432\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.6583, -2.9817, -0.2665,  0.9687], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.1981, -1.9085, -4.8906, -3.5482, -4.4685, -3.4863, -4.1981, -2.9666,\n",
      "        -3.0286, -4.8906], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9672, -2.1749, -4.6585, -3.9672, -4.4755, -3.7506, -3.9672, -3.1124,\n",
      "        -3.3461, -4.6585], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06528518348932266\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 1.4823, -1.2613,  3.3657,  4.8966], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.6441, -4.3700, -4.7242, -4.4291, -2.9559, -3.0899, -1.9111, -2.9559,\n",
      "        -3.0899, -3.4929], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9764, -4.4802, -4.7073, -4.4802, -3.0589, -3.3454, -2.2047, -3.0589,\n",
      "        -3.3454, -3.7960], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04553208127617836\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 2.8150, -2.2684,  6.3019,  8.9460], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.3406, -5.1620, -4.6867, -3.7427, -3.5044, -2.9530, -5.1620, -4.6867,\n",
      "        -3.7427, -3.1532], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4841, -5.0054, -4.7150, -3.9881, -3.8233, -2.9930, -5.0054, -4.7150,\n",
      "        -3.9881, -3.3473], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.033253248780965805\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.2700, -4.4490, -4.9577, -5.1109], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.9542, -3.8406, -3.5182, -4.2898, -3.8406, -4.6587, -0.3280, -3.8406,\n",
      "        -3.8406, -3.8406], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.9361e+00, -4.0032e+00, -3.8414e+00, -4.2809e+00, -4.0032e+00,\n",
      "        -4.7247e+00,  5.8591e-04, -4.0032e+00, -4.0032e+00, -4.0032e+00],\n",
      "       grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03494786098599434\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-4.8857, -4.0359, -4.6308, -4.9986], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-3.9409, -1.9316, -3.1626, -2.9523, -4.1253, -4.1253, -3.9409,  9.0137,\n",
      "        -3.5263, -5.0765], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.0132, -2.2855, -2.8852, -2.8852, -4.4809, -4.4809, -4.0132, 10.0000,\n",
      "        -3.8464, -4.9950], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1551903486251831\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-4.4379, -3.5185, -4.1742, -4.7137], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.9610, -4.3251, -4.6672, -3.6403, -4.3251, -4.4379, -4.0403, -3.1210,\n",
      "        -4.1539, -2.9610], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.8656, -4.4771, -4.7286, -3.8089, -4.4771, -4.1666, -4.0216, -2.8656,\n",
      "        -4.0216, -2.8656], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.025318894535303116\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-4.4049, -4.1388, -4.1388, -3.3618, -3.5295, -2.9636, -4.2851, -4.6833,\n",
      "        -4.1388, -4.6949], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.2729, -4.0256, -4.0256, -3.3378, -3.8078, -2.8996, -4.4716, -4.7219,\n",
      "        -4.0256, -4.7219], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.017494264990091324\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-3.8254, -3.5361, -3.0846, -3.4501], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.2404, -4.9925, -3.4501, -3.5361, -4.7055, -4.2132, -4.2132, -2.9734,\n",
      "        -4.4491, -3.9980], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7761, -4.9566, -4.1598, -3.7761, -4.5982, -4.0301, -4.0301, -2.9251,\n",
      "        -4.1598, -4.1598], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10401924699544907\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.2887, -3.0019, -2.2038, -2.7078], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.7172, -4.7353, -1.4841, -4.8727, -4.6123, -4.1367, -2.9608, -3.5528,\n",
      "        -3.2887, -4.7172], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7230, -4.4796,  0.1494, -4.6006, -4.7230, -4.2877, -2.9835, -3.7516,\n",
      "        -3.7516, -4.7230], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3097112774848938\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.9235, -1.9440, -1.3825, -3.0630], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.4857, -3.7707, -1.9440, -4.2639, -5.0076, -4.2639, -5.2643,  1.5560,\n",
      "        -4.0469, -3.0427], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.0475, -3.6574, -2.2442, -4.0475, -4.9609, -4.0475, -4.9609,  3.7153,\n",
      "        -4.1359, -3.0318], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5153372287750244\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.6458, -3.2350, -0.3988,  1.0875], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.6897, -2.8412, -4.4652, -4.2482, -5.1876, -3.5987, -3.1440, -4.6897,\n",
      "        -4.6959, -4.5263], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7507, -3.0609, -4.3162, -4.0756, -4.7507, -3.5571, -3.0609, -4.7507,\n",
      "        -4.5272, -4.5272], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03356441110372543\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 1.6969, -1.6415,  3.6355,  5.1953], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.5082,  5.1953, -4.5842, -4.2100, -4.3687, -3.1431, -1.9632, -4.5842,\n",
      "        -2.7488, -4.5842], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4739,  7.3396, -4.5683, -4.1032, -4.5683, -3.0662, -2.0487, -4.5683,\n",
      "        -3.0662, -4.5683], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4765193462371826\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 3.2259, -2.9272,  6.6622,  9.2029], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.5996, -3.6787, -4.8447,  0.9160, -4.0045,  5.1848,  5.1848, -4.1779,\n",
      "        -4.3537, -4.1779], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6041, -3.4283, -4.8218,  3.6664, -4.1265,  7.2826,  7.2826, -4.1265,\n",
      "        -4.6041, -4.1265], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.6511528491973877\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-3.4592, -4.9097,  9.0567, -3.6726, -4.6713, -4.3141, -4.6011, -3.1737,\n",
      "        -3.1737, -4.1951], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3677, -4.9899, 10.0000, -3.3594, -4.6066, -4.6066, -4.6066, -2.9970,\n",
      "        -2.9970, -4.1446], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11574554443359375\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-4.6463, -4.0606, -4.5620, -4.8659], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.2361, -4.9567, -4.6019, -4.5620, -4.0707, -4.0707, -4.9567, -4.3046,\n",
      "        -4.0707, -4.3046], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3266, -4.9553, -4.5964, -4.3266, -4.0862, -4.0862, -4.9553, -4.5964,\n",
      "        -4.0862, -4.5964], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.023462019860744476\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-4.0455, -3.4145, -3.8921, -4.4238], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-3.4145, -3.6061, -3.6061, -4.0455, -3.2936,  1.2432, -3.1112,  8.8853,\n",
      "        -2.5767, -4.7562], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3191, -3.3191, -3.3191, -4.0731, -3.3278,  3.6596, -3.0016, 10.0000,\n",
      "        -3.0016, -4.9158], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.7475589513778687\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-3.3695, -3.5370, -2.5954, -3.1392], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.4287, -4.7906, -2.5954, -3.5370, -4.1571, -3.9491, -3.3294, -3.3384,\n",
      "        -4.6358, -3.0312], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5344, -4.8450, -2.9522, -3.3359, -4.2400, -4.0046, -3.2843, -3.3359,\n",
      "        -4.8450, -2.9522], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02438514307141304\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.0953, -2.9382, -2.1068, -2.8225], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.4371, -4.3875, -3.2636, -2.9382, -3.4659, -4.7912, -4.5551, -2.9382,\n",
      "        -3.9757, -4.3077], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4793, -4.4848, -3.2406, -2.8961, -3.3948, -4.6350, -4.7731, -2.8961,\n",
      "        -3.9373, -4.4793], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.012322613969445229\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-4.7672, -4.7672, -4.4966, -3.9566, -3.3775, -4.4444, -3.0220, -4.3131,\n",
      "        -4.4444, -4.4966], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5696, -4.5696, -4.6994, -3.8780, -3.4587, -4.6994, -3.4587, -4.4223,\n",
      "        -4.6994, -4.6994], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.050575900822877884\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1647, -1.6627, -0.4809, -2.0791], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.3181, -3.3181, -4.3104, -3.9268, -4.2638, -3.3181,  0.4004, -1.6627,\n",
      "        -4.3525,  0.4004], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4840, -3.4840, -4.3655, -3.8172, -4.5093, -3.4840,  0.8293, -1.4328,\n",
      "        -4.3655,  0.8293], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05787701532244682\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4763, -2.6737,  0.5320,  2.1328], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.0679, -2.7009, -4.2813, -3.0679, -4.4751, -4.4405, -3.6821, -3.8831,\n",
      "        -3.2699, -4.4405], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.1158, -2.8254, -4.4554, -3.1158, -4.4554, -4.3139, -3.7611, -3.7611,\n",
      "        -3.7333, -4.3139], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03186987340450287\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 2.1410, -1.3776,  3.7670,  5.1481], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.3261, -2.6416, -3.2030, -4.4287, -3.0119, -1.9979, -2.7612, -4.3029,\n",
      "        -3.8412, -4.5834], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4085, -2.7981, -3.4857, -4.4572, -3.0792, -1.3471, -2.7981, -4.2596,\n",
      "        -3.7107, -4.4085], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05907876417040825\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 3.7072, -2.2583,  6.5202,  8.6395], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.5835, -4.4963, -3.1693, -2.5835, -4.4083, -4.3673, -4.4083, -3.6358,\n",
      "        -4.3673,  0.7762], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.6645, -4.3765, -3.4575, -2.6645, -4.3946, -4.3765, -4.3946, -3.6174,\n",
      "        -4.3765,  1.0653], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.019505169242620468\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.4305, -3.7135, -4.3980, -4.3815], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-3.0424, -2.5430,  5.1328,  0.9163, -4.4940, -4.2106, -3.5649, -4.2311,\n",
      "        -3.8042, -3.5649], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.0228, -2.5128,  6.7523,  1.1390, -4.2084, -4.2084, -3.6403, -4.2084,\n",
      "        -3.6403, -3.6403], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.27941519021987915\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-4.2512, -3.5314, -4.1533, -4.5302], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.1763, -3.8832, -2.9081, -4.4256, -3.3882, -4.3462, -2.5157,  8.5635,\n",
      "        -2.5157, -2.9081], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.1918, -3.8261, -3.0025, -4.3447, -3.3373, -4.3002, -2.3768, 10.0000,\n",
      "        -2.3768, -3.0025], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2134803831577301\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-3.4910, -2.7924, -3.2384, -3.8408], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-3.5755, -3.5755, -2.7924, -4.4715, -4.4242, -3.8968, -4.4715,  1.1900,\n",
      "        -3.1294, -4.4715], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5995, -3.5995, -3.2777, -4.1781, -4.3347, -3.8018, -4.1781,  1.2976,\n",
      "        -3.2777, -4.1781], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05455281212925911\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-3.2930, -3.1380, -2.3991, -3.3267], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 2.2725, -3.1380, -3.4423, -4.3818, -4.3603, -1.2685, -3.5100, -3.9656,\n",
      "        -4.2008, -3.9656], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.6981, -3.1592, -3.5125, -4.1802, -4.3463, -1.1388, -3.6015, -4.1802,\n",
      "        -4.2518, -4.1802], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.219834566116333\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.4532, -2.4840, -1.1397, -2.0957], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.4428, -3.1393, -4.3001, -3.4428, -3.4428, -3.9705, -4.3854, -2.4840,\n",
      "        -4.2963,  5.2381], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5969, -3.0648, -4.1774, -3.5969, -3.5969, -3.7971, -4.2359, -2.0257,\n",
      "        -4.3509,  6.7654], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.26899638772010803\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9372, -1.5191, -0.0832, -1.8558], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.0637, -2.4575, -3.1234, -2.8009, -3.7605, -3.4128, -3.4128, -4.3389,\n",
      "        -3.8414, -3.7030], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.2155, -1.9695, -3.0038, -2.9542, -4.1599, -3.5882, -3.5882, -4.2155,\n",
      "        -4.1668, -3.7880], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06484347581863403\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.3834, -2.3128,  1.6001,  2.8752], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.7484, -4.2194, -2.8021, -3.5331, -3.0762, -1.1019, -2.4021, -2.0585,\n",
      "        -3.0762, -3.5331], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7647, -4.3063, -2.9322, -4.1209, -2.9986, -1.0879, -1.9917, -3.3288,\n",
      "        -2.9986, -4.1209], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.25101137161254883\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 2.5253, -1.2832,  4.1422,  5.2334], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.5110, -4.1821, -4.0785, -4.1647, -3.8694, -4.2171, -4.1647, -2.3493,\n",
      "        -2.3493, -2.3493], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.1819, -4.1530, -4.0897, -4.1530, -4.0897, -4.0897, -4.1530, -2.1819,\n",
      "        -2.1819, -2.1819], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.025839421898126602\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 4.2031, -1.8922,  6.7991,  8.4308], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.3825,  8.4308, -3.2013, -4.2414, -4.2414, -3.8001, -2.8488, -0.2364,\n",
      "        -2.7307, -2.9199], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3772, 10.0000, -3.1267, -4.0394, -4.0394, -3.6840, -2.8743,  1.5217,\n",
      "        -3.1267, -3.1267], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5854352116584778\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-4.2897, -3.3987, -2.2632, -3.9146, -1.9528, -2.3518, -4.2416, -4.2374,\n",
      "        -2.9527, -4.3451], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.0990, -3.3414, -2.6150, -4.0033, -2.6150, -2.6150, -4.0033, -4.1418,\n",
      "        -3.1166, -4.1418], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0813320204615593\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3337, -3.4621, -3.9675, -4.3180], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-3.5097, -3.1609, -3.5097, -2.2407, -3.5097, -2.8304, -4.2005, -3.8866,\n",
      "        -3.5097, -2.8304], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4379, -3.0737, -3.4379, -2.6729, -3.4379, -2.8355, -3.9827, -3.9827,\n",
      "        -3.4379, -2.8355], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02716911770403385\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.9701, -2.9473, -3.6087, -4.2217], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.5674, -4.2980, -4.2737, -4.0212, -3.4053, -2.8061,  8.4656, -3.4053,\n",
      "        -1.8644, -3.4844], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.0079, -4.0985, -4.1029, -3.8633, -3.3107, -2.8287, 10.0000, -3.3107,\n",
      "        -0.9859, -3.4319], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.34350988268852234\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.2389, -2.2513, -2.4351, -3.6740], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.0571,  1.6300, -4.0571, -3.7772, -2.5738, -3.4385, -2.7712, -2.2187,\n",
      "        -4.0571, -3.7772], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9722,  1.4641, -3.9722, -3.9722, -2.8786, -3.4348, -2.8280, -2.5820,\n",
      "        -3.9722, -3.9722], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03533255308866501\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-1.9262, -2.2149, -2.2149,  2.8028, -2.7401, -3.9464, -3.0292, -2.7708,\n",
      "         5.4021, -1.2798], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.4663, -2.4663, -2.4663,  3.8619, -2.8320, -3.9778, -3.0137, -2.7336,\n",
      "         6.8085, -0.8511], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.37124496698379517\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8291, -1.2768,  0.2340, -1.6775], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.3455,  1.7951, -1.4338, -2.7070, -2.7169, -2.7070, -3.4858, -3.9219,\n",
      "        -3.5525, -3.3455], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4452,  1.6307, -0.7894, -2.8312, -2.8312, -2.8312, -3.8448, -4.1017,\n",
      "        -3.8448, -3.4452], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07526952773332596\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-4.0929, -2.1498, -2.6842, -3.0692, -2.6842, -1.6975,  8.7987, -1.6975,\n",
      "        -4.0929, -2.7577], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.1440, -2.1006, -2.8239, -2.9231, -2.8239, -2.1006, 10.0000, -2.1006,\n",
      "        -4.1440, -3.3489], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2185511589050293\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-3.4943, -2.0926, -2.7497, -3.4943, -3.4699, -3.0043, -3.5064, -3.5064,\n",
      "        -3.2931, -3.4699], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7918, -1.9071, -2.5470, -3.7918, -3.9472, -3.3413, -3.9472, -3.9472,\n",
      "        -3.4334, -3.9472], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12302343547344208\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 2.9151, -0.9444,  4.4887,  5.6380], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.6320,  5.6380, -2.7187, -3.9188, -1.9898, -3.3370,  5.6380, -3.6320,\n",
      "        -2.0080, -3.8621], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9050,  7.0725, -2.6543, -4.0577, -0.8284, -3.4065,  7.0725, -3.9050,\n",
      "        -1.8119, -4.0577], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5718660354614258\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 4.6885, -1.3392,  7.1875,  8.9544], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.0851, -2.6567, -3.1660, -3.4379, -3.7473,  3.1236, -3.3898, -4.0120,\n",
      "        -1.1906, -3.9854], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7818, -2.8008, -3.3567, -3.6435, -3.8494,  4.1009, -3.3567, -4.0131,\n",
      "        -0.9760, -4.0131], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12049238383769989\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.3141, -3.3898, -4.1888, -4.2400], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.7988, -4.2400, -4.1181, -3.9145,  2.9480,  5.6832, -2.1359,  5.6832,\n",
      "        -2.4460, -1.7988], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7659, -4.0508, -3.9617, -3.5533,  4.1148,  7.0414, -1.7659,  7.0414,\n",
      "        -2.9224, -1.7659], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5607986450195312\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.6113, -2.7648, -3.4674, -3.8154], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-3.5956, -3.4436, -3.8723, -1.7182, -0.8393, -2.5178, -2.5178, -2.4071,\n",
      "        -4.2567, -0.2454], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2541, -3.2541, -3.7532, -1.7553, -1.2209, -2.9610, -2.9610, -2.9610,\n",
      "        -4.0205,  1.8577], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5492473244667053\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-3.3735, -2.4086, -3.1329, -3.7153], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-3.8933, -3.8933, -3.4185, -3.0489, -2.7075, -1.0713, -2.4086, -1.6865,\n",
      "        -3.3735, -4.2537], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7440, -3.7440, -3.2420, -3.2420, -2.7462, -1.1340, -2.8586, -1.6951,\n",
      "        -3.1678, -4.0197], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04180518537759781\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-4.2058, -2.6177, -3.9829, -4.2157, -3.3719, -4.0117, -2.4522, -1.9877,\n",
      "        -1.6720, -1.6720], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9323, -2.5789, -3.9323, -4.0347, -3.4514, -3.9323, -2.7198, -1.6175,\n",
      "        -1.6175, -1.6175], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.033883221447467804\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-3.3534, -2.4629, -3.1160, -3.6926], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-3.1160, -2.5624, -4.1616, -3.9536, -3.8410, -3.5215,  0.0752, -3.7512,\n",
      "        -4.1546, -1.6754], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.7425, -2.5678, -3.9549, -3.7719, -3.7719, -3.4341,  1.7991, -3.7719,\n",
      "        -4.0596, -1.5703], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3219987750053406\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-2.6139, -2.4356, -1.5897, -2.7387], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.8104, -1.7309, -4.0943, -1.0098, -2.4356, -1.6079, -4.1658, -3.2828,\n",
      "        -3.7099, -4.1120], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.8002, -0.7390, -3.9774, -0.7390, -2.4307,  1.7430, -4.0896, -3.2688,\n",
      "        -3.8002, -3.9775], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.233193039894104\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.0229, -1.6424, -0.5782, -2.0544], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.2924, -2.6966, -2.3941, -2.6966, -3.8449, -2.3659, -3.4952, -1.6424,\n",
      "        -3.8545, -2.7196], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2542, -3.2265, -2.5205, -3.2265, -3.7910, -2.3584, -3.4223, -1.5204,\n",
      "        -3.7910, -3.2265], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08633998036384583\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.6418, -0.9075,  0.3924, -1.6561], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.7784, -4.1375, -3.2574, -3.8895, -4.1330, -3.2287, -3.8895, -3.2629,\n",
      "        -4.1140, -2.4348], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.1913, -3.9427, -3.2397, -3.7830, -4.0660, -3.2397, -3.7830, -3.2397,\n",
      "        -4.0660, -2.3388], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02480604127049446\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.7259, -1.3533,  1.5268,  2.3669], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.6268, -3.2455, -3.2103, -4.1266, -1.5789, -3.2211, -2.6753, -3.6909,\n",
      "        -4.1266, -1.8917], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.7025, -3.2288, -3.2288, -4.0574, -1.5900, -3.1521, -3.1521, -3.7774,\n",
      "        -4.0574, -1.5900], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03466884046792984\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 3.3805, -0.2304,  4.5752,  5.4552], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.2540, -2.3404, -3.3907, -3.6931, -3.1981, -1.5577, -4.0192, -4.1148,\n",
      "        -3.1981, -4.1305], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.3828, -2.3828, -3.4094, -3.7721, -3.2176, -1.6443, -3.9103, -4.0467,\n",
      "        -3.2176, -4.0467], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.005674120504409075\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 5.0532, -0.1794,  6.8457,  7.9901], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.1835, -4.1534, -3.9616, -1.5452, -3.7863, -4.1214, -3.5968, -0.7905,\n",
      "        -3.1835, -0.7905], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2114, -3.9020, -3.7720, -1.6829, -3.7720, -4.0378, -3.4161, -0.5699,\n",
      "        -3.2114, -0.5699], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.025683987885713577\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.0523, -3.3717, -3.9694, -4.0945], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.0523, -3.2206, -3.5701, -3.5657, -2.1513, -4.0523,  1.8850, -3.4168,\n",
      "        -3.1721, -3.1721], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.0345, -3.3608, -3.3608, -3.4309, -2.3982, -4.0345,  3.8253, -3.4309,\n",
      "        -3.2108, -3.2108], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3911033570766449\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.3849, -2.7039, -3.2388, -3.8057], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-3.0650, -3.1433, -4.0335, -3.1433, -1.7960, -4.0335, -3.7227, -2.1110,\n",
      "        -2.1901, -1.5129], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.9988, -3.1984, -3.8824, -3.1984, -1.7062, -3.8824, -3.7706, -2.3768,\n",
      "        -2.3763, -1.7062], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02092347852885723\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-2.8903, -2.1853, -2.5606, -3.3254], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-3.9281, -2.1853, -3.0126, -3.6328, -0.6885, -3.6780,  1.8743, -3.8570,\n",
      "        -3.9281, -2.1675], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.8707, -2.3424, -2.9668, -3.7650, -0.4963, -3.7650,  3.7403, -3.7650,\n",
      "        -3.8707, -2.3424], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3616611957550049\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-3.3199, -2.4146, -2.0432, -3.8333, -3.7314, -3.6787, -3.5879, -1.4680,\n",
      "        -2.3597, -3.5879], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4355, -2.3418, -2.2622, -3.9879, -3.7490, -3.9879, -3.7490, -1.6466,\n",
      "        -2.5581, -3.7490], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.030963454395532608\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9991, -1.4405, -0.6789, -2.2930], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.7476, -1.9991, -3.0359, -2.7732, -2.7179, -3.6596, -2.8983,  2.0553,\n",
      "         1.4487, -2.9773], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9577, -2.1784, -3.1472, -2.8876, -2.8876, -3.8112, -3.1472,  3.6551,\n",
      "         0.8498, -3.1472], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.31621235609054565\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2644, -0.5601,  0.7034, -1.3277], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.5802, -1.3997, -3.5802, -3.5442, -3.2620, -3.5442, -2.9839, -2.9859,\n",
      "        -3.5154, -3.6608], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6855, -1.5776, -3.6855, -3.7576, -3.2096, -3.7576, -3.1001, -3.1001,\n",
      "        -3.6855, -3.9117], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02660227194428444\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 1.0138, -0.7480,  1.6422,  2.2933], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 1.6422, -3.1293, -3.5407, -3.6147, -1.3603, -1.1326, -3.5810, -3.6147,\n",
      "         5.0662, -2.8419], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.0640, -3.3674, -3.6433, -3.8632, -1.5503, -1.5503, -3.7050, -3.8632,\n",
      "         5.7764, -3.0492], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12984402477741241\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.3807, 0.2818, 4.3552, 5.0207], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 2.3742, -3.4522,  7.4713, -1.8756, -2.8905, -3.4225, -3.6061, -1.3130,\n",
      "        -1.8756, -2.9145], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.5187, -3.6511, 10.0000, -1.9927, -2.9972, -3.6015, -3.8173, -1.5151,\n",
      "        -1.9927, -2.9972], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.7906758785247803\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.0937, 0.5037, 6.5765, 7.5186], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.4284,  1.8189, -1.8235, -2.8501, -2.8387, -3.3897, -3.4365, -2.8501,\n",
      "        -2.8718,  1.8189], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.2677,  1.2916, -1.9207, -2.9419, -2.9419, -3.5548, -3.5548, -2.9419,\n",
      "        -2.9419,  1.2916], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06652016192674637\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.6256, -3.0248, -3.6815, -3.5598], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-3.4606, -1.7550, -1.7550, -1.2092, -1.7550, -3.3681, -0.4352, -1.2092,\n",
      "        -3.6256, -1.2092], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5097, -1.8769, -1.8769, -1.3917, -1.8769, -3.5507, -0.2429, -1.3917,\n",
      "        -3.7223, -1.3917], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02265709452331066\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.1494, -2.4612, -3.1233, -3.4783], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.1666, -3.3446, -2.8156, -2.4612, -2.8156, -1.8646, -1.1666, -1.9675,\n",
      "        -3.5432, -3.6144], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3056, -3.5148, -2.8469, -2.5587, -2.8469, -2.2291, -1.3056, -2.1826,\n",
      "        -3.6853, -3.6853], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0283442921936512\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-2.3771, -1.6820, -2.0515, -2.6821], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.1387, -2.7902, -3.5541, -3.3506, -2.9452, -0.3303, -3.5541, -0.3303,\n",
      "        -2.7902, -3.4478], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.2513, -2.8053, -3.6507, -3.4805, -3.1854, -0.1909, -3.6507, -0.1909,\n",
      "        -2.8053, -3.4346], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01454216055572033\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-1.8638, -1.6207, -0.7763, -2.2078], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.6647, -1.6356,  6.4612, -1.1111, -3.5876, -2.7692, -1.6356, -3.6830,\n",
      "        -1.1111, -3.5876], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.7656, -1.6987,  5.9133, -1.2060, -3.6231, -2.7656, -1.6987, -3.4493,\n",
      "        -1.2060, -3.6231], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.039344944059848785\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.5993, -1.0907, -0.1759, -1.7519], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.5845, -3.4272, -3.1340, -1.9930, -3.4013, -1.2361, -1.6009,  2.0780,\n",
      "        -2.6352, -3.6093], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.6472, -3.3717, -3.1275, -2.1125, -3.4299, -1.1583, -1.6472,  1.8166,\n",
      "        -2.7310, -3.5981], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.010799320414662361\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.0040, -0.2691,  0.9183, -0.9903], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.4888, -2.7130, -0.1358, -3.6271, -1.2342, -3.6271, -1.5764, -1.5764,\n",
      "        -3.4284, -0.6685], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4141, -2.6989, -0.1735, -3.5767, -2.1230, -3.5767, -1.6017, -1.6017,\n",
      "        -3.3498, -1.1223], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10155868530273438\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 1.7383, -0.2518,  2.0784,  3.2731], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.5628,  2.0784, -1.9618, -3.4295, -2.5926, -3.4295,  5.1182, -3.4603,\n",
      "        -3.4295, -2.7081], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.6137,  1.9458, -2.1039, -3.3333, -2.6742, -3.3333,  6.0057, -3.4087,\n",
      "        -3.3333, -2.6742], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0866214781999588\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.4256, 0.4428, 4.0009, 5.1302], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.0365, -3.4617, -3.4770, -2.8672, -0.0821, -1.5542, -3.1010, -3.4617,\n",
      "        -3.4838, -1.5305], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.0739, -3.3210, -3.4065, -2.6523, -0.1974, -1.6221, -2.8858, -3.3210,\n",
      "        -3.4065, -1.6221], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.017070459201931953\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.2437, 0.6975, 6.1107, 7.8201], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.6819, -1.4017, -3.4679, -1.0318,  0.8949, -1.5552, -3.5488, -3.3848,\n",
      "        -1.0318, -3.3848], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.8905, -2.0774, -3.4137, -1.0445,  2.0546, -1.6103, -3.5499, -3.3165,\n",
      "        -1.0445, -3.3165], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.18605360388755798\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-1.9125, -0.6702, -3.6489, -3.5036,  6.0633, -0.0234, -1.5297, -1.5594,\n",
      "        -1.5297, -0.0234], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.0577, -1.0210, -3.5518, -3.4287,  6.0385, -0.1523, -1.6032, -1.6032,\n",
      "        -1.6032, -0.1523], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.020577294752001762\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.5928, -2.7185, -3.5920, -3.5184], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-3.6475, -1.8195, -1.8905, -1.0268, -2.5749, -1.8905, -0.6825,  5.1447,\n",
      "        -1.0268,  6.0198], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5578, -2.0438, -2.0438, -0.9989, -2.6376, -2.0438, -0.9989,  6.0405,\n",
      "        -0.9989,  6.0405], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10139328241348267\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.0718, -2.1162, -2.9220, -3.2882], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.1162, -3.5338, -3.4202,  7.8104, -3.3263, -2.6939,  5.1473, -3.4591,\n",
      "        -3.7482, -3.5338], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2133, -3.4662, -3.5651, 10.0000, -3.3236, -2.6427,  6.0294, -3.3236,\n",
      "        -3.4662, -3.4662], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5712506771087646\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.4892, -1.3798, -1.7320, -2.9279], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.6906, -3.6146, -3.4527, -3.6664,  7.8848, -1.5902,  5.2091, -3.2952,\n",
      "         1.9806, -0.1638], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.6492, -3.5686, -3.3333, -3.5686, 10.0000, -1.6628,  6.0963, -3.3333,\n",
      "         2.1325, -0.0205], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5339337587356567\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3052, -1.0119,  0.1397, -1.3241], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 3.5761, -0.1477, -3.2533, -2.9058, -3.2533,  8.0288,  1.1559, -1.0119,\n",
      "        -1.8409, -0.1477], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.7897,  0.0404, -3.3431, -2.9270, -3.3431, 10.0000,  2.2185, -0.8743,\n",
      "        -2.0154,  0.0404], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5196822285652161\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.6711, -0.1083,  1.2663, -0.4818], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.9256, -3.4236,  5.4619, -3.5361,  0.2108, -3.5361, -1.6111,  8.2288,\n",
      "        -2.6526, -2.6526], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.4320, -3.3474,  6.4059, -3.5691,  0.1397, -3.5691, -1.6210, 10.0000,\n",
      "        -2.6498, -2.6498], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4297927916049957\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 1.9594, -0.2444,  2.0793,  3.7882], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.5057, -3.5057, -3.5057, -1.6230, -0.0683, -2.6295, -3.0130, -2.6097,\n",
      "         8.4711, -1.7845], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5631, -3.5631, -3.5631, -1.6149,  0.2404, -2.6432, -3.0786, -2.6432,\n",
      "        10.0000, -1.9876], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.24895115196704865\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.7229, 0.4812, 4.1392, 5.8342], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.7650, -3.4660, -0.9561, -3.0546, -3.6036, -0.9561, -1.0813, -0.9561,\n",
      "        -3.4660, -1.7650], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.9732, -3.5494, -0.6962, -3.3461, -3.3461, -0.6962,  0.3280, -0.6962,\n",
      "        -3.5494, -1.9732], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.24404513835906982\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.7098, 0.8006, 6.3488, 9.0063], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.6005, -2.2056,  2.1127, -3.4135, -3.1646, -0.9150, -2.2578, -2.6330,\n",
      "         0.0170,  2.1127], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.6499, -2.3486,  2.6119, -3.3231, -3.3231, -0.7203, -2.3486, -2.6148,\n",
      "         0.3210,  2.6119], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06936083734035492\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.6033, -2.7677, -3.6540, -3.4849], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.5964,  0.0615, -0.8704, -3.1745, -1.9371,  1.4576, -2.6523, -3.6033,\n",
      "        -2.6523, -3.4059], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.5933,  0.3118, -0.7445, -3.2957, -2.3100,  2.6669, -2.5933, -3.4909,\n",
      "        -2.5933, -3.4591], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17172107100486755\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.0011, -2.1792, -2.9292, -3.2626], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.5362, -2.0788, -3.4237, -3.4267, -2.7314,  6.5119, -3.4300, -3.5234,\n",
      "        -2.6636, -2.6636], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7288, -2.2273, -3.4255, -3.2712, -2.9613,  7.4381, -3.4255, -3.4583,\n",
      "        -2.5675, -2.5675], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10167215764522552\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-2.1855, -1.3519, -1.7994, -2.3621], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.6532, -3.6369, -3.5630, -2.6532, -3.0106, -3.1307,  0.2635, -0.7823,\n",
      "        -2.7070, -3.2560], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.5433, -3.4363, -3.4363, -2.5433, -2.9267, -2.7792,  0.3966, -0.7629,\n",
      "        -2.9267, -3.2509], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.027736296877264977\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-3.1729, -0.7486, -3.2688,  6.4034, -2.9847, -3.5496, -1.6948, -0.8189,\n",
      "        -1.3104, -0.7486], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3821, -0.7393, -3.2399,  7.6601, -2.8963, -3.2399, -1.7971, -0.7393,\n",
      "        -0.7393, -0.7393], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.20706681907176971\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2740, -0.7233,  0.2708, -1.2086], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.6112, -3.4898,  6.4785, -2.6112, -0.7233, -3.3768, -3.4194,  2.2698,\n",
      "        -3.3768, -3.2921], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.5058, -3.2286,  7.7359, -2.5058, -0.7563, -3.2286, -3.3473,  2.6569,\n",
      "        -3.2286, -3.2286], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.18755610287189484\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4036,  0.3088,  1.7149,  0.0192], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 4.0719, -1.9556, -1.4532, -3.3534, -2.6607, -0.4036, -0.7023, -3.2705,\n",
      "        -2.5673, -2.6237], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.8980, -2.0984, -1.7136, -3.3112, -2.8333, -0.7435, -0.7435, -3.2230,\n",
      "        -2.4885, -2.4885], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09461837261915207\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.0158, 0.0065, 2.4606, 4.1281], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.5039, -3.2825, -3.2825, -1.5314, -2.5039,  9.8148, -3.0265, -2.6556,\n",
      "        -2.0028, -2.6556], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.4735, -3.2198, -3.2198, -1.7140, -2.4735, 10.0000, -2.7608, -2.8025,\n",
      "        -2.0786, -2.8025], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01969338394701481\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.0727, 0.7994, 4.9381, 6.6857], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.4407, -1.4257,  1.8470, -1.4257, -3.2492, -2.4406, -1.1625, -2.4406,\n",
      "         9.8764, -3.5759], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2501, -1.5986,  2.7716, -1.5986, -3.2190, -2.4624, -1.5986, -2.4624,\n",
      "        10.0000, -3.3968], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11903493106365204\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0681, 1.1909, 7.5417, 9.9326], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.2155, -2.3603, -2.7110, -1.1192,  2.6678, -3.0079, -0.6484, -2.3603,\n",
      "        -3.3498, -1.4118], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2272, -2.4598, -2.7538, -1.9936,  2.7966, -3.2272, -0.5338, -2.4598,\n",
      "        -3.2356, -1.4756], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08813786506652832\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.5050, -2.6584, -3.2750, -3.4718], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.3111, -3.0647,  2.7385, -3.2135, -1.3874, -2.1720, -3.2135, -3.2135,\n",
      "        -3.0647, -3.4718], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.9323, -3.2245,  2.7875, -3.2259, -1.3855, -2.7324, -3.2259, -3.2259,\n",
      "        -3.2245, -3.3925], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07602275907993317\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-2.6491, -1.8800, -2.1860, -2.9566], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.5177,  0.4584, -3.2520, -3.3684, -3.2918, -3.2520, -1.1828, -3.5007,\n",
      "        -3.1459,  6.8009], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.4401,  0.9138, -3.2146, -3.2111, -3.3722, -3.2146, -1.3291, -3.3722,\n",
      "        -3.2111,  7.9725], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16621169447898865\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-2.2489, -1.2077, -1.4965, -2.5980], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-3.4985, -2.5980, -1.6401, -3.5661, -2.8957, -0.6239, -1.2077, -2.2837,\n",
      "        -3.2253, -1.8399], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3486, -2.6559, -1.7749, -3.2058, -2.7929, -0.6213, -1.2820, -2.4297,\n",
      "        -3.2058, -2.0869], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.027258748188614845\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-1.9262, -1.2766, -0.2620, -2.3653], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.2620, -1.9848, -2.9631, -3.2776, -0.6085, -2.8985, -2.2677, -1.2766,\n",
      "        -0.6085, -2.2677], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6635, -1.7049, -3.1913, -3.1913, -0.6635, -3.1913, -2.4208, -1.2358,\n",
      "        -0.6635, -2.4208], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04394813999533653\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.6073, -0.5860,  0.3412, -1.9209], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.2491, -0.5860, -2.4191, -3.3862, -3.4322, -3.2799,  0.5731, -2.6108,\n",
      "        -3.2799, -1.2491], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.2391, -0.6929, -2.4169, -3.1772, -3.3079, -3.1772,  0.9931, -2.4169,\n",
      "        -3.1772, -1.2391], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03058803081512451\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3396,  0.6117,  2.2210,  0.0683], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 2.2210,  6.8762, -1.2310, -3.2556, -1.8997, -2.2861, -2.5450, -2.2861,\n",
      "        -2.5450,  0.3189], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 2.6168,  7.9337, -1.2401, -3.1704, -1.6028, -2.4138, -2.5962, -2.4138,\n",
      "        -2.5962,  0.9989], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.18705815076828003\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.9441, 0.1225, 2.8821, 3.9813], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.5061, -3.3067, -0.5320, -3.1338, -2.5225, -0.9411, -2.4033,  3.9813,\n",
      "        -3.5061, -1.2783], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2356, -3.2767, -0.6772, -2.5769, -2.4137, -1.5452, -2.4137,  5.2049,\n",
      "        -3.2356, -1.2253], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.23552067577838898\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.2046, 0.9376, 5.6200, 6.9066], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.5094, -1.9271, -3.4200, -0.5191, -3.1299, -0.5191, -3.1299,  2.9356,\n",
      "        -3.1744, -0.5191], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.7118, -1.5751, -3.2584, -0.6260, -3.1689, -0.6260, -3.1689,  2.6372,\n",
      "        -3.2725, -0.6260], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03268614038825035\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0360, 1.3409, 8.1433, 9.8718], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 9.8718, -1.0378, -2.5253, -1.3140, -3.3113, -2.7494, -2.4164, -2.8293,\n",
      "         0.6669, -1.9022], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([10.0000, -1.4957, -2.5929, -1.1667, -3.2954, -3.2728, -2.4397, -3.1748,\n",
      "         1.0474, -2.7006], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14286990463733673\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-2.2986, -0.5023, -0.6946,  8.1259, -0.5023, -1.3179, -1.8250,  6.9275,\n",
      "        -1.0418, -0.5349], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.4382, -0.5035, -0.5035,  7.8628, -0.5035, -1.2006, -2.1861,  7.8628,\n",
      "        -1.4814,  1.0177], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.37479180097579956\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-2.7130, -1.8716, -2.3080, -3.0959], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.4737,  9.7960, -2.7130, -1.2320, -2.3534, -0.4674,  4.1131, -3.0025,\n",
      "        -3.0025, -0.3244], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.1628, 10.0000, -2.6845, -1.2920, -2.4247, -0.5454,  5.2266, -3.1512,\n",
      "        -3.1512, -0.5454], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14868858456611633\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-2.4344, -1.2670, -1.8995, -2.8922], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.2223, -2.9344, -2.9923, -1.2223, -2.5399, -2.3917, -2.3917, -2.3917,\n",
      "        -1.2670, -2.3917], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3662, -3.1357, -3.1357, -1.3662, -2.5514, -2.4104, -2.4104, -2.4104,\n",
      "        -1.3662, -2.4104], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.011391752399504185\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-1.9066, -1.2172, -0.4604, -2.3441], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.4058, -2.9792, -0.4604, -2.4264,  2.9145, -2.4463, -1.2517, -3.0000,\n",
      "        -2.3814, -1.2517], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5498, -3.1263, -0.5498, -2.3992,  2.7726, -2.5051, -1.4144, -3.1263,\n",
      "        -2.7418, -1.4144], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.027354801073670387\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2529, -0.3882,  0.5200, -1.4085], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.1861, -2.4552,  0.7348, -3.2332, -1.2140, -3.4173, -1.5472, -2.9597,\n",
      "        -3.0206, -3.0267], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.1244, -2.3924,  0.6357, -3.2729, -1.4384, -3.1295, -1.3451, -3.1295,\n",
      "        -3.1244, -3.1244], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.024228129535913467\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4506,  0.7293,  1.7560, -0.1063], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 4.2668, -2.1966, -3.2350, -0.3799, -0.3621, -1.3291, -3.0418, -3.0418,\n",
      "        -3.1778, -2.2909], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 5.2000, -2.3825, -3.1264, -0.5063,  0.5804, -1.6331, -3.1264, -3.1264,\n",
      "        -3.2674, -2.1327], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.196120947599411\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-2.2516, -2.8020, -0.9557, -0.3619, -3.1227, -3.0771, -1.1991, -1.5099,\n",
      "        -1.1991, -2.9938], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.1274, -3.1182, -1.2851, -0.5116, -3.1237, -3.1237, -1.4492, -1.2851,\n",
      "        -1.4492, -2.7720], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04732746630907059\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.2427, 0.2915, 2.9688, 4.3802], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.2165, -3.1797,  4.3802, -1.2701, -0.6911, -3.4900, -3.1621, -0.3564,\n",
      "        -1.1851, -0.9441], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2309, -3.2309,  5.1652, -1.6220, -0.5011, -3.1220, -3.1238, -0.5011,\n",
      "        -1.4212, -1.2625], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10938926041126251\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.1705, 1.0096, 5.3281, 6.8250], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.3615, -1.1712, -2.4495, -1.3733, -0.3615,  4.4613, -2.9008, -0.3615,\n",
      "        -1.3733, -2.3712], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5120, -1.3818, -2.3155, -1.5999, -0.5120,  5.1425, -3.1341, -0.5120,\n",
      "        -1.5999, -2.7647], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09061785042285919\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.8673, 1.3876, 7.5477, 9.5621], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.4185,  0.6889, -2.4185, -3.1313,  9.5621, -0.9568, -3.1313, -3.1313,\n",
      "        -2.4185, -2.6681], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2930,  0.3173, -2.2930, -3.1148, 10.0000, -1.2380, -3.1148, -3.1148,\n",
      "        -2.2930, -2.3258], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05741922929883003\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-1.2513, -3.1217, -0.3858,  1.4477, -3.0728,  1.4477, -1.5986, -2.3586,\n",
      "        -2.0926, -3.0644], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.2429, -3.1144, -0.4459,  3.2082, -3.1872,  3.2082, -1.2429, -2.2685,\n",
      "        -2.2685, -3.1924], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.639726459980011\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.1727, -2.4658, -3.2079, -3.1737], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.1750, -3.0715, -1.8934, -3.1463, -2.2894, -0.1750, -3.1727, -1.4033,\n",
      "         9.5119, -0.3944], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3833, -3.1755, -2.2376, -3.1122, -2.2376, -0.3833, -3.2192, -1.5153,\n",
      "        10.0000, -0.3833], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04730590060353279\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-2.8060, -1.8909, -2.7457, -3.3105], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-3.0843, -1.2288, -3.1727, -0.3893, -1.2288, -3.0843, -2.2191, -1.8596,\n",
      "        -3.1671, -0.3893], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.1630, -1.1049, -3.1059, -0.3082, -1.1049, -3.1630, -2.2003, -2.1059,\n",
      "        -3.2353, -0.3082], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.012630343437194824\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-2.1602, -2.1602,  6.6660, -2.8047,  0.8177, -3.3370,  9.4511, -1.1573,\n",
      "        -2.3900, -1.0092], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.1606, -2.1606,  7.5060, -3.0947,  0.6447, -3.2401, 10.0000, -1.1034,\n",
      "        -2.3067, -1.2208], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11849243938922882\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-2.2202, -1.1638, -1.9021, -2.7124], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.1638, -0.3702, -3.1450, -2.0733, -3.1677, -1.1578, -1.2896, -0.3702,\n",
      "        -1.1638, -2.4882], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.1073, -0.2374, -3.0772, -2.1308, -3.1341, -1.1073, -1.4250, -0.2374,\n",
      "        -1.1073, -2.6565], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.009988931939005852\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-1.5517, -1.1534, -0.1474, -2.0660], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.1490, -3.3330, -1.1534, -0.1474, -1.1534, -3.2085, -0.3560, -0.3560,\n",
      "        -2.1130, -3.3330], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.0585, -3.1158, -1.1327, -0.2453, -1.1327, -3.1158, -0.2453, -0.2453,\n",
      "        -2.0999, -3.1158], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.014616662636399269\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.0565, -0.3397,  0.8312, -1.4423], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.2075, -1.1526, -3.1876, -2.1445, -1.4423, -3.2912, -1.1526, -2.0872,\n",
      "        -2.4782, -2.0872], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.9870, -1.1591, -3.1092, -1.9870, -1.3532, -3.0454, -1.1591, -2.0731,\n",
      "        -2.6055, -2.0731], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.016458438709378242\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-3.5331, -3.3038, -1.1538, -1.1067,  2.1466, -0.3279,  0.8619, -0.3279,\n",
      "        -3.2623, -2.0573], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2302, -3.2302, -1.1630, -1.2063,  2.9841, -0.2411,  0.9319, -0.2411,\n",
      "        -3.1098, -2.0540], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08518334478139877\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([ 7.7426, -3.1352, -2.3423, -0.1781,  5.3945, -0.3125, -1.1145, -0.1781,\n",
      "         0.8568, -0.3125], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 7.4302, -3.0282, -2.2599, -0.2289,  4.9342, -0.2289, -1.2024, -0.2289,\n",
      "         1.0196, -0.2289], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03811177611351013\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.3124, -1.1332, -1.9996, -2.7991], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 7.6983, -3.5018, -3.2420,  0.2170, -2.8291, -3.2420, -1.9996, -1.5767,\n",
      "        -3.1552, -1.4983], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 7.4267, -3.2258, -3.1086, -1.1985, -3.1086, -3.1086, -2.0199, -1.1752,\n",
      "        -3.2258, -1.2893], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.24774155020713806\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.7067, -0.1865, -1.0950, -2.4851], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-3.0844, -3.2297,  2.3060, -1.3912, -0.2355, -1.0204, -1.1470, -3.2297,\n",
      "        -1.1103, -3.2264], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.0190, -3.0190,  2.8804, -1.2734, -0.2252, -1.2119, -1.2119, -3.0190,\n",
      "        -1.1678, -3.1020], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.049661461263895035\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.1268,  0.9694,  2.3382,  0.0599], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.6924, -3.1871, -0.2369, -1.9588, -3.1850,  0.9694, -3.1850, -1.1434,\n",
      "        -1.6924, -3.0599], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.9073, -3.0168, -0.2077, -1.9857, -3.1011,  1.1044, -3.1011, -1.2132,\n",
      "        -1.9073, -3.0168], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0162014402449131\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.3102, 0.2624, 2.9871, 4.2248], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.2873, -0.9901, -2.2417,  6.5451, -3.1298, -3.0409, -3.3032, -3.1298,\n",
      "        -3.0651, -0.2237], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.1900, -1.2027, -1.9760,  7.3806, -3.0175, -3.2236, -3.2236, -3.0175,\n",
      "        -3.0175, -0.1900], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08915473520755768\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.1454, 0.8897, 5.1950, 6.5406], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.0691, -1.5161, -2.2896, -2.9583, -1.1281, -2.4736, -0.2098, -2.2224,\n",
      "        -3.3470, -3.0548], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.1071, -1.1740, -2.2503, -3.1071, -1.1740, -2.5217, -0.1746, -2.2503,\n",
      "        -3.2262, -3.0129], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.016492439433932304\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.9086, 1.2530, 7.4060, 9.2691], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.0110, -0.9711,  1.0384, -2.9877, -3.0110, -3.2902,  2.4173, -3.2902,\n",
      "        -1.8893, -0.1967], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.1085, -1.1578,  1.1756, -3.0105, -3.1085, -3.2276,  2.7494, -3.2276,\n",
      "        -1.9629, -0.1658], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.019769303500652313\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.0442, -2.4744, -3.1573, -3.2385], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.0945, -2.0224, -2.9355,  1.0638, -2.9355,  6.5293, -1.8736, -0.9693,\n",
      "        -1.1081, -0.9268], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.0550, -1.8724, -3.0084,  1.2034, -3.0084,  7.3210, -1.9576, -1.1298,\n",
      "        -1.1298, -1.0550], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16517651081085205\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-2.5532, -1.6791, -2.4719, -3.2483], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.8650, -1.8650,  9.1948,  2.8092, -0.8476, -2.9776, -1.8650, -2.9973,\n",
      "        -2.9155, -0.1614], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.9456, -1.9456, 10.0000,  2.6589, -1.1055, -3.1148, -1.9456, -3.0064,\n",
      "        -3.0064, -0.1605], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.078413225710392\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-2.0280, -0.9518, -1.6232, -2.6877], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 1.1195, -2.9866, -1.0434,  4.0253, -2.8087, -1.6645, -2.1648, -2.9125,\n",
      "        -2.9125, -3.1312], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.1832, -3.1142, -1.0649,  4.8682, -3.0012, -1.8566, -1.8566, -3.0012,\n",
      "        -3.0012, -3.1142], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09163714200258255\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-1.3233, -1.0000, -0.0070, -2.0953], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.0044, -1.6357, -2.8796, -1.0000,  2.4221, -2.8301, -1.8674, -2.9863,\n",
      "        -0.8295, -0.1233], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.9887, -1.1670, -2.9887, -1.0063,  2.6217, -3.1081, -1.9230, -3.1081,\n",
      "        -0.9546, -0.1751], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03852711617946625\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.8232, -0.1009,  0.9543, -1.5721], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.8716, -2.6909,  2.4319, -2.9343, -3.2594,  4.0253, -2.5506,  6.5235,\n",
      "        -3.1408,  1.1732], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.9799, -2.4812,  2.6228, -3.1861, -3.1861,  4.8711, -2.4812,  7.2538,\n",
      "        -3.1861,  1.1887], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1416780948638916\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([ 0.1130, -0.7741, -2.5387, -2.9765, -1.0056, -2.8619, -0.9233,  2.4566,\n",
      "        -0.9189, -2.3168], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.2110, -0.8983, -2.4739, -3.0851, -0.8983, -2.9689, -0.8860,  2.6484,\n",
      "        -0.8860, -2.1983], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13132014870643616\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.6185,  0.1669, -0.7418, -2.5710], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.7418, -2.8855, -0.7418, -0.5890, -1.8810, -3.0752, -2.8522, -2.1651,\n",
      "        -1.8810, -0.0409], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.8497, -2.9486, -0.8497, -0.8497, -1.8835, -3.1300, -2.9486, -1.8835,\n",
      "        -1.8835, -0.0854], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.018882161006331444\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-1.9857, -3.0318, -0.8663, -0.8327,  2.7227, -3.0318,  6.4789, -0.0069,\n",
      "        -3.0679, -0.0069], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7796, -2.9223, -0.8399, -0.8399,  2.6407, -3.0446,  7.1728, -0.0830,\n",
      "        -3.0966, -0.0830], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.055607426911592484\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.5927,  0.2574, -0.6969, -2.5732], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.8467,  5.0170, -2.1141, -0.8467, -0.8002, -3.0385,  0.0174, -2.5234,\n",
      "        -2.3004,  0.0174], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.8226,  4.8240, -1.8611, -0.8226, -0.8226, -2.9027, -0.0713, -2.4234,\n",
      "        -2.4234, -0.0713], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01622552052140236\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4012,  1.3020,  2.3451, -0.8265], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 1.0534, -2.9749, -2.8955, -0.7811, -1.9636, -1.9636, -3.0289, -1.3375,\n",
      "         2.6981, -1.9636], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.1106, -2.8848, -2.8848, -0.8056, -1.8642, -1.8642, -2.8848, -1.1627,\n",
      "         2.6411, -1.8642], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.009636002592742443\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.2363, 0.2891, 2.6979, 4.0616], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.9671, -2.2754, -2.3019,  2.3335, -2.9874,  0.0443, -0.9644,  0.0443,\n",
      "        -2.9637, -2.2086], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.8680, -2.4098, -2.1252,  2.6554, -2.9877, -0.0095, -0.7170, -0.0095,\n",
      "        -2.8757, -2.1252], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02444620616734028\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.1892, 0.9826, 4.9687, 6.4887], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.0502, -2.0786, -1.5305, -0.8214, -1.5445, -2.9243, -2.9548, -0.6238,\n",
      "        -2.2752, -2.9548], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.0415, -1.8625, -1.1984, -0.7434, -1.1984, -2.8707, -2.9719, -0.7019,\n",
      "        -2.4061, -2.9719], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.030958209186792374\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.8425, 1.3731, 6.9894, 8.9773], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.9228,  4.0831, -1.9248,  1.3133, -2.9674, -0.7741, -0.7741, -2.9228,\n",
      "         1.2106, -2.7877], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.9618,  4.8485, -1.8679,  1.1178, -3.0536, -0.7160, -0.7160, -2.9618,\n",
      "         1.1178, -2.8670], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0659351497888565\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.8260, -2.2802, -2.9421, -2.9507], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.0416, -2.1649, -1.5519, -1.8988,  8.9596, -2.7475, -1.8988, -2.7680,\n",
      "         0.0416, -1.8988], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.1361, -2.1532, -1.7147, -1.8671, 10.0000, -2.9484, -1.8671, -2.8600,\n",
      "         0.1361, -1.8671], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11786910146474838\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-2.2984, -1.5465, -2.3087, -2.9039], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.8548, -0.7741, -2.9307, -1.8732,  6.5268,  0.0426, -2.2587, -2.9307,\n",
      "        -2.7599, -2.9209], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.9279, -0.6564, -3.0463, -1.8644,  7.1087,  0.1747, -2.3919, -3.0463,\n",
      "        -2.8537, -2.9279], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04286935552954674\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-1.6688, -0.7521, -1.3554, -2.2248], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.8553, -1.8553, -2.8441, -2.7747, -2.1153,  0.3157,  0.0535, -2.9154,\n",
      "        -2.7421,  2.7888], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.8549, -1.8549, -2.9038, -2.8417, -2.1533,  2.7910,  0.1938, -2.9038,\n",
      "        -2.9038,  2.7910], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6182239651679993\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-0.7961, -0.7054,  0.3723, -1.4086], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.4042,  0.3723, -2.8916,  4.1368,  0.1013, -2.7979, -1.4925,  2.7498,\n",
      "        -0.7054,  0.1013], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.1542,  0.1696, -2.8608,  4.8723,  0.1696, -2.8160, -1.6434,  2.7232,\n",
      "        -0.6649,  0.1696], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11800935119390488\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3537,  0.1595,  1.2682, -0.8898], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 2.7185, -0.6715,  4.0871, -2.9274,  0.1595, -2.8150, -1.8560, -2.8945,\n",
      "        -3.0471, -2.1981], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 2.6784, -0.7088,  4.8393, -2.8179,  0.1414, -2.9783, -1.8053, -2.7894,\n",
      "        -2.9783, -2.3114], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06390256434679031\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-1.9793, -2.8204,  0.2954, -2.8327, -2.9413, -2.8147, -2.1663,  8.9238,\n",
      "         0.4768, -2.9185], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.0791, -2.9496,  0.1289, -2.7645, -2.7814, -2.7814, -2.2790, 10.0000,\n",
      "         1.0728, -2.7645], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1635686755180359\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3310,  0.5238, -0.4389, -2.3860], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.9472,  2.7175, -1.3857, -0.3408, -2.2113, -0.3353, -2.9172, -2.7573,\n",
      "        -2.1375,  2.7175], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.0518,  2.6836, -1.5354,  0.1255, -2.2471, -0.7661, -2.7402, -2.7402,\n",
      "        -2.2471,  2.6836], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.048362936824560165\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4339,  1.5204,  2.2057, -1.0551], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.8284,  0.2500, -0.4141, -2.7413, -1.3488, -0.5519,  0.3042, -2.8321,\n",
      "        -2.2429, -1.8295], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.7158,  0.0797, -0.4854, -2.7158, -1.4967, -0.7750,  0.0797, -2.7263,\n",
      "        -1.7285, -1.7285], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04555175453424454\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.3191, 0.8122, 2.7275, 4.1270], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 6.7235, -2.9352,  1.5423, -3.0579, -1.8139,  9.0058, -2.8882, -1.3302,\n",
      "        -0.8009,  2.1795], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 7.1052, -2.7141,  0.9615, -2.8904, -1.7208, 10.0000, -2.7001, -1.4723,\n",
      "        -0.4508,  2.7143], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.20212188363075256\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.1460, 1.5715, 4.7970, 6.4824], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.8097, -1.3513, -0.8247,  4.1671, -0.3282, -2.8863,  1.5524, -2.8863,\n",
      "         0.3425,  1.5524], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.7026, -1.4480, -0.7031,  4.8342, -0.4289, -2.7026,  0.9968, -2.7026,\n",
      "         0.1072,  0.9968], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12311084568500519\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.8449, 2.2121, 6.8725, 9.1296], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.7620,  0.3349, -2.7940,  0.6437, -1.6996,  0.3349,  0.4143, -2.1587,\n",
      "        -0.7651, -0.2652], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.6812,  0.1763, -2.6963,  1.0693, -1.7017,  0.1763,  0.1763, -1.9209,\n",
      "        -0.4207, -0.4207], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.050363827496767044\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([ 2.3637,  0.3264, -2.6778,  2.3637, -0.4723, -2.6895, -2.1988, -2.6778,\n",
      "        -2.6895, -0.7830], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 2.8949,  0.2634, -2.6783,  2.8949, -0.5664, -2.6925, -2.1951, -2.6783,\n",
      "        -2.6925, -0.4156], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07121193408966064\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6852, -1.8746, -2.4819, -2.6028], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.5680, -0.1510, -0.1510, -1.5619, -2.5458,  2.4697,  4.1695, -2.4588,\n",
      "        -2.8244,  1.4900], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.3587, -0.4143, -0.4143, -1.6928, -2.6742,  2.9275,  4.9031, -2.6742,\n",
      "        -2.8867,  1.2228], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10853469371795654\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2939, -0.9921, -1.7615, -2.8364], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.0992, -0.4720, -2.7958, -2.6434, -2.7958, -2.6557,  9.2438,  0.3192,\n",
      "        -0.4507, -2.7661], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4057, -0.4366, -2.8865, -2.8865, -2.8865, -2.6800, 10.0000,  0.4477,\n",
      "        -0.4366, -2.8865], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07743364572525024\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3340, -0.2501, -0.4139, -1.7952], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 6.5659, -2.5354, -1.8353, -2.5229, -0.4740,  6.5659, -0.2501,  0.3290,\n",
      "         2.6793,  1.6884], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 7.3585, -2.6518, -1.6659, -2.6720, -0.4083,  7.3585,  0.5195,  0.5195,\n",
      "         2.9270,  1.4114], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.20919939875602722\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.1806,  0.3304,  1.6710, -0.4788], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.3304, -1.3701,  1.6710, -0.4604, -1.4533, -1.8379, -2.5281,  3.1105,\n",
      "        -1.4533, -2.5281], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.5039, -1.3424,  1.4841, -0.4134, -1.6452, -1.8387, -2.6274,  2.9000,\n",
      "        -1.6452, -2.6274], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02056807279586792\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.1533,  1.5397,  2.8238, -0.6277], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.5378, -2.0314, -0.3000, -1.2678, -0.3422, -1.2678, -2.0314, -1.4641,\n",
      "         0.6244, -2.5445], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.6036, -2.1415, -0.3817, -1.3080, -0.4380, -1.3080, -2.1415, -1.6268,\n",
      "         0.4589, -2.6342], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.010956770740449429\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.5178, 1.0081, 3.0777, 4.2615], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.8505, -2.5583, -0.0910, -0.6313, -2.5818, -2.0140, -1.2382, -0.3084,\n",
      "         3.0777, -0.2715], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.8126, -2.5830,  0.4200, -1.0819, -2.5830, -2.1144, -1.2776, -0.4744,\n",
      "         2.8354, -0.4744], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06051262095570564\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.3736, 1.7569, 5.1030, 6.5712], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.6484, -2.4570, -0.0862, -0.2716, -0.2716,  0.3350, -2.8981, -0.6287,\n",
      "        -2.5915,  6.5712], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.5987, -2.5615, -0.3466, -0.4972, -0.4972,  0.3189, -2.7965, -0.3466,\n",
      "        -2.5615,  7.3289], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08482708036899567\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1851, 2.5020, 7.2906, 9.2294], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.5117,  0.3251, -1.5061,  0.5721,  6.5712, -2.3296,  0.3251,  0.5721,\n",
      "        -2.9222, -2.7058], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.2335,  0.2396, -1.5691,  0.2396,  7.3064, -1.5691,  0.2396,  0.2396,\n",
      "        -2.7828, -2.5458], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14810018241405487\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.8507, -1.9818, -2.6668, -2.9088], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.7277, -0.9144, -0.0955, -2.6668, -0.0955, -1.0206, -0.0955, -1.5442,\n",
      "         0.5529, -2.9088], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.7088, -0.5024, -0.3451, -2.5878, -0.3451, -0.9066, -0.3451, -1.5973,\n",
      "         0.2093, -2.7836], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1475081741809845\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-2.1894, -1.1800, -1.9629, -2.6637], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.6092, -2.7376, -2.8848,  2.9861,  0.3200, -2.6637, -2.6411, -1.9752,\n",
      "        -2.6820,  2.9637], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.6181, -2.7777, -2.7777,  2.7308,  0.1692, -2.7777, -2.5899, -2.0620,\n",
      "        -2.5466,  2.7308], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01968332566320896\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-1.4213, -0.3176, -0.8650, -1.8787], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.6337, -0.5525, -0.7086, -2.1388, -1.9696, -2.6417, -1.6639,  0.3868,\n",
      "         0.3226, -2.6337], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.5953, -0.6519, -0.3188, -1.6378, -2.0674, -2.7727, -1.6378,  0.1355,\n",
      "         0.1355, -2.5953], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05414187163114548\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-0.9387, -0.4418,  0.3109, -1.4729], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 2.9303, -2.6155, -2.8038, -0.8277, -1.7115,  4.1464, -0.8277, -1.7115,\n",
      "        -2.5930,  0.3207], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 2.7317, -2.6026, -2.7709, -0.8325, -1.6647,  4.9753, -0.8325, -1.6647,\n",
      "        -2.5558,  0.1222], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07729555666446686\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.5187,  0.3187,  1.2535, -0.9429], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.9336, -1.7403, -1.2096,  1.6172, -1.9641, -2.5508, -0.1366, -0.4702,\n",
      "         0.3187,  0.2628], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.7634, -1.6881, -1.3244,  1.6206, -2.0886, -2.5593, -0.3162, -0.7634,\n",
      "         0.1282,  0.1282], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.023316020146012306\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-1.7734, -2.6967,  2.9005, -0.1336,  1.2777, -0.3807, -1.7964, -1.7494,\n",
      "         9.2040,  0.7563], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7070, -2.7739,  2.8133, -0.3193,  1.6105, -0.7875, -1.7745, -1.7070,\n",
      "        10.0000,  1.6105], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16942784190177917\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4172,  0.3186,  1.3250, -0.7793], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 1.3250, -0.5124,  0.3186, -2.4996, -2.0675, -2.4569,  0.3186, -2.4996,\n",
      "        -2.4569, -2.4993], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.5726, -0.7811,  0.1925, -2.5711, -1.7793, -2.5711,  0.1925, -2.5711,\n",
      "        -2.5711, -2.7764], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.036152057349681854\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.1046,  1.5511,  2.8214, -0.1761], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.3228,  0.3228, -2.7086, -1.2882,  0.3228, -1.7468, -0.0952,  1.3981,\n",
      "        -0.3438, -1.6990], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.2583,  0.2583, -2.7735, -1.3885,  0.2583, -1.7262, -0.2926,  1.5392,\n",
      "        -0.7579, -1.7262], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.025833735242486\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.6925, 1.0687, 2.9959, 4.4023], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.7339,  2.7902,  1.4651, -1.7491,  2.9959, -2.4548,  1.5232, -0.0843,\n",
      "        -2.4146, -1.7339], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7325,  2.9621,  1.5112, -1.7325,  2.9621, -2.6296,  1.5112, -0.2827,\n",
      "        -2.5742, -1.7325], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.012862672097980976\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-2.5621,  0.3559, -0.8215,  0.2775,  0.0931, -1.2057, -1.9628,  0.8082,\n",
      "         6.7754,  1.5230], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.7665,  0.3707, -0.2726,  0.3707,  0.3707, -1.4190, -2.1377,  1.4865,\n",
      "         7.3996,  1.4865], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13561543822288513\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.4102, 2.4700, 7.0511, 9.3323], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 3.0101,  0.3538,  2.7050,  6.7867,  0.3196, -1.2240,  0.3196, -0.5303,\n",
      "        -2.5792, -2.5792], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 2.9955,  0.3748,  2.9955,  7.3990,  0.3748, -1.4158,  0.3748, -0.6815,\n",
      "        -2.7580, -2.7580], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.058970678597688675\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.7077, -1.9463, -2.6168, -2.6328], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.0579, -1.2385, -0.5279,  0.3216, -2.5247, -0.8334,  4.4246, -2.0579,\n",
      "        -0.5279, -2.1492], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.8055, -1.4052, -0.6927,  0.3722, -2.6157, -0.6927,  5.1124, -1.8055,\n",
      "        -0.6927, -2.1146], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07144485414028168\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-2.2058, -1.2387, -2.1170, -2.6822], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.6910, -0.6427, -1.9450, -2.6682, -0.7611, -0.8185, -0.1232,  1.4649,\n",
      "        -2.5173, -2.5522], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.6850, -0.6962, -2.1148, -2.7505, -0.2262, -0.6843, -0.2262,  1.3668,\n",
      "        -2.5327, -2.6129], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.036675576120615005\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-1.2973, -0.4287, -1.0417, -1.7114], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.6866, -2.4755, -2.7180,  0.3099,  4.4545,  2.5996, -2.4755, -1.6505,\n",
      "         9.2760,  0.3099], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.6504, -2.5179, -2.7479,  0.3848,  5.1097,  3.0091, -2.5179, -1.6504,\n",
      "        10.0000,  0.3848], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11380401998758316\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-0.7702, -0.5228,  0.3786, -1.3326], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.4599, -1.6083, -1.2121, -0.2116, -0.7739, -0.1564,  0.3156, -2.5993,\n",
      "         0.3156, -0.5228], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.4992, -1.6143, -1.3685, -0.2210, -0.6581, -0.2210,  0.3976, -2.5910,\n",
      "         0.3976, -0.6593], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.007584844715893269\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-2.4486,  2.5909, -1.6484,  6.8084, -2.7705, -1.9241, -2.4486,  0.3192,\n",
      "        -2.4486, -2.5179], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.4836,  3.0742, -1.5827,  7.3712, -2.7317, -2.0827, -2.4836,  0.4022,\n",
      "        -2.4836, -2.4836], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.059292398393154144\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-0.7181, -0.5050,  0.4145, -1.3433], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.2244, -1.2074,  0.4145, -2.4407, -0.5050,  0.3259, -2.4407, -1.5478,\n",
      "        -2.5234, -2.5395], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6270, -1.3322,  0.4073, -2.4690, -0.6270,  0.4073, -2.4690, -1.5548,\n",
      "        -2.4690, -2.5693], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02046675980091095\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.2572,  0.3260,  1.5574, -0.7767], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.5100, -2.5296, -2.6467, -0.1731, -0.4851, -2.8338,  0.1967, -0.6287,\n",
      "        -2.6330, -0.4851], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.5276, -2.4552, -2.5581, -0.1978, -0.5874, -2.7250,  0.4016, -1.3136,\n",
      "        -2.5581, -0.5874], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05637582018971443\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.2037,  1.4692,  2.6318, -0.1768], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.6534, -2.6562, -1.4791, -1.1802, -0.5568,  0.3202,  4.5524, -1.8348,\n",
      "        -0.5568, -2.4867], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.5487, -2.5487, -1.5011, -1.2789, -0.1879,  0.3849,  5.1378, -1.5011,\n",
      "        -0.1879, -2.4421], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07651197165250778\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.9265, 1.1718, 3.1715, 4.5882], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.4232, -2.8392, -2.6344,  1.4761, -2.8392, -1.4232, -0.4428,  0.3206,\n",
      "         0.5581,  1.4761], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4615, -2.7028, -2.5306,  1.3885, -2.7028, -1.4615, -0.4977,  0.3829,\n",
      "         0.3829,  1.3885], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.010382594540715218\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.7393, 1.8153, 5.1003, 6.8225], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.3729, -0.2337, -2.4781,  0.2389, -2.6868, -1.1241,  0.3195,  1.4774,\n",
      "        -1.1241, -2.8052], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4297, -0.4544, -2.4098,  0.3911, -2.6907, -1.2103,  0.3911,  1.4127,\n",
      "        -1.2103, -2.6907], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01170613057911396\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.4692, 2.4262, 7.0471, 9.2853], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.3255,  1.4762, -2.3283, -2.4877, -1.5562, -2.4434, -1.0987, -2.5557,\n",
      "        -2.7629, -2.3806], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4037,  1.4390, -2.4005, -2.5088, -1.4037, -2.4005, -1.1873, -2.5088,\n",
      "        -2.6820, -2.4005], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.005523861851543188\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.6450, -1.8578, -2.5030, -2.7167], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 3.2873, -0.4050, -0.5733, -1.2907, -2.5126, -1.5428,  1.4753, -2.4045,\n",
      "         2.7363,  6.8323], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.2295, -0.3593, -0.3593, -1.3829, -2.4990, -1.3829,  1.4627, -2.3885,\n",
      "         3.2295,  7.3597], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06073883920907974\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-2.1450, -1.1085, -1.9846, -2.7731], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 3.3067, -0.1714, -0.1628, -0.3918, -2.4753, -2.4753, -0.1628, -0.1714,\n",
      "        -2.4458,  0.3078], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.2408, -0.2143, -0.3221, -0.3221, -2.4882, -2.4882, -0.3221, -0.2143,\n",
      "        -2.4882,  0.3984], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.007396765053272247\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-1.0492, -0.1541, -0.7113, -1.5247], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.3362,  0.3091,  0.3091, -1.8355, -2.3031,  1.5560, -2.3031,  0.8683,\n",
      "        -1.5124,  1.4856], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.3612,  0.4004,  0.4004, -1.9932, -2.3612,  1.5274, -2.3612,  1.5274,\n",
      "        -1.3547,  1.5274], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05108597129583359\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-0.4649, -0.3407,  0.8332, -1.0149], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.3066, -2.3417, -1.2496, -1.8233,  0.3228, -1.4890, -1.0865, -2.6216,\n",
      "        -1.2496, -1.0149], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.3401, -2.4651, -1.3389, -1.9779,  0.3760, -1.3389, -1.1243, -2.6410,\n",
      "        -1.3389, -1.1243], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.009531358256936073\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4817,  0.3405,  1.4943, -1.0302], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.3453,  2.7953,  0.3405, -1.0109, -0.6888,  1.4943, -0.4542, -0.1870,\n",
      "        -2.3453, -1.4619], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.4476,  3.2453,  0.3449, -1.1053, -0.5997,  1.5157, -0.2413, -0.1878,\n",
      "        -2.4476, -1.3256], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03047581948339939\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.3994, 1.5255, 2.8004, 0.0370], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.7949, -2.4185,  0.3578, -0.4443, -1.2804, -2.2747,  0.3578, -1.2804,\n",
      "        -2.2747, -1.2804], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.9363, -2.4252,  0.3169, -0.2408, -1.3137, -2.2896,  0.3169, -1.3137,\n",
      "        -2.2896, -1.3137], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.006855932530015707\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.0158, 1.2205, 3.2656, 4.6637], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.2989, -2.2267, -2.1690, -2.4937, -1.2989, -1.2989,  0.3720,  9.1820,\n",
      "         4.6637, -2.4174], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3034, -2.2663, -2.2663, -2.4052, -1.3034, -1.3034,  0.2907, 10.0000,\n",
      "         5.1144, -2.4052], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08979451656341553\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.7812, 1.8409, 5.1083, 6.8060], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.1952, -2.6464, -2.4022,  0.0550,  0.3864, -1.0302, -0.3933, -2.4933,\n",
      "         1.5569, -0.0521], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.1402, -2.5931, -2.3851, -0.1402,  0.2834, -1.0469, -0.2308, -2.3851,\n",
      "         1.5377, -0.2308], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.012554384768009186\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([ 7.0158, -1.3153, -2.4787, -1.5256, -1.3153, -0.9704, -2.4538,  0.3936,\n",
      "        -0.2189,  1.5654], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 7.3010, -1.2867, -2.3731, -1.5917, -1.2867, -1.0357, -2.3731,  0.2860,\n",
      "        -0.2028,  1.5522], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.012123427353799343\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.8158, 1.8439, 5.1404, 6.8235], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 6.8235, -2.6160, -0.9585,  9.2355,  7.0556,  0.9265,  0.0534,  0.3941,\n",
      "        -1.3116, -1.5748], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 7.3119, -2.5701, -1.0273, 10.0000,  7.3119,  0.3018, -0.1087,  0.3018,\n",
      "        -1.2831, -1.2831], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14064407348632812\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.5762, 2.4585, 7.1313, 9.2741], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.2129, -1.3048, -0.6287, -0.9532,  0.9475, -1.3048,  9.2741, -0.0228,\n",
      "         1.5085, -1.3048], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.1473, -1.2880, -0.5764, -1.0205,  0.3576, -1.2880, 10.0000, -0.1473,\n",
      "         1.6104, -1.2880], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09131012111902237\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.4806, -1.7371, -2.3820, -2.5635], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 9.3493, -2.4175, -0.2329,  9.3493, -2.2502, -2.5635, -2.1179,  1.6030,\n",
      "        -2.5635, -1.6173], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([10.0000, -2.5634, -0.1469, 10.0000, -2.1971, -2.5634, -2.1971,  1.6489,\n",
      "        -2.5634, -1.5517], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08909008651971817\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.9819, -0.9426, -1.8387, -2.7080], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.3420,  2.9934,  4.8722, -0.2425, -1.9819, -2.2117,  1.7034, -2.3420,\n",
      "         0.0965, -2.3356], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.3827,  3.3850,  5.3020, -0.1366, -1.8483, -2.1894,  1.6941, -2.3827,\n",
      "        -0.0575, -2.3827], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03969553858041763\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.9064,  0.0070, -0.5390, -1.4739], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.7131, -1.7131,  0.1951, -0.2469, -1.2611, -1.2611,  1.0733, -1.2611,\n",
      "        -0.9823, -2.3164], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.8373, -1.8373, -0.0341, -0.1257, -1.2875, -1.2875,  1.7470, -1.2875,\n",
      "        -0.9937, -2.3803], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05582340806722641\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-0.2978, -0.2421,  0.9519, -1.0055], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.2100,  0.5065, -1.2980, -0.2421, -2.1751,  1.8560, -0.4592, -2.1751,\n",
      "         0.5065, -2.0852], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.1682,  0.6704, -1.2768, -0.1432, -2.3735,  1.7603, -0.5618, -2.3735,\n",
      "         0.6704, -2.1682], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01709730178117752\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.1131,  0.5582,  1.8775, -0.7789], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.4168,  0.0496,  0.5582, -1.2678,  3.0709, -2.4780, -2.0799, -0.8893,\n",
      "        -1.6820,  1.1805], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.5138,  0.0625,  0.6898, -1.2585,  3.5247, -2.5138, -2.1585, -0.9470,\n",
      "        -1.7819,  1.7638], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05938958376646042\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5946, 1.6901, 3.0628, 0.0624], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.5045,  9.8481, -2.1076, -2.5045, -2.5045,  0.0738, -1.6584, -0.7819,\n",
      "        -0.8246,  7.2193], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.4926, 10.0000, -2.1454, -2.4926, -2.4926,  0.1290, -1.7421, -0.5126,\n",
      "        -0.9128,  7.8633], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05300026014447212\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2553, 1.3763, 3.5838, 5.0470], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.1653, -0.2381, -2.6386, -2.1279, -2.1279, -1.9082, -2.1517, -0.2381,\n",
      "         7.2639,  9.9084], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.2511,  0.1887, -2.4785, -2.1315, -2.1315, -1.7086, -2.1315,  0.1887,\n",
      "         7.9175, 10.0000], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08733581751585007\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.0760, 2.0324, 5.5241, 7.3148], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.1419,  3.5944, -1.3874, -1.6281,  0.7194, -0.1957,  0.8125, -2.1531,\n",
      "        -0.7440,  0.8125], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.2688,  3.5633, -1.3508, -1.6696,  0.7399,  0.2181,  0.7399, -2.1132,\n",
      "        -0.8527,  0.7399], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.021571099758148193\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 6.9601,  2.7474,  7.6477, 10.0103], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 7.3618, -2.1485,  1.9689, -1.2439,  1.9689, -0.1253, -2.3034, -0.1253,\n",
      "         0.7624, -0.1449], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 8.0092, -2.0951,  1.7594, -1.1304,  1.7594, -0.2866, -2.2197, -0.2866,\n",
      "         0.7720,  0.2347], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07258833199739456\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.4620, -1.6088, -2.3668, -2.4833], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.1337, -2.1175, -2.1748,  0.2183,  5.1193, -1.6088, -2.1337, -0.1192,\n",
      "        -0.0926, 10.0473], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.0826, -2.0826, -2.0826, -0.2902,  5.6698, -1.6106, -2.0826, -0.2902,\n",
      "         0.2373, 10.0000], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07169225066900253\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.7279, -0.6586, -1.5786, -2.3844], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.1212,  0.7913,  1.7823, -1.6064, -1.6064, -1.2870, -1.1959,  2.0359,\n",
      "         5.1737, -2.2054], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.0354,  0.8323,  1.8083, -1.5927, -1.5927, -1.0354, -1.0354,  1.8083,\n",
      "         5.7078, -2.1780], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.043698184192180634\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.8019,  0.2175, -0.3957, -1.3195], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 5.6660, -2.0507,  0.9066, -2.0697, -1.1886,  7.4879, -1.0481, -1.0481,\n",
      "        10.1154,  2.0727], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 5.7391, -2.0698,  0.8654, -2.0698, -1.0083,  8.1039, -1.0083, -1.0083,\n",
      "        10.0000,  1.8460], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.048721421509981155\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-0.2696, -0.0928,  0.9545, -0.9073], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.6245,  7.5170, -2.2368,  0.9545,  5.3038,  0.2154, -1.3149,  3.8050,\n",
      "        -2.0664, -0.7801], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.5684,  8.1135, -2.4450,  0.8856,  5.7653, -0.1409, -1.2648,  3.7734,\n",
      "        -2.1469, -0.8062], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07576890289783478\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.1651,  0.7858,  2.1338, -0.3171], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.1651, -2.0156, 10.1256, -1.9685, -1.7882, -1.1668, -2.0156, -1.9760,\n",
      "        -2.3822, -0.6206], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.0706, -2.0501, 10.0000, -2.0501, -2.0501, -0.9756, -2.0501, -2.0501,\n",
      "        -2.4382, -0.8181], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02331322245299816\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7453, 1.7800, 3.2886, 0.2803], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.8711, -1.9721, -1.3616, -0.8711, -0.0520, -0.6111,  1.7800,  1.1123,\n",
      "        -1.0694, -0.8711], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-9.7064e-01, -2.0274e+00, -1.2495e+00, -9.7064e-01,  1.0836e-03,\n",
      "        -8.2173e-01,  1.9598e+00,  9.3174e-01, -9.7064e-01, -9.7064e-01],\n",
      "       grad_fn=<AddBackward0>)\n",
      "LOSS: 0.016717443242669106\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.4870, 1.4654, 3.9497, 5.4237], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.9027, -1.9027, -2.2849, -1.4019,  0.0391, -0.9726, -0.7041, -1.5427,\n",
      "         0.7850,  0.7850], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.0075, -2.0075, -2.4176, -1.2437,  0.1887, -0.9648, -0.8184, -1.5431,\n",
      "         0.9404,  0.9404], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01484721153974533\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.2100, 2.0762, 5.8187, 7.5687], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.0252, -0.5893,  0.8016,  0.8016, -1.3210,  2.1418,  3.3210,  0.8016,\n",
      "         0.3004,  7.5687], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.1063, -0.8067,  0.9276,  0.9276, -1.2349,  1.9889,  3.8956,  0.9276,\n",
      "         0.1880,  8.0704], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07268434017896652\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 6.9387,  2.7486,  7.8459, 10.0368], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 3.3486,  0.0713, -0.8767, -1.9231,  1.1760, -2.0610,  5.4290, -2.1329,\n",
      "         2.1103,  5.4290], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.8861,  0.0584, -0.9324, -1.9681,  0.8993, -2.0864,  5.8055, -2.0864,\n",
      "         2.0138,  5.8055], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06663762032985687\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.4150, -1.5181, -2.2091, -2.3509], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 9.9860, -2.0969, -1.9394,  0.0379, -0.8804, -1.9394,  1.5250,  1.1716,\n",
      "         0.8577,  9.9860], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([10.0000, -2.0684, -1.9430,  0.0544, -0.9144, -1.9430,  3.8767,  0.8904,\n",
      "         0.8904, 10.0000], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5613032579421997\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.5682, -0.5277, -1.3171, -2.1184], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.7473, -2.4062,  0.0813,  0.3052, -1.4848,  2.0746,  0.3052, -2.1620,\n",
      "        -1.5682, -1.9801], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.8671, -2.3363,  0.0057,  0.0057, -1.4749,  2.0889,  0.0057, -2.0362,\n",
      "        -1.4749, -1.9068], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02345229499042034\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-1.9372, -2.1882,  0.2508, -1.9953, -2.4276,  9.8016, -2.1882,  2.0029,\n",
      "        -2.1314,  0.3888], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.8849, -2.0185,  0.2447, -1.8849, -2.3189, 10.0000, -2.0185,  2.1266,\n",
      "        -2.3189,  0.2447], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01950329914689064\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-2.1687,  7.3749, -0.2550, -1.9275, -1.9866,  2.0642, -0.0396,  1.0109,\n",
      "         0.1632,  9.7407], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.0058,  7.7666, -0.3496, -1.8700, -1.8700,  2.1572,  0.0307,  0.8578,\n",
      "         0.0307, 10.0000], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0327720120549202\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.0296,  1.0396,  2.0843, -0.5707], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.5926,  1.0396,  1.0396, -0.5108,  9.7003,  0.1659, -0.2246,  1.0396,\n",
      "         2.0843,  2.0843], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4597,  0.8759,  0.8759, -0.7201, 10.0000,  0.2614,  0.0538,  0.8759,\n",
      "         2.1926,  2.1926], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.034179236739873886\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.8290, 2.1128, 3.5915, 0.2919], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 9.6862,  0.3087,  2.1437, -1.8791, -0.9026, -0.9026,  1.0560,  1.0560,\n",
      "        -0.5240,  0.3996], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([10.0000,  0.0921,  2.2323, -1.8526, -0.8475, -0.8475,  0.9294,  0.9294,\n",
      "        -0.7222,  0.2602], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.025084014981985092\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.4143, 1.9092, 4.0149, 5.1782], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 1.2848,  0.0938,  1.2848, -0.1889,  0.2112, -0.7619, -1.8937,  2.2276,\n",
      "        -2.3186, -0.8739], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.0048,  0.1563,  1.0048, -0.3315,  0.1563, -0.7293, -1.8471,  2.2791,\n",
      "        -2.2953, -0.8481], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.019110964611172676\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.2091, 2.6768, 5.9855, 7.3807], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.7461, -1.8637,  7.9731, -1.8136, -0.8463,  0.3577,  9.7043, -1.9507,\n",
      "        -1.8637,  7.3807], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.0988, -1.8495,  7.7339, -1.8495, -0.8518,  0.2498, 10.0000, -2.0037,\n",
      "        -1.8495,  7.7339], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04100131243467331\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.8983, 3.5390, 7.9802, 9.7259], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.9384, -0.8291, -0.7193,  1.3153, -1.8343, -1.9140, -0.8291,  0.1670,\n",
      "        -0.8006, -1.4507], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.9977, -0.8497, -0.7438,  1.1548, -1.8449, -1.9977, -0.8497,  0.2516,\n",
      "        -0.7438, -1.5341], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.005518927704542875\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2979, -1.4503, -2.0601, -2.2238], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.4222,  1.1249, -1.7766, -0.8164,  0.2832,  5.2003, -0.6074,  0.3891,\n",
      "         7.4278,  0.7931], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.2520,  1.2079, -1.8398, -0.8452,  0.1732,  5.6851, -0.7452,  0.2520,\n",
      "         7.7687,  1.2079], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.061384640634059906\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.5482, -0.6184, -1.3086, -2.0796], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 3.7394, -0.8172, -1.5482,  0.4025, -1.7760, -0.8172,  5.2076, -2.2127,\n",
      "         0.4025,  0.1822], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.6869, -0.8339, -1.5566,  0.2604, -1.8321, -0.8339,  5.6944, -2.2999,\n",
      "         0.2604,  0.1598], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02919589914381504\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.7684,  0.2962, -0.2743, -1.2869], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 1.2698, -1.5542,  1.1758, -0.8274, -1.7868,  1.1758,  2.4808, -0.6261,\n",
      "         2.2057, -1.7734], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.2328, -1.5635,  1.2328, -0.8197, -1.8214,  1.2328,  2.3602, -0.7334,\n",
      "         2.3602, -1.8214], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.006146897561848164\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-1.7823, -1.4315, -1.7951, -1.7823, -2.2147,  0.3086, -0.6345, -1.7823,\n",
      "         0.3133, -2.1032], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.8119, -1.5710, -1.8119, -1.8119, -2.2884,  0.1140, -0.7223, -1.8119,\n",
      "         0.2794, -2.2884], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01087837666273117\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-0.0945,  0.2114,  1.2248, -0.7576], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 1.4320, -0.8591,  5.2358, -1.1955, -1.7927, -0.8591,  2.1499,  0.2684,\n",
      "        -0.8940, -1.7927], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 2.3379, -0.7913,  5.6931, -1.2081, -1.8046, -0.7913,  3.7123,  0.2888,\n",
      "        -0.7913, -1.8046], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.34914258122444153\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.3838,  1.2838,  2.4301, -0.1841], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.7841,  0.3373, -1.8146, -0.8867, -0.7841, -2.1268,  1.1838, -0.2049,\n",
      "        -2.2929, -0.8867], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6964,  0.0654, -1.7822, -0.7657, -0.6964, -1.9242,  1.1871, -0.0835,\n",
      "        -2.2654, -0.7657], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.017620325088500977\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.9167, 2.3325, 3.5985, 0.2129], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 1.1855, -0.8849, -1.8252,  0.3412,  0.2935, -1.8395, -1.8884, -0.2048,\n",
      "        -2.0730, -0.8849], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.1740, -0.7490, -1.7664,  0.0669,  0.0669, -1.7664, -1.9141, -0.0518,\n",
      "        -1.9141, -0.7490], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.022181125357747078\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.4887, 2.3558, 3.9433, 5.1591], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 3.5764, -0.7558, -1.9030, -1.8205,  2.4344,  0.2859,  0.5737,  3.5764,\n",
      "        -1.8361, -2.2581], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.6432, -0.7089, -1.9190, -1.7607,  2.2188,  0.3918,  0.3918,  3.6432,\n",
      "        -1.7607, -2.2365], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.011184955015778542\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.2747, 3.2655, 5.8347, 7.3206], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.2460,  9.5958,  1.3601,  1.3601,  1.3601,  2.4395,  5.1437,  3.5664,\n",
      "         1.3601,  7.3206], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2283, 10.0000,  1.1955,  1.1955,  1.1955,  2.2098,  5.5885,  3.6294,\n",
      "         1.1955,  7.6362], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06262333691120148\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.9477, 4.2948, 7.7684, 9.5930], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 3.5889, -1.3612, -0.7726,  0.2494, -1.9250, -1.8036, -1.6877,  0.3769,\n",
      "        -1.9055,  7.3159], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.6383, -1.6208, -0.7302,  0.4349, -1.9264, -1.7575, -1.6208,  0.2106,\n",
      "        -1.9264,  7.6337], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.024175357073545456\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1943, -1.3721, -1.9775, -2.2150], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.4831,  9.5874,  7.7893, -1.9725, -1.7870,  0.2661, -0.7224,  0.3881,\n",
      "        -1.7870,  1.6116], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.7605, 10.0000,  7.6287, -1.9358, -1.7609,  0.2759, -0.7311,  0.2759,\n",
      "        -1.7609,  2.2582], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07065319269895554\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-0.7001,  7.3302, -1.7652,  1.3313, -1.3763, -0.6841,  1.0368,  0.4052,\n",
      "         5.1790, -1.8903], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.7608,  7.6463, -1.7578,  1.2787, -1.6301, -0.7205,  1.2787,  0.3258,\n",
      "         5.5972, -1.9406], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04144023731350899\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3696, -0.1397, -0.9520, -1.9035], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.3756,  2.3812,  0.3756,  5.2014, -0.6876, -0.6539, -0.6539,  1.3321,\n",
      "        -0.6539, -2.2020], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.5363,  2.2510,  0.5363,  5.6061, -0.7531, -0.7065, -0.7065,  1.2808,\n",
      "        -0.7065, -2.2494], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02497507631778717\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.5183,  1.0457,  0.5804, -0.9892], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 1.3264, -0.6246,  0.1817, -0.9892, -0.0281, -1.8704, -0.0539, -0.6246,\n",
      "        -0.6246,  0.4371], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.2938, -0.6964,  0.5723, -1.1157, -0.0588, -1.9466, -0.0588, -0.6964,\n",
      "        -0.6964,  0.4156], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.019229451194405556\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.5353,  1.3231,  2.5600, -0.0236], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 5.2652, -1.6946,  0.2257,  0.3485, -1.8402,  0.4785,  1.3231,  9.6374,\n",
      "         9.6374, -0.6103], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 5.6179, -1.7427,  0.5916,  0.5916, -1.9474,  0.5916,  1.3040, 10.0000,\n",
      "        10.0000, -0.6864], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06130428984761238\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.0068, 2.4006, 3.6567, 0.3288], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.4729,  7.3901,  9.6961,  2.4006, -1.9109,  0.3697,  3.6567,  3.6567,\n",
      "         1.3246,  1.3246], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.5066,  7.7265, 10.0000,  2.2911, -1.9347,  0.5914,  3.7981,  3.7981,\n",
      "         1.3279,  1.3279], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.030835652723908424\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.6678, 2.6410, 4.1144, 5.3926], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 1.7644,  0.4966,  2.4144, -1.3992,  1.7359, -1.6338, -1.6338, -0.8047,\n",
      "         0.4966, -0.5363], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 2.3303,  0.5623,  2.3303, -1.5796,  1.3586, -1.7242, -1.7242, -0.6388,\n",
      "         0.5623, -0.6388], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05651447921991348\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.3620, 3.5355, 5.8966, 7.4682], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 1.3471, -2.1442,  1.7374,  0.3459, -1.8997,  0.3459, -1.6159,  2.6742,\n",
      "         1.3471,  5.4402], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.3784, -2.2582,  1.3784,  0.5637, -1.9021,  0.5637, -1.7069,  3.8962,\n",
      "         1.3784,  5.7214], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1819269061088562\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.0739, 4.7068, 7.8263, 9.8198], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.6056, -0.5712, -0.5346,  0.4025, -0.7906,  0.5220, -1.6651,  0.5220,\n",
      "        -0.5712,  2.6413], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.6749, -0.6378, -0.5715,  0.4829, -0.5715,  0.4829, -1.6749,  0.4829,\n",
      "        -0.6378,  2.3228], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.017412293702363968\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2114, -1.3794, -1.9635, -2.1982], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.0681, -0.5532, -0.5532,  0.2784, -1.6357, -1.9755,  7.8105, -1.6277,\n",
      "        -1.7638,  1.4460], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2415, -0.5453, -0.5453,  0.4073, -1.6485, -2.2415,  7.8474, -1.6485,\n",
      "        -1.8480,  1.3640], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.013331755995750427\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.5272, -0.5156, -1.3802, -2.0979], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.5156,  9.8355,  0.6435,  0.5335,  0.6278,  0.1110, -1.3457, -1.6537,\n",
      "        -0.6918,  5.4561], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5370, 10.0000,  0.3346,  0.3346,  0.6954,  0.0302, -1.4641, -1.6226,\n",
      "        -0.5202,  5.7222], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.028878699988126755\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5428,  0.5582, -0.1754, -0.9904], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.8414, -1.8414, -1.8414,  1.4994,  1.4571,  0.1181, -2.2197,  5.4686,\n",
      "         0.6178, -1.3253], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.8193, -1.8193, -1.8193,  1.3472,  1.3472,  0.0455, -2.1928,  5.7243,\n",
      "         0.3114, -1.4477], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02169584296643734\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.1518,  0.5425,  1.4612, -0.3493], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.5425,  0.5944,  0.6516,  0.1518,  9.8653, -1.6629, -0.6451, -1.8455,\n",
      "        -1.6629,  0.5425], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.3151,  0.3151,  0.7236,  0.3151, 10.0000, -1.5806, -0.5034, -1.8134,\n",
      "        -1.5806,  0.3150], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02661142311990261\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5426, 1.4746, 2.6504, 0.2097], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.5664,  1.4746, -1.7361,  3.7477,  2.6504,  1.4746,  1.5210,  5.5132,\n",
      "        -1.8352, -1.6361], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5173,  1.3854, -1.5690,  3.9618,  2.3730,  1.3854,  1.3854,  5.7324,\n",
      "        -1.8239, -1.5690], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0240169744938612\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([ 2.6895,  1.4388, -2.2342,  1.4388, -0.2928, -1.9345,  0.5930,  0.5067,\n",
      "         0.1720,  2.6895], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 2.4226,  1.4205, -2.1955,  1.4205, -0.4663, -1.8274,  0.4207,  0.4207,\n",
      "         0.0402,  2.4226], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.024062324315309525\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.7242, 3.0369, 4.2529, 5.5572], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 1.3997,  0.7820, -1.8720,  7.4862, -0.5180, -2.2311,  0.8205, -0.6659,\n",
      "         2.6167, -1.8720], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.4275,  0.7090, -1.8388,  7.9337, -0.5616, -2.2023,  0.7090, -0.4759,\n",
      "         2.4822, -1.8388], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.027793843299150467\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.3013, 3.9121, 5.9713, 7.4901], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.6465, -1.4377, -1.6465,  0.0235, -0.4985,  1.6553,  0.4670, -1.6465,\n",
      "         1.3617,  7.4901], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.5844, -1.8501, -1.5844,  0.0283, -0.5841,  1.4355,  0.4898, -1.5844,\n",
      "         1.4355,  7.9392], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0444999523460865\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.0425, 5.1408, 8.0446, 9.9247], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.4618,  0.8527,  0.5476,  1.3373,  9.9247,  2.6212,  5.5806,  1.6666,\n",
      "         0.5476, -0.6553], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.5898,  0.7017,  0.4999,  1.4437, 10.0000,  2.5832,  5.7420,  1.4437,\n",
      "         0.4999, -0.6040], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.014051404781639576\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2910, -1.3545, -1.8235, -2.2415], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 5.5940, -0.4895,  4.0349, -0.5611,  1.3239,  2.7321,  5.5940, -1.8308,\n",
      "         4.0349,  9.9227], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 5.7469, -0.6223,  4.0346, -0.5198,  1.4589,  2.6314,  5.7469, -1.8222,\n",
      "         4.0346, 10.0000], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.010050401091575623\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.6110, -0.5628, -1.1620, -2.1542], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.3990, -1.6212,  0.3990, -0.4971, -1.4470, -1.8095,  1.8879,  1.3218,\n",
      "         7.4961,  1.3218], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.4912, -1.5921,  0.4912, -0.6345, -1.5921, -1.8054,  2.6678,  1.4556,\n",
      "         7.9299,  1.4556], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0890117958188057\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5887,  0.5358,  0.0255, -0.9796], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 2.6895,  1.6094,  0.8928,  2.6634, -2.2521, -1.3374, -0.5434,  1.3467,\n",
      "         0.4057, -2.3011], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 2.6445,  1.4206,  0.7403,  2.6445, -2.2036, -1.4891, -0.5178,  1.4206,\n",
      "         0.4484, -2.2036], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01041023526340723\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-0.0204,  0.4030,  1.5487, -0.4902], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.6226, -0.5625, -1.4877, -1.6387, -1.7784,  3.1615, -1.3296, -0.5625,\n",
      "        -0.0204, -1.4877], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4678, -0.6164, -1.5702, -1.5702, -1.7523,  4.0548, -1.4678, -0.6164,\n",
      "         0.3938, -1.5702], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10375847667455673\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.3071,  1.4174,  2.6010, -0.0476], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 7.4704,  2.6010,  0.0127,  9.8640, -1.6513, -2.3110, -1.5219, -0.6092,\n",
      "        -1.6035, -1.5219], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 7.8776,  2.5981,  0.1356, 10.0000, -1.5544, -2.1893, -1.5544, -0.6018,\n",
      "        -1.4363, -1.5544], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.025379741564393044\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.2167, 2.7656, 3.9735, 0.8600], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.6146,  5.5877, -2.2895, -0.7703,  3.9735, -0.4516, -1.5480, -1.6519,\n",
      "        -2.2895,  0.4272], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4858,  5.7184, -2.1817, -0.9729,  4.0290, -0.4858, -1.5390, -1.5390,\n",
      "        -2.1817,  0.2762], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.013778862543404102\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.7796, 3.3130, 4.2829, 5.5849], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 1.4781,  1.3039,  0.4295,  0.6196,  0.4295,  0.4693, -1.8936, -1.3083,\n",
      "        -1.7379,  1.4781], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.2791,  1.2791,  0.2391,  0.8873,  0.2391,  0.8873, -1.6829, -1.3851,\n",
      "        -1.6829,  1.2791], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04521263390779495\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.3457, 4.2272, 5.9326, 7.4622], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 5.5924, -0.6683, -0.6683,  0.6570, -1.7187,  3.9697, -0.6683, -1.3181,\n",
      "        -2.2476, -1.9260], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 5.7160, -0.5645, -0.5645,  0.8867, -1.6895,  4.0331, -0.5645, -1.3727,\n",
      "        -2.1863, -1.6895], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.016790572553873062\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.0641, 5.5051, 7.9098, 9.8044], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.5272, -1.8792,  0.8783,  1.4593,  1.4593,  9.8044,  0.3988,  5.6051,\n",
      "        -1.5272, -1.3225], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.5321, -1.7098,  0.8730,  1.3222,  1.3222, 10.0000,  0.2593,  5.7176,\n",
      "        -1.5321, -1.3682], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.013884373009204865\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([ 5.6275,  0.4244,  1.4309, -1.4930,  1.4440, -1.3237,  2.0663, -1.4930,\n",
      "        -2.1391, -0.6003], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 5.7262,  0.2996,  1.3605, -1.5403,  1.3605, -1.3657,  2.6258, -1.5403,\n",
      "        -2.1913, -0.5603], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03608802706003189\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3327, -0.3931, -1.0202, -1.9295], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 2.6489, -0.5567,  2.7631, -1.3230, -0.5567,  4.3715,  0.5579, -0.5567,\n",
      "        -2.0935,  0.4337], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 2.6323, -0.5524,  2.6323, -1.3538, -0.5524,  4.0813,  0.3300, -0.5524,\n",
      "        -2.1907,  0.3300], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.017471132799983025\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4769,  0.5395,  0.0194, -0.9224], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 1.5340, -0.4591,  4.3890, -1.7107,  0.1047,  2.6738,  0.5395, -0.5233,\n",
      "        -0.5233, -1.3289], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.4064, -0.5144,  4.1005, -1.7808,  0.3806,  2.6426,  0.3806, -0.5483,\n",
      "        -0.5483, -1.3442], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.021128010004758835\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.1459,  0.3482,  1.5716, -0.4211], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.5029, -1.3394, -0.5029, -1.4257,  4.3909, -1.2489,  0.9532,  2.6911,\n",
      "        -0.9223, -1.6823], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5454, -1.3351, -0.5454, -1.5510,  4.1208, -1.3351,  0.8902,  2.6448,\n",
      "        -1.3351, -1.7979], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.028964588418602943\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.4450,  1.3360,  2.6904, -0.0173], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.9484,  2.6904, -1.6549, -1.4737, -0.4083, -1.3416,  5.7036,  1.5895,\n",
      "        -1.6982, -0.5015], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.9050,  2.6370, -1.8092, -1.5498, -0.5713, -1.2985,  5.7546,  1.4213,\n",
      "        -1.8092, -0.5334], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0106984106823802\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.3368, 2.7247, 4.0309, 0.9194], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.4366,  1.3277, -1.4984, -0.5085, -0.6153,  1.3277,  3.4083, -1.4984,\n",
      "        -1.4491, -1.4491], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.4175,  1.4184, -1.5538, -0.5189, -0.5189,  1.4184,  4.1392, -1.5538,\n",
      "        -1.5538, -1.5538], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.058851540088653564\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.9164, 3.4537, 4.3177, 5.6946], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.4499,  2.1613,  0.9154,  1.3426,  0.4499, -1.5347,  4.0098,  7.4930,\n",
      "         9.8184, -0.5287], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.3937,  2.6088,  0.9452,  1.4006,  0.3937, -1.5461,  4.1251,  7.8365,\n",
      "        10.0000, -0.4997], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03760739043354988\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.4116, 4.3953, 5.8577, 7.4886], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 7.4886,  0.3720,  1.0331,  0.9130,  7.4886, -1.7926, -2.0711,  0.3720,\n",
      "        -1.6981,  0.5825], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 7.8256,  0.3771,  0.9882,  0.9882,  7.8256, -1.7928, -2.1734,  0.3771,\n",
      "        -1.7928,  0.9882], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.041886426508426666\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.0946, 5.7440, 7.7179, 9.7839], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 1.5128,  2.6238,  9.7839,  3.9508, -1.1179, -1.6966,  1.3883,  7.4880,\n",
      "         2.2410,  4.2473], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.3614,  2.5557, 10.0000,  4.0998, -1.1286, -1.7678,  1.3614,  7.8055,\n",
      "         2.5557,  4.0998], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.032403476536273956\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.0224, -1.2622, -1.7169, -2.1075], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.8998,  1.4188,  0.4139, -0.5528,  5.6555,  0.6717, -1.6307, -0.0896,\n",
      "        -0.8204, -1.2622], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.0606,  1.3396,  0.3327, -0.3955,  5.7473,  1.0606, -1.5062, -0.5847,\n",
      "        -0.7492, -1.0806], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05217994004487991\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-1.6449,  2.5881, -1.6449, -1.1191,  0.2481,  1.0248, -1.8182, -0.5281,\n",
      "        -1.8186, -2.1060], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4947,  2.5068, -1.4947, -1.0821,  0.3363,  1.0777, -1.7122, -0.3547,\n",
      "        -2.1155, -2.1155], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.019312726333737373\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.1115,  0.3147, -0.5978, -1.7368], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.2093, -1.6407, -1.8068,  0.7507,  0.4920,  3.8818, -1.7391,  1.4359,\n",
      "        -0.4930,  5.6455], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.1094, -1.4870, -1.6910,  1.0882,  0.3049,  4.0809, -1.6910,  1.3208,\n",
      "        -0.3244,  5.7594], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.029262736439704895\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.2142,  1.4973,  1.0566, -0.6259], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 1.4275, -2.0478,  9.7376, -0.7532,  7.5148,  3.8887,  7.5148, -1.5788,\n",
      "         7.5148,  4.1558], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.3325, -2.0640, 10.0000, -0.7145,  7.7638,  4.0798,  7.7638, -1.4792,\n",
      "         7.7638,  4.0798], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03178844600915909\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.3664,  1.4158,  2.6152, -0.2259], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 2.8185, -1.5894, -1.1590, -0.3643,  0.4332,  1.4158,  1.4158,  1.4158,\n",
      "        -0.3643, -0.3643], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 2.5186, -1.4701, -1.1730, -0.2678,  0.3511,  1.3536,  1.3536,  1.3536,\n",
      "        -0.2678, -0.2678], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01506207324564457\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.2943, 2.7939, 3.9496, 0.7790], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 7.5753, -1.5575, -1.7273, -1.8943, -1.7273,  1.3795, -1.2574, -1.1520,\n",
      "        -0.2378, -1.7273], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 7.7559, -1.4700, -1.6646, -2.0368, -1.6646,  1.3940, -1.2141, -1.2141,\n",
      "        -0.5851, -1.6646], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.019886497408151627\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.9125, 3.6404, 4.2056, 5.6575], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 2.7020, -1.9522, -1.3982, -0.4406,  1.0296,  1.3491,  1.0296,  9.7228,\n",
      "        -1.1542,  1.3491], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 2.5932, -2.0388, -1.4719, -0.2511,  1.0369,  1.4318,  1.0369, 10.0000,\n",
      "        -1.2671,  1.4318], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.016402238979935646\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.5008, 4.6157, 5.8810, 7.5901], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.1240, -0.1240, -1.6578,  2.7346, -1.4807, -1.7761,  2.7570, -1.4724,\n",
      "         1.6371, -1.4807], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.2484, -0.2484, -1.6682,  2.6326, -1.4682, -2.0437,  2.6326, -1.6682,\n",
      "         1.4611, -1.4682], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01982184872031212\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.0680, 5.8508, 7.6702, 9.7417], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 1.3147,  5.6750,  9.7417,  1.3147,  0.3821, -0.0828,  5.6750,  2.2371,\n",
      "        -0.0828, -1.1580], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.4804,  5.8488, 10.0000,  1.4804,  0.4858, -0.2417,  5.8488,  2.6657,\n",
      "        -0.2417, -1.3379], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04593288525938988\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.8867, -1.1483, -1.4762, -1.9268], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.9268, -0.4887,  1.3363,  9.7601, -1.3907, -1.9268,  4.2561,  1.3363,\n",
      "         1.6390,  0.3763], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.0335, -0.2132,  1.4730, 10.0000, -1.4399, -2.0335,  4.1272,  1.4730,\n",
      "         1.4730,  0.4751], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02499367855489254\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3928, -0.3655, -1.0069, -2.1443], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 1.3797,  4.0648, -1.1339,  4.2473, -1.3928, -1.5894,  7.6605, -1.6134,\n",
      "         0.3843, -1.3694], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.4619,  4.1492, -1.3289,  4.1492, -1.3289, -1.6377,  7.8119, -1.6377,\n",
      "         0.4391, -1.4033], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00955935101956129\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.7234,  0.5322, -0.1395, -1.3982], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.3982,  7.6874, -1.5988, -1.3992, -0.1112, -1.1303, -0.1112, -1.3349,\n",
      "        -1.0264,  1.5563], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3121,  7.8346, -1.6158, -1.3121, -0.1722, -1.3121, -0.1722, -1.3725,\n",
      "        -0.6615,  1.4505], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02231612801551819\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.0808,  0.4214,  1.5177, -0.6342], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 2.7213,  5.7591,  1.5177, -1.5850,  1.5513,  9.8385, -1.5891, -0.6686,\n",
      "        -0.6686, -1.1481], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 2.6484,  5.9429,  1.4492, -1.6017,  1.4492, 10.0000, -1.6017, -0.6599,\n",
      "        -0.6599, -1.3506], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01218880619853735\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([ 0.4191, -2.0261,  9.8659,  2.8402,  4.0561, -1.1532, -1.1532,  0.5720,\n",
      "        -1.5343,  1.4977], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.3326, -2.0379, 10.0000,  2.6505,  4.2044, -1.2581, -1.2581,  0.3326,\n",
      "        -1.5855,  1.4533], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01674862951040268\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.4603,  1.5202,  2.7512, -0.1043], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.3353,  0.4183,  0.5726,  9.8979, -1.1810, -1.1810,  1.4812, -2.0717,\n",
      "         2.3739, -1.2426], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3047,  0.3330,  0.3330, 10.0000, -1.2317, -1.2317,  1.4761, -2.0629,\n",
      "         2.6740, -1.3047], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01751812919974327\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.3619, 2.8469, 4.0997, 0.8606], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 1.5011, -0.3089,  4.0997,  2.8469,  0.5620, -0.6339,  1.2160,  9.9337,\n",
      "        -0.3229, -0.7103], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.5007, -0.1649,  4.2485,  2.6897,  0.3510, -0.6671,  1.1592, 10.0000,\n",
      "        -0.1649, -0.4942], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.019251760095357895\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.9927, 3.7923, 4.2789, 5.8558], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.1647,  4.1364,  9.9695, -1.5331,  2.8216,  1.2538,  7.8308, -1.2775,\n",
      "         1.5487,  5.8558], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.1873,  4.2702, 10.0000, -1.5635,  2.7228,  1.1717,  7.9725, -1.2757,\n",
      "         1.5394,  6.0477], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.009377766400575638\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.6251, 4.8146, 5.9831, 7.8551], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 1.5550, -1.1583, -1.7132, -0.1682,  1.5550, -0.9595,  3.7985, -0.1682,\n",
      "        -0.1865, -1.2745], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.5722, -1.2600, -1.5604, -0.2060,  1.5722, -1.1678,  4.2895, -0.2060,\n",
      "        -0.5394, -1.1678], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04576369374990463\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.1889,  6.0974,  7.7476, 10.0108], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.2797, -1.2188,  2.4488,  0.4727, -1.2797,  1.5707,  2.8850, -1.2188,\n",
      "        -1.2188, -0.1756], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.1580, -1.2447,  2.7906,  0.4044, -1.1580,  1.5965,  2.7906, -1.2447,\n",
      "        -1.2447, -0.5395], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.029515251517295837\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1255, -1.2654, -1.6877, -2.2317], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 1.5881, -0.1875, -1.2654,  7.8754, -0.1875, -0.1875, -1.6877,  7.8754,\n",
      "        -0.1916, -1.1393], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.6036, -0.5214, -1.1688,  8.0186, -0.5214, -0.5214, -1.5587,  8.0186,\n",
      "        -0.2214, -1.2282], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.041029900312423706\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.2836, -0.2530, -0.7959, -2.0601], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.2530, -1.2475,  1.6630, -0.2057,  0.4922,  1.3980, -0.4717, -0.2057,\n",
      "         2.5183, -1.1439], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4773, -1.2277,  1.6085, -0.2279,  0.4248,  1.6085, -0.6803, -0.2279,\n",
      "         2.8160, -1.2227], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.024191871285438538\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.6282,  0.6394,  0.0372, -1.3070], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 4.2341, -1.4161, -1.1605,  0.3683, -0.2294, 10.0200,  4.2341,  1.6078,\n",
      "         0.6394, 10.0200], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.2818, -1.2161, -1.2161,  0.4273, -0.2252, 10.0000,  4.2818,  1.5918,\n",
      "         0.4899, 10.0000], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.007454328238964081\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.1504,  0.4966,  1.6643, -0.6240], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.6154, -0.3774, -1.2205,  0.6825,  0.4966, -1.8288, -2.2380,  1.6126,\n",
      "        -1.1910,  4.2374], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.5906, -0.3857, -1.2235,  0.4979,  0.4979, -2.0869, -2.0869,  1.5836,\n",
      "        -1.2235,  4.2714], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.012724909000098705\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.5038,  1.6148,  2.8700, -0.1292], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.2202,  0.8741, -1.3682, -0.5767, -2.2108,  0.5104, -2.2108, -0.4357,\n",
      "        -1.6670,  7.8870], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.2330,  1.3756, -1.2330, -0.3592, -2.0599,  0.5207, -2.0599, -0.3592,\n",
      "        -1.6163,  8.0093], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.038627155125141144\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.4165, 2.8948, 4.2567, 0.8263], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.5421, -0.4887, -0.2151,  1.6095, -0.7072, -1.2406,  1.4349, -1.5182,\n",
      "         0.7321, -0.2151], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3411, -0.3411, -0.1947,  1.5874, -0.6474, -1.2479,  1.3792, -1.6365,\n",
      "         0.5490, -0.1947], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.011778870597481728\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.9916, 3.9266, 4.3112, 5.8421], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.5322, 10.0078, -0.2887,  1.4406,  1.6011,  4.3112, -2.0912,  0.5314,\n",
      "         1.6011,  2.6427], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3399, 10.0000, -0.1813,  1.3784,  1.5963,  4.2579, -2.0234,  0.5889,\n",
      "         1.5963,  2.8412], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.010262001305818558\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.6831, 5.0229, 6.0453, 7.9038], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.1218, -1.3488,  2.8857,  5.8427, -1.4457,  6.0453,  1.4428,  1.5993,\n",
      "        -1.9688, -2.0326], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.3877, -1.2612,  2.8380,  6.1134, -1.6492,  6.1134,  1.3909,  1.5971,\n",
      "        -2.0060, -2.0060], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03936683386564255\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.2283,  6.2982,  7.7627, 10.0146], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.1690,  7.7627, -1.4403, -0.1690, -0.1690,  1.6084, 10.0146,  1.8269,\n",
      "        -1.6786,  1.6084], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.1522,  8.0132, -1.6415, -0.1522, -0.1522,  1.6148, 10.0000,  1.6148,\n",
      "        -1.6415,  1.6148], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0150768356397748\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.9418, -1.1166, -1.4516, -1.9453], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.1584,  1.8392, -1.2446, -1.9453, -1.0462, -0.1584,  1.4431,  0.5670,\n",
      "        -0.1584, -1.2400], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.1396,  1.6397, -1.2635, -2.0050, -1.5425, -0.1396,  1.4081,  0.6553,\n",
      "        -0.1396, -1.2635], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.030075300484895706\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.6082, -0.5957, -1.1084, -2.3902], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.3852,  1.6495, -1.6251,  7.9049, -0.5957,  5.8660,  0.1170,  0.1170,\n",
      "        -1.2519, -0.1538], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3726,  1.6616, -1.6058,  8.0004, -0.3726,  6.1145,  0.2886,  0.2886,\n",
      "        -1.2616, -0.1239], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.018121737986803055\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5217,  0.6636,  0.1243, -1.1548], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.5714,  7.9021, -1.5974, -2.3536, -0.1453, -0.6523,  5.8777,  9.9950,\n",
      "        -0.1453,  1.6818], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.6401,  7.9955, -1.5870, -1.9973, -0.1117, -0.6286,  6.1119, 10.0000,\n",
      "        -0.1117,  1.6981], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01984121836721897\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.3376,  0.5734,  1.8209, -0.3157], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.9276,  0.5734,  9.9919, -0.2852, -0.5498,  7.9013,  7.9013, -1.9276,\n",
      "         1.3400, -0.1314], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.0107,  0.6388, 10.0000, -0.4311, -0.4311,  7.9927,  7.9927, -2.0107,\n",
      "         1.7337, -0.1022], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02261713519692421\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.8444, 1.7406, 3.0436, 0.4293], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 9.9836,  1.8048, -2.1888,  0.5856,  0.4258,  4.2625, -1.1302,  1.7406,\n",
      "         1.7406,  4.2625], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([10.0000,  1.7392, -2.0172,  0.6243,  0.1944,  4.3203, -1.4606,  1.7392,\n",
      "         1.7392,  4.3203], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02049153670668602\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.5612, 2.9006, 4.2607, 1.0386], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 5.9222,  0.3468,  5.9222, -1.1694,  0.6027, -0.1152,  0.6027, -0.1152,\n",
      "        -1.2110,  4.2607], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 6.1083,  0.6083,  6.1083, -1.4163,  0.6083, -0.0750,  0.6083, -0.0750,\n",
      "        -1.4163,  4.3300], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.024878785014152527\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.0949, 4.0368, 4.4138, 5.9361], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 7.8897, -1.4571,  1.7994, -1.2136, -0.2354, -2.0307,  4.2610,  1.7994,\n",
      "        -0.2354, -0.2354], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 7.9687, -1.5114,  1.7592, -1.2414, -0.4764, -2.0952,  4.3425,  1.7592,\n",
      "        -0.4764, -0.4764], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01982991024851799\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.7179, 5.0934, 6.0989, 7.8686], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 1.4261,  0.5693, -2.1130,  0.5693, -0.0996,  0.9612, -0.0996,  1.4261,\n",
      "        -2.1130, -1.4648], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.4564,  0.5290, -2.1323,  0.5290, -0.0504,  0.5290, -0.0504,  1.4564,\n",
      "        -2.1323, -1.4956], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01984466053545475\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.2260, 6.3647, 7.8200, 9.9118], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.6144,  4.4284, -1.2718,  7.8495, -2.1902,  1.6754, -1.2959,  1.8297,\n",
      "         1.8297,  9.9118], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.5079,  4.3298, -1.2333,  7.9206, -2.1663,  1.7628, -1.2459,  1.7628,\n",
      "         1.7628, 10.0000], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.005504783242940903\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2260, -1.3281, -1.8389, -2.2504], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.2504,  0.2297,  0.6200, -0.0594,  0.4232,  0.6200, -1.3070,  0.6200,\n",
      "         5.9225,  1.8255], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.1953,  0.5072,  0.5072, -0.0349,  0.2075,  0.5072, -1.2335,  0.5072,\n",
      "         6.0554,  1.7682], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.019168870523571968\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.1254, -0.1921, -0.6844, -1.7560], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.2804, -1.2804, -1.2804, -0.6844,  1.4589,  1.4589,  7.8310, -1.3645,\n",
      "        -1.4730,  0.9249], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.2405, -1.2405, -1.2405, -0.6393,  1.4660,  1.4660,  7.8970, -1.1729,\n",
      "        -1.4677,  0.5325], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.020197881385684013\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.6890,  0.5866, -0.0088, -1.4149], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 4.3376,  3.1025, -1.3861,  7.8252,  4.4938, -1.3861, -1.4644,  0.5866,\n",
      "        -2.3233,  0.5666], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.3444,  2.9039, -1.1505,  7.8883,  4.3444, -1.1505, -1.4639,  0.5861,\n",
      "        -2.2475,  0.5861], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.018296636641025543\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.2732,  0.5807,  1.8103, -0.4557], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 2.3415e-02,  3.3257e-04,  5.8067e-01, -1.2620e+00,  9.8689e+00,\n",
      "         2.3415e-02, -1.4512e+00, -2.3068e+00,  5.7225e-01,  2.3415e-02],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.0416, -0.0416,  0.6293, -1.2464, 10.0000, -0.0416, -1.4579, -2.2390,\n",
      "         0.6293, -0.0416], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0042134239338338375\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.8143, 1.7461, 3.0892, 0.4712], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.0384,  0.5971, -1.3624, -1.1465,  4.5261,  0.0384,  1.7461, -1.2718,\n",
      "         1.7461,  1.7461], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.0351,  0.6608, -1.1391, -1.2441,  4.3648, -0.0351,  1.7803, -1.2441,\n",
      "         1.7803,  1.7803], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.010454924777150154\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.5592, 2.9476, 4.3853, 1.0322], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.2490,  0.2899,  0.0345,  5.9731,  0.0345,  4.3853, -0.2584,  3.0669,\n",
      "         1.7384, -1.3311], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.1980,  0.6760, -0.0231,  6.0470, -0.0231,  4.3758, -0.0231,  2.9468,\n",
      "         1.7602, -1.1343], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.027289878576993942\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.1117, 4.0956, 4.5139, 5.9860], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.3177, -1.4188, -1.5358,  1.8652, -1.4188,  0.6444, -0.1493,  1.7299,\n",
      "        -1.4188, -1.2861], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.6787, -1.4234, -1.4234,  1.7264, -1.4234,  0.6787, -0.4874,  1.7264,\n",
      "        -1.4234, -1.1344], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.030081767588853836\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.6673, 5.0942, 6.1234, 7.8351], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.2404, -1.2360,  1.7241, -2.1343, -1.6807,  1.7241,  5.9981, -0.1735,\n",
      "        -1.4604,  4.3723], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.1561, -1.1877,  1.6927, -2.1164, -2.1164,  1.6927,  6.0515, -0.4770,\n",
      "        -1.1561,  4.3983], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03897593170404434\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.1773, 6.3517, 7.8086, 9.8808], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 4.3635,  7.8369,  6.0099,  1.7141, -2.0444,  0.6663, -0.0422, -1.1690,\n",
      "        -2.0444,  2.9583], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.4089,  7.8927,  6.0532,  1.6624, -2.0594,  0.6475, -0.0255, -1.1715,\n",
      "        -2.0594,  2.9271], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0011787217808887362\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.0107, -1.1220, -1.6696, -1.9664], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 1.7012,  9.8809,  2.9301, -0.2755,  0.6365,  9.8809,  2.7343, -1.1439,\n",
      "         1.7012,  7.8375], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.6371, 10.0000,  2.9245, -0.4271,  0.6359, 10.0000,  2.9245, -1.1579,\n",
      "         1.6371,  7.8928], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.009906860068440437\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.2082, -0.3249, -0.8378, -1.7592], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 9.8952, -1.2082,  6.0368, -1.4868,  1.0800,  7.8488,  1.8083,  0.6744,\n",
      "        -1.1193, -0.1615], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([10.0000, -1.2924,  6.0639, -1.4705,  1.4695,  7.9057,  1.6168,  0.6275,\n",
      "        -1.1453, -0.0280], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.023142660036683083\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5815,  0.6961,  0.0556, -1.2369], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.8243,  7.8633, -0.3653,  4.3498,  0.6729, -1.0222, -0.3653,  2.8954,\n",
      "         2.7372,  4.4715], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.9200,  7.9222, -0.3735,  4.4511,  0.6114, -1.3288, -0.3735,  2.9149,\n",
      "         2.9149,  4.4511], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.015315577387809753\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.4120,  0.6718,  1.7744, -0.2656], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.0819, -0.8879,  1.6813,  9.9263,  7.8743, -1.5174,  1.6813,  2.7432,\n",
      "         1.4355, -0.0819], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.5084e-03, -1.1054e+00,  1.5963e+00,  1.0000e+01,  7.9337e+00,\n",
      "        -1.4877e+00,  1.5963e+00,  2.9082e+00,  1.4689e+00,  3.5086e-03],\n",
      "       grad_fn=<AddBackward0>)\n",
      "LOSS: 0.011450066231191158\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7614, 1.6732, 2.8806, 0.4774], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 1.4424,  1.7649, -1.0297,  2.9804,  2.7546,  1.6732, -0.0895, -1.8742,\n",
      "        -0.0895,  2.8806], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.4792,  1.5926, -1.0812,  2.9014,  2.9014,  1.5926,  0.0046, -1.8741,\n",
      "         0.0046,  2.9014], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0086126197129488\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6485, 2.9700, 4.3284, 1.1581], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 4.3284, -1.5511,  6.0934, -0.9932,  1.7487,  2.8882,  4.3284,  0.7714,\n",
      "        -1.5511, -0.5617], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.4841, -1.5055,  6.1070, -0.7191,  1.5994,  2.8955,  4.4841,  0.5738,\n",
      "        -1.5055, -0.7191], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.021411973983049393\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.1825, 4.1187, 4.4758, 6.0967], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 1.6553,  1.6530, -0.4535,  1.1604,  0.7738, -1.4137, -0.9862,  7.9025,\n",
      "         1.6530, -1.1380], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.6286,  1.6286, -0.3036,  1.4987,  0.5798, -1.0570, -1.0570,  7.9622,\n",
      "         1.6286, -1.0570], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.031890545040369034\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.7123, 5.0662, 6.0963, 7.9151], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.5612,  0.6106, -1.4809,  1.6336,  0.6106, -1.8589,  1.6336,  1.5164,\n",
      "        -0.0434,  0.6106], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.5436,  0.6140, -1.5436,  1.6727,  0.6140, -1.8560,  1.6727,  1.4828,\n",
      "        -0.0300,  0.6140], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0008644155459478498\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.2371, 6.2958, 7.8243, 9.9828], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.9117, -0.0908, -0.2566,  1.8235, -1.3546, -0.9540,  4.4149, -0.9540,\n",
      "         0.7358,  0.5976], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.0817, -0.0465, -0.0465,  1.7084, -1.5503, -1.4370,  4.5080, -1.4370,\n",
      "         0.6411,  0.6411], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06126102805137634\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.8483, -1.0060, -1.4620, -1.7300], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.0583, -1.0702, -1.0060, -1.0060, -1.2588,  4.5517,  3.0569,  2.9241,\n",
      "         9.9894,  0.7177], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.0955, -1.4170, -1.4170, -1.4170, -1.4170,  4.5112,  3.0071,  3.0071,\n",
      "        10.0000,  0.6693], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.049798112362623215\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.1935, -0.3988, -0.7307, -1.7034], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.0038,  6.1217, -1.0885,  3.0939, -0.2615, -1.0038, -0.7307, -1.5204,\n",
      "        -1.8368,  7.9361], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.1078,  6.1425, -1.3589,  3.0360, -0.3768, -1.1078, -0.7826, -1.5518,\n",
      "        -1.9797,  7.9932], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.013916810043156147\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5453,  0.6776,  0.2021, -1.1891], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.0400,  1.6177, -1.0426, -0.3291, -1.0426,  1.5825,  1.6177,  1.8959,\n",
      "         0.6776, -0.0197], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.0581,  1.8065, -1.1195, -0.3901, -1.1195,  1.4500,  1.8065,  1.8065,\n",
      "         0.7063, -0.1270], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.012506936676800251\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.4081,  0.6004,  1.8835, -0.3056], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.6847, -1.6081,  0.8348, -1.0705,  7.9316,  3.1219, -2.0968,  1.6440,\n",
      "         0.6004,  3.1219], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.5355, -2.1209,  0.6951, -1.1251,  7.9860,  3.0678, -2.1209,  1.8097,\n",
      "         0.6951,  3.0678], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03534905984997749\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7942, 1.6726, 3.1056, 0.4976], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.7329,  9.9716, -1.2720,  1.8711, -0.7247, -1.4849,  1.5546, -0.0631,\n",
      "        -1.1029, -2.1900], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.5232, 10.0000, -1.1898,  1.7950, -0.7261, -1.5232,  1.4703, -0.1345,\n",
      "        -1.1898, -2.1448], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.008061565458774567\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6978, 3.0257, 4.5288, 1.2630], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.2825,  1.7007, -1.0370, -1.2015, -0.5779,  0.6869, -1.1149, -0.6771,\n",
      "        -1.7530,  1.8560], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.1544,  1.7848, -1.1544, -1.1272, -0.7023,  0.6704, -1.1272, -0.3818,\n",
      "        -1.5201,  1.7848], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0205183457583189\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.0880, 4.1092, 4.5323, 6.0487], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 1.7258,  1.8429,  0.6637, -0.1483,  4.5338,  1.5201,  2.7699, -0.1413,\n",
      "         3.0892, -2.2811], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.7803,  1.7803,  0.6586, -0.1354,  4.4438,  1.4929,  3.0804, -0.3831,\n",
      "         3.0804, -2.1526], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01874290034174919\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.6373, 5.1196, 6.1449, 7.9054], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 3.0743, -1.7293,  9.9480,  0.6960, -1.2757, -1.7293, -0.1226,  7.9054,\n",
      "        -1.2762,  1.7553], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.0649, -1.5518, 10.0000,  0.6343, -1.1090, -1.5518, -0.1313,  7.9532,\n",
      "        -1.1369,  1.7668], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.011931236833333969\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.1213, 6.3831, 7.8279, 9.9497], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 1.5084, -1.3070,  9.9497, -1.3070, -0.1345,  7.9086,  0.7000, -0.4881,\n",
      "         9.9497,  1.5084], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.5413, -1.1481, 10.0000, -1.1481, -0.1306,  7.9548,  0.6244, -0.3700,\n",
      "        10.0000,  1.5413], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.007954683154821396\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2766, -1.2644, -1.6793, -2.2643], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 1.7957, -0.0843, -1.7194,  4.5043, -0.6291, -1.7194, -1.2644, -1.1853,\n",
      "        -1.1504, -1.2644], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.7644, -0.1341, -1.5662,  4.4400, -0.6495, -1.5662, -1.0981, -1.0981,\n",
      "        -1.1570, -1.0981], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.011786622926592827\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.1975, -0.1201, -0.5273, -1.8025], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.0763,  0.3891,  1.8071,  2.8552,  0.6990, -0.1397,  1.4642,  6.0587,\n",
      "        -1.6357, -0.1201], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.1375,  0.3178,  1.7678,  3.0493,  0.6318, -0.1375,  1.7678,  6.1406,\n",
      "        -1.5720, -0.3848], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.022552674636244774\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.7339,  0.6894,  0.1339, -1.4542], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 3.0573,  0.6894, -2.1625, -1.2293, -1.2774,  9.9868, -0.1386,  1.8072,\n",
      "         4.1384, -2.1625], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.0324,  0.6265, -2.1064, -1.1248, -1.1732, 10.0000, -0.3795,  1.7516,\n",
      "         4.4636, -2.1064], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.019972702488303185\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.2602,  0.7091,  1.8020, -0.4474], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 4.4648,  0.2602, -1.6237,  6.0762, -1.2072, -0.1480, -1.6245, -1.6237,\n",
      "        -1.2593,  4.4648], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.4686,  0.6218, -1.5842,  6.1544, -1.1527, -0.1284, -1.5842, -1.5842,\n",
      "        -1.1783,  4.4686], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.015151428990066051\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6991, 1.8339, 3.0249, 0.4417], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 6.0858, -1.4074,  3.0249,  3.0249, -1.5140, -2.0372, -1.1327,  2.9319,\n",
      "         7.9547, -1.2286], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 6.1592, -1.1807,  3.0017,  3.0017, -1.5904, -2.0666, -1.1801,  3.0017,\n",
      "         7.9989, -1.1801], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.007601288612931967\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7024, 3.1350, 4.4269, 1.3227], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.5444,  6.0973,  1.8402, -1.1653, -1.9753, -1.5444,  0.7266, -0.6632,\n",
      "         1.5574, -1.1953], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.5926,  6.1639,  1.7043, -1.2138, -2.0488, -1.5926,  0.6053, -0.6769,\n",
      "         1.6600, -1.1806], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.006093182135373354\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.0927, 4.1802, 4.4654, 6.1087], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 3.1359, -0.1563, -1.9309,  6.1087, -0.2037,  2.9963, -0.1371, -0.2726,\n",
      "        -1.1171, -0.2726], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 2.9752, -0.1241, -2.0385,  6.1668, -0.1241,  2.9752, -0.1241, -0.3399,\n",
      "        -1.1834, -0.3399], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.006225568242371082\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.6637, 5.1982, 6.0556, 7.9646], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 1.8308,  6.1193,  1.8308,  0.3649,  2.9842, -1.2793,  1.6365,  0.7247,\n",
      "        -1.9000,  1.8308], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.6912,  6.1681,  1.6912,  0.6170,  2.9717, -1.2804,  1.6912,  0.6170,\n",
      "        -2.0317,  1.6912], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.015641948208212852\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.1842,  6.4528,  7.7226, 10.0141], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 2.9935, -0.1531, -1.0921,  1.6105,  1.3533, -0.2964,  0.7426, -0.3545,\n",
      "        -1.1484, -1.4763], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 2.9750, -0.1448, -1.1881,  1.6887,  1.6887, -0.3317,  0.6303, -0.3317,\n",
      "        -1.3191, -1.6161], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01913262903690338\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.9700, -1.1584, -1.3932, -1.8747], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.1490, -1.4656, -0.1490, -1.1093, -1.4983, 10.0209, -1.5109, -0.1432,\n",
      "        -1.1093,  1.7751], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.1633, -1.6282, -0.1633, -1.1937, -1.6282, 10.0000, -1.6282, -0.1633,\n",
      "        -1.1937,  1.6991], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.007837490178644657\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3742, -0.4140, -0.7779, -2.0158], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.8826, -1.8826,  1.6239,  1.7512,  0.9149,  0.6961,  3.0008,  0.7312,\n",
      "         0.7312,  3.0008], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.0457, -2.0457,  1.6558,  1.7007,  1.6558,  0.6596,  2.9920,  0.6596,\n",
      "         0.6596,  2.9920], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06174423545598984\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5165,  0.7265,  0.2571, -1.1995], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 1.4481,  0.7265,  6.1525,  0.7265, -1.9093,  1.7282, -0.5165,  0.7265,\n",
      "         1.7282, -1.9093], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.6131,  0.6839,  6.1672,  0.6839, -2.0456,  1.7054, -0.3461,  0.6839,\n",
      "         1.7054, -2.0456], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.010011059232056141\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.5492,  0.7073,  1.9067, -0.2061], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([10.0165, -1.4729, -0.6744,  7.9608, -1.4729, -1.9508,  0.5492,  7.9608,\n",
      "        -1.1551, -1.0833], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([10.0000, -1.6093, -0.7917,  8.0148, -1.6093, -2.0511,  0.7160,  8.0148,\n",
      "        -1.2019, -1.2019], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.011120581068098545\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.8335, 1.6851, 3.0150, 0.4565], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.0927, -1.3103,  1.6851,  6.1530,  1.9288, -0.0868,  1.9288,  0.7029,\n",
      "         1.5457,  1.6493], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.1979, -1.1979,  1.7135,  6.1590,  1.7135, -0.1329,  1.7135,  0.7359,\n",
      "         1.5201,  1.7135], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.012524734251201153\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.9214, 3.0461, 4.4619, 1.5354], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 3.0195, -2.1414, -1.9813,  0.0572, -0.4665,  0.6972,  0.0572, -1.1177,\n",
      "         1.9094,  1.9094], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.0157, -2.0509, -2.0509, -0.1175, -0.3726,  0.7185, -0.1175, -1.1992,\n",
      "         1.7175,  1.7175], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01636463962495327\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.2189, 4.1509, 4.5249, 6.1404], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.4512, -1.2621,  0.8785, -0.4625,  1.8593,  1.4541,  9.9879, -1.5727,\n",
      "         1.8593,  1.3596], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.5844, -1.1977,  0.6734, -0.3745,  1.7278,  1.4452, 10.0000, -1.5844,\n",
      "         1.7278,  1.4452], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.011395720764994621\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.7572, 5.1182, 6.0835, 7.9309], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 1.3998,  7.9309, -1.1645, -0.2559,  0.6907,  0.6907,  1.3649,  7.7598,\n",
      "         1.8116, -1.2939], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.4070,  7.9803, -1.1943, -0.0583,  0.6304,  0.6304,  1.4070,  7.9803,\n",
      "         1.7418, -1.1943], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.011484886519610882\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.2851, 6.3593, 7.7898, 9.9640], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.2319, -1.3142, -0.6243, -0.0580, -1.6163, -1.1852, -1.6163,  7.9211,\n",
      "         3.0422, -1.1880], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.0667, -1.1923, -0.7808, -0.0453, -1.5619, -1.4063, -1.5619,  7.9676,\n",
      "         3.0261, -1.1923], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.012409074231982231\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.0963, -1.2073, -1.5465, -2.2735], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 1.3327, -1.0915, -0.4040,  3.1165,  0.9210,  6.1245, -1.6276,  1.7283,\n",
      "        -1.2073,  4.4906], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.3308, -1.4005, -0.4014,  3.0415,  0.5938,  6.1242, -1.5660,  1.8049,\n",
      "        -1.4005,  4.5120], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0255668256431818\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3909, -0.4165, -0.8249, -2.1682], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.3218,  1.7291,  1.3073, -0.4179, -1.1943, -1.7275, -0.5456,  1.7761,\n",
      "        -1.6353,  1.7761], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.1927,  1.8208,  1.3021, -0.4068, -1.1927, -1.5647, -0.4068,  1.8208,\n",
      "        -1.5647,  1.8208], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00799442920833826\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5560,  0.6512,  0.1598, -1.2879], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.3556, -0.3933, -1.2564, -1.3080,  1.6037,  1.7282, -2.3556, -2.1330,\n",
      "         7.9087, -1.6343], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.1308, -0.4139, -1.3540, -1.1918,  1.2746,  1.8327, -2.1308, -2.1308,\n",
      "         7.9455, -1.5712], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.024916602298617363\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.4376,  0.6309,  1.8038, -0.3924], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.6309,  1.7308, -1.2028, -2.3532, -0.1757,  1.2826, -1.6169, -1.1794,\n",
      "         4.6219,  3.1596], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 6.2344e-01,  1.8437e+00, -1.1919e+00, -2.1512e+00,  1.5492e-03,\n",
      "         1.2669e+00, -1.5804e+00, -1.1919e+00,  4.5118e+00,  3.0865e+00],\n",
      "       grad_fn=<AddBackward0>)\n",
      "LOSS: 0.010431510396301746\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.8220, 1.7334, 3.1668, 0.4453], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.0205, -1.3334,  0.0205, -2.3310,  6.1320,  7.9099,  1.8187, -0.1336,\n",
      "        -1.5893, -1.2973], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 6.8382e-03, -1.3162e+00,  6.8382e-03, -2.1675e+00,  6.1189e+00,\n",
      "         7.9390e+00,  1.8501e+00,  6.8382e-03, -1.5891e+00, -1.3162e+00],\n",
      "       grad_fn=<AddBackward0>)\n",
      "LOSS: 0.004949191119521856\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.9103, 3.0319, 4.5668, 1.5860], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 3.0319,  1.8401,  0.6393, -0.3356,  9.9332, -2.2971, -1.6028,  9.9332,\n",
      "         0.6393, -1.1525], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.1101,  1.8594,  0.6561, -0.4247, 10.0000, -2.1822, -1.5989, 10.0000,\n",
      "         0.6561, -1.1967], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.003910399973392487\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.1753, 4.1239, 4.6587, 6.1545], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.3156,  4.6587,  1.7355, -2.2570,  2.4972,  1.2929,  1.7355,  1.1188,\n",
      "        -1.1700, -1.1391], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4210,  4.5390,  1.8654, -2.1910,  3.1202,  1.2474,  1.8654,  1.2474,\n",
      "        -1.1977, -1.1977], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.047456689178943634\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.6837, 5.0917, 6.2117, 7.9309], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 5.6837,  3.0472,  3.0472,  1.7614, -1.5023, -1.1353,  0.6650, -0.2851,\n",
      "        -1.1412, -1.1515], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 6.1378,  3.0913,  3.0913,  1.8435, -1.6031, -1.1862,  0.6464, -0.4015,\n",
      "        -1.1862, -1.1862], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0246709194034338\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.2179, 6.3411, 7.8624, 9.9385], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.7747,  1.7914,  9.9385, -0.1922,  0.4311,  3.1291,  1.2969, -0.2583,\n",
      "        -1.3110,  9.9385], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.6223,  1.8162, 10.0000,  0.0571,  0.6831,  3.0590,  1.3060, -0.3783,\n",
      "        -1.2325, 10.0000], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01825663633644581\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1908, -1.2935, -1.6060, -2.1531], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 4.4824,  0.0357,  9.9387,  7.9246,  3.0949, -0.5504, -1.2443,  0.0357,\n",
      "         0.7532, -0.2359], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.5423,  0.0743, 10.0000,  7.9448,  3.0341, -0.5609, -1.2124,  0.0743,\n",
      "         0.6127, -0.3510], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0048528509214520454\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.2456, -0.2209, -0.6356, -1.8399], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 9.9415,  1.8196,  7.9263, -0.5532, -2.1824, -1.1262, -1.0420,  1.3070,\n",
      "         3.0954,  0.0394], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([10.0000,  1.7559,  7.9473, -0.5148, -2.1512, -1.1227, -1.1227,  1.3538,\n",
      "         3.0198,  0.0893], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.002728267339989543\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4835,  0.7763,  0.2695, -1.0885], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.2091,  1.8047,  0.7221,  1.2226, -0.1179,  1.5259, -0.2091,  0.1540,\n",
      "         1.3210,  3.1037], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3013,  1.7352,  0.6243,  1.3715,  0.1004,  1.3715, -0.3013,  0.1004,\n",
      "         1.3715,  3.0126], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.013876358047127724\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.4312,  0.7288,  1.8121, -0.2915], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 1.8121, -0.9857, -1.0893, -2.0711, -1.5800, -1.2605, -1.2531, -2.0711,\n",
      "         2.6510,  1.8188], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.7213, -1.0829, -1.0829, -2.1345, -1.5804, -1.1874, -1.1874, -2.1345,\n",
      "         3.0071,  1.7213], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.017176523804664612\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6396, 1.8189, 3.0071, 0.3072], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.1547, -1.2620,  1.8888,  6.1577, -1.4708, -0.9661,  9.9486,  6.1577,\n",
      "         1.8189, -0.3450], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.1247, -1.1873,  1.7064,  6.1370, -1.5765, -1.0605, 10.0000,  6.1370,\n",
      "         1.7064, -0.4225], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.008203146047890186\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8068, 3.0884, 4.4193, 1.4308], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 6.1535,  7.9300,  4.5609, -1.2653, -2.0546,  0.0795,  0.8487,  4.5609,\n",
      "        -2.1455,  0.7292], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 6.1370,  7.9547,  4.5381, -1.1899, -2.1186,  0.1308,  0.6282,  4.5381,\n",
      "        -2.1186,  0.6282], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.007385297678411007\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.2922, 4.2154, 4.5633, 6.1520], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.0542, -1.2434, -0.9499, -1.2607, -1.5040,  0.8511,  6.1520,  0.0939,\n",
      "         0.0939,  0.0939], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.1191, -1.1981, -1.0313, -1.1981, -1.5845,  0.6516,  6.1388,  0.1313,\n",
      "         0.1313,  0.1313], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.006744795944541693\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.8776, 5.1803, 6.1460, 7.9332], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 1.7873,  7.9332, -0.9503,  6.1497,  3.0348,  6.1497,  0.7329, -0.2334,\n",
      "        -1.5295,  9.9542], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.7313,  7.9588, -1.0229,  6.1399,  2.9746,  6.1399,  0.6890, -0.2430,\n",
      "        -1.5933, 10.0000], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0021053755190223455\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.4414, 6.4271, 7.8350, 9.9580], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.5484, -2.0722, -1.2497, -2.0722,  1.7840, -2.0722,  4.4202,  1.3445,\n",
      "         7.9362, -2.0722], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.5986, -2.1247, -1.2215, -2.1247,  1.7436, -2.1247,  4.5331,  1.4914,\n",
      "         7.9622, -2.1247], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0050992402248084545\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1190, -1.2552, -1.5118, -2.0993], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.9663,  7.9374,  0.1473, -1.2887,  7.9374, -0.9663,  1.7538, -0.3122,\n",
      "        -0.0103,  3.0645], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.0093,  7.9633,  0.1148, -1.6025,  7.9633, -1.0093,  1.7581, -0.2594,\n",
      "         0.1148,  2.9885], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.012879553250968456\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.2509, -0.2618, -0.5966, -1.9379], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 1.4887,  0.1531,  4.5910,  4.4423,  1.9825, -0.5966, -0.2618,  3.0199,\n",
      "        -0.9827,  0.1531], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.4989,  0.1066,  4.5185,  4.5185,  1.7642, -0.3534, -0.2653,  2.9980,\n",
      "        -0.9952,  0.1066], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.012298425659537315\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4313,  0.8068,  0.3751, -1.1247], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 9.9530,  1.2182, -1.0003,  1.7305,  1.7305,  0.8068,  9.9530, -0.3365,\n",
      "         7.9344, -0.2601], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([10.0000,  1.5011, -0.9867,  1.7812,  1.7812,  0.8012, 10.0000, -0.2738,\n",
      "         7.9577, -0.2738], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.009446412324905396\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.5431,  0.7456,  2.0227, -0.3379], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.1686,  7.9385, -1.2244, -1.0092,  3.1054,  0.6037,  0.0283,  1.7287,\n",
      "         9.9572, -0.9636], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.1005,  7.9615, -1.2291, -0.9745,  3.0230,  0.8204,  0.1005,  1.7948,\n",
      "        10.0000, -0.9745], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.007168092764914036\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6270, 1.7365, 3.1147, 0.1326], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.2478,  0.1689, -1.2496,  9.9637, -1.6160, -1.0097,  9.9637,  2.0209,\n",
      "         0.5540,  2.0209], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.2831,  0.1037, -1.2230, 10.0000, -1.5466, -0.9581, 10.0000,  1.8032,\n",
      "         0.8188,  1.8032], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.018120501190423965\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8330, 3.0091, 4.4787, 1.3036], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 3.1334, -1.5123, -2.2149,  4.2125, -1.5313,  0.0617,  9.9749,  4.4787,\n",
      "        -0.3738,  3.1334], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.0308, -1.5344, -2.1221,  4.5065, -1.5344,  0.1063, 10.0000,  4.5065,\n",
      "        -0.2870,  3.0308], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.012753032147884369\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.3989, 4.2264, 4.5879, 6.1149], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 1.3144,  4.4854, -1.3062, -0.9978,  4.5879, -2.2194,  1.4825, -0.9491,\n",
      "         0.3011,  0.1505], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.4837,  4.5034, -1.5193, -0.9278,  4.5034, -2.1171,  1.4837, -0.9278,\n",
      "         0.1087,  0.1087], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.013614240102469921\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0546, 5.2330, 6.2025, 7.9685], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 6.1160, -0.9970,  9.9938, -0.9970, -0.2400,  0.7849,  1.7901,  4.5759,\n",
      "         6.1160,  1.9187], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 6.1717, -0.9179, 10.0000, -0.9179, -0.2936,  0.7268,  1.8234,  4.5044,\n",
      "         6.1717,  1.8234], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.004026440903544426\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.6563,  6.5086,  7.8721, 10.0073], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 3.1400,  4.4904,  1.6612, -0.9841,  6.1236,  0.7030,  0.7807, -1.5547,\n",
      "        10.0073,  1.8091], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.0413,  4.5112,  1.8260, -0.9084,  6.1805,  0.6969,  0.6969, -1.4739,\n",
      "        10.0000,  1.8260], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.006022621877491474\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.0674, -1.2162, -1.5564, -2.1813], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.5866,  3.1300,  0.1158, -0.9744, 10.0196,  0.7005,  0.7005, -1.5195,\n",
      "         0.7753, -0.4056], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.5059,  3.0448,  0.1336, -0.8996, 10.0000,  0.6729,  0.6729, -1.4534,\n",
      "         0.6729, -0.3023], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.004713102243840694\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.1219, -0.2466, -0.5768, -1.8854], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 1.8256, -1.2163,  7.9951, -1.2163, -1.5600, -2.1640,  7.9951, -0.9464,\n",
      "         0.6953, -2.1640], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.8116, -1.2219,  8.0279, -1.2219, -1.4397, -2.0947,  8.0279, -0.8967,\n",
      "         0.6627, -2.0947], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0030061888974159956\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.3927,  0.7442,  0.2744, -1.1357], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.6859,  1.8238,  4.5171, -0.3927, -2.1263,  0.7442, -1.0912,  1.4226,\n",
      "         0.7442, -1.5435], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.6593,  1.8118,  4.5381, -0.3302, -2.0912,  0.6593, -0.8971,  1.4436,\n",
      "         0.6593, -1.4368], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.007034367881715298\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6391,  0.6771,  1.8668, -0.2834], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.9688, -1.4098,  0.6771,  1.8136,  0.7156,  0.7156, -1.8385, -1.0562,\n",
      "         4.5402,  2.7006], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.2458, -1.4381,  0.6801,  1.8258,  0.6801,  0.6801, -2.0896, -0.9099,\n",
      "         4.5501,  3.0862], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03133826702833176\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.8806, 1.8143, 3.1445, 0.3423], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.0302,  4.5661,  2.7152, -0.9485,  1.8143, -0.9054,  8.0208, -1.1897,\n",
      "         1.5314,  6.1718], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.0708,  4.5546,  3.0897, -0.9217,  1.8300, -0.9217,  8.0542, -1.2530,\n",
      "         1.4436,  6.2187], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.015828697010874748\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.0518, 3.0782, 4.5279, 1.5266], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.9086,  6.1514, -1.1713, -1.9951,  1.8221,  0.6914,  1.8926, -0.9086,\n",
      "         6.1736, -0.9086], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9285,  6.2205, -1.2526, -2.0541,  1.8253,  0.7034,  1.8253, -0.9285,\n",
      "         6.2205, -0.9285], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0022939748596400023\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.5019, 4.2886, 4.5476, 6.1735], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 4.5127,  0.1325,  8.0213,  8.0213,  0.0716, 10.0599,  1.8935, -1.0768,\n",
      "         8.0213,  0.1325], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.5561,  0.1462,  8.0539,  8.0539,  0.1462, 10.0000,  1.8221, -0.9356,\n",
      "         8.0539,  0.1462], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.003965179435908794\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1708, 5.2989, 6.1493, 8.0183], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 3.1363, -0.2758, -0.9409,  3.1363, -1.3051, -0.2425, -0.9409, -1.3051,\n",
      "         0.6981, -1.9587], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.0506, -0.4029, -0.9461,  3.0506, -1.3894,  0.1549, -0.9461, -1.3894,\n",
      "         0.7015, -2.0365], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.020915161818265915\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.7863,  6.5745,  7.8026, 10.0488], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 1.5639,  0.6919,  8.0175,  0.6500,  3.1278,  1.8379, -0.0593, -1.1496,\n",
      "        -1.1562, -1.3012], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.5830,  0.6956,  8.0439,  0.6956,  3.0433,  1.8150, -0.4150, -1.2576,\n",
      "        -1.2576, -1.3874], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.016670528799295425\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.8939, -1.1484, -1.3699, -1.9617], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 1.8370, -0.9634,  4.4851,  0.6896,  0.1190,  4.4851,  1.8684,  4.5231,\n",
      "        -1.3130,  0.6896], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.7983, -0.9666,  4.5469,  0.6816,  0.1353,  4.5469,  1.7983,  4.5469,\n",
      "        -1.3840,  0.6816], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0020076902583241463\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.1232, -0.2781, -0.5400, -1.8982], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.1490,  0.3145,  6.1504,  0.3356, -0.9876,  1.8535, -1.9048, -0.9244,\n",
      "        10.0291, -0.9244], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.2503,  0.5289,  6.2051,  0.5289, -0.9748,  1.7898, -2.0341, -0.9748,\n",
      "        10.0000, -0.9748], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01234085951000452\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4449,  0.6742,  0.2176, -1.1813], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 7.9993, -0.2696,  3.0999, -1.1533,  3.0999, 10.0197,  0.6741,  0.1044,\n",
      "        10.0197,  7.9993], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 8.0178, -0.3933,  3.0377, -1.2427,  3.0377, 10.0000,  0.6509,  0.1279,\n",
      "        10.0000,  8.0178], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.003356724279001355\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.5885,  0.6629,  1.8152, -0.3102], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.1651, -0.4520, -1.1651, -2.0188,  1.8152, 10.0076, -1.3795, -2.0188,\n",
      "         3.0933, -1.1651], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.2379, -0.3837, -1.2379, -2.0486,  1.7840, 10.0000, -1.3941, -2.0486,\n",
      "         3.0402, -1.2379], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.002639675047248602\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.8285, 1.8339, 3.0856, 0.3469], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 1.8339,  1.8339,  4.4926,  0.0903,  0.6503,  1.5152,  0.6941, -1.0064,\n",
      "         4.4926, -1.1902], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.7771,  1.7771,  4.5104,  0.1224,  0.6151,  1.7343,  0.6151, -0.9857,\n",
      "         4.5104, -1.2240], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0065192170441150665\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.0296, 3.1116, 4.5068, 1.5403], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.0898,  0.6904,  0.6904,  0.3608,  0.6346, -2.1006,  1.8202,  3.0890,\n",
      "        -1.0072,  1.7493], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.1115,  0.6105,  0.6105,  0.3898,  0.6105, -2.0977,  1.7801,  3.0561,\n",
      "        -0.9945,  1.7401], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.001759708160534501\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.4690, 4.2998, 4.5118, 6.1154], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.0694,  1.8017, -1.2505,  7.9776,  1.5021,  0.6738, -0.4769,  7.9776,\n",
      "        -1.2505, -2.0554], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.0059,  1.7894, -1.2173,  7.9856,  1.7894,  0.6210, -0.7161,  7.9856,\n",
      "        -1.2173, -2.1254], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.015393746085464954\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1503, 5.3139, 6.1365, 7.9752], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.2667,  4.5369,  0.8071,  3.0906,  1.7898,  1.4936,  0.6652,  7.9752,\n",
      "         0.6652, -1.8940], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.2110,  4.5003,  0.6273,  3.0832,  1.7816,  1.7816,  0.6273,  7.9810,\n",
      "         0.6273, -2.1401], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.018328208476305008\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.7175, 6.5723, 7.7792, 9.9708], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 4.5110,  1.7737, -1.0097,  1.8239, -1.4621,  1.8239,  4.5416, -0.0140,\n",
      "         9.9708, -1.1797], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.4933,  1.7532, -1.0126,  1.7532, -1.4261,  1.7532,  4.4933,  0.0787,\n",
      "        10.0000, -1.2127], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0024919421412050724\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1256, -1.2527, -1.4589, -2.1782], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.3160,  9.9668,  3.0358,  4.5438,  4.5438, -1.9586, -1.2527, -0.1607,\n",
      "         0.6479,  9.9668], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4133, 10.0000,  3.0894,  4.4898,  4.4898, -2.1274, -1.2156,  0.0686,\n",
      "         0.6458, 10.0000], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01028277724981308\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.2602, -0.2514, -0.5404, -2.0124], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.1779,  0.0548,  6.0993, -2.0124,  0.2005,  0.6514,  0.0548,  1.7530,\n",
      "        -0.3329, -1.4416], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.1116,  0.0502,  6.1717, -2.1116,  0.3888,  0.6473,  0.0502,  1.7175,\n",
      "        -0.4162, -1.4232], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.006350323557853699\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5720,  0.6558,  0.2203, -1.2553], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.6466,  7.9665,  6.0990,  4.5471,  0.2714,  4.5471,  1.7380, -0.2674,\n",
      "        -0.2674, -1.7811], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.6475,  7.9685,  6.1699,  4.4891,  0.3872,  4.4891,  1.7071, -0.4097,\n",
      "        -0.4097, -2.0927], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.016372615471482277\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.4322,  0.6413,  1.8260, -0.3873], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.9913,  1.7273, -0.2954,  9.9669,  1.7273,  9.9669,  1.5310, -2.0153,\n",
      "         6.1049, -1.5650], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.0026,  1.6991, -0.3945, 10.0000,  1.6991, 10.0000,  1.6991, -1.4397,\n",
      "         6.1703, -1.4397], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.039326559752225876\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5664, 1.7095, 2.9882, 0.2759], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.6293,  4.5389,  0.6866,  0.6866,  7.9749,  0.4564,  1.7095,  1.6187,\n",
      "        -1.1847, -1.1579], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.6415,  4.5104,  0.6415,  0.6415,  7.9809,  0.3771,  1.6894,  1.7689,\n",
      "        -1.0070, -1.2999], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.008604144677519798\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.9526, 3.1149, 4.5379, 1.6578], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.3682,  2.9835,  6.1455, -1.1500,  0.0440,  1.6900, -0.9835,  1.8620,\n",
      "        -0.3682,  4.5379], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3803,  3.0841,  6.1884, -1.3314, -0.0096,  1.6851, -1.0224,  1.7565,\n",
      "        -0.3803,  4.5309], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00607619434595108\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.3861, 4.2975, 4.5369, 6.1667], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.2519,  3.1084, -2.1125, -1.1392,  0.6095, -1.1518,  0.4934,  6.1667,\n",
      "        -0.6853,  1.6779], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3508,  3.0780, -2.0366, -1.3508,  0.6510, -1.3508,  0.3797,  6.1976,\n",
      "        -0.5559,  1.6830], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01331766415387392\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0339, 5.2815, 6.1230, 8.0045], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.0923, -0.3874, -0.9905, -0.9905,  1.8359,  1.8565,  3.0498,  1.0972,\n",
      "        -2.0349, -0.6665], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.0496, -0.3754, -1.0496, -1.0496,  1.6807,  1.7448,  3.0733,  1.7448,\n",
      "        -2.0481, -0.5565], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04778432473540306\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.5915,  6.5278,  7.7585, 10.0287], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 1.6708,  6.1990,  2.9770, -2.0283,  0.7020,  4.5141, -1.0986,  6.0251,\n",
      "         0.7401,  1.8257], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.6793,  6.2086,  3.0627, -2.0488,  0.6432,  4.5791, -1.0551,  6.2086,\n",
      "         0.4023,  1.6793], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.018675444647669792\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1492, -1.1643, -1.3079, -2.0259], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 1.6877, 10.0290, -1.1643, 10.0290, -0.3594, -0.5972,  4.5435,  0.1416,\n",
      "        -2.0259,  0.7103], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.6887, 10.0000, -1.3235, 10.0000, -0.3607, -0.3607,  4.5857,  0.0587,\n",
      "        -2.0478,  0.6319], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.009826844558119774\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.5376, -0.3449, -0.6404, -2.1982], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.1752,  1.7411,  0.5938,  6.2112, -0.0707,  1.7001, -1.7528,  1.3282,\n",
      "        -0.0707,  4.2941], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3104,  1.6661,  0.6281,  6.2037,  0.0900,  1.7007, -1.6413,  1.6661,\n",
      "         0.0900,  4.5901], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02910243533551693\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.6110,  0.6912,  0.2929, -1.1500], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.1122, -1.5262,  1.8123, -1.1870, -1.0812,  1.7151, -0.0581, -0.9950,\n",
      "        -0.0581,  3.0196], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.0966, -1.2936,  1.7177, -1.2936, -1.0523,  1.7177,  0.0966, -1.0523,\n",
      "         0.0966,  3.0489], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.012753416784107685\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.3290,  0.5904,  1.8100, -0.3381], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.9685,  4.4964,  0.6780,  0.6780,  7.7509,  3.0417,  0.4996,  3.0919,\n",
      "        -1.7506,  1.7283], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.0299,  4.5893,  0.6290,  0.6290,  8.0077,  3.0467,  0.5834,  3.0467,\n",
      "        -1.6502,  1.7375], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.010235450230538845\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6017, 1.7370, 3.0758, 0.4776], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.0122,  0.6017,  4.5119, -1.4640, -1.2205,  3.0758,  6.2038,  1.7370,\n",
      "         6.2038, -0.9394], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.0675,  0.6442,  4.5834, -1.2655, -1.2655,  3.0607,  6.1782,  1.7683,\n",
      "         6.1782, -1.0110], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.006236847955733538\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.9761, 3.0807, 4.5288, 1.8093], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 6.4179e-01,  1.6005e+00, -1.2383e+00, -2.1947e+00,  6.4179e-01,\n",
      "        -1.4196e+00,  9.5997e-03,  9.9773e+00,  1.7450e+00,  7.9636e+00],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.6556,  1.5133, -1.2542, -2.1144,  0.6556, -1.2542,  0.0504, 10.0000,\n",
      "         1.7967,  7.9796], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.004712389782071114\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.4450, 4.3057, 4.6150, 6.1890], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 3.1342,  1.8206,  0.5746,  6.1890,  0.5746, -0.9382,  1.8484,  2.7566,\n",
      "         3.1342,  4.6150], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.0879,  1.4809,  0.6636,  6.1588,  0.6636, -0.9719,  1.8208,  3.0879,\n",
      "         3.0879,  4.5701], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02500496432185173\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0824, 5.2811, 6.1742, 7.9397], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 9.9465,  0.3909, -0.5687, -1.2648,  1.7874, -0.9130,  1.1490, -1.1185,\n",
      "         1.1490,  7.9397], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([10.0000,  0.6535, -0.5451, -1.2235,  1.4903, -0.9482,  1.4903, -0.9482,\n",
      "         1.4903,  7.9519], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.042567409574985504\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.6903, 6.5536, 7.8319, 9.9262], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 4.6037,  1.8298,  0.6340,  3.1399,  6.1577, -1.2740, -0.5812,  1.9819,\n",
      "        -0.8987, -1.2628], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.5419,  1.8259,  0.6468,  3.0593,  6.1314, -1.1972, -0.4294,  1.8259,\n",
      "        -0.9272, -1.1972], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.006955851800739765\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1339, -1.2597, -1.3787, -2.1299], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.1002,  0.6366,  9.9091, -2.1299,  0.4443,  7.9109, -1.6969,  4.4940,\n",
      "         4.4940,  1.8249], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.0968,  0.6424, 10.0000, -2.1337,  0.6424,  7.9182, -1.6274,  4.5296,\n",
      "         4.5296,  1.8313], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.005505320616066456\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.1754, -0.1809, -0.3993, -1.9426], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 4.4815, -1.3114,  1.8213, -0.8701, -1.6770,  4.4815,  0.2317, -0.8701,\n",
      "         6.1367,  3.1510], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.5209, -1.6134,  1.8359, -0.8934, -1.6134,  4.5209,  0.1227, -0.8934,\n",
      "         6.1128,  3.0333], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01259422767907381\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5264,  0.6495,  0.2863, -1.2006], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 9.8892,  0.6260,  0.2398,  0.6495,  1.8135,  0.2398,  3.1464, -2.1275,\n",
      "         1.8135,  1.8135], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([10.0000,  0.6322,  0.1507,  0.6322,  1.8318,  0.1507,  3.0271, -2.1222,\n",
      "         1.8318,  1.8318], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.004376204218715429\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.5330,  0.6458,  1.8081, -0.3426], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.1259,  1.8500,  2.0649,  0.6610,  1.8081, -0.7713,  1.8182,  0.6610,\n",
      "        -1.1061,  1.4701], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.1127,  1.8191,  1.8191,  0.6273,  1.8191, -0.8788,  1.4930,  0.6273,\n",
      "        -1.1188,  1.4930], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.018196115270256996\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.8721, 1.8574, 3.1269, 0.5259], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 6.1038, -1.3070,  1.8574, -1.3070,  0.5560, -0.8864, -2.1306,  6.1038,\n",
      "        -1.8917,  0.1447], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 6.1001, -1.5266,  1.8142, -1.5266,  0.6262, -0.8698, -2.1075,  6.1001,\n",
      "        -2.1075,  0.2014], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.015384351834654808\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.9343, 3.0734, 4.4598, 1.5029], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.1271, -0.9222, -1.5490,  0.1446,  1.8694,  1.3586,  0.2082,  1.8694,\n",
      "         1.4476,  7.8140], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.0835, -0.8698, -1.4784,  0.2227,  1.8062,  1.5247,  0.2227,  1.8062,\n",
      "         1.5247,  7.8921], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.006355093326419592\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.4900, 4.3980, 4.5641, 6.0807], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 2.8101,  0.7001,  1.4422,  0.7001,  7.8786,  3.0727, -0.9414, -0.0849,\n",
      "         4.4625,  4.4625], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.0163,  0.6278,  1.5291,  0.6278,  7.8874,  3.0163, -0.8685, -0.3889,\n",
      "         4.4726,  4.4726], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01617337390780449\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1698, 5.3858, 6.1729, 7.8731], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 4.4635,  1.8654,  0.7054,  0.5052, -1.3638, -1.4711,  0.7054,  4.4635,\n",
      "         6.0705,  0.2079], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.4635,  1.8127,  0.6363,  0.8047, -1.4037, -1.4037,  0.6363,  4.4635,\n",
      "         6.0858,  0.2575], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.011085395701229572\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.7882, 6.6621, 7.8673, 9.8706], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 3.1506,  1.4333,  6.0695, -0.1107,  1.8378,  1.8574,  9.8706, -1.0019,\n",
      "         6.0695,  7.8735], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.0238,  1.5439,  6.0862, -0.3738,  1.8355,  1.8355, 10.0000, -0.8764,\n",
      "         6.0862,  7.8835], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.013116496615111828\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.9934, -1.1563, -1.3484, -2.0683], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 7.8821,  0.2256, -1.0028,  3.1701,  1.4774,  1.8433,  0.2256,  0.6935,\n",
      "         1.4774,  1.9214], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 7.8953,  0.2679, -0.8826,  3.0364,  1.5420,  1.8531,  0.2679,  0.6706,\n",
      "         1.5420,  1.8531], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.004969318397343159\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.1277, -0.1751, -0.4170, -1.9866], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 4.6191, -0.4640,  1.8786,  0.2448, -0.9953,  1.4503,  1.5020,  1.8764,\n",
      "        -1.4058,  6.0874], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.4787, -0.3600,  1.8717,  0.2650, -0.8904,  1.5350,  1.5350,  1.8717,\n",
      "        -1.3428,  6.1023], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.005445660557597876\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4364,  0.7093,  0.3409, -1.1363], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 7.9046,  0.2633, -1.3938,  1.8994,  7.9046,  3.2105, -1.1285,  0.6288,\n",
      "         1.8056,  0.2633], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 7.9233,  0.2568, -1.3356,  1.8895,  7.9233,  3.0737, -1.1895,  0.6492,\n",
      "         1.8895,  0.2568], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0034151121508330107\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.7066,  0.6559,  1.9124, -0.2056], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.1647,  0.1128,  0.2767,  0.7102, -0.2401,  4.5444,  1.9124,  3.2172,\n",
      "         0.2767,  6.1164], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.2161,  0.2516,  0.2516,  0.7211, -0.3609,  4.5047,  1.8955,  3.0900,\n",
      "         0.2516,  6.1245], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.005598762072622776\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.0490, 1.7688, 3.2142, 0.7165], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 7.9716,  9.9430, -0.9234,  9.9430,  7.9265,  1.7586,  1.9179,  6.1288,\n",
      "        -0.3896,  1.7688], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 7.9487, 10.0000, -0.8948, 10.0000,  7.9487,  1.8928,  1.8928,  6.1338,\n",
      "        -0.3552,  1.8928], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.004356385674327612\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.9995, 2.9953, 4.5689, 1.5381], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 4.5689,  3.1999,  1.7593,  1.3763, -0.8989,  1.7593,  9.9632,  4.6568,\n",
      "        -1.1740,  0.4872], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.5300,  3.1120,  1.8799,  1.4955, -0.8879,  1.8799, 10.0000,  4.5300,\n",
      "        -1.3146,  0.5683], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.009646167978644371\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.5457, 4.3565, 4.6468, 6.1612], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 4.6468,  0.2765, -0.3260,  1.9014,  0.2862,  7.9586, -0.8938,  6.1612,\n",
      "         3.1691,  7.9586], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.5450,  0.2439, -0.4536,  1.8522,  0.2439,  7.9869, -0.8766,  6.1627,\n",
      "         3.1103,  7.9869], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0037253189366310835\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2359, 5.3422, 6.2380, 7.9754], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.2051, -0.7365,  0.2632,  1.3880, -1.1528,  3.1356,  7.9754,  1.8807,\n",
      "        -0.8498,  1.0587], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.2875, -0.8648,  0.2492,  1.4914, -1.2740,  3.1064,  8.0056,  1.8221,\n",
      "        -0.8648,  0.6926], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.018827393651008606\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.8645,  6.6347,  7.9188, 10.0222], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 3.0943,  4.5619,  4.5619,  0.7761,  0.6635,  0.6635,  0.2548,  0.2548,\n",
      "        -1.1170,  1.8815], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.1057,  4.5687,  4.5687,  0.6933,  0.6933,  0.6933,  0.2609,  0.2609,\n",
      "        -1.2731,  1.7849], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0042650131508708\n",
      "---\n",
      "end game\n"
     ]
    }
   ],
   "source": [
    "# Deep Q Network\n",
    "import gym \n",
    "import gym_examples\n",
    "from dqn import Agent\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('gym_examples/GridWorld-v0', size=4)\n",
    "agent = Agent(gamma=0.9, epsilon=0.5, batch_size=10, n_actions=4, eps_end=0.01, input_dims=[16], lr=0.003)\n",
    "scores, eps_hist,avg_scores = [], [], []\n",
    "n_games = 10\n",
    "\n",
    "for i in range(n_games):\n",
    "    score = 0\n",
    "    done=False\n",
    "    observation = env.reset()[0]['agent']\n",
    "    counter = 0\n",
    "\n",
    "    while not done:\n",
    "        rewards = []\n",
    "        agent_location = env.get_agent_location()\n",
    "        print(f\"Agent's Location: {agent_location}\")\n",
    "        # choose an action based on current state of env\n",
    "        action = agent.choose_action(observation)\n",
    "        observation_, reward, done, truncated, info = env.step(action) #observation_ is just next_state\n",
    "        observation_ = observation_['agent']\n",
    "        score += reward\n",
    "        rewards\n",
    "        agent.store_transitions(observation, action, reward, observation_, done)\n",
    "        agent.learn()\n",
    "        observation = observation_\n",
    "        # end of an episode\n",
    "        counter += 1\n",
    "        print('---')\n",
    "\n",
    "    scores.append(score)\n",
    "    eps_hist.append(agent.epsilon)\n",
    "    print('end game')\n",
    "\n",
    "    avg_score = np.mean(scores)\n",
    "    \n",
    "    avg_scores.append(avg_score)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming avg_scores is filled with the average score after each game\n",
    "episodes = list(range(1, n_games + 1))  # Creating a list of episode numbers\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(episodes, avg_scores, marker='o', linestyle='-', color='b', label='Average Score per Episode')\n",
    "plt.title('Average Scores Over Episodes')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Score')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (50,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/andyyang/Desktop/DS598 Reinforcement Learning/gym-examples/ps2.ipynb Cell 5\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/andyyang/Desktop/DS598%20Reinforcement%20Learning/gym-examples/ps2.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m all_rewards \u001b[39m=\u001b[39m scores  \u001b[39m# Example data\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/andyyang/Desktop/DS598%20Reinforcement%20Learning/gym-examples/ps2.ipynb#X16sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Calculate the mean and standard deviation across runs for each episode\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/andyyang/Desktop/DS598%20Reinforcement%20Learning/gym-examples/ps2.ipynb#X16sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m mean_rewards \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mmean(all_rewards, axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/andyyang/Desktop/DS598%20Reinforcement%20Learning/gym-examples/ps2.ipynb#X16sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m std_rewards \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mstd(all_rewards, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/andyyang/Desktop/DS598%20Reinforcement%20Learning/gym-examples/ps2.ipynb#X16sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m episodes \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(\u001b[39mlen\u001b[39m(mean_rewards))\n",
      "File \u001b[0;32m~/Desktop/DS598 Reinforcement Learning/gym-examples/.env/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3504\u001b[0m, in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m   3501\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3502\u001b[0m         \u001b[39mreturn\u001b[39;00m mean(axis\u001b[39m=\u001b[39maxis, dtype\u001b[39m=\u001b[39mdtype, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m-> 3504\u001b[0m \u001b[39mreturn\u001b[39;00m _methods\u001b[39m.\u001b[39;49m_mean(a, axis\u001b[39m=\u001b[39;49maxis, dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   3505\u001b[0m                       out\u001b[39m=\u001b[39;49mout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/DS598 Reinforcement Learning/gym-examples/.env/lib/python3.11/site-packages/numpy/core/_methods.py:102\u001b[0m, in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_mean\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m, where\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m--> 102\u001b[0m     arr \u001b[39m=\u001b[39m asanyarray(a)\n\u001b[1;32m    104\u001b[0m     is_float16_result \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     rcount \u001b[39m=\u001b[39m _count_reduce_items(arr, axis, keepdims\u001b[39m=\u001b[39mkeepdims, where\u001b[39m=\u001b[39mwhere)\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (50,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming `all_rewards` is a list of lists, where each inner list contains the rewards per episode for a single run.\n",
    "# Let's say you have 10 runs, and each run is 100 episodes long.\n",
    "all_rewards = scores  # Example data\n",
    "\n",
    "# Calculate the mean and standard deviation across runs for each episode\n",
    "mean_rewards = np.mean(all_rewards, axis=0)\n",
    "std_rewards = np.std(all_rewards, axis=0)\n",
    "episodes = np.arange(len(mean_rewards))\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(episodes, mean_rewards, label='Mean Rewards')\n",
    "plt.fill_between(episodes, mean_rewards - std_rewards, mean_rewards + std_rewards, alpha=0.2)\n",
    "plt.title('Learning Curve over 10 Runs')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9873709528509469"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.choice([1,2,3], 2, replace=False)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(2, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = np.array([1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, lr, input_dims, fc1_dims, fc2_dims, n_actions):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.n_actions = n_actions\n",
    "        self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)\n",
    "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "        self.fc3 = nn.Linear(self.fc2_dims, self.n_actions)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.loss = F.mse_loss\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        actions = self.fc3(x)\n",
    "\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = DeepQNetwork(lr =0.003, n_actions=4, input_dims=[16], fc1_dims=128, fc2_dims=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = net.forward(T.tensor([0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0], dtype=T.float32))\n",
    "T.argmax(w).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "memo = T.tensor([\n",
    "    [0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0]], dtype=T.float32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = net.forward(memo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0018,  0.0231,  0.0456, -0.0359],\n",
       "        [-0.0359,  0.0097,  0.0867, -0.0367],\n",
       "        [-0.0359,  0.0097,  0.0867, -0.0367],\n",
       "        [ 0.0365,  0.0507,  0.0973, -0.0360],\n",
       "        [-0.0475,  0.0321,  0.0967, -0.0469]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0456, 0.0867, 0.0867, 0.0973, 0.0967], grad_fn=<MaxBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.max(actions, dim=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
