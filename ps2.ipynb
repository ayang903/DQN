{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete random movements on environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent's INITIAL Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [1 3], Target Location: [0 0]\n",
      "Agent's Location: [1 3], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 3], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [1 3], Target Location: [0 0]\n",
      "Agent's Location: [0 3], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 1], Target Location: [0 0]\n",
      "Agent's Location: [0 0], Target Location: [0 0]\n",
      "Episode finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andyyang/Desktop/DS598 Reinforcement Learning/gym-examples/.env/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import gym_examples\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('gym_examples/GridWorld-v0', size=4)\n",
    "\n",
    "state = env.reset()\n",
    "agent_location = env.get_agent_location()\n",
    "target_location = env.get_target_location()\n",
    "print(f\"Agent's INITIAL Location: {agent_location}, Target Location: {target_location}\")\n",
    "\n",
    "done = False\n",
    "\n",
    "\n",
    "while not done:\n",
    "    # Select action randomly\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # take step, returns transition\n",
    "    next_state, reward, done, truncated, info = env.step(action)\n",
    "    \n",
    "    agent_location = env.get_agent_location()\n",
    "    target_location = env.get_target_location()\n",
    "    print(f\"Agent's Location: {agent_location}, Target Location: {target_location}\")\n",
    "\n",
    "    #update state\n",
    "    state = next_state\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(\"Episode finished\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-0.0218, -0.0784,  0.0679,  0.0578], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.0013, -0.0464,  0.0637,  0.0546], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.0174, -0.0597,  0.0616,  0.0634], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.0174, -0.0597,  0.0616,  0.0634], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.0066, -0.0508,  0.0809,  0.0624], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([ 0.0679,  0.0637,  0.0616,  0.0809,  0.0634,  0.0624,  0.0616,  0.0634,\n",
      "        -0.0597,  0.0634], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9427, -0.9429, -0.9429, -0.9272, -0.9429, -0.9429, -0.9429, -0.9429,\n",
      "        -0.9272, -0.9429], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.9870325326919556\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.0112, -0.0825, -0.0129,  0.1397], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.0825,  0.1397,  0.0782, -0.0184,  0.1397, -0.0129,  0.1397, -0.0136,\n",
      "         0.0427,  0.1397], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9296, -0.8743, -0.8743, -0.9144, -0.8743, -0.8743, -0.8743, -0.8743,\n",
      "        -0.9296, -0.8743], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.8968302607536316\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.0072, -0.1086, -0.0637,  0.2043], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.0637, -0.0293, -0.0845,  0.2043,  0.2043,  0.2043, -0.0974,  0.2043,\n",
      "         0.1266, -0.0637], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.8161, -0.8860, -0.8161, -0.8161, -0.8161, -0.8161, -0.8661, -0.8161,\n",
      "        -0.8161, -0.8161], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.8046396970748901\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-0.1326,  0.2658, -0.1543,  0.2658, -0.1198,  0.2658, -0.1198, -0.1326,\n",
      "         0.2658, -0.1758], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.8455, -0.7608, -0.7608, -0.7608, -0.7608, -0.7608, -0.7608, -0.8455,\n",
      "        -0.7608, -0.8209], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6837469339370728\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.0114, -0.1628, -0.1723,  0.2327], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.1817, -0.1817,  0.3279,  0.3279,  0.3279, -0.1723, -0.2241,  0.3279,\n",
      "        -0.1649,  0.3279], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.7049, -0.7049, -0.7049, -0.7049, -0.7049, -0.7906, -0.7049, -0.7049,\n",
      "        -0.7906, -0.7049], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6885680556297302\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([ 0.3926, -0.3000,  0.3926,  0.3926, -0.2027, -0.2432,  0.3926,  0.3926,\n",
      "        -0.2027,  0.2965], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6467, -0.6467, -0.6467, -0.6467, -0.7332, -0.7332, -0.6467, -0.6467,\n",
      "        -0.7332, -0.6467], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.7212889194488525\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.0042, -0.2520, -0.3235,  0.4607], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.4607, -0.3235, -0.3235,  0.4607,  0.4607,  0.3595,  0.4607,  0.4607,\n",
      "        -0.3165, -0.3813], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5854, -0.5854, -0.5854, -0.5854, -0.5854, -0.5854, -0.5854, -0.5854,\n",
      "        -0.6765, -0.5854], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6672433614730835\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([ 0.5320, -0.4688, -0.4032,  0.4165, -0.3044,  0.5320,  0.5320, -0.4032,\n",
      "         0.5320, -0.3952], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5212, -0.5212, -0.5212, -0.5212, -0.6252, -0.5212, -0.5212, -0.5212,\n",
      "        -0.5212, -0.6252], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5502551794052124\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.0098, -0.3632, -0.4879,  0.6061], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.6061, -0.5950,  0.6061,  0.6061, -0.4765, -0.5586, -0.3632, -0.4879,\n",
      "         0.6061, -0.4879], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4545, -0.5537, -0.4545, -0.4545, -0.5763, -0.4545, -0.5763, -0.4545,\n",
      "        -0.4545, -0.4545], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.45696964859962463\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.0116, -0.4250, -0.5676,  0.6676], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.5676,  0.5203, -0.6400,  0.6676, -0.4250,  0.6676, -0.4250,  0.6676,\n",
      "        -0.5676,  0.6676], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3992, -0.3992, -0.3992, -0.3992, -0.5317, -0.3992, -0.5317, -0.3992,\n",
      "        -0.3992, -0.3992], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5534826517105103\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.0124, -0.4860, -0.6296,  0.7132], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.7132,  0.5507, -0.6150, -0.6296, -0.7376,  0.7132,  0.7132, -0.6296,\n",
      "         0.5507,  0.7132], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3581, -0.3581, -0.5044, -0.3581, -0.4720, -0.3581, -0.3581, -0.3581,\n",
      "        -0.3581, -0.3581], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6472967863082886\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-0.5358, -0.6477,  0.7425, -0.6663,  0.7425, -0.7698,  0.5534, -0.6663,\n",
      "        -0.6663,  0.7425], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5019, -0.5019, -0.3318, -0.3318, -0.3318, -0.4611, -0.3318, -0.3318,\n",
      "        -0.3318, -0.3318], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.46991318464279175\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([ 0.7446, -0.6511, -0.6686,  0.5341, -0.6686, -0.6686, -0.6686,  0.7446,\n",
      "         0.7446, -0.5687], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3298, -0.5194, -0.3298, -0.3298, -0.3298, -0.3298, -0.3298, -0.3298,\n",
      "        -0.3298, -0.5194], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.46884188055992126\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.0076, -0.5856, -0.6440,  0.7248], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.5856,  0.7248, -0.7335, -0.6440,  0.7248, -0.6440,  0.7248,  0.7248,\n",
      "         0.7248, -0.6440], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5506, -0.3477, -0.3477, -0.3477, -0.3477, -0.3477, -0.3477, -0.3477,\n",
      "        -0.3477, -0.3477], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6164480447769165\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-0.6051,  0.6848,  0.6848, -0.6082, -0.6051,  0.6848,  0.4566,  0.6848,\n",
      "         0.6848,  0.4566], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3837, -0.3837, -0.3837, -0.5890, -0.3837, -0.3837, -0.3837, -0.3837,\n",
      "        -0.3837, -0.3837], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.7218957543373108\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([ 0.6450,  0.6450,  0.6450,  0.4093, -0.5658, -0.5658, -0.5658, -0.6727,\n",
      "         0.6450,  0.6450], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4195, -0.4195, -0.4195, -0.4195, -0.4195, -0.4195, -0.4195, -0.4195,\n",
      "        -0.4195, -0.4195], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6481064558029175\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([ 0.6009, -0.5896,  0.6009, -0.5236, -0.5490,  0.6009,  0.6009,  0.6009,\n",
      "         0.3593, -0.6459], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4592, -0.6766, -0.4592, -0.4592, -0.6766, -0.4592, -0.4592, -0.4592,\n",
      "        -0.4592, -0.6173], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6317771673202515\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.0234, -0.5016, -0.5231,  0.3108], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.4872,  0.5610, -0.4872,  0.5610,  0.5610, -0.4872, -0.5896,  0.5610,\n",
      "        -0.4872,  0.3108], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4951, -0.4951, -0.4951, -0.4951, -0.4951, -0.4951, -0.7203, -0.4951,\n",
      "        -0.4951, -0.4951], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5128200054168701\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([ 0.5254,  0.2640, -0.5935, -0.5853,  0.5254, -0.4570,  0.5254, -0.5012,\n",
      "        -0.5935,  0.5254], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5271, -0.5271, -0.7624, -0.6904, -0.5271, -0.5271, -0.5271, -0.7624,\n",
      "        -0.7624, -0.5271], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5198435187339783\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-0.4362,  0.4972,  0.2225,  0.2225,  0.4972,  0.4972, -0.6065,  0.4972,\n",
      "        -0.6065, -0.4362], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5525, -0.5525, -0.5525, -0.5525, -0.5525, -0.5525, -0.7998, -0.5525,\n",
      "        -0.7998, -0.5525], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5710448026657104\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([ 0.4780, -0.4219, -0.5431, -0.4219, -0.4219, -0.5548, -0.6283,  0.4780,\n",
      "         0.4780,  0.4780], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5698, -0.5698, -0.5698, -0.5698, -0.5698, -0.7440, -0.8346, -0.5698,\n",
      "        -0.5698, -0.5698], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.45361775159835815\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([ 0.1526,  0.4626,  0.4626,  0.4626,  0.4626,  0.4626, -0.4173, -0.5529,\n",
      "         0.4626,  0.4626], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5837, -0.5837, -0.5837, -0.5837, -0.5837, -0.5837, -0.5837, -0.7634,\n",
      "        -0.5837, -0.5837], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.8276742696762085\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([ 0.4465,  0.1198, -0.4168,  0.4465, -0.4168, -0.4727, -0.4168,  0.4465,\n",
      "         0.1198,  0.4465], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5981, -0.5981, -0.5981, -0.5981, -0.5981, -0.8922, -0.5981, -0.5981,\n",
      "        -0.5981, -0.5981], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5670611262321472\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.0181, -0.7596, -0.5406,  0.3516], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.4239,  0.0884,  0.4404, -0.4239, -0.4239,  0.4404, -0.4239,  0.0884,\n",
      "         0.4404, -0.7100], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6036, -0.6036, -0.6036, -0.6036, -0.6036, -0.6036, -0.6036, -0.6036,\n",
      "        -0.6036, -0.9204], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.44013944268226624\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-0.4384,  0.4441,  0.4441, -0.4384, -0.5560,  0.0726,  0.4441, -0.4941,\n",
      "         0.0331, -0.4384], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6003, -0.6003, -0.6003, -0.6003, -0.6003, -0.6921, -0.6003, -0.9347,\n",
      "        -0.8206, -0.6003], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.48605695366859436\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([ 2.1830e-04, -7.8094e-01, -6.0876e-01,  2.0864e-01],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.4582,  0.3614,  0.4627,  0.4627, -0.4582,  0.4627, -0.7818, -0.7818,\n",
      "         0.2086,  0.4627], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5835, -0.8060, -0.5835, -0.5835, -0.5835, -0.5835, -0.9472, -0.9472,\n",
      "        -0.8122, -0.5835], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6869902014732361\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([ 0.4709, -0.5338, -0.0052,  0.4709, -0.4831, -0.4831,  0.4709,  0.4709,\n",
      "        -0.8244, -0.8244], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5762, -0.9536, -0.8095, -0.5762, -0.5762, -0.5762, -0.5762, -0.5762,\n",
      "        -0.9536, -0.9536], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.52593994140625\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-0.5136,  0.4800,  0.3552, -0.5136, -0.5136, -0.5136, -0.5136,  0.4800,\n",
      "        -0.6146,  0.0159], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5680, -0.5680, -0.7936, -0.5680, -0.5680, -0.5680, -0.5680, -0.5680,\n",
      "        -0.5680, -0.5680], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.38744115829467773\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.0500, -0.8159, -0.6374,  0.2401], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 2.4011e-01,  4.8680e-01,  4.8680e-01,  4.0767e-04,  2.3127e-01,\n",
      "        -5.8576e-01,  4.8680e-01,  3.4329e-01, -5.4511e-01, -5.4511e-01],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.7839, -0.5619, -0.5619, -0.5619, -0.7919, -0.9679, -0.5619, -0.7839,\n",
      "        -0.5619, -0.5619], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.7127882242202759\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.0645, -0.8530, -0.6629,  0.2475], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.2475, -0.5784, -0.7202, -0.5784,  0.4860, -0.5784,  0.4860,  0.4860,\n",
      "        -0.9648,  0.4860], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.7772, -0.5626, -0.7772, -0.5626, -0.5626, -0.5626, -0.5626, -0.5626,\n",
      "        -0.9672, -0.5626], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.545235276222229\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([ 0.2868, -0.0454, -0.6107,  0.4776,  0.4776,  0.4776, -0.7513,  0.2493,\n",
      "        -0.0780,  0.4776], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.7756, -0.5701, -0.5701, -0.5701, -0.5701, -0.5701, -0.7756, -0.7756,\n",
      "        -0.7986, -0.5701], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.7367075085639954\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-0.0998, -1.0229, -0.7796,  0.2231], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.6409, -0.6409,  0.4683, -0.6409,  0.2231,  0.2560,  0.2231,  0.4683,\n",
      "        -0.6409,  0.4683], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5785, -0.5785, -0.5785, -0.5785, -0.7992, -0.7696, -0.7992, -0.5785,\n",
      "        -0.5785, -0.5785], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6445210576057434\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-0.1212, -1.0567, -0.8045,  0.2108], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.4482,  0.0219, -0.6667, -0.0984,  0.4482,  0.4482, -0.6667, -1.0785,\n",
      "         0.4482,  0.2531], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5966, -0.8102, -0.5966, -0.5966, -0.5966, -0.5966, -0.5966, -0.9803,\n",
      "        -0.5966, -0.7722], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6377847790718079\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-0.1528, -1.0850, -0.8240,  0.2046], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.2046,  0.4352,  0.4352, -0.7165,  0.0018, -0.6857, -0.8240,  0.2046,\n",
      "        -0.7457, -0.1623], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.8159, -0.6083, -0.6083, -0.9983, -0.8334, -0.6083, -0.7707, -0.8159,\n",
      "        -0.6083, -0.8159], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5492388606071472\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-0.1968, -1.1127, -0.8392,  0.2141], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.4322, -0.0302,  0.2141, -0.7010,  0.4322, -0.7010, -0.1377,  0.2141,\n",
      "         0.4322, -1.1291], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6110, -0.8387, -0.8073, -0.6110, -0.6110, -0.6110, -0.6110, -0.8073,\n",
      "        -0.6110, -1.0272], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.625575065612793\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([ 0.2246, -0.1500, -0.1500,  0.4340,  0.4340,  0.2835, -0.7099,  0.4340,\n",
      "         0.4340,  0.4340], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.7979, -0.6094, -0.6094, -0.6094, -0.6094, -0.7449, -0.6094, -0.6094,\n",
      "        -0.6094, -0.6094], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.7978463768959045\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-0.1737,  0.2248, -0.7168, -1.1630,  0.4306, -0.7721, -0.7168,  0.4306,\n",
      "         0.4306,  0.4306], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6124, -0.7977, -0.6124, -1.0953, -0.6124, -0.6124, -0.6124, -0.6124,\n",
      "        -0.6124, -0.6124], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5641743540763855\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([ 0.4182,  0.2879, -0.7180,  0.4182,  0.4182,  0.4182,  0.4182,  0.4182,\n",
      "        -0.7878, -0.3430], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6236, -0.7409, -0.6236, -0.6236, -0.6236, -0.6236, -0.6236, -0.6236,\n",
      "        -1.1249, -0.8066], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.7908235788345337\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.1702, -0.9685, -0.8077, -0.2360], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([ 0.3988, -0.7228, -1.0769,  0.3988,  0.2056,  0.2056, -1.1808,  0.3988,\n",
      "        -0.2360, -0.7228], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6411, -0.6411, -0.8673, -0.6411, -0.8150, -0.8150, -1.1532, -0.6411,\n",
      "        -0.6411, -0.6411], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5549237132072449\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3954, -1.1839, -0.8574,  0.1190], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.7246, -1.1789,  0.1843, -0.8256, -0.8783,  0.1843, -0.7246, -0.7246,\n",
      "        -0.7796,  0.2681], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6655, -1.1783, -0.8342, -1.1783, -0.7587, -0.8342, -0.6655, -0.6655,\n",
      "        -0.6655, -0.7587], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3290947675704956\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-0.7253, -0.7253, -1.1750, -0.3209, -0.7253, -0.8818,  0.3378,  0.3378,\n",
      "        -0.7253, -0.4502], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6960, -0.6960, -1.1961, -0.6960, -0.6960, -0.7793, -0.6960, -0.6960,\n",
      "        -0.6960, -0.8620], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2461954653263092\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([ 0.3092,  0.0529, -0.7225, -0.7225, -0.3649, -0.4839,  0.3092, -0.8796,\n",
      "        -0.8623,  0.2259], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.7218, -0.7967, -0.7218, -0.7218, -0.7218, -0.8819, -0.7218, -0.7967,\n",
      "        -1.2165, -0.7967], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4311177730560303\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.5211, -1.0856, -0.7815,  0.2123], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.2810, -0.7255,  0.2810,  0.2810,  0.1139,  0.1139, -0.7255,  0.1139,\n",
      "        -1.1718,  0.0149], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.7471, -0.7471, -0.7471, -0.7471, -0.8975, -0.8975, -0.7471, -0.8975,\n",
      "        -1.2342, -0.8090], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6923230290412903\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-0.7326,  0.0850, -1.1749, -0.7326,  0.1969,  0.2433, -0.7326, -0.2779,\n",
      "         0.2433,  0.2433], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.7810, -0.9235, -1.2501, -0.7810, -0.8228, -0.7810, -0.7810, -1.0332,\n",
      "        -0.7810, -0.7810], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5787714719772339\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-0.8937, -1.1846, -1.1074,  0.0618, -0.7431, -1.1846,  0.0618, -1.1846,\n",
      "        -0.3097,  0.0618], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.8320, -1.2787, -1.0665, -0.9444, -0.8090, -1.2787, -0.9444, -1.2787,\n",
      "        -1.0665, -0.9444], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.36463189125061035\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-0.4480, -1.0012, -0.7228, -0.0177], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.6403, -0.7548,  0.1891, -0.5401, -0.0964,  0.0418,  0.1817,  0.1891,\n",
      "        -0.7548,  0.1891], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9623, -0.8298, -0.8298, -0.8298, -0.8365, -0.9623, -0.8365, -0.8298,\n",
      "        -0.8298, -0.8298], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5906108021736145\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-0.5797, -0.9885, -1.1534,  0.0239, -0.0369,  0.0239,  0.1649, -0.6910,\n",
      "        -0.6910,  0.1649], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.8516, -1.3543, -1.1139, -0.9785, -0.9785, -0.9785, -0.8516, -0.9785,\n",
      "        -0.9785, -0.8516], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.533715546131134\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-0.6939, -1.2733, -0.9264,  0.0232], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.1525, -0.7896, -0.1525,  0.0232, -0.6204,  0.1838, -0.4363, -0.7498,\n",
      "         0.1451, -0.7896], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.8346, -0.8694, -0.8346, -0.9791, -0.8694, -0.8346, -1.1372, -0.9791,\n",
      "        -0.8694, -0.8694], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.46202221512794495\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([ 0.0316, -0.8112,  0.1342,  0.1342,  0.0316,  0.1342,  0.1342, -0.9414,\n",
      "        -1.0714,  0.2034], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9715, -0.8792, -0.8792, -0.8792, -0.9715, -0.8792, -0.8792, -0.8170,\n",
      "        -1.4451, -0.8170], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.7321592569351196\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.8813, -1.2447, -0.8613,  0.2071], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.1096, -1.3088,  0.0238,  0.0238,  0.1096,  0.0238, -0.9618, -0.8393,\n",
      "        -0.9618, -0.2109], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9014, -1.4886, -0.9786, -0.9786, -0.9014, -0.9786, -0.8136, -0.9014,\n",
      "        -0.8136, -0.8136], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5501807928085327\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([ 7.2457e-02, -5.8381e-01, -8.6677e-01,  2.0032e-01,  7.2457e-02,\n",
      "        -1.3432e+00,  7.2457e-02,  2.0032e-01,  7.7170e-04, -8.6677e-01],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9348, -1.2376, -0.9348, -0.8197, -0.9348, -1.5254, -0.9348, -0.8197,\n",
      "        -0.9993, -0.9348], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6594597101211548\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([ 0.0365, -0.8999, -0.8171, -0.1979, -0.3055, -0.0209,  0.0365,  0.0365,\n",
      "         0.1924, -0.9946], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9671, -0.9671, -0.9671, -1.0188, -0.8269, -1.0188, -0.9671, -0.9671,\n",
      "        -0.8269, -1.0188], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6029981374740601\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.0518, -1.3652, -0.9232,  0.1832], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.0398, -1.0518, -0.9353, -1.1596, -0.9353, -0.0066, -0.9353, -0.0066,\n",
      "        -1.0518, -1.4348], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.0358, -1.0358, -1.0059, -1.6231, -1.0059, -1.0059, -1.0059, -1.0059,\n",
      "        -1.0358, -1.6231], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3255230784416199\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.0964, -1.4144, -0.9574,  0.1692], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.1692, -0.0649, -0.0536, -0.3056, -1.4933, -1.4933, -0.9400, -0.0649,\n",
      "        -0.0536, -1.0463], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.8477, -1.0584, -1.0482, -1.0584, -1.6511, -1.6511, -1.0482, -1.0584,\n",
      "        -1.0482, -0.8477], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5654549598693848\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.1332, -1.4745, -0.9902,  0.1418], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.1095, -1.1332, -0.7436, -1.1332, -0.1095,  0.1418,  0.1418, -0.0915,\n",
      "        -1.5660, -0.1095], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.0985, -1.0823, -1.4371, -1.0823, -1.0985, -0.8724, -0.8724, -1.0823,\n",
      "        -1.6692, -1.0985], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6470411419868469\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-0.1214, -1.6441, -1.6441, -0.1214, -0.1666, -0.1666,  0.1104, -1.0971,\n",
      "        -1.0881, -0.1666], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.1092, -1.7047, -1.7047, -1.1092, -1.1499, -1.1499, -0.9006, -0.9006,\n",
      "        -1.1499, -1.1499], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5924620032310486\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-0.1654, -0.8167, -1.3824,  0.0617, -0.2363, -0.2363, -0.2363, -0.1654,\n",
      "        -0.6106, -1.7278], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.1489, -1.5495, -1.7350, -0.9444, -1.2127, -1.2127, -1.2127, -1.1489,\n",
      "        -0.9444, -1.7350], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6579593420028687\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.0727, -1.8141, -1.2038, -0.3066], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.3066,  0.0183, -0.3066, -1.8141, -0.2114, -0.3066,  0.0183, -1.1047,\n",
      "        -0.6745, -1.6773], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.2759, -0.9836, -1.2759, -1.7799, -1.1902, -1.2759, -0.9836, -1.2759,\n",
      "        -0.9836, -1.6071], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5915294885635376\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.1174, -1.8918, -1.2711, -0.3874], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.9162, -1.8918, -0.9162, -0.3874, -1.2711, -0.0358, -1.3007, -1.1549,\n",
      "        -0.3874, -1.2983], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.6795, -1.8246, -1.6795, -1.3487, -1.3487, -1.0322, -1.3487, -1.3487,\n",
      "        -1.3487, -1.2426], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.40596693754196167\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-0.4299, -0.4299, -0.4299, -1.9631, -1.3664, -0.2956, -1.7341, -1.5838,\n",
      "        -0.7767, -0.0612], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3869, -1.3869, -1.3869, -1.9089, -1.2661, -1.2661, -1.9089, -1.9089,\n",
      "        -1.0551, -1.0551], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4903872609138489\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-0.0978, -0.3394, -0.0978, -1.4090, -0.0978, -1.4161, -0.0978, -0.4882,\n",
      "        -1.4161, -0.4882], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.0880, -1.3055, -1.0880, -1.4394, -1.0880, -1.4394, -1.0880, -1.4394,\n",
      "        -1.4394, -1.4394], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6666873097419739\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4702, -1.9214, -1.3492, -0.1556], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.4986, -1.3770, -0.5591, -0.3978, -0.5591, -0.1556, -0.5591, -0.5591,\n",
      "        -0.1556, -0.3978], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.5032, -1.1400, -1.5032, -1.3580, -1.5032, -1.1400, -1.5032, -1.5032,\n",
      "        -1.1400, -1.3580], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.7403730154037476\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.5192, -1.9842, -1.4227, -0.2387], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.4736, -1.5782, -0.6495, -0.6495, -1.5706, -0.2387, -1.5782, -0.2387,\n",
      "        -0.4736, -0.2387], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4262, -1.5846, -1.5846, -1.5846, -1.5846, -1.2148, -1.5846, -1.2148,\n",
      "        -1.4262, -1.2148], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6422463655471802\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.5686, -2.0506, -1.5031, -0.3394], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.0529, -0.7523, -0.7523, -1.6622, -1.0529, -1.5031, -0.3394, -0.7523,\n",
      "        -1.5686, -1.6622], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3054, -1.6771, -1.6771, -1.6771, -1.3054, -1.6771, -1.3054, -1.6771,\n",
      "        -1.5081, -1.6771], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.36608123779296875\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-2.2954, -1.7781, -0.4391, -2.1139, -0.8618, -0.6604, -0.8618, -0.6604,\n",
      "        -1.7532, -0.8618], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2350, -1.7756, -1.3952, -2.0440, -1.7756, -1.5943, -1.7756, -1.5943,\n",
      "        -1.7756, -1.7756], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.517295241355896\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-0.9834, -1.2797, -0.7693, -0.9834, -1.6626, -1.6754, -0.5500, -1.6754,\n",
      "        -1.8468, -0.9834], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.8851, -1.4950, -1.6924, -1.8851, -1.6924, -1.8851, -1.4950, -1.8851,\n",
      "        -1.8851, -1.8851], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4320715069770813\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.7212, -2.2253, -1.7891, -0.6677], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.9533, -0.6677, -1.7466, -1.7466, -0.8779, -1.1023, -0.6677, -0.6677,\n",
      "        -1.9533, -1.4035], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.9921, -1.6009, -1.6009, -1.6009, -1.7901, -1.9921, -1.6009, -1.6009,\n",
      "        -1.9921, -1.6009], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4320985674858093\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-1.2321, -2.2906, -2.0522, -1.2321, -0.8055, -1.2321, -1.5806, -1.6150,\n",
      "        -0.8055, -2.1494], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.1089, -2.3886, -2.1089, -2.1089, -1.7249, -2.1089, -2.3886, -1.7249,\n",
      "        -1.7249, -2.1089], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4676527976989746\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8686, -2.3748, -2.0160, -0.9229], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.1551, -1.8686, -1.3403, -0.9229, -0.9229, -2.3748, -2.0160, -1.9036,\n",
      "        -1.5832, -1.3403], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2062, -1.9748, -2.2062, -1.8307, -1.8307, -2.4760, -2.2062, -1.8307,\n",
      "        -1.9748, -2.2062], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3366659879684448\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9865, -2.4883, -2.1515, -1.0374], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.6132, -1.1540, -1.4342, -1.4342, -1.0374, -1.1540, -1.8171, -1.4342,\n",
      "        -1.1540, -1.7223], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.6354, -2.0386, -2.2908, -2.2908, -1.9336, -2.0386, -2.5500, -2.2908,\n",
      "        -2.0386, -1.9336], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5934353470802307\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-1.7927, -1.9265, -1.1432, -1.5261, -1.2289, -2.1235, -2.1235, -2.4595,\n",
      "        -2.2848, -1.2289], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.0289, -2.0289, -2.0289, -2.3735, -2.1060, -2.1060, -2.1060, -2.3735,\n",
      "        -2.3735, -2.1060], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3123261332511902\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-2.1800, -2.5633, -2.4165, -2.4948, -1.2474, -1.3140, -2.1800, -2.1355,\n",
      "        -2.7621, -1.8745], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.1227, -2.4630, -2.4630, -2.4630, -2.1227, -2.1826, -2.1227, -2.6871,\n",
      "        -2.9219, -2.1227], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.19317063689231873\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.4073, -2.8298, -2.5502, -1.3378], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.3378, -2.6167, -1.3873, -2.6167, -2.3127, -1.3873, -1.3378, -1.7129,\n",
      "        -1.3378, -2.5346], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2040, -2.5417, -2.2485, -2.5417, -2.7473, -2.2485, -2.2040, -2.5417,\n",
      "        -2.2040, -2.6100], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4627155661582947\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.5735, -2.9594, -2.6815, -1.4386], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.4386, -2.3478, -1.8043, -3.3815, -1.8043, -1.4386, -2.5735, -2.6815,\n",
      "        -1.8043, -1.8043], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2948, -2.2948, -2.6239, -3.2636, -2.6239, -2.2948, -2.3239, -2.6239,\n",
      "        -2.6239, -2.6239], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.42350226640701294\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-1.5632, -1.5853, -2.4813, -1.9407, -3.0514, -2.4813, -2.8306, -1.5853,\n",
      "        -3.0765, -2.7902], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.4069, -2.4268, -2.4069, -2.4268, -3.4311, -2.4069, -2.7337, -2.4268,\n",
      "        -2.8892, -2.7337], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.25670501589775085\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-1.7073, -1.7073, -3.0363, -1.7019, -3.2060, -2.0731, -3.0363, -2.2340,\n",
      "        -2.8475, -3.2060], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.5366, -2.5366, -2.8658, -2.5317, -3.5627, -2.8658, -2.8658, -2.5366,\n",
      "        -3.0106, -3.5627], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3123108148574829\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.7218, -3.3782, -3.0713, -2.2367], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.6503, -2.6503, -1.8466, -2.9380, -2.9134, -3.3782, -1.8466, -1.8466,\n",
      "        -2.9134, -3.0713], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.6619, -2.6619, -2.6619, -3.1344, -2.6400, -3.6442, -2.6619, -2.6619,\n",
      "        -2.6400, -3.0130], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.22568261623382568\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-1.9931, -2.7455, -3.1988, -2.4176, -1.9708, -3.1988, -1.9931, -2.9926,\n",
      "        -2.9466, -3.5731], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.7938, -2.7938, -3.1759, -3.1759, -2.7737, -3.1759, -2.7938, -3.2681,\n",
      "        -2.7737, -3.6934], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.26254329085350037\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.0550, -4.0398, -3.8853, -3.4549], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-2.9783, -2.1412, -2.8492, -2.6045, -2.6045, -3.0550, -3.4549, -2.6045,\n",
      "        -2.1412, -2.1307], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.9176, -2.9271, -2.9271, -3.3441, -3.3441, -3.3996, -3.3441, -3.3441,\n",
      "        -2.9271, -2.9176], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.36360710859298706\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-2.2907, -3.4705, -2.2989, -3.0079, -2.9690, -2.6598, -2.2989, -2.7986,\n",
      "        -2.8071, -3.6109], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.0616, -3.5187, -3.0690, -3.0616, -3.0616, -3.0690, -3.0690, -3.5187,\n",
      "        -3.0616, -3.5187], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2553613483905792\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-3.0043, -3.1273, -3.6320, -3.0043, -3.6320, -4.2081, -2.4332, -4.2081,\n",
      "        -2.4332, -2.4332], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7038, -3.1899, -3.7038, -3.7038, -3.7038, -3.9156, -3.1899, -3.9156,\n",
      "        -3.1899, -3.1899], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.28817781805992126\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-4.3168, -2.5854, -3.3815, -3.1674, -3.2599, -3.1988, -3.1988, -3.1988,\n",
      "        -4.3168, -3.1988], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.0433, -3.3268, -3.8109, -3.3719, -3.3268, -3.8789, -3.8789, -3.8789,\n",
      "        -4.0433, -3.8789], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.27802330255508423\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4298, -4.3526, -3.9268, -3.3595], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.3095, -2.7734, -3.3595, -3.1689, -3.3595, -2.7215, -3.3595, -4.4318,\n",
      "        -3.3595, -3.3095], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4961, -3.4961, -4.0235, -3.4961, -4.0235, -3.4494, -4.0235, -4.2403,\n",
      "        -4.0235, -3.4961], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.30292099714279175\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.6292, -4.4043, -4.0470, -3.5314], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.8764, -3.5314, -2.8764, -3.8400, -4.2981, -3.5314, -3.5314, -3.5314,\n",
      "        -4.0470, -2.8764], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5887, -4.1783, -3.5887, -4.0395, -4.1783, -4.1783, -4.1783, -4.1783,\n",
      "        -4.1782, -3.5887], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.32675236463546753\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.8494, -4.4749, -4.1999, -3.7231], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.0238, -3.0486, -3.5146, -4.4852, -3.7231, -3.0486, -3.7231, -3.3413,\n",
      "        -3.0486, -4.0926], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7214, -3.7438, -3.7438, -4.3508, -4.3508, -3.7438, -4.3508, -3.7214,\n",
      "        -3.7438, -4.1631], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2944332957267761\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1000, -4.5616, -4.3648, -3.9245], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.9245, -4.3648, -4.3648, -3.1599, -3.6467, -3.6632, -4.6696, -4.3631,\n",
      "        -3.2222, -3.2222], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5321, -4.5321, -4.5321, -3.8439, -3.9000, -3.9000, -4.5321, -4.2969,\n",
      "        -3.9000, -3.9000], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.19552166759967804\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-3.3666, -4.1456, -4.1178, -3.8261, -4.6163, -4.5981, -3.3666, -3.8985,\n",
      "        -4.1456, -4.1309], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.0299, -4.7060, -4.7060, -4.0299, -4.4300, -4.7060, -4.0299, -4.4082,\n",
      "        -4.7060, -3.9694], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.22280268371105194\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-4.8043, -4.2117, -4.2117, -4.9159, -3.9387, -3.4932, -4.2117, -3.4932,\n",
      "        -4.2117, -4.4365], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5174, -4.7905, -4.7905, -4.7905, -4.0772, -4.1439, -4.7905, -4.1439,\n",
      "        -4.7905, -4.7905], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2429487407207489\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-4.7663, -5.1456, -3.6470, -3.6470, -4.9260, -4.7663, -3.6470, -4.3216,\n",
      "        -4.1281, -4.7663], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8894, -4.8894, -4.2823, -4.2823, -5.4644, -4.8894, -4.2823, -4.8894,\n",
      "        -4.1953, -4.8894], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.19388195872306824\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-3.8205, -5.1189, -5.1189, -3.8205, -3.6862, -4.7413, -4.7413, -4.4425,\n",
      "        -5.1189, -3.6862], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4384, -4.9983, -4.9983, -4.4384, -4.3176, -4.3176, -4.3176, -4.9983,\n",
      "        -4.9983, -4.3176], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.22725459933280945\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-3.9174, -4.9670, -3.9174, -4.0219, -4.6564, -5.5080, -4.6564, -5.0912,\n",
      "        -5.3388, -4.6564], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5257, -4.6197, -4.5257, -4.6197, -5.1907, -4.6983, -5.1907, -4.9256,\n",
      "        -5.5821, -5.1907], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2816644608974457\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0443, -5.5133, -6.2384, -5.4487], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-5.4542, -4.2905, -4.9445, -4.1905, -5.9180, -4.9445, -4.9445, -4.1905,\n",
      "        -5.9180, -4.1905], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5399, -4.8614, -5.4500, -4.7715, -5.4500, -5.4500, -5.4500, -4.7715,\n",
      "        -5.4500, -4.7715], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2550564408302307\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8385, -5.2369, -5.8268, -4.9462], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-6.0377, -4.8385, -4.6028, -6.0377, -4.8553, -4.6028, -5.3259, -4.5092,\n",
      "        -5.2769, -5.5601], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4835, -4.8091, -5.1426, -5.4835, -5.0583, -5.1426, -5.1426, -5.0583,\n",
      "        -5.4835, -5.4903], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16216036677360535\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-4.2993, -4.7717, -5.2811, -5.1091], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-5.5770, -5.9267, -5.6352, -5.0142, -4.9159, -5.5770, -5.9267, -4.9159,\n",
      "        -5.8369, -4.9159], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5641, -5.5641, -5.5128, -5.4355, -5.4243, -5.5641, -5.5641, -5.4243,\n",
      "        -5.5128, -5.4243], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13361740112304688\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-5.1600, -4.8055, -5.8143, -5.1600, -5.1600, -4.7777, -5.8143, -5.1600,\n",
      "        -5.1600, -5.8143], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6064, -4.8581, -5.7169, -5.6064, -5.6064, -5.3000, -5.7169, -5.6064,\n",
      "        -5.6064, -5.7169], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13003042340278625\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-5.3119, -6.1296, -5.7438, -5.5023, -6.0194, -6.1296, -5.3119, -5.5023,\n",
      "        -6.1296, -6.1296], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3431, -5.7645, -5.7645, -5.6450, -5.7645, -5.7645, -5.3431, -5.6450,\n",
      "        -5.7645, -5.7645], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06413330137729645\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-5.1075, -6.3031, -5.7296, -6.3031, -5.7369, -6.4347, -5.1638, -6.3031,\n",
      "        -5.7369, -5.3326], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0861, -5.8586, -5.8586, -5.8586, -5.7344, -5.8586, -5.5967, -5.8586,\n",
      "        -5.7344, -5.7344], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12906049191951752\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-5.8046, -5.8472, -6.4148, -5.8456, -6.3370, -6.3370, -6.3370, -6.3370,\n",
      "        -5.8263, -5.5669], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.7326, -5.7326, -6.0102, -5.8593, -6.0102, -6.0102, -6.0102, -6.0102,\n",
      "        -5.8593, -5.8593], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06959109008312225\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3595, -5.3066, -5.5155, -5.7582], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.9201, -4.9120, -5.8049, -5.7202, -6.2807, -5.9201, -5.3489, -5.8049,\n",
      "        -5.8049, -5.9201], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.2228, -5.0238, -5.9445, -5.7522, -6.2228, -6.2228, -5.9445, -5.9445,\n",
      "        -5.9445, -6.2228], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0704902932047844\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9460, -4.7449, -5.0717, -5.3036], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.8053, -5.8214, -5.9841, -4.8564, -5.5602, -4.9947, -6.2713, -6.2713,\n",
      "        -5.4793, -6.0210], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.8495, -5.7596, -5.8495, -4.9578, -5.7596, -5.3708, -6.2393, -6.2393,\n",
      "        -5.6775, -6.2393], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.030429178848862648\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.4748, -4.0852, -4.5279, -4.5694], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.2627, -5.6819, -5.8170, -5.1654, -6.2627, -5.8665, -5.6295, -5.6295,\n",
      "        -5.0417, -5.6964], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.1268, -5.7148, -5.7148, -5.2844, -6.1268, -6.1268, -5.7148, -5.7148,\n",
      "        -5.1595, -5.6031], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01675240322947502\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-5.8182, -5.5646, -6.2267, -4.4567, -4.5746, -5.8182, -6.2267, -6.2612,\n",
      "        -5.8182, -5.7103], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5922, -5.4619, -6.0081, -4.5817, -4.6568, -5.5922, -6.0081, -6.0081,\n",
      "        -5.5922, -5.5819], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03622611239552498\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.6818, -4.0277, -4.7272, -4.5901], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.7391, -5.5844, -5.7391, -5.7920, -6.1112, -6.4429, -5.9361, -6.0487,\n",
      "        -6.1112, -6.3286], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6029, -5.4550, -5.6029, -5.2175, -6.0260, -5.6029, -5.6029, -5.6417,\n",
      "        -6.0260, -6.0260], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14723090827465057\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.6236, -4.0854, -4.6683, -4.4345], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.6331, -5.6900, -5.0637, -5.6331, -5.9771, -5.3847, -5.9211, -6.0514,\n",
      "        -5.3348, -5.8402], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.7367, -5.5573, -5.4648, -5.7367, -5.7367, -5.7367, -6.1210, -5.8014,\n",
      "        -5.3682, -6.1210], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.056415267288684845\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-6.0259, -6.1986, -5.5561, -5.7899, -5.7857, -6.1986, -5.5744, -5.7899,\n",
      "        -5.5561, -6.0582], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.9153, -6.1617, -5.9153, -6.1617, -6.1617, -6.1617, -5.8181, -6.1617,\n",
      "        -5.9153, -6.1617], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07611231505870819\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.4912, -4.0496, -4.5565, -4.3188], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.7641, -5.3104, -5.6186, -6.0368, -5.7613, -5.0095, -5.7613, -5.6324,\n",
      "        -5.6348, -5.6186], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5085, -5.8011, -5.8011, -5.8011, -6.0858, -5.1932, -6.0858, -5.9248,\n",
      "        -5.4844, -5.8011], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07807597517967224\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-5.8118, -6.0513, -5.8118, -6.0962, -5.7639, -6.0649, -5.7233, -5.2588,\n",
      "        -5.6665, -5.9478], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.9714, -5.7329, -5.9714, -5.8741, -5.8741, -5.9714, -5.7329, -5.4543,\n",
      "        -5.6208, -5.8741], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02683105692267418\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-5.5172, -5.9671, -5.8704, -5.8818, -5.8704, -5.8422, -5.4197, -5.8704,\n",
      "        -5.8704, -6.0666], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6934, -5.5959, -5.8778, -5.8262, -5.8778, -5.6934, -5.5959, -5.8778,\n",
      "        -5.8778, -5.8778], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.026099776849150658\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-5.9232, -5.9632, -6.1392, -5.9299, -4.2402, -5.9760, -5.9760, -5.5003,\n",
      "        -4.2461, -5.9232], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.8267, -5.6728, -5.8267, -5.6728, -4.4516, -5.8053, -5.8053, -5.4505,\n",
      "        -5.4505, -5.8267], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.18227127194404602\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7892, -4.2638, -4.8155, -4.9855], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.6709, -6.0282, -5.9396, -5.9601, -6.0019, -4.3000, -5.9601, -5.9601,\n",
      "        -6.0019, -5.3213], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6642, -5.7791, -5.7791, -5.7892, -5.6642, -4.8374, -5.7892, -5.7892,\n",
      "        -5.6642, -5.5570], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07478495687246323\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-5.9970, -5.8367, -6.0071, -5.9970, -5.9339, -5.9970, -5.9970, -5.8367,\n",
      "        -6.1436, -5.8367], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6961, -5.8032, -5.7978, -5.6961, -5.8032, -5.6961, -5.6961, -5.8032,\n",
      "        -5.8032, -5.8032], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.054230112582445145\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5964, -4.2571, -4.5995, -4.7867], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.8942, -5.7903, -5.7558, -5.6478, -5.8455, -5.9062, -5.4816, -5.8942,\n",
      "        -5.7214, -5.8455], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.7767, -5.7767, -5.8867, -5.8867, -5.8867, -5.8751, -5.4924, -5.7767,\n",
      "        -5.7767, -5.8868], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.010959500446915627\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5142, -4.2716, -4.5217, -4.6694], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.6547, -5.7995, -5.7607, -5.7607, -5.9101, -5.4130, -5.7607, -5.6547,\n",
      "        -5.7607, -5.7114], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.9390, -5.8444, -5.9507, -5.9507, -5.9507, -5.5614, -5.9507, -5.9390,\n",
      "        -5.9507, -5.9507], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.038899678736925125\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.4685, -4.2348, -4.4724, -4.5971], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.6733, -5.7796, -5.7307, -5.6733, -5.7503, -5.4909, -5.3928, -5.3559,\n",
      "        -5.0719, -5.6353], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.9198, -5.8729, -5.8729, -5.9198, -5.9198, -5.6534, -5.5647, -5.4709,\n",
      "        -5.0137, -5.8729], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.030822202563285828\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-4.1687, -5.7661, -4.4280, -5.7625, -5.7071, -5.4485, -5.7853, -5.6157,\n",
      "        -5.7039, -5.1939], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7518, -5.8636, -5.4139, -5.9037, -5.9037, -5.6084, -5.8636, -5.8636,\n",
      "        -5.8560, -5.4139], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15449172258377075\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-5.4466, -5.8394, -3.5957, -5.7503, -5.7503, -5.1864, -5.8394, -5.8284,\n",
      "        -5.8394, -5.7503], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4466, -5.8002, -4.2361, -5.8217, -5.8217, -5.2858, -5.8002, -5.8217,\n",
      "        -5.8002, -5.8217], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04400132596492767\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6046, -4.1047, -4.6295, -4.7552], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.6724, -5.8932, -5.6972, -5.7976, -5.6972, -5.8932, -5.7520, -5.8932,\n",
      "        -5.8762, -5.6724], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.7404, -5.7404, -5.7459, -5.7459, -5.7459, -5.7404, -5.6685, -5.7404,\n",
      "        -5.7404, -5.7404], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.011206249706447124\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-5.7133, -4.2268, -4.8486, -5.9540, -3.4841, -3.9196, -5.9148, -5.8142,\n",
      "        -4.0557, -5.8142], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.7001, -4.7315, -5.1540, -5.6874, -4.1357, -4.5277, -5.7001, -5.6874,\n",
      "        -4.7191, -5.6874], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17318817973136902\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7387, -4.1074, -4.7559, -4.8291], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.8216, -5.9094, -5.9657, -5.9094, -5.8216, -5.9094, -5.7492, -5.5434,\n",
      "        -5.9859, -5.8321], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6767, -5.6900, -5.6900, -5.6900, -5.6767, -5.6900, -5.5831, -5.3015,\n",
      "        -5.6767, -5.6767], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.046828873455524445\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6244, -4.0377, -4.6044, -4.6704], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-3.9263, -5.7882, -5.6837, -5.7882, -4.7407, -5.7740, -3.5195, -5.7565,\n",
      "        -5.8381, -4.8555], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5337, -5.7247, -5.6164, -5.7247, -4.5337, -5.7247, -4.1676, -5.7247,\n",
      "        -5.7320, -5.1347], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09370052814483643\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5781, -4.0368, -4.5347, -4.5711], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.4483, -5.7216, -5.7751, -5.7644, -5.1870, -5.7751, -5.7216, -5.7751,\n",
      "        -5.3216, -5.9938], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7623, -5.7894, -5.7894, -5.7911, -5.0463, -5.7894, -5.7894, -5.7894,\n",
      "        -5.4162, -5.6683], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.024381956085562706\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-5.6848, -5.9085, -5.2669, -5.6848, -5.6922, -5.5655, -4.0246, -5.6922,\n",
      "        -5.6922, -5.3947], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.8553, -5.8553, -5.4588, -5.8553, -5.8372, -5.7249, -4.7702, -5.8372,\n",
      "        -5.8372, -5.4588], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07463289052248001\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9563, -4.4394, -4.9341, -4.8893], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.4140, -5.6995, -5.4391, -4.9438, -4.5028, -5.7022, -4.5790, -5.2974,\n",
      "        -5.7022, -5.9080], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4495, -5.8297, -5.8297, -5.1626, -4.9915, -5.8726, -4.6692, -5.7310,\n",
      "        -5.8726, -5.8726], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07130536437034607\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-5.4935, -5.7683, -5.9072, -5.8590, -5.7683, -5.3605, -5.4935, -5.6397,\n",
      "        -5.7683, -5.6829], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4623, -5.8607, -5.8607, -5.8607, -5.8607, -5.6953, -5.4623, -5.8607,\n",
      "        -5.8607, -5.7761], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01993091031908989\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-5.2215, -5.8572, -5.7214, -3.7365, -5.7214, -5.7026, -5.8876, -5.8572,\n",
      "        -5.9055, -5.8572], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4331, -5.8220, -5.7037, -4.3628, -5.7037, -5.8220, -5.6297, -5.8220,\n",
      "        -5.8220, -5.8220], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05292205139994621\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-5.7466, -3.9910, -5.3213, -5.7074, -5.6715, -5.7639, -5.8989, -3.9910,\n",
      "        -5.0074, -5.6379], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6369, -4.5919, -5.3456, -5.5837, -5.6369, -5.7892, -5.7892, -4.5919,\n",
      "        -5.0247, -5.3456], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08497197926044464\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.1101, -4.4307, -5.2405, -5.3921], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.7565, -5.9954, -5.8087, -5.3149, -4.4307, -5.9954, -4.5013, -5.7565,\n",
      "        -5.9954, -5.7457], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5952, -5.7834, -5.7834, -5.3362, -4.5951, -5.7834, -5.0016, -5.5952,\n",
      "        -5.7834, -5.5625], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04988573491573334\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-5.4673, -5.7317, -5.7515, -5.8192, -5.7317, -5.4673, -3.9994, -5.3310,\n",
      "        -5.2067, -5.6430], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4095, -5.5756, -5.5756, -5.7979, -5.5756, -5.4095, -4.5994, -5.3430,\n",
      "        -5.0697, -5.3430], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.055579811334609985\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-5.7806, -5.6897, -5.7221, -5.9637, -5.6897, -5.8006, -5.9637, -5.9637,\n",
      "        -5.6897, -4.0330], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.8391, -5.5879, -5.5879, -5.8391, -5.5879, -5.8391, -5.8391, -5.8391,\n",
      "        -5.5879, -4.6297], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04565588757395744\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2403, -3.7920, -4.3063, -4.3357], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.7745, -3.7920, -4.8469, -5.9893, -5.6629, -5.6629, -5.6629, -6.0236,\n",
      "        -5.7119, -5.9159], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.8926, -4.4128, -4.4128, -5.8926, -5.6222, -5.6222, -5.6222, -5.6222,\n",
      "        -5.8926, -5.8926], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0796414241194725\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2110, -3.8327, -4.2739, -4.3185], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.7542, -5.6152, -5.6008, -4.0167, -5.6181, -5.6008, -5.3726, -5.8459,\n",
      "        -5.7071, -5.8459], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.9222, -5.6608, -5.6752, -4.6150, -5.6752, -5.6752, -5.5419, -5.9222,\n",
      "        -5.6752, -5.9222], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04439288005232811\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-5.6064, -5.9079, -5.9066, -5.4764, -5.6064, -5.9079, -5.9079, -5.6316,\n",
      "        -5.9079, -5.9079], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.7076, -5.9287, -5.7076, -5.5010, -5.7076, -5.9287, -5.9287, -5.6788,\n",
      "        -5.9287, -5.9287], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.006512614898383617\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-5.8841, -5.7575, -5.4720, -5.8841, -5.1865, -5.8841, -4.9027, -5.8841,\n",
      "        -5.6480, -5.7575], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.9248, -5.9248, -5.5336, -5.9248, -5.3148, -5.9248, -4.4888, -5.9248,\n",
      "        -5.9248, -5.9248], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03307695314288139\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.3960, -4.1371, -4.5304, -4.6215], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.6404, -5.5318, -5.4314, -5.6404, -5.6272, -3.8924, -5.0369, -5.1937,\n",
      "        -5.6981, -4.5210], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.7477, -5.5332, -5.5332, -5.7477, -5.6743, -4.5031, -5.1249, -5.3189,\n",
      "        -5.7477, -4.6500], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.045117683708667755\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-5.8740, -5.6798, -5.7231, -4.5154, -5.4277, -5.7309, -5.6798, -5.8740,\n",
      "        -4.3334, -4.0497], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.8586, -5.7558, -5.7558, -4.6447, -5.5547, -5.8586, -5.7558, -5.8586,\n",
      "        -4.7761, -4.6447], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06122765690088272\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.4254, -4.1568, -4.6530, -4.7217], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.7943, -5.8874, -5.7943, -5.4377, -4.5375, -5.6399, -5.8874, -5.8874,\n",
      "        -4.1568, -5.5783], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.8148, -5.8148, -5.8148, -5.3096, -4.7353, -5.7530, -5.8148, -5.8148,\n",
      "        -4.7411, -5.6406], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04302758723497391\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.4454, -4.1713, -4.7347, -4.7717], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.8931, -5.8384, -5.7769, -4.7464, -5.6960, -5.7055, -5.7769, -5.7784,\n",
      "        -5.3143, -5.9029], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.7829, -5.7829, -5.7564, -5.0613, -5.5135, -5.7564, -5.7564, -5.7564,\n",
      "        -5.5135, -5.7829], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02056947909295559\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-5.7577, -5.7577, -4.6092, -5.8758, -4.4632, -5.9035, -4.4960, -5.9035,\n",
      "        -5.9035, -5.7364], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.7639, -5.7639, -4.6581, -5.7690, -4.7705, -5.7690, -4.6368, -5.7690,\n",
      "        -5.7690, -5.6272], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0194383691996336\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.4930, -4.2174, -4.8607, -4.8443], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.8897, -5.3140, -4.6433, -5.7409, -6.0159, -5.8256, -5.8897, -5.4056,\n",
      "        -5.8256, -4.3494], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.7661, -5.5028, -4.6493, -5.6353, -5.7661, -5.7805, -5.7661, -5.3458,\n",
      "        -5.7805, -4.6349], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.022891147062182426\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.5293, -4.2483, -4.9005, -4.8544], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.5683, -4.0728, -5.8626, -4.0408, -4.7195, -4.3766, -5.8626, -6.0353,\n",
      "        -4.0408, -5.7253], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6368, -4.6655, -5.7768, -4.6368, -4.6368, -4.8235, -5.7768, -5.7768,\n",
      "        -4.6368, -5.6497], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1360081136226654\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.5509, -4.2626, -4.8955, -4.8307], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.5509, -5.8830, -5.9154, -4.8402, -5.3426, -5.7252, -4.2626, -5.3184,\n",
      "        -5.4657, -5.8386], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8364, -5.8083, -5.8083, -5.0860, -5.5404, -5.8140, -4.8364, -5.4068,\n",
      "        -5.5404, -5.8083], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.054945290088653564\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.5940, -4.2757, -4.8994, -4.8290], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.6987, -5.3746, -5.3746, -4.2757, -4.0886, -4.4393, -4.0886, -5.9247,\n",
      "        -5.3575, -5.6987], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6946, -5.5400, -5.5400, -4.8481, -4.6797, -4.7496, -4.6797, -5.8372,\n",
      "        -5.5255, -5.6946], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12134383618831635\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-5.5150, -5.8530, -6.0185, -4.9111, -5.7627, -5.5032, -5.8530, -5.8530,\n",
      "        -4.7182, -5.8530], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4630, -5.8827, -5.8827, -4.7796, -5.8415, -5.5385, -5.8827, -5.8827,\n",
      "        -4.7732, -5.8827], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.005244930740445852\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.6949, -4.3142, -4.8999, -4.8413], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.8431, -5.8793, -5.8793, -5.5326, -5.8286, -5.2256, -5.8793, -5.7649,\n",
      "        -5.5542, -5.4718], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7800, -5.9264, -5.9264, -5.5342, -5.8532, -5.4932, -5.9264, -5.8532,\n",
      "        -5.4932, -5.5456], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.009986626915633678\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-4.5355, -5.7439, -4.2369, -5.7871, -5.7871, -5.9238, -5.2791, -5.0223,\n",
      "        -4.3091, -5.7871], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8188, -5.7512, -4.8132, -5.8468, -5.8468, -5.9486, -5.4975, -5.0820,\n",
      "        -4.8782, -5.8468], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07988961786031723\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.7802, -4.3116, -4.8912, -4.8996], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.5470, -4.3116, -5.8270, -5.8905, -5.3312, -5.8915, -5.9819, -5.8270,\n",
      "        -5.9819, -5.8270], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8211, -4.8804, -5.8446, -5.7663, -5.4951, -5.9721, -5.9721, -5.8446,\n",
      "        -5.9721, -5.8446], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.044858988374471664\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-5.0106, -6.0462, -5.5460, -5.9279, -4.3182, -5.8740, -4.5786, -6.0462,\n",
      "        -4.3164, -5.9082], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1933, -5.9914, -5.4970, -5.7839, -4.8864, -5.8398, -4.8065, -5.9914,\n",
      "        -4.8848, -5.7839], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07769496738910675\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8688, -4.3065, -4.9052, -4.9570], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.9641, -6.1751, -5.8807, -6.0982, -5.1386, -5.8807, -6.0982, -4.8626,\n",
      "        -4.3065, -4.8395], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.0338, -5.8571, -5.8571, -6.0338, -5.1836, -5.8571, -6.0338, -4.9158,\n",
      "        -4.8759, -4.9407], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04546818509697914\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9030, -4.3528, -4.9300, -5.0033], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.9468, -5.9700, -4.3924, -4.8477, -6.1263, -6.0261, -5.9700, -5.9468,\n",
      "        -5.9050, -5.4422], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.8980, -5.8801, -4.9532, -4.9955, -6.0722, -6.0722, -5.8801, -5.8980,\n",
      "        -5.8980, -5.5647], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03773189336061478\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-5.9992, -5.9730, -6.3148, -5.9992, -5.9995, -6.1390, -5.9730, -5.9992,\n",
      "        -6.0357, -5.6403], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.9393, -5.9514, -5.9393, -5.9393, -6.1188, -6.1188, -5.9514, -5.9393,\n",
      "        -6.1188, -5.6014], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.017574522644281387\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-5.5795, -4.5114, -6.1209, -6.0440, -6.0361, -5.9794, -5.9749, -5.6577,\n",
      "        -5.8471, -4.5635], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6520, -5.0602, -6.1748, -6.0233, -6.1748, -6.0216, -6.0216, -5.6520,\n",
      "        -6.0216, -5.1072], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0659080296754837\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-4.5697, -4.9641, -5.7578, -5.8383, -5.9960, -6.1197, -6.1197, -5.0108,\n",
      "        -5.9960, -4.9641], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1127, -4.9358, -5.6637, -5.6953, -6.0786, -6.2142, -6.2142, -5.1127,\n",
      "        -6.0786, -4.9358], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03676281124353409\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-5.0223, -6.0973, -6.0900, -4.6152, -5.7014, -6.1373, -5.1177, -6.1373,\n",
      "        -5.2546, -5.0745], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9701, -6.2438, -6.1251, -5.1537, -5.7349, -6.2438, -4.9701, -6.2438,\n",
      "        -5.5200, -5.1134], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04329555854201317\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-6.0887, -6.1219, -5.0888, -5.7998, -5.1197, -6.1219, -6.1219, -4.6477,\n",
      "        -4.6477, -4.5961], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.1480, -6.2513, -5.1829, -5.7035, -5.1365, -6.2513, -6.2513, -5.1829,\n",
      "        -5.1829, -5.1365], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0937102884054184\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3494, -4.8290, -5.3850, -5.5268], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.2117, -6.1767, -6.2308, -6.1516, -4.8290, -6.1516, -6.2308, -6.2308,\n",
      "        -6.3095, -4.6198], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.2249, -6.2544, -6.2544, -6.1656, -5.1578, -6.1656, -6.2544, -6.2544,\n",
      "        -6.2249, -5.1578], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.041304346174001694\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.1826, -4.6305, -5.2331, -5.2707], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.2138, -5.7983, -5.3017, -5.7983, -6.2883, -6.2883, -6.3617, -4.7220,\n",
      "        -5.9085, -5.2331], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2498, -5.7775, -5.5612, -5.7775, -6.2602, -6.2602, -6.1842, -5.2498,\n",
      "        -5.6067, -5.2380], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04722754284739494\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.1929, -4.6506, -5.2823, -5.3229], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.3380, -4.6506, -4.9025, -4.6506, -5.3538, -4.7695, -5.1598, -6.2841,\n",
      "        -4.7695, -4.9025], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.2697, -5.1856, -5.1856, -5.1856, -5.1856, -5.2926, -5.2926, -6.2697,\n",
      "        -5.2926, -5.1856], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13305500149726868\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.1990, -4.6840, -5.3140, -5.3625], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.6840, -5.8982, -6.2865, -6.3883, -6.3085, -5.1129, -6.3575, -6.1513,\n",
      "        -6.1533, -4.6713], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2156, -5.7809, -6.3084, -6.3084, -6.3595, -5.2042, -6.2750, -6.2750,\n",
      "        -5.7809, -5.2042], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07588940113782883\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-5.2116, -6.5065, -5.3881, -6.4133, -4.8969, -6.4034, -5.3111, -4.8963,\n",
      "        -6.4034, -6.3396], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4067, -6.3447, -5.6112, -6.3561, -5.4072, -6.3447, -5.4067, -5.4067,\n",
      "        -6.3447, -6.4345], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06631596386432648\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.6590, -5.2128, -5.7979, -5.9291], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.4582, -6.5414, -6.3858, -4.9755, -5.8186, -6.0963, -4.9513, -5.4896,\n",
      "        -4.9513, -5.2463], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.4173, -6.4173, -6.3991, -5.4780, -5.7217, -5.9194, -5.4562, -5.3245,\n",
      "        -5.4562, -5.3457], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08573164790868759\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3396, -4.9074, -5.4388, -5.5035], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.4593, -5.8899, -6.1283, -5.4893, -6.4593, -4.9074, -6.4593, -6.5065,\n",
      "        -4.9096, -5.0820], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.4677, -5.8537, -5.8537, -5.4167, -6.4677, -5.4167, -6.4677, -6.5178,\n",
      "        -5.4186, -5.5738], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08426547050476074\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-5.2059, -6.3794, -6.5334, -6.5670, -5.5067, -5.0300, -6.5434, -5.1058,\n",
      "        -5.6824, -5.4098], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6853, -6.5521, -6.6491, -6.6491, -6.0215, -5.5270, -6.6313, -5.5953,\n",
      "        -5.9044, -5.5953], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11227764934301376\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-5.4583, -5.1778, -5.5034, -5.6030], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.1619, -5.7718, -5.5718, -5.3210, -5.1778, -6.5103, -6.6031, -6.6031,\n",
      "        -6.1619, -5.1434], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.1947, -5.8253, -5.5898, -5.7889, -5.6601, -6.6268, -6.7294, -6.7294,\n",
      "        -6.1947, -5.6290], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07381276786327362\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-5.2461, -6.5638, -6.1336, -5.2461, -6.6787, -6.4420, -5.2452, -6.5720,\n",
      "        -6.5720, -6.5720], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.7215, -6.6895, -6.1790, -5.7215, -6.7782, -6.6895, -5.7207, -6.6895,\n",
      "        -6.6895, -6.6895], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0808529406785965\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-5.9671, -5.7058, -6.0281, -6.2526], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.5213, -6.6612, -6.7988, -5.8163, -6.3567, -6.6612, -5.5853, -6.3567,\n",
      "        -5.5853, -6.7988], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.8734, -6.7211, -6.8734, -5.7042, -6.3302, -6.7211, -5.7654, -6.3302,\n",
      "        -5.7654, -6.8734], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02211645245552063\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-5.8099, -5.7635, -5.3729, -5.3164, -6.4136, -5.5312, -5.8099, -6.9240,\n",
      "        -5.3164, -5.5312], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.7847, -5.9781, -5.8356, -5.7847, -6.2935, -5.9781, -5.7847, -6.8837,\n",
      "        -5.7847, -5.9781], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11155538260936737\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-5.7779, -5.3648, -5.7413, -5.9823], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.8062, -6.4287, -7.0453, -6.8849, -7.0453, -6.8849, -5.8848, -7.0453,\n",
      "        -7.0453, -7.0453], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.9110, -6.3517, -6.9110, -6.7328, -6.9110, -6.7328, -6.0065, -6.9110,\n",
      "        -6.9110, -6.9110], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.016809921711683273\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-6.6522, -6.6522, -6.9774, -6.8866, -6.4877, -6.1977, -5.6121, -5.9656,\n",
      "        -7.0908, -6.0342], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.7785, -6.7785, -6.7785, -6.9647, -6.4619, -6.2080, -6.0509, -6.0509,\n",
      "        -6.9647, -5.8944], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03135900944471359\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.1260, -5.5978, -6.0551, -6.2572], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-7.1187, -6.6775, -6.9740, -6.9740, -5.9762, -7.0139, -5.6598, -5.5225,\n",
      "        -7.1187, -5.5978], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.0165, -6.5891, -6.8334, -6.8334, -5.9702, -6.9162, -6.0938, -5.9702,\n",
      "        -7.0165, -6.0380], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0660426989197731\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.2176, -5.7109, -6.1362, -6.2903], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-7.0138, -7.1144, -6.9826, -6.5344, -6.1593, -5.6379, -5.7109, -5.7109,\n",
      "        -6.2903, -6.0407], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.9949, -7.1003, -6.9186, -6.4366, -6.2735, -6.0741, -6.1398, -6.1398,\n",
      "        -6.3477, -6.0735], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.058982063084840775\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.3187, -5.8370, -6.2274, -6.3354], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.9934, -7.1149, -7.0739, -6.9934, -7.0168, -6.3354, -6.0890, -6.6757,\n",
      "        -6.9934, -5.8296], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.0081, -7.1885, -7.1885, -7.0081, -7.0781, -6.4315, -6.2533, -6.6915,\n",
      "        -7.0081, -6.2467], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02333800308406353\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.4179, -5.9321, -6.3236, -6.3916], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.8928, -6.0956, -6.7799, -7.0964, -5.9321, -5.9321, -7.0274, -5.9321,\n",
      "        -6.4179, -5.8928], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.3035, -6.3389, -6.9081, -7.2461, -6.3389, -6.3389, -7.0789, -6.3389,\n",
      "        -6.3035, -6.3035], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09476171433925629\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.5042, -6.0147, -6.4071, -6.4475], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-7.0804, -6.0380, -7.2603, -6.5042, -6.6677, -7.0809, -6.0380, -6.0325,\n",
      "        -7.2987, -6.0325], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.1535, -6.4292, -7.1535, -6.4292, -6.5668, -7.2041, -6.4292, -6.4292,\n",
      "        -7.2041, -6.4292], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06776589900255203\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-6.0945, -7.1398, -6.9326, -6.1171, -6.1171, -7.0935, -7.1496, -6.1171,\n",
      "        -6.0519, -7.3094], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.4850, -7.3654, -6.9128, -6.5054, -6.5054, -7.2393, -7.2393, -6.5054,\n",
      "        -6.4467, -7.2393], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08462174236774445\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.6410, -6.1725, -6.5663, -6.5727], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-7.2084, -6.6353, -6.2081, -6.1725, -6.5398, -7.0654, -6.6956, -7.2516,\n",
      "        -6.9961, -6.2081], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.3220, -6.5872, -6.5872, -6.5552, -6.5552, -7.1015, -6.5188, -7.4131,\n",
      "        -6.9912, -6.5872], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05082342028617859\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-6.3063, -7.3186, -7.3602, -6.8696, -7.3186, -7.3980, -7.3464, -6.6304,\n",
      "        -6.4881, -6.5808], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.6757, -7.4048, -7.2695, -6.9674, -7.4048, -7.4579, -7.4048, -6.5734,\n",
      "        -6.6273, -6.5734], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.019874192774295807\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.9936, -6.5844, -7.0270, -7.0380], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.3121, -6.7621, -7.3973, -6.3086, -7.4165, -6.3919, -7.5570, -7.4154,\n",
      "        -7.4165, -7.1918], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.6808, -6.8032, -7.2512, -6.6778, -7.4726, -6.7527, -7.4726, -7.5299,\n",
      "        -7.4726, -7.1321], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04556111618876457\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.7953, -6.3696, -6.8832, -6.8012], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.3696, -7.3941, -7.5329, -6.7546, -7.5275, -6.3696, -7.2688, -6.3696,\n",
      "        -6.3408, -7.6273], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.7326, -7.5187, -7.6088, -6.7067, -7.5419, -6.7326, -7.1996, -6.7326,\n",
      "        -6.7067, -7.5419], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0565163716673851\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-7.6600, -7.7923, -6.4406, -6.5742, -7.2178, -6.4406, -6.8131, -7.6481,\n",
      "        -7.6481, -7.6600], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.6864, -7.5487, -6.7965, -6.9167, -7.0961, -6.7965, -6.7817, -7.6113,\n",
      "        -7.6113, -7.6864], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04499996453523636\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.2406, -6.8911, -7.3871, -7.4191], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-7.0320, -6.5764, -6.5307, -6.5307, -7.7560, -6.2895, -7.4347, -7.5392,\n",
      "        -7.7920, -7.8762], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.0111, -6.9188, -6.8776, -6.8776, -7.6912, -6.9188, -7.3522, -7.6029,\n",
      "        -7.6912, -7.6029], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08543829619884491\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.9737, -6.6467, -7.1250, -7.0956], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-7.8345, -7.8498, -6.8038, -7.6071, -7.8345, -7.8385, -7.8345, -7.8498,\n",
      "        -6.6362, -7.2687], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.7878, -7.7878, -7.1234, -7.6916, -7.7878, -7.8575, -7.7878, -7.7878,\n",
      "        -6.9726, -7.5560], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03195897489786148\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-7.1798, -8.1921, -6.7524, -7.8899, -7.4883, -7.1599, -7.3682, -7.9182,\n",
      "        -6.9217, -7.4557], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.0247, -7.9469, -7.0772, -7.8742, -7.4439, -7.0772, -7.2575, -7.8742,\n",
      "        -7.2295, -7.3551], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03177651762962341\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-7.8129, -7.9699, -7.9081, -7.9699, -7.4519, -7.5048, -7.9699, -7.0616,\n",
      "        -7.0616, -8.0505], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.8344, -7.9794, -7.9794, -7.9794, -7.3921, -7.8344, -7.9794, -7.3554,\n",
      "        -7.3554, -7.8758], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03212729096412659\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-6.9691, -7.1876, -8.0612, -7.2793, -7.2916, -8.0468, -7.1876, -7.1876,\n",
      "        -7.7835, -8.0104], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.2722, -7.4689, -8.1753, -7.2828, -7.6805, -7.9683, -7.4689, -7.4689,\n",
      "        -7.9683, -8.0652], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05367008596658707\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-7.4469, -7.2911, -7.4767, -7.6066], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-7.0820, -8.0932, -8.1521, -7.2911, -7.0727, -7.6957, -7.6989, -7.4767,\n",
      "        -8.0984, -8.0984], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.3654, -8.0366, -8.1225, -7.5620, -7.3654, -7.9059, -7.7965, -7.3654,\n",
      "        -8.1225, -8.1225], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.031069615855813026\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-7.5765, -7.3621, -7.5209, -7.7125], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-7.4976, -7.9770, -7.9711, -7.9584, -7.3218, -7.9803, -7.3621, -8.1821,\n",
      "        -7.5209, -8.2578], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.4273, -8.0876, -8.2938, -7.9503, -7.3591, -8.1625, -7.6259, -8.1625,\n",
      "        -7.4273, -8.2938], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02359987422823906\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-7.7133, -7.3924, -7.5655, -7.8136], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-8.3926, -8.1033, -7.3924, -7.9236, -8.0110, -8.1305, -7.0226, -8.0110,\n",
      "        -7.1916, -8.0944], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.1722, -7.8044, -7.6531, -7.9943, -8.1722, -8.1237, -7.3203, -8.1722,\n",
      "        -7.4724, -8.1237], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0431310310959816\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-7.2371, -8.1939, -7.2371, -8.2711, -7.0564, -8.0996, -8.3823, -8.3823,\n",
      "        -7.4278, -7.7623], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.5134, -8.1511, -7.5134, -8.2688, -7.3507, -8.1888, -8.2688, -8.2688,\n",
      "        -7.4231, -7.7267], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.027615174651145935\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-8.5440, -7.8323, -8.1809, -7.9129, -8.4268, -8.4037, -7.4640, -8.5440,\n",
      "        -8.4037, -8.2683], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.2246, -7.7176, -8.2246, -7.7133, -8.2246, -8.2667, -7.7176, -8.2246,\n",
      "        -8.2667, -8.1961], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.040691956877708435\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.0601, -7.5783, -7.8344, -8.0050], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-8.3091, -8.2438, -7.2485, -8.3311, -8.4220, -7.9552, -7.4255, -8.2438,\n",
      "        -7.5977, -7.9444], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.3578, -8.3110, -7.5237, -7.8954, -8.3578, -7.8925, -7.6829, -8.3110,\n",
      "        -7.8380, -7.8380], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04202670231461525\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-8.1653, -7.8099, -7.7767, -7.7583, -8.4206, -8.7270, -7.8105, -8.4220,\n",
      "        -7.3768, -7.7583], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.1954, -7.9232, -7.9583, -7.9825, -8.5109, -8.4030, -7.6391, -8.5109,\n",
      "        -7.6391, -7.9825], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.036650825291872025\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.8524, -7.5124, -7.6526, -7.6251], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-7.8955, -7.5124, -8.4645, -8.3517, -7.6692, -7.8955, -8.4442, -8.5016,\n",
      "        -8.2694, -8.2694], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.1059, -7.7611, -8.2453, -8.4424, -7.9022, -8.1059, -8.5998, -8.5998,\n",
      "        -8.4424, -8.4424], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03548060357570648\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-8.4557, -8.2404, -8.2404, -8.0457, -8.4557, -8.5946, -8.3081, -8.2404,\n",
      "        -8.4557, -8.5798], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.6101, -8.4164, -8.4164, -8.2353, -8.6101, -8.6101, -8.4773, -8.4164,\n",
      "        -8.6101, -8.6101], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.023017605766654015\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-7.8941, -8.1163, -8.6737, -8.2693, -8.2532, -8.4813, -8.4813, -8.6960,\n",
      "        -8.7062, -8.1313], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.1303, -8.3182, -8.3732, -8.1047, -8.4278, -8.6331, -8.6331, -8.6331,\n",
      "        -8.6331, -8.3046], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03299100697040558\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.0914, -7.9490, -8.0690, -7.9440], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-8.2094, -8.8467, -8.5543, -8.5266, -7.9490, -8.2978, -8.2570, -8.0612,\n",
      "        -8.0218, -8.0849], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.3055, -8.5944, -8.5944, -8.6740, -8.2047, -8.4680, -8.0825, -8.2047,\n",
      "        -8.2047, -7.9528], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.029247760772705078\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-8.6084, -8.3058, -8.2421, -8.2580, -8.8850, -8.5006, -8.5006, -8.3058,\n",
      "        -8.8850, -8.1400], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.6506, -8.3247, -8.4639, -8.3247, -8.4929, -8.6506, -8.6506, -8.3247,\n",
      "        -8.4929, -8.2480], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.042017024010419846\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-8.2317, -8.7914, -8.4487, -8.9063, -8.8060, -8.2317, -8.5617, -8.3711,\n",
      "        -8.4487, -8.8060], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.3205, -8.7055, -8.6038, -8.7320, -8.7320, -8.3205, -8.7055, -8.3770,\n",
      "        -8.6038, -8.7320], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.013330424204468727\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.2586, -8.3906, -8.4105, -8.3391], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-8.3267, -8.6299, -8.1069, -8.4425, -8.3267, -8.2586, -8.2124, -8.7650,\n",
      "        -8.6299, -8.2941], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.3912, -8.7669, -8.4327, -8.4397, -8.3912, -8.4405, -8.4397, -8.7669,\n",
      "        -8.7669, -8.4397], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02579689957201481\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-8.2140, -8.4839, -8.4405, -8.6807], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-8.4564, -8.8239, -8.7392, -8.4564, -8.7027, -8.4199, -8.5050, -8.4108,\n",
      "        -8.4564, -8.2389], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.4010, -8.8324, -8.8190, -8.4010, -8.8324, -8.3802, -8.4150, -8.8324,\n",
      "        -8.4010, -8.4150], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02509482577443123\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-8.7577, -8.8149, -8.7577, -8.5556, -8.1651, -9.1656, -8.3710, -8.2136,\n",
      "        -8.8811, -8.6708], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.8037, -8.8024, -8.8037, -8.4266, -8.4107, -8.3682, -8.5209, -8.1582,\n",
      "        -8.8037, -8.8824], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07935426384210587\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-8.3319, -8.8334, -8.8251, -8.8251, -8.4423, -8.3810, -8.8251, -8.7613,\n",
      "        -8.5177, -8.5374], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.4261, -8.8300, -8.8300, -8.8300, -8.5706, -8.2640, -8.8300, -8.4401,\n",
      "        -8.4261, -8.5706], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.015178379602730274\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-8.4501, -8.7084, -8.5597, -8.7824], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-8.8625, -8.9426, -8.8625, -8.3973, -8.5827, -8.4137, -8.3483, -8.9037,\n",
      "        -8.5550, -8.5550], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.8775, -8.8910, -8.8775, -8.5846, -8.5974, -8.5974, -8.5974, -8.9079,\n",
      "        -8.5846, -8.5846], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.013598388060927391\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-8.4398, -8.6946, -8.5606, -8.8077], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-8.4745, -8.5768, -8.5283, -8.5283, -8.4492, -8.9008, -8.5283, -8.5768,\n",
      "        -8.8345, -8.4528], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.2767, -8.6076, -8.5238, -8.5238, -8.6042, -8.8762, -8.5238, -8.6076,\n",
      "        -8.8991, -8.5238], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.007496907375752926\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-8.5584, -8.8294, -8.9375, -8.7647, -8.9375, -8.8532, -8.7698, -8.9460,\n",
      "        -8.6097, -8.8323], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.5606, -8.6658, -8.8882, -8.8846, -8.8882, -8.9097, -8.8882, -8.8846,\n",
      "        -8.6330, -8.9097], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.007354319095611572\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-8.6001, -8.7368, -8.6899, -9.2188], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-8.4771, -8.5879, -8.8172, -8.7709, -8.9665, -8.7742, -8.4471, -8.4538,\n",
      "        -8.9643, -8.5791], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.6663, -8.6024, -8.9129, -8.8646, -8.7401, -8.5937, -8.6024, -8.7401,\n",
      "        -8.9129, -8.4091], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02753925323486328\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-8.7274, -8.7785, -8.7311, -9.2669], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-8.9631, -8.6042, -8.7819, -8.8686, -8.9631, -8.9631, -8.6078, -8.4367,\n",
      "        -8.6078, -8.6042], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.9470, -8.6201, -8.9037, -8.8273, -8.9470, -8.9470, -8.6769, -8.3800,\n",
      "        -8.6769, -8.6201], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0030572409741580486\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-8.8414, -8.8257, -8.7687, -9.3084], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.9215, -8.7322, -8.7731, -8.9571, -8.6292, -8.9394, -8.6556, -8.5991,\n",
      "        -8.6766, -8.8414], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.8958, -8.7341, -8.8958, -8.9378, -8.6332, -8.9378, -8.6572, -8.6572,\n",
      "        -8.7341, -8.8918], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.002532863523811102\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.8132, -8.7378, -8.6414, -8.9608], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.5840, -8.6414, -8.5029, -8.4844, -9.0315, -9.0315, -8.8132, -8.6628,\n",
      "        -8.7136, -8.7378], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.5288, -8.6333, -8.3772, -8.6307, -8.8820, -8.8820, -8.9134, -8.6359,\n",
      "        -8.7590, -8.7590], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.009833580814301968\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.7282, -8.7031, -8.5412, -8.9163], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.9227, -8.8258, -8.7006, -8.6291, -9.0017, -8.6584, -8.5718, -8.9227,\n",
      "        -9.0017, -8.9953], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.9407, -8.6894, -8.6894, -8.9407, -8.9015, -8.6669, -8.5602, -8.9407,\n",
      "        -8.9015, -8.6871], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.023177968338131905\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-8.5513, -8.9383, -8.5469, -8.9454, -8.8681, -9.0323, -8.8681, -8.7082,\n",
      "        -8.7616, -8.6573], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.6962, -8.9616, -8.6492, -8.7916, -8.9813, -9.0243, -8.9813, -8.8535,\n",
      "        -8.7965, -8.7916], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.012170353904366493\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.8959, -8.7265, -8.7863, -8.9297], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-8.6321, -8.8856, -8.7863, -8.5175, -8.6376, -8.8856, -8.7022, -9.1226,\n",
      "        -8.8856, -8.5175], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-9.0210, -8.9961, -8.8539, -8.6657, -8.7677, -8.9961, -8.8320, -9.0210,\n",
      "        -8.9961, -8.6657], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.028040770441293716\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-9.0414, -8.6464, -8.9608, -9.0168], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-8.6109, -8.8860, -8.2833, -8.7989, -8.9863, -8.7336, -9.0485, -8.5563,\n",
      "        -8.7953, -8.4466], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.8739, -8.6019, -8.4550, -8.8678, -8.9053, -8.7818, -8.9072, -8.7007,\n",
      "        -8.4550, -8.6019], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03737204149365425\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-8.3707, -9.0510, -8.9382, -8.4540, -9.1754, -8.8143, -9.4445, -8.7041,\n",
      "        -8.4540, -8.7041], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.4964, -8.9027, -8.8595, -8.6086, -8.6283, -8.6283, -8.8364, -8.8374,\n",
      "        -8.6086, -8.8374], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08310271054506302\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-9.1275, -8.8983, -8.9342, -8.8998, -8.8319, -8.8357, -8.6624, -8.5404,\n",
      "        -8.7452, -8.8983], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.8962, -8.8723, -8.8723, -8.9696, -8.7119, -8.9236, -8.6864, -8.6864,\n",
      "        -8.7757, -8.8723], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01084949728101492\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.9875, -8.8089, -8.9254, -9.1415], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-8.8848, -8.4759, -8.8089, -8.8848, -8.7786, -9.0442, -8.8567, -8.6352,\n",
      "        -8.7930, -8.8848], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.9920, -8.6661, -8.8114, -8.9920, -8.9280, -8.9792, -8.9008, -8.7717,\n",
      "        -8.7717, -8.9920], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.011820551939308643\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-8.6734, -8.6149, -8.9162, -8.8478, -8.8551, -8.5581, -8.4799, -8.8407,\n",
      "        -8.7136, -9.0350], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.8061, -8.8423, -8.8061, -8.8341, -8.6319, -8.7190, -8.6319, -8.9275,\n",
      "        -8.8423, -9.0487], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.020473146811127663\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.8681, -8.8941, -8.9462, -9.2239], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-8.9624, -8.9943, -8.5365, -8.7107, -8.7693, -8.9158, -8.9943, -9.1039,\n",
      "        -8.7543, -9.1363], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7611, -8.9635, -8.6828, -8.8397, -8.8924, -8.9781, -8.9635, -8.9264,\n",
      "        -8.8924, -8.9813], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.017406677827239037\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-8.8177, -8.9030, -8.8948, -8.9895, -8.7845, -8.8962, -8.9895, -8.9030,\n",
      "        -8.8962, -8.7845], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.9040, -8.9389, -8.9068, -8.9389, -8.8580, -8.9389, -8.9389, -8.9389,\n",
      "        -8.9389, -8.8580], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0029705199413001537\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-8.6698, -8.9455, -9.0782, -9.0544, -8.8820, -9.3565, -8.9766, -8.9775,\n",
      "        -8.8795, -8.9775], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.8028, -8.8676, -8.8859, -8.7806, -8.8274, -8.8859, -8.8859, -8.8676,\n",
      "        -8.8298, -8.8676], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03950504958629608\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-9.0995, -9.2650, -9.2991, -9.3625], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-8.8162, -8.9079, -8.8938, -8.9108, -9.0995, -8.7428, -8.8642, -9.1698,\n",
      "        -8.7233, -8.9108], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.8510, -8.8864, -8.8457, -8.8436, -8.8693, -8.8457, -8.6963, -8.8510,\n",
      "        -9.1570, -8.8436], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03945683315396309\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-8.8058, -8.9074, -8.8544, -9.1221], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-8.7653, -8.7801, -8.9286, -8.9286, -8.8068, -8.7512, -8.8262, -8.8731,\n",
      "        -8.7669, -8.7801], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7675, -8.9021, -8.8902, -8.8902, -8.9261, -8.8902, -8.9261, -8.8761,\n",
      "        -9.0565, -8.9021], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.016015607863664627\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-8.8543, -8.9111, -8.7758, -9.0833], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.9795, -8.7413, -8.8667, -8.7781, -8.7842, -9.1920, -8.7885, -8.7842,\n",
      "        -8.7842, -8.6743], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.9003, -8.7997, -8.7997, -8.9003, -8.8506, -8.8506, -8.7155, -8.8506,\n",
      "        -8.8506, -8.6808], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.016422679647803307\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-9.1820, -9.1910, -8.9944, -9.1343], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.9508, -8.6699, -8.7745, -9.1343, -8.7745, -8.7439, -8.6556, -8.7893,\n",
      "        -8.7621, -8.6384], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.6192, -8.6977, -8.8971, -8.8971, -8.8971, -9.0120, -8.6192, -8.7346,\n",
      "        -8.8120, -8.7868], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02977917529642582\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-9.1632, -9.1735, -8.8856, -9.1508], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.7559, -8.8849, -8.6406, -8.9638, -8.7541, -9.1332, -8.7541, -8.9613,\n",
      "        -8.6452, -8.7973], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.9443, -8.5852, -8.5852, -8.7885, -8.7654, -9.0018, -8.7654, -8.9971,\n",
      "        -8.6663, -8.8852], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.018610302358865738\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-9.2060, -9.1066, -8.8612, -9.1554], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-9.1554, -8.8035, -8.7303, -8.6451, -9.1544, -8.8296, -8.9181, -8.8612,\n",
      "        -8.8296, -8.8035], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7484, -8.8958, -8.6441, -8.6597, -8.9061, -8.8824, -8.9061, -8.9751,\n",
      "        -8.8824, -8.8958], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.027062907814979553\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-9.1942, -9.0123, -8.8594, -9.1223], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-9.1223, -8.8341, -8.5878, -8.8306, -8.6563, -8.8076, -8.7932, -8.9252,\n",
      "        -8.6802, -9.0169], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7950, -8.9074, -8.6612, -8.9735, -8.6588, -8.9074, -8.7950, -8.8692,\n",
      "        -8.6588, -8.9074], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.016388345509767532\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-9.1565, -8.9013, -8.8705, -9.0390], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.8096, -8.9131, -8.5933, -9.0114, -8.8945, -8.8945, -8.6998, -8.8461,\n",
      "        -8.6071, -8.8945], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.8913, -8.9706, -8.7107, -8.7923, -8.9901, -8.9901, -8.9528, -8.9834,\n",
      "        -8.7574, -8.9901], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.020470863208174706\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-9.1440, -9.0189, -8.8620, -8.7420, -8.5969, -8.5613, -8.8731, -8.8841,\n",
      "        -8.9255, -8.5613], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7746, -8.7746, -8.9349, -8.8906, -8.7139, -8.7051, -8.9375, -8.7746,\n",
      "        -9.0139, -8.7051], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.030254101380705833\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.7640, -8.4656, -8.5084, -8.7032], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-8.9631, -8.7050, -8.8919, -8.9584, -8.8425, -8.9399, -9.1818, -8.7492,\n",
      "        -8.8512, -8.8757], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-9.0266, -8.7578, -8.9582, -8.9661, -8.9661, -8.9661, -8.9582, -8.8743,\n",
      "        -8.9661, -8.9582], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.011288688518106937\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.9862, -8.7296, -8.6729, -8.8225], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.8285, -8.6082, -8.6082, -9.0103, -8.8686, -8.8639, -8.8639, -8.7578,\n",
      "        -8.6929, -8.6919], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-9.0478, -8.7128, -8.7128, -9.0478, -8.9817, -8.9651, -8.9651, -8.8016,\n",
      "        -8.7645, -8.8283], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.013031339272856712\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.9303, -8.7119, -8.6517, -8.9637], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.9557, -8.7826, -9.0641, -8.8890, -9.0201, -8.7826, -8.9557, -8.8239,\n",
      "        -9.0641, -8.6232], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.9083, -8.7609, -8.9436, -8.9083, -8.7273, -8.7609, -8.9083, -8.9083,\n",
      "        -8.9436, -8.7865], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.015434217639267445\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.9070, -8.7273, -8.6665, -8.9899], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.8429, -8.8209, -9.0853, -8.8355, -9.0853, -8.7177, -8.5910, -9.0679,\n",
      "        -8.7177, -8.8870], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.9119, -8.7955, -8.9519, -8.9519, -8.9519, -8.7180, -8.7100, -9.0020,\n",
      "        -8.7180, -8.9119], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.007366628851741552\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.8872, -8.7358, -8.7173, -8.9979], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.7465, -8.7357, -9.1491, -8.4740, -9.0868, -8.7239, -8.8281, -8.7465,\n",
      "        -8.8547, -9.0868], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7812, -8.7203, -8.7099, -8.8599, -8.9940, -8.7812, -8.9940, -8.7812,\n",
      "        -8.7721, -8.9940], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03993409499526024\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.8757, -8.7829, -8.7874, -8.9565], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-9.0579, -8.7829, -8.8757, -9.0515, -8.5461, -8.9798, -8.6569, -8.6119,\n",
      "        -9.0579, -8.7781], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7507, -8.9047, -8.9003, -8.7729, -8.9003, -9.0050, -8.8113, -8.7507,\n",
      "        -8.7507, -8.9047], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0466863177716732\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.8718, -8.7905, -8.8913, -8.8921], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-8.9147, -8.9147, -8.9121, -8.7081, -9.0161, -8.8626, -9.0161, -8.9932,\n",
      "        -8.8202, -8.9966], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-9.0233, -9.0233, -8.8373, -8.8373, -8.8066, -8.9764, -8.8066, -8.9764,\n",
      "        -9.0209, -9.0233], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01878645084798336\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-9.2002, -8.7925, -8.9064, -9.1038, -8.8905, -8.9064, -8.9318, -8.6796,\n",
      "        -8.7844, -8.7498], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.9134, -8.8186, -8.8711, -8.8987, -8.8199, -8.8711, -9.0386, -8.8186,\n",
      "        -8.9059, -8.7801], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.017883818596601486\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-9.0634, -8.8774, -8.9386, -8.7483, -8.9386, -9.0422, -9.0634, -8.6881,\n",
      "        -8.2151, -8.7864], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-9.0695, -8.9897, -8.8588, -8.7321, -8.8588, -8.9897, -9.0695, -8.8193,\n",
      "        -8.9013, -8.8681], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.052315086126327515\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-8.9117, -8.5839, -8.7346, -8.4829, -8.9692, -8.9788, -8.8637, -8.8766,\n",
      "        -8.5143, -8.5839], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.8257, -8.7255, -8.6338, -8.8308, -8.6346, -8.7255, -8.7367, -8.6629,\n",
      "        -8.8257, -8.7255], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05134809762239456\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.9227, -8.8877, -8.9967, -8.7129], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-8.9950, -8.9950, -8.5546, -8.6956, -8.5546, -8.8877, -8.8370, -8.7740,\n",
      "        -8.6145, -8.9227], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.8609, -8.8609, -8.6991, -8.8260, -8.6991, -8.7530, -8.8609, -8.7530,\n",
      "        -8.8416, -8.8238], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.017531823366880417\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.7217, -8.6703, -8.8127, -8.6080], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-8.8293, -8.5631, -8.8846, -8.9294, -8.8679, -8.6918, -8.5631, -8.7671,\n",
      "        -9.1248, -8.6080], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7068, -8.7068, -8.8226, -8.6760, -8.9110, -8.8516, -8.7068, -8.6760,\n",
      "        -8.8574, -8.8574], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.029382523149251938\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-8.6559, -8.8834, -8.8733, -8.7007, -8.8733, -8.6193, -8.9948, -8.5623,\n",
      "        -8.9447, -8.9464], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7245, -9.0517, -8.9230, -9.1071, -8.9230, -8.7574, -8.9543, -8.6459,\n",
      "        -9.0142, -8.6459], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03258611261844635\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-9.0126, -8.9960, -8.9154, -8.8343, -8.8926, -8.8343, -8.6742, -8.8324,\n",
      "        -9.0126, -8.9102], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-9.0366, -8.7996, -8.7945, -8.9509, -8.9971, -8.9509, -8.7509, -8.9438,\n",
      "        -9.0366, -8.8082], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.012111609801650047\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.9365, -8.9352, -8.7962, -8.9734], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.7297, -8.9795, -8.9107, -8.7677, -8.5030, -9.0104, -8.5030, -8.7977,\n",
      "        -8.6159, -8.9795], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.6527, -8.9789, -8.7350, -8.8909, -8.6527, -8.9166, -8.6527, -8.8659,\n",
      "        -8.8660, -8.9789], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01727474108338356\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.6127, -8.5478, -8.4157, -8.7721], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-9.0420, -8.4157, -8.4477, -8.7086, -8.5641, -8.9623, -8.9243, -8.6088,\n",
      "        -8.5761, -8.7086], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.8565, -8.5741, -8.4820, -8.7015, -8.6450, -8.8378, -8.7077, -8.7761,\n",
      "        -8.7761, -8.7015], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01977764256298542\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.5891, -8.5380, -8.3552, -8.7859], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.7194, -8.7248, -8.9428, -8.3442, -9.0759, -8.8989, -8.4730, -8.7248,\n",
      "        -8.5380, -9.0759], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.6872, -8.6684, -8.8232, -8.4168, -8.8174, -8.8420, -8.4571, -8.6684,\n",
      "        -8.8224, -8.8174], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.024499904364347458\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-8.7348, -8.7581, -8.9565, -8.8850, -8.8049, -8.6747, -8.6787, -9.0552,\n",
      "        -9.0552, -8.5945], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.6775, -8.7979, -8.6175, -8.8300, -8.6948, -8.5117, -8.6479, -8.8135,\n",
      "        -8.8135, -8.6479], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.028215596452355385\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-8.5891, -8.5229, -8.9611, -8.5891, -8.7018, -8.5229, -8.5891, -8.8685,\n",
      "        -8.5231, -8.9611], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.6708, -8.9166, -8.8929, -8.6708, -8.6096, -8.9166, -8.6708, -8.8770,\n",
      "        -8.5781, -8.8929], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0350923091173172\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.6153, -8.6245, -8.5173, -8.6651], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.7797, -8.7797, -8.5938, -8.6076, -8.3381, -9.0108, -8.8547, -8.6245,\n",
      "        -8.5938, -8.8698], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7922, -8.7922, -8.6376, -8.7922, -8.6102, -8.6656, -9.0467, -8.8984,\n",
      "        -8.6376, -8.8439], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.034402985125780106\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.7249, -8.7784, -8.5502, -8.6534], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.8653, -8.6534, -8.6569, -8.6707, -8.6534, -8.6453, -8.6426, -8.7626,\n",
      "        -8.6534, -8.7339], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.8605, -8.6951, -8.5741, -8.8036, -8.6951, -8.5741, -8.5686, -8.8022,\n",
      "        -8.6951, -9.0384], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.013461433351039886\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.8180, -8.9100, -8.5904, -8.6551], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-9.0533, -8.6922, -8.5904, -8.9145, -8.9145, -8.6919, -8.7926, -8.9774,\n",
      "        -8.5837, -8.9145], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7253, -8.7743, -8.7313, -8.7253, -8.7253, -8.8227, -9.0245, -8.5308,\n",
      "        -8.5308, -8.7253], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05146927759051323\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.9062, -8.9235, -8.7011, -8.6525], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-8.5999, -9.0938, -8.9235, -8.6525, -8.2187, -8.6216, -8.6525, -8.9235,\n",
      "        -8.8733, -8.6510], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7399, -8.8450, -8.7399, -8.7873, -8.3968, -8.7594, -8.7873, -8.7399,\n",
      "        -8.7478, -8.5813], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.025662284344434738\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.9596, -8.8547, -8.8318, -8.6431], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-8.7543, -8.6911, -8.8985, -8.8928, -8.1802, -8.5709, -8.8318, -8.5709,\n",
      "        -8.6911, -8.7756], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.5513, -8.5513, -8.8177, -9.0233, -8.7120, -8.7138, -8.7787, -8.7138,\n",
      "        -8.5513, -8.7138], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04341454803943634\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-8.6630, -8.5634, -8.5626, -8.7745, -8.5993, -9.0785, -8.7745, -8.3856,\n",
      "        -8.9003, -8.6630], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7067, -8.7070, -8.7063, -8.7067, -8.3986, -8.8724, -8.7067, -8.4827,\n",
      "        -8.7995, -8.7067], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.015671733766794205\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-8.4580, -8.7320, -8.7226, -8.5607, -8.6638, -8.6579, -8.5851, -8.8615,\n",
      "        -8.7172, -8.2778], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.6120, -8.8504, -8.8182, -8.6120, -8.8276, -8.8276, -8.7085, -8.6120,\n",
      "        -8.4496, -8.7553], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04821602255105972\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-8.5904, -8.2473, -8.7565, -8.5904, -8.7565, -8.5776, -9.0304, -8.7915,\n",
      "        -8.6936, -8.8739], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7314, -8.4226, -8.8243, -8.7314, -8.8243, -8.3677, -8.8243, -8.6457,\n",
      "        -8.6022, -8.7314], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.021609563380479813\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-8.5871, -8.9369, -8.8560, -8.8924, -8.3893, -8.8924, -8.6357, -8.3893,\n",
      "        -8.3893, -8.7636], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7157, -8.7667, -8.7319, -8.8005, -8.5504, -8.8005, -8.6817, -8.5504,\n",
      "        -8.5504, -8.5504], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02031695283949375\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.8884, -8.6742, -8.9127, -8.8855], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-8.2651, -8.3904, -8.8855, -8.9127, -8.8855, -8.7002, -8.7002, -8.5741,\n",
      "        -8.8855, -8.6318], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.4386, -8.5514, -8.8067, -8.8067, -8.8067, -8.7660, -8.7660, -8.6375,\n",
      "        -8.8067, -8.5514], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.010499360971152782\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.5219, -8.4022, -8.5033, -8.6869], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-8.7256, -8.5033, -8.5562, -8.7131, -8.6879, -8.5562, -8.8312, -8.4713,\n",
      "        -8.3046, -8.5033], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.6832, -8.5620, -8.6242, -8.5620, -8.5620, -8.6242, -8.8262, -8.6242,\n",
      "        -8.4742, -8.5620], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.010871773585677147\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-8.8372, -8.4870, -8.7701, -8.6460, -8.4420, -8.4378, -8.7701, -8.3653,\n",
      "        -8.4420, -8.9651], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7814, -8.5909, -8.7814, -8.8584, -8.4977, -8.5940, -8.7814, -8.5287,\n",
      "        -8.4977, -8.8584], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.012800408527255058\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.6405, -8.6006, -8.6015, -8.9391], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-8.4052, -8.4694, -8.7359, -8.8473, -8.9391, -8.3995, -8.9783, -8.8054,\n",
      "        -8.9783, -8.2258], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.5047, -8.8021, -8.6316, -8.7406, -8.5596, -8.7406, -8.8361, -8.7573,\n",
      "        -8.8361, -8.5211], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05331585928797722\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.6297, -8.6076, -8.5297, -8.8195], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.4558, -8.7810, -8.7810, -8.9320, -8.3170, -8.7784, -8.7810, -8.6076,\n",
      "        -8.3881, -8.6894], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.5358, -8.7653, -8.7653, -8.8253, -8.6933, -8.5883, -8.7653, -8.6767,\n",
      "        -8.5439, -8.6191], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.023032883182168007\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-8.6844, -8.7328, -8.5175, -8.6844, -8.8659, -8.6539, -8.7446, -8.5915,\n",
      "        -8.7522, -8.4158], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.4321, -8.6101, -8.5286, -8.4321, -8.7885, -8.7885, -8.7545, -8.6101,\n",
      "        -8.5888, -8.4321], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.019395580515265465\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.5018, -8.4602, -8.3899, -8.5523], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.7909, -8.4314, -8.6865, -8.5552, -8.6370, -8.6865, -8.6783, -8.6620,\n",
      "        -8.3087, -8.6865], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.8104, -8.4778, -8.7904, -8.8265, -8.4778, -8.7904, -8.8104, -8.6657,\n",
      "        -8.6657, -8.7904], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.027883771806955338\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.5021, -8.4972, -8.3912, -8.5258], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.5202, -8.6629, -8.4972, -8.5531, -8.7522, -8.6709, -8.7345, -8.5754,\n",
      "        -8.6437, -8.6914], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.5277, -8.8038, -8.5521, -8.6325, -8.8038, -8.8038, -8.4303, -8.4303,\n",
      "        -8.6156, -8.7966], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.017496995627880096\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.5043, -8.5155, -8.4099, -8.5103], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.4357, -8.7584, -8.6753, -8.5155, -8.4623, -8.7198, -8.6109, -8.8597,\n",
      "        -8.5576, -8.6538], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7233, -8.4740, -8.8078, -8.5689, -8.5857, -8.8078, -8.5857, -8.6963,\n",
      "        -8.4740, -8.7884], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.025943120941519737\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-8.4578, -8.7521, -8.5284, -8.6639, -8.4973, -8.5284, -8.5072, -8.7032,\n",
      "        -8.5542, -8.6739], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.5863, -8.5413, -8.6755, -8.7975, -8.4869, -8.6755, -8.6120, -8.8049,\n",
      "        -8.5525, -8.7960], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.015845801681280136\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.3581, -8.4005, -8.2953, -8.3387], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.4433, -8.7006, -8.6671, -8.4951, -8.5999, -8.6476, -8.5338, -8.4487,\n",
      "        -8.6476, -8.7255], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.5990, -8.8004, -8.8004, -8.5202, -8.6899, -8.6891, -8.4657, -8.6891,\n",
      "        -8.6891, -8.6146], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.013890037313103676\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.4137, -8.4031, -8.2946, -8.3711], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.6949, -8.4031, -8.7160, -8.7118, -8.7258, -8.8370, -8.4304, -8.4947,\n",
      "        -8.7235, -8.6422], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7036, -8.5874, -8.8444, -8.7779, -8.7650, -8.6812, -8.5874, -8.5874,\n",
      "        -8.7779, -8.7779], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01353532075881958\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.4788, -8.4466, -8.3020, -8.4277], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.5729, -8.4743, -8.7355, -8.7314, -8.4466, -8.6063, -8.7314, -8.7499,\n",
      "        -8.7314, -8.7314], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.4718, -8.4414, -8.7314, -8.7480, -8.5567, -8.5902, -8.7480, -8.8337,\n",
      "        -8.7480, -8.7480], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.003181458916515112\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.5564, -8.5130, -8.3195, -8.4997], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.3383, -8.5094, -8.4206, -8.6087, -8.7520, -8.2265, -8.7981, -8.7834,\n",
      "        -8.4087, -8.5709], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.5044, -8.8109, -8.4039, -8.5678, -8.7144, -8.5678, -8.6292, -8.8109,\n",
      "        -8.5044, -8.4876], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.028378164395689964\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.6336, -8.5709, -8.3772, -8.5698], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.7604, -8.5661, -8.4868, -8.4072, -8.7604, -8.5709, -8.7604, -8.5250,\n",
      "        -8.7699, -8.5088], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7095, -8.7095, -8.6381, -8.4761, -8.7095, -8.4761, -8.7095, -8.7341,\n",
      "        -8.4211, -8.4211], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.023807406425476074\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-8.7694, -8.7490, -8.7490, -8.9149, -8.7158, -8.8019, -8.9158, -8.7490,\n",
      "        -8.8004, -8.6183], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7826, -8.7379, -8.7379, -8.8102, -8.7379, -8.7236, -8.7826, -8.7379,\n",
      "        -8.8102, -8.8442], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.008698909543454647\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-8.6029, -8.3544, -8.7933, -8.7359, -8.7235, -8.6536, -8.6414, -8.5585,\n",
      "        -8.7705, -8.5585], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7426, -8.5145, -8.5372, -8.7882, -8.5372, -8.7882, -8.8372, -8.7027,\n",
      "        -8.8854, -8.7027], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.025935988873243332\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.7871, -8.5704, -8.6951, -8.6862], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-9.0802, -8.7668, -8.6496, -8.7719, -8.5858, -8.4813, -8.7668, -8.5138,\n",
      "        -8.4341, -8.7871], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7515, -8.8901, -8.7515, -8.7846, -8.5647, -8.7133, -8.8901, -8.6542,\n",
      "        -8.5907, -8.7133], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.025294587016105652\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.5526, -8.2479, -8.5396, -8.4911], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-8.4398, -8.3914, -9.0966, -8.7614, -8.2479, -8.5526, -8.4359, -8.8314,\n",
      "        -9.0052, -8.7614], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.5958, -8.5958, -8.7303, -8.8648, -8.4231, -8.5923, -8.5923, -8.8648,\n",
      "        -8.8648, -8.8648], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02992379665374756\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-8.6618, -8.8142, -8.8142, -8.9353, -8.7800, -8.7014, -8.9026, -8.7542,\n",
      "        -9.0049, -8.3927], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7314, -8.6622, -8.6622, -8.7323, -8.5534, -8.7649, -8.7649, -8.8459,\n",
      "        -8.6086, -8.5534], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.035784825682640076\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.5223, -8.2858, -8.6271, -8.4959], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-8.4890, -9.0177, -8.7474, -8.9172, -8.8678, -8.8429, -8.6277, -8.7850,\n",
      "        -8.9685, -8.2858], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.6777, -8.6401, -8.8726, -8.9810, -8.7016, -8.7657, -8.7649, -8.5818,\n",
      "        -8.8075, -8.4572], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03469417616724968\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-8.6197, -8.8975, -8.6607, -8.6197, -8.7257, -8.6565, -8.9405, -8.7257,\n",
      "        -8.7257, -8.8975], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7578, -8.8242, -8.5166, -8.7578, -8.8242, -8.7909, -8.8142, -8.8242,\n",
      "        -8.8242, -8.8242], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.013272644951939583\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-8.4236, -8.7525, -8.7248, -8.7491, -8.7491, -8.9400, -8.7036, -8.7491,\n",
      "        -8.7829, -8.9067], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.5603, -8.8543, -8.7403, -8.8200, -8.8200, -8.9993, -8.7427, -8.8200,\n",
      "        -8.7747, -8.8200], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.005699913948774338\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-8.5607, -8.8190, -8.5527, -8.7304, -8.7304, -8.8337, -8.7820, -8.8337,\n",
      "        -8.4578, -8.8023], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.6701, -8.6944, -8.4903, -8.6293, -8.6293, -8.7781, -8.7359, -8.7781,\n",
      "        -8.6120, -8.6787], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.009916452690958977\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-8.9510, -8.8551, -8.5502, -8.6807, -8.4624, -8.8094, -8.8388, -8.8649,\n",
      "        -8.8362, -8.8649], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.6952, -8.6550, -8.8812, -8.5408, -8.6161, -8.6952, -8.7494, -8.7494,\n",
      "        -8.5632, -8.7494], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.038048941642045975\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.3645, -8.5509, -8.4705, -8.6095], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-8.9829, -8.8247, -8.6894, -9.0818, -8.8437, -8.6499, -8.6900, -8.4443,\n",
      "        -8.8345, -8.6900], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.9154, -8.7674, -8.7136, -8.6942, -8.8426, -8.5878, -8.5999, -8.5999,\n",
      "        -8.7142, -8.5999], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.021737050265073776\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-8.7230, -8.7580, -8.5908, -8.8194, -8.5883, -8.7230, -8.9031, -8.6517,\n",
      "        -8.6870, -8.7458], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7295, -8.7659, -8.6457, -8.8712, -8.7295, -8.7295, -8.7238, -8.6781,\n",
      "        -8.6457, -8.7295], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.006059812381863594\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.6498, -8.5055, -8.5084, -8.7129], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-8.6282, -8.6079, -8.8205, -8.6079, -8.6915, -8.7028, -8.7368, -8.6179,\n",
      "        -8.4809, -8.5219], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.6418, -8.8631, -8.8401, -8.8631, -8.6697, -8.8401, -8.8631, -8.7255,\n",
      "        -8.5453, -8.6697], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.020367177203297615\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.7248, -8.4628, -8.3064, -8.6063], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.6326, -8.3064, -8.8228, -8.8361, -9.0525, -8.6504, -8.8638, -8.6504,\n",
      "        -8.6836, -8.3887], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7694, -8.4693, -8.7672, -8.7324, -8.6145, -8.6460, -8.6460, -8.6460,\n",
      "        -8.7694, -8.4693], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.031224075704813004\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-8.9848, -8.3008, -8.7915, -8.7885, -8.4355, -8.4355, -8.6435, -8.6248,\n",
      "        -8.7332, -8.6248], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.4830, -8.4397, -8.4830, -8.7498, -8.5920, -8.5920, -8.7044, -8.6328,\n",
      "        -8.5231, -8.6328], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04646644741296768\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.7438, -8.2775, -8.2929, -8.4873], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-8.4245, -8.3990, -9.0093, -8.7536, -8.5699, -8.5463, -8.6435, -8.7536,\n",
      "        -9.0576, -8.6435], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.5393, -8.4498, -8.6996, -8.7976, -8.7976, -8.6541, -8.6458, -8.7976,\n",
      "        -8.5796, -8.6458], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04075226932764053\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.6648, -8.2672, -8.2888, -8.4357], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-8.5376, -8.7164, -8.7865, -8.6872, -8.9551, -8.6872, -8.4047, -8.6605,\n",
      "        -8.4252, -8.6648], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.6528, -8.8088, -8.5827, -8.8088, -8.7148, -8.8088, -8.5606, -8.8494,\n",
      "        -8.5827, -8.5792], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02428613044321537\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.5446, -8.2621, -8.2629, -8.4162], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-8.6616, -8.5896, -8.6072, -8.5137, -8.5807, -8.7793, -8.4003, -8.2629,\n",
      "        -8.5421, -8.5807], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.6879, -8.5414, -8.5414, -8.6081, -8.7954, -8.6498, -8.5603, -8.4359,\n",
      "        -8.6879, -8.7954], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.020203210413455963\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-8.6854, -8.5835, -8.5856, -8.5094, -8.6583, -8.4751, -8.4051, -8.6778,\n",
      "        -8.5709, -8.5094], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7925, -8.6791, -8.7270, -8.6479, -8.7270, -8.5578, -8.5841, -8.7165,\n",
      "        -8.5688, -8.6479], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.012410947121679783\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-8.7558, -8.6395, -8.6131, -8.6982, -8.7144, -8.6372, -8.5675, -8.5196,\n",
      "        -8.5962, -8.5962], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7735, -8.6677, -8.6395, -8.7735, -8.8013, -8.7545, -8.7737, -8.6395,\n",
      "        -8.6516, -8.6516], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.009183540940284729\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.6306, -8.6709, -8.4404, -8.6992], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.2093, -8.5261, -8.7459, -8.7459, -8.6093, -8.2093, -8.7022, -8.6593,\n",
      "        -8.7035, -8.7525], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.3884, -8.7433, -8.6841, -8.6841, -8.6882, -8.3884, -8.4897, -8.6841,\n",
      "        -8.6882, -8.7504], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01711697317659855\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.3621, -8.4230, -8.1844, -8.4821], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.7690, -8.7138, -8.4821, -8.6891, -8.4821, -8.4102, -8.4102, -8.6891,\n",
      "        -8.7087, -8.8182], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.6119, -8.5994, -8.4575, -8.6375, -8.4575, -8.3660, -8.3660, -8.6375,\n",
      "        -8.5419, -8.5163], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.016717202961444855\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-8.5228, -8.6466, -8.4712, -8.4905, -8.8252, -8.8038, -8.5286, -8.7278,\n",
      "        -8.7060, -8.8116], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7434, -8.6415, -8.4753, -8.6758, -8.5003, -8.7434, -8.6758, -8.6758,\n",
      "        -8.5745, -8.7434], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.023854518309235573\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-8.5839, -8.2822, -8.6796, -8.5500, -8.5948, -8.6371, -8.7420, -8.4146,\n",
      "        -8.5638, -8.2822], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.6748, -8.4540, -8.7075, -8.7889, -8.6937, -8.7075, -8.6937, -8.7551,\n",
      "        -8.7075, -8.4540], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.027878856286406517\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.6261, -8.6194, -8.4685, -8.5936], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.5849, -8.6854, -8.5849, -8.3947, -8.6421, -8.8365, -8.6424, -8.6373,\n",
      "        -8.5794, -8.6471], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.6961, -8.7981, -8.6961, -8.4900, -8.6961, -8.8947, -8.8168, -8.8947,\n",
      "        -8.6216, -8.7140], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.015576491132378578\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.5065, -8.4022, -8.2971, -8.4925], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.6474, -8.6568, -8.6656, -8.6205, -8.5137, -8.5137, -8.7068, -8.7786,\n",
      "        -8.6568, -8.7082], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.6585, -8.6623, -8.6756, -8.6674, -8.6623, -8.6623, -8.6756, -8.7676,\n",
      "        -8.6623, -8.6674], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.004942653700709343\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-8.2790, -8.5922, -8.6655, -8.4768, -8.8001, -8.2790, -8.7604, -8.4664,\n",
      "        -8.3022, -8.7613], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.4511, -8.5421, -8.6590, -8.6448, -8.6291, -8.4511, -8.6291, -8.6197,\n",
      "        -8.4720, -8.6892], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.019401872530579567\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.6105, -8.4183, -8.3051, -8.5783], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.6105, -8.6706, -8.6706, -8.6571, -8.8648, -8.7154, -8.4183, -8.6882,\n",
      "        -8.6882, -8.6571], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.5461, -8.6348, -8.6348, -8.6594, -8.6957, -8.6957, -8.4746, -8.6180,\n",
      "        -8.6180, -8.6594], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00486917607486248\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.6171, -8.4164, -8.3486, -8.6001], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.6906, -8.6171, -8.8227, -8.7257, -8.4894, -8.6023, -8.6906, -8.7257,\n",
      "        -8.7778, -8.3486], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.6404, -8.5778, -8.7527, -8.7075, -8.6404, -8.7421, -8.6404, -8.7075,\n",
      "        -8.7110, -8.5138], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.008621727116405964\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.6083, -8.4133, -8.4125, -8.6104], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.6477, -8.6854, -8.6854, -8.4125, -8.6854, -8.5430, -8.8408, -8.5616,\n",
      "        -8.8353, -8.6786], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.9170, -8.6854, -8.6854, -8.5712, -8.6854, -8.6850, -8.8108, -8.6337,\n",
      "        -8.7692, -8.8108], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.014586172997951508\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.6079, -8.4490, -8.4454, -8.6228], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.6836, -8.5645, -8.8982, -8.7360, -8.6836, -8.7360, -8.7320, -8.9791,\n",
      "        -8.6292, -8.5590], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7031, -8.6858, -8.7802, -8.6804, -8.7031, -8.6804, -8.8302, -8.9789,\n",
      "        -8.8516, -8.7031], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.011547915637493134\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-8.7040, -8.4425, -8.8569, -8.6736, -8.8199, -8.5206, -8.5206, -8.7705,\n",
      "        -8.8236, -8.5312], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.8528, -8.5983, -8.8501, -8.8528, -8.8501, -8.6078, -8.6078, -8.8135,\n",
      "        -8.9843, -8.6078], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.012826847843825817\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.5862, -8.6390, -8.4218, -8.5795], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.5514, -8.4218, -8.6153, -8.8017, -8.4438, -8.5514, -8.8017, -8.5514,\n",
      "        -8.8831, -8.6177], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.6962, -8.5796, -8.6410, -8.7999, -8.6410, -8.6962, -8.7999, -8.6962,\n",
      "        -8.7929, -8.5781], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01371382363140583\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.6099, -8.7412, -8.4235, -8.6073], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.7412, -8.8453, -9.0071, -8.4209, -8.4124, -8.7532, -8.6728, -8.4209,\n",
      "        -8.8356, -8.8496], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.5711, -8.6198, -8.9366, -8.6198, -8.5711, -8.7020, -8.7020, -8.6198,\n",
      "        -8.8055, -8.5788], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.026675458997488022\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.6077, -8.7665, -8.4654, -8.6020], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.7410, -8.9383, -8.4806, -8.9408, -8.6764, -8.7187, -8.8723, -9.1540,\n",
      "        -8.7817, -8.7817], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7582, -8.9036, -8.6326, -9.0434, -8.8087, -8.7035, -8.6504, -8.8738,\n",
      "        -8.6504, -8.6504], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02151922509074211\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.5974, -8.7694, -8.5292, -8.5793], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.7614, -8.9160, -8.8410, -8.8572, -8.7614, -8.6581, -8.7943, -8.7542,\n",
      "        -9.1135, -8.7694], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.8336, -8.7639, -8.8591, -8.7639, -8.8336, -8.7639, -8.7138, -8.7465,\n",
      "        -8.9717, -8.7138], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.008349398151040077\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.5857, -8.7276, -8.5976, -8.5599], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-9.0341, -8.5566, -8.7643, -8.7456, -8.9305, -8.7643, -8.9587, -9.0835,\n",
      "        -8.5976, -8.9698], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7826, -8.8057, -8.8879, -8.7010, -8.8879, -8.8879, -9.0259, -9.0270,\n",
      "        -8.7039, -9.0728], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01892462745308876\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-8.9128, -8.9017, -8.6945, -8.9429, -8.8703, -8.7741, -8.9197, -8.8271,\n",
      "        -8.8273, -8.7375], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-9.0182, -8.8966, -8.7114, -8.9362, -8.9833, -8.8966, -8.9409, -8.7807,\n",
      "        -9.0381, -8.9409], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.012766366824507713\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.0894, -8.2006, -8.2539, -8.1002], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-9.1334, -8.9022, -8.9617, -8.6758, -8.9222, -8.6438, -8.5700, -8.6925,\n",
      "        -8.8437, -8.6758], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.9098, -9.0119, -9.0655, -8.7130, -8.9435, -8.8232, -9.0119, -8.7130,\n",
      "        -8.8232, -8.7130], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03043501079082489\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-8.9473, -8.9260, -9.0839, -8.8245, -9.0839, -8.7197, -8.7186, -8.8551,\n",
      "        -8.8243, -8.8551], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.9207, -9.0334, -8.9215, -8.6821, -8.9215, -8.6821, -8.7585, -8.9215,\n",
      "        -8.7591, -8.9215], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.010132668539881706\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-8.9611, -9.0863, -9.0440, -9.0507, -9.1562, -9.1082, -9.0267, -8.5868,\n",
      "        -9.1347, -8.9669], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-9.0650, -9.0112, -9.0650, -8.8968, -9.0304, -8.9184, -8.9825, -8.7635,\n",
      "        -8.9825, -8.9155], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.015135872177779675\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-8.9969, -9.1144, -9.2209, -9.0662], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-8.9570, -8.9524, -8.6262, -9.1614, -8.8555, -9.0091, -9.0936, -8.9524,\n",
      "        -8.9782, -9.0957], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.9601, -8.9637, -8.8033, -8.9637, -8.7636, -8.7683, -8.9637, -8.9637,\n",
      "        -9.0804, -8.8033], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.024995598942041397\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-8.9435, -8.7266, -8.9175, -8.7386, -8.9702, -8.9175, -8.6837, -8.9702,\n",
      "        -8.9621, -8.9930], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-9.0124, -8.8539, -8.9854, -8.8539, -9.0704, -8.9854, -8.3306, -9.0704,\n",
      "        -9.0659, -9.0937], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.020908448845148087\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-9.0404, -8.9808, -9.0952, -8.7055, -8.9808, -8.8519, -8.8789, -8.6468,\n",
      "        -8.8519, -9.1053], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-9.0886, -9.0639, -9.0886, -8.8273, -9.0639, -8.9668, -8.8349, -8.7316,\n",
      "        -8.9668, -9.1473], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.006830052938312292\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-9.3330, -9.0062, -8.9976, -9.0600], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.6530, -8.9329, -8.9364, -9.0184, -8.9364, -8.7913, -8.8616, -8.8535,\n",
      "        -8.9786, -8.9434], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7833, -9.0396, -9.0078, -9.0313, -9.0078, -8.9121, -8.9549, -8.9549,\n",
      "        -8.9121, -8.8054], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.009575563482940197\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-9.0780, -8.7606, -8.5622, -8.6620, -8.6578, -8.7624, -8.8745, -9.0113,\n",
      "        -9.0484, -9.0113], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-9.0273, -8.8551, -8.7920, -8.7920, -8.7920, -8.8862, -8.9870, -8.9309,\n",
      "        -8.9870, -8.9309], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.014391325414180756\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-9.3801, -9.0688, -8.8984, -9.6043, -9.2628, -8.7457, -8.8829, -8.8971,\n",
      "        -8.6485, -8.8716], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.8325, -8.9844, -8.9142, -9.1070, -8.6980, -8.8691, -8.5995, -9.0013,\n",
      "        -8.7765, -8.9844], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10090859979391098\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-9.4553, -8.9775, -8.9273, -9.0497], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.8658, -8.8314, -8.7409, -9.2274, -8.9140, -8.8314, -8.7409, -8.8696,\n",
      "        -8.6720, -8.9296], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.8775, -9.0346, -8.8475, -8.9718, -8.9482, -9.0346, -8.8475, -9.0367,\n",
      "        -8.7755, -9.0367], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.022202495485544205\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-9.3888, -9.0273, -8.9589, -9.0212], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-9.4920, -8.8252, -9.2374, -8.9589, -8.9635, -8.8192, -8.7821, -8.5250,\n",
      "        -8.9589, -8.9589], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-9.1910, -8.8900, -9.1910, -9.0630, -8.8707, -8.8628, -8.8707, -8.6725,\n",
      "        -9.0630, -9.0630], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01695447973906994\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-9.3040, -9.1077, -8.9815, -8.9981], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-9.3040, -8.8206, -8.9929, -9.0811, -8.8582, -8.9815, -8.9163, -8.9815,\n",
      "        -9.1426, -8.5556], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-9.0247, -9.0834, -9.0532, -9.0107, -8.8847, -9.0834, -9.0247, -9.0834,\n",
      "        -8.8847, -8.7001], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.027624109759926796\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-9.1466, -9.1494, -8.9726, -8.9917], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.9917, -8.5857, -9.2647, -8.8566, -8.5857, -8.9367, -8.7496, -9.1803,\n",
      "        -8.9628, -8.9257], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-9.0753, -8.7138, -9.0809, -8.9477, -8.7138, -8.9110, -8.8747, -9.0477,\n",
      "        -8.9191, -8.9477], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01181858777999878\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-9.0097, -9.0213, -8.9772, -8.5274, -8.9680, -9.0450, -9.0251, -8.5647,\n",
      "        -8.9029, -9.1528], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.9388, -8.9525, -9.0794, -8.6747, -9.0712, -9.1405, -9.1087, -8.9525,\n",
      "        -9.0126, -9.1087], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.023301035165786743\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-9.0680, -9.3563, -9.0932, -9.1139], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-9.1139, -8.9262, -8.7538, -8.9262, -8.7980, -9.4171, -8.5447, -8.9460,\n",
      "        -9.0437, -8.7005], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-9.1612, -8.9182, -8.8758, -8.9182, -9.0436, -9.1261, -8.6699, -9.0274,\n",
      "        -8.9128, -8.6699], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0202651247382164\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-8.4953, -9.3230, -8.5932, -9.0090, -9.1125, -9.0090, -8.7576, -8.8888,\n",
      "        -9.1594, -9.0090], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.9100, -9.0863, -8.6308, -8.9692, -9.1032, -8.9692, -9.0135, -8.9692,\n",
      "        -9.1476, -8.9692], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.030629198998212814\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-8.9902, -9.3341, -9.1151, -9.1245], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-9.1160, -9.1160, -9.1963, -9.1160, -8.9961, -9.0197, -9.1876, -9.2879,\n",
      "        -8.9301, -8.9813], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.8912, -8.8912, -9.1566, -8.8912, -8.9344, -8.8559, -8.9765, -8.9765,\n",
      "        -8.8559, -8.9059], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03365595266222954\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-9.0809, -9.1642, -8.9413, -8.6106, -8.7684, -8.7122, -8.9428, -9.2385,\n",
      "        -9.0211, -8.6106], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-9.0432, -9.1643, -8.8915, -8.6697, -8.9692, -8.7818, -8.9845, -8.9845,\n",
      "        -9.1162, -8.6697], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.013135462999343872\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-8.7470, -8.6886, -9.2516, -8.7470, -9.1944, -8.9264, -8.8014, -8.7470,\n",
      "        -8.9208, -8.9264], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.8197, -8.8197, -9.1772, -8.8197, -9.2750, -9.0338, -8.9213, -8.8197,\n",
      "        -9.0287, -9.0338], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.009414711967110634\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-9.2375, -9.0581, -9.1487, -9.0880], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-8.6822, -9.1570, -8.9181, -9.0651, -8.8074, -9.1765, -9.0880, -9.0880,\n",
      "        -9.0011, -9.0022], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.8140, -9.1126, -8.9838, -9.0546, -8.7741, -9.2589, -9.1523, -9.1523,\n",
      "        -9.1126, -9.0546], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00551188550889492\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-9.0942, -8.6338, -8.7946, -8.7927, -9.2787, -8.7748, -8.8582, -9.1883,\n",
      "        -8.9633, -8.7248], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.9877, -8.5556, -8.9151, -8.7196, -8.8973, -8.7871, -8.7196, -8.9877,\n",
      "        -8.8973, -8.8523], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02630309760570526\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-9.3553, -8.9454, -9.1215, -9.1322], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-8.5403, -8.6913, -8.8378, -9.4082, -8.5403, -8.9605, -8.7646, -8.9454,\n",
      "        -8.8946, -8.5823], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.6862, -8.8221, -8.7241, -9.0509, -8.6862, -8.8881, -8.7621, -8.9557,\n",
      "        -8.5291, -8.7241], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03594675660133362\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-8.8631, -9.1375, -8.7146, -8.9979, -9.0902, -8.8115, -8.8504, -8.4223,\n",
      "        -8.7770, -9.1952], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.5800, -8.7874, -8.8431, -9.0649, -8.9303, -8.7874, -8.9303, -8.7536,\n",
      "        -8.7536, -8.9175], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.044367384165525436\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-8.9867, -8.9446, -8.6279, -8.9356, -8.7903, -8.5117, -8.8240, -8.6279,\n",
      "        -8.7825, -8.7825], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-9.0383, -9.0028, -8.7651, -8.8373, -9.0197, -8.6605, -8.8373, -8.7651,\n",
      "        -8.9042, -8.9042], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.015798401087522507\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-9.1451, -8.9942, -8.7976, -9.1286], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.8877, -8.6850, -9.0450, -8.7171, -8.7409, -8.6247, -8.9086, -8.8877,\n",
      "        -8.9427, -9.0722], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.8454, -8.6006, -8.7622, -8.8454, -8.7622, -8.6006, -8.9178, -8.8454,\n",
      "        -8.8668, -9.0312], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.011567948386073112\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-9.2677, -9.1141, -8.8664, -9.0649], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-9.0176, -9.0896, -8.9362, -9.0469, -8.8556, -8.9034, -9.2677, -8.7043,\n",
      "        -8.8664, -9.1750], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.8059, -8.9798, -8.9798, -8.7124, -8.8575, -8.8575, -8.8749, -8.7388,\n",
      "        -8.7124, -9.0130], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03782963007688522\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.9720, -8.9158, -8.6419, -8.7847], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.8021, -8.7601, -9.0263, -8.8021, -8.7601, -8.9510, -8.8021, -8.8021,\n",
      "        -8.9428, -8.9291], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.8473, -8.8423, -8.9007, -8.8473, -8.8423, -8.8642, -8.8473, -8.8473,\n",
      "        -8.8642, -8.8642], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.005545216146856546\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.9200, -8.9140, -8.7068, -8.8013], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.6616, -8.7921, -8.4559, -8.7068, -8.7770, -8.6073, -8.7304, -8.6644,\n",
      "        -8.6768, -8.8161], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-9.2306, -8.5900, -8.6103, -8.8361, -8.8666, -8.5900, -8.8361, -8.5900,\n",
      "        -8.7918, -8.7918], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04441153258085251\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-8.7848, -8.7258, -8.8109, -8.6352, -8.8415, -8.8669, -8.6352, -8.8729,\n",
      "        -8.8813, -8.8844], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-9.1034, -8.8532, -8.8514, -8.5962, -8.8988, -8.8988, -8.5962, -8.9163,\n",
      "        -8.5750, -8.7717], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02351086586713791\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.6937, -8.6564, -8.6433, -8.7898], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.6235, -8.8419, -8.9189, -8.9189, -8.4200, -8.7930, -8.4200, -8.8587,\n",
      "        -8.7390, -8.7403], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7790, -8.8300, -8.7464, -8.7464, -8.5549, -8.7497, -8.5549, -8.8678,\n",
      "        -8.8651, -8.8678], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01543803047388792\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.6382, -8.6019, -8.6017, -8.7906], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.3817, -8.6344, -8.8507, -8.7415, -8.7707, -8.7415, -8.8965, -8.7707,\n",
      "        -8.5212, -8.7467], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.5306, -8.7246, -8.7930, -8.7315, -8.8723, -8.7315, -8.9380, -8.8723,\n",
      "        -8.6087, -8.8604], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.007678162306547165\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.6028, -8.5928, -8.5296, -8.7937], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.6060, -8.8927, -9.0589, -9.1345, -8.6060, -8.9203, -8.7901, -8.4481,\n",
      "        -8.3879, -8.8687], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7454, -8.6868, -9.0034, -9.0034, -8.7454, -8.9251, -8.7529, -8.5857,\n",
      "        -8.4787, -8.7454], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.014530633576214314\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-8.7525, -8.7977, -8.8603, -8.6467, -8.9160, -8.6467, -8.5859, -8.7910,\n",
      "        -8.2758, -8.5916], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.6196, -8.6623, -8.7020, -8.6295, -8.6263, -8.6295, -8.5635, -8.8199,\n",
      "        -8.4483, -8.5635], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01773666962981224\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.6090, -8.5785, -8.4299, -8.7305], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.6090, -8.6732, -8.7305, -8.3973, -8.5631, -8.3973, -8.6187, -8.2926,\n",
      "        -8.6187, -8.2926], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.6345, -8.6345, -8.6397, -8.4633, -8.7068, -8.4633, -8.8059, -8.4633,\n",
      "        -8.8059, -8.4633], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.016814233735203743\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-8.6922, -8.6922, -8.5498, -8.8019, -8.7541, -8.8019, -8.3890, -8.4069,\n",
      "        -9.0157, -8.7166], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.8230, -8.8230, -8.6948, -8.6948, -8.6831, -8.6948, -8.8230, -8.4639,\n",
      "        -8.4639, -8.6409], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.058510202914476395\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-9.0304, -8.9581, -8.9021, -9.0061], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.7018, -8.6195, -8.7215, -8.8329, -8.9322, -8.5981, -8.7530, -8.5981,\n",
      "        -8.7157, -8.5471], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.8016, -8.7069, -8.7124, -8.7000, -8.7382, -8.7382, -8.7382, -8.7382,\n",
      "        -8.8521, -8.6317], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.013825143687427044\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-8.6623, -8.6749, -8.9355, -8.6740, -8.9355, -8.9124, -8.6108, -8.9355,\n",
      "        -8.4740, -8.7943], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7961, -8.8074, -9.0360, -8.7876, -9.0360, -8.9956, -8.6748, -9.0360,\n",
      "        -8.6024, -8.7877], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.010618915781378746\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-8.6566, -8.7952, -8.7036, -8.5828, -8.8206, -8.5828, -8.6195, -8.8202,\n",
      "        -8.6868, -8.7145], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7910, -8.8007, -8.8332, -8.6165, -8.9905, -8.6165, -8.7575, -8.8347,\n",
      "        -8.6566, -8.6566], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00895481277257204\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.5465, -8.3522, -8.4845, -8.3832], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-8.8859, -8.9970, -9.0979, -8.6586, -8.7034, -8.4938, -8.7706, -8.6161,\n",
      "        -8.8123, -8.6886], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.9505, -8.7816, -8.9703, -8.6328, -8.7990, -8.6445, -8.6445, -8.7545,\n",
      "        -8.8198, -8.8198], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.015168605372309685\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-8.8030, -8.8508, -8.6597, -8.7504, -8.6597, -8.9103, -8.8030, -8.6715,\n",
      "        -8.6326, -8.7504], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7696, -8.6570, -8.7938, -8.6548, -8.7938, -8.7359, -8.7696, -8.8199,\n",
      "        -8.7693, -8.6548], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.016509126871824265\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.8520, -8.5599, -8.7705, -8.5538], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-8.6600, -8.9021, -8.7820, -8.4593, -8.8963, -8.8836, -8.8963, -8.6301,\n",
      "        -8.8313, -8.7437], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7716, -8.5697, -8.7728, -8.8058, -8.8133, -8.8220, -8.8133, -8.7114,\n",
      "        -8.7716, -8.8694], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02865775302052498\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-9.1132, -8.9256, -9.0281, -8.8037], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-8.6838, -8.6382, -8.4868, -8.7142, -8.6774, -8.6927, -8.7142, -8.7142,\n",
      "        -8.6838, -8.7204], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.8154, -8.7081, -8.8096, -8.8428, -8.8096, -8.7526, -8.8428, -8.8428,\n",
      "        -8.8154, -8.7520], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02154221385717392\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-8.7930, -8.7957, -8.7672, -8.6246, -8.7683, -8.9240, -8.8349, -8.8022,\n",
      "        -8.7397, -8.7374], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7622, -8.6843, -8.7520, -8.6990, -8.9137, -8.7399, -8.6843, -8.4187,\n",
      "        -8.8430, -8.7239], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.025472650304436684\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.5397, -8.5031, -8.3630, -8.3181], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-8.7219, -8.7868, -8.9228, -8.8163, -8.8045, -8.7075, -8.3181, -8.7239,\n",
      "        -8.6982, -8.6218], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.8497, -8.7709, -8.9150, -8.7592, -8.9283, -8.8367, 10.0000, -8.7888,\n",
      "        -8.8284, -8.9150], grad_fn=<AddBackward0>)\n",
      "LOSS: 33.57117462158203\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-8.6225, -8.6239, -8.3687, -7.9676], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-8.6239, -8.0029, -7.9738, -7.9676, -8.5076, -8.0029, -7.9440, -7.9676,\n",
      "        -7.9676, -8.0029], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.2628, -8.2026, -8.1764, -8.1709, -8.1503, -8.2026, -8.2328, -8.1709,\n",
      "        -8.1709, -8.2026], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06260974705219269\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-8.0760, -8.2184, -8.1726, -7.3711, -7.4063, -8.0073, -8.1989, -7.3931,\n",
      "        -8.2652, -8.1486], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.6234, -7.6538, -7.6291, -7.6340, -7.6657, -7.6538, -7.6234, -7.6526,\n",
      "        -7.6291, -7.7364], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2053510844707489\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-7.7884, -7.7810, -7.5472, -6.8527], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-7.8340, -6.9207, -7.7884, -7.7565, -7.8739, -7.6398, -6.9428, -7.8032,\n",
      "        -7.7565, -6.9207], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.1675, -7.1247, -7.1675, -7.2291, -7.2116, -7.2539, -7.2177, -7.2291,\n",
      "        -7.2291, -7.1247], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.24622027575969696\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-7.2400, -7.2968, -7.2709, -7.2480, -7.2010, -6.6120, -6.5883, -7.2217,\n",
      "        -6.6100, -6.6868], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.9508, -6.9772, -6.9490, -6.9551, -6.9772, -6.8463, -6.9267, -6.9508,\n",
      "        -7.0513, -6.9295], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09216512739658356\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-6.8037, -6.7790, -6.7428, -6.2299], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-6.8657, -6.8678, -6.7606, -6.9360, -6.8037, -5.4919, -6.7995, -6.7980,\n",
      "        -6.7790, -6.9015], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.7289, -6.6836, -6.6566, -6.7900, -6.6069, 10.0000, -6.6566, -6.6069,\n",
      "        -6.8287, -6.5263], grad_fn=<AddBackward0>)\n",
      "LOSS: 24.03215789794922\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-6.2280, -6.2304, -6.2187, -5.7292], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-6.2823, -6.3212, -5.3197, -5.7292, -5.3197, -6.1630, -5.6965, -6.2281,\n",
      "        -6.1630, -6.0306], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.3213, -6.2706, -5.4297, -6.1563, -5.4297, -6.0800, -6.1269, -6.3883,\n",
      "        -6.0800, -6.2386], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.047862205654382706\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-5.9143, -5.2424, -5.3145, -5.8697, -5.8837, -5.8580, -5.8591, -5.4936,\n",
      "        -5.3039, -5.8580], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.9260, -5.7182, -5.7831, -5.9662, -5.9036, -5.9516, -5.9465, -6.0118,\n",
      "        -5.7735, -5.9516], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09699361771345139\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.2592, -5.3470, -5.3277, -4.8589], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.3219, -4.4102, -5.3219, -5.4846, -4.8669, -5.4071, -5.4071, -4.8589,\n",
      "        -5.4071, -5.1527], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3730, -4.6409, -5.3730, -5.6102, -5.5895, -5.5280, -5.5280, -5.3730,\n",
      "        -5.5280, -5.6142], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11174807697534561\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-4.5277, -5.1974, -4.5277, -5.1077, -4.8175, -5.1600, -4.9584, -4.5277,\n",
      "        -5.2167, -4.8175], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0749, -5.3358, -5.0749, -5.2452, -5.0349, -5.3774, -5.0349, -5.0749,\n",
      "        -5.3774, -5.0349], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11098742485046387\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6800, -4.8216, -4.7931, -4.3297], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.2448, -4.9052, -4.9604, -4.6800, -4.8672, -4.6800, -4.3297, -5.0066,\n",
      "        -4.3297, -4.8672], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8204, -5.1460, -5.0830, -4.8204, -5.0010, -4.8204, -4.8967, -5.0584,\n",
      "        -4.8967, -5.0010], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11251743882894516\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.4584, -4.6181, -4.5724, -4.0901], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.8234, -4.6945, -4.0901, -4.6961, -3.9936, -4.8168, -4.0901, -3.9936,\n",
      "        -4.8284, -4.8179], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9206, -4.9002, -4.6811, -4.8767, -4.5943, -4.9662, -4.6811, -4.5943,\n",
      "        -4.8323, -4.9206], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15373781323432922\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2732, -4.4576, -4.3942, -3.8836], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.3942, -3.9202, -4.3002, -4.5994, -4.6854, -4.6854, -3.7773, -3.9202,\n",
      "        -4.6758, -4.5434], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4953, -4.5282, -4.4953, -4.6939, -4.6352, -4.6352, -4.3996, -4.5282,\n",
      "        -4.7902, -4.7902], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12627029418945312\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-4.0847, -4.3197, -4.4228, -3.5220, -4.2768, -3.8498, -4.5482, -3.7005,\n",
      "        -4.5831, -4.5482], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.1698, -4.5322, -4.5969, -3.4922, -4.2337, -4.5322, -4.5969, -4.3305,\n",
      "        -4.6010, -4.5969], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09530089050531387\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-3.8574, -4.1311, -3.4317, -3.6598, -4.3476, -4.1311, -4.2985, -3.5431,\n",
      "        -3.9980, -3.5431], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.0174, -4.1888, -4.0885, -4.2397, -4.3803, -4.1888, -4.2938, -4.1888,\n",
      "        -4.0885, -4.1888], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1643074005842209\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.9429, -4.1695, -4.0663, -3.4484], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.5317, -4.3837, -3.5317, -3.7366, -3.5567, -4.4023, -4.2348, -3.7366,\n",
      "        -2.9576, -4.1778], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.1163, -4.1785, -4.1163, -3.8836, -3.9635, -4.3630, -4.2818, -3.8836,\n",
      "        -3.2268, -4.2010], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10111268609762192\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.8532, -4.0938, -3.9816, -3.3311], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.3440, -4.2545, -4.3937, -4.1358, -3.9587, -3.0702, -4.3000, -4.1654,\n",
      "        -3.6381, -4.1792], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.2743, -4.2805, -4.2743, -4.1086, -3.9682, -3.1223, -4.2228, -4.1719,\n",
      "        -3.7632, -4.2228], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.004686959553509951\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.7732, -4.0224, -3.9035, -3.2292], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.2000, -3.2000, -3.7732, -4.1362, -3.0626, -3.8058, -4.2308, -3.8861,\n",
      "        -3.5550, -4.2308], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.8800, -3.8800, -3.9063, -4.0273, -3.7564, -4.0952, -4.1286, -3.8800,\n",
      "        -3.6578, -4.1286], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15509526431560516\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.7135, -3.9565, -3.8370, -3.1418], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.9991, -4.1354, -3.8254, -4.1641, -4.0938, -2.6379, -3.8327, -3.1418,\n",
      "        -3.6874, -3.1418], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9503, -3.9428, -3.8053, -4.0470, -3.9669, -2.9541, -3.9950, -3.8276,\n",
      "        -3.6739, -3.8276], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11369027942419052\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.6695, -3.8929, -3.7792, -3.0685], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.9056, -3.2503, -3.9544, -3.9544, -3.5531, -3.9476, -3.0456, -4.0610,\n",
      "        -2.8923, -3.7289], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.8700, -3.9501, -4.0466, -4.0466, -4.0832, -3.8797, -3.7411, -3.8797,\n",
      "        -3.6031, -3.9501], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1864258497953415\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.6440, -3.8466, -3.7303, -3.0024], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.9961, -3.4244, -3.6966, -4.0168, -2.9818, -2.9818, -3.5060, -3.8444,\n",
      "        -3.2382, -3.5615], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.0283, -3.4267, -3.8674, -3.8674, -3.6837, -3.6837, -4.0283, -3.8149,\n",
      "        -3.8258, -3.6837], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16714730858802795\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.6264, -3.8178, -3.6931, -2.9426], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.4473, -4.0329, -3.6914, -3.9692, -3.9988, -3.6914, -3.6475, -3.6914,\n",
      "        -4.0329, -2.9426], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6483, -3.8673, -3.6317, -3.9706, -3.8218, -3.6317, -3.8131, -3.6317,\n",
      "        -3.8673, -3.6483], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06627219915390015\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.6065, -3.7855, -3.6530, -2.8896], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.0078, -3.9456, -4.0078, -4.1576, -3.6554, -3.9764, -4.0119, -3.8730,\n",
      "        -3.6554, -3.9764], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.8315, -3.9158, -3.8315, -3.9158, -3.5867, -3.7827, -3.8315, -3.8315,\n",
      "        -3.5867, -3.7827], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.024021584540605545\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.5706, -3.7328, -3.6060, -2.8484], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.9176, -3.4462, -2.8354, -2.8484, -3.9915, -2.8354, -3.7794, -3.9288,\n",
      "        -2.6764, -2.8484], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.8676, -3.5519, -3.5519, -3.5635, -3.8539, -3.5519, -3.6800, -3.7529,\n",
      "        -3.4087, -3.5635], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2659275531768799\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.5397, -3.6823, -3.5656, -2.8191], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.7429, -3.3900, -2.8191, -3.8791, -3.6356, -3.5656, -2.6501, -2.3989,\n",
      "        -3.8060, -3.8791], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6659, -3.8309, -3.5372, -3.7315, -3.6783, -3.3851, -3.3851, -2.6572,\n",
      "        -3.6502, -3.7315], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2271224558353424\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-3.8912, -3.3512, -3.8912, -3.3512, -3.7511, -3.2858, -2.3618, -3.6318,\n",
      "        -3.5272, -3.8683], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.8063, -3.5130, -3.8063, -3.5130, -3.6267, -3.2802, -2.6307, -3.5821,\n",
      "        -3.3667, -3.7940], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.018830865621566772\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3436, -3.4689, -3.3607, -2.6163], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.5885, -3.4689, -2.9045, -3.7078, -2.6163, -3.5036, -3.8412, -3.5885,\n",
      "        -2.7628, -2.7628], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5550, -3.6141, -3.3547, -3.7865, -3.3547, -3.4830, -3.7865, -3.5550,\n",
      "        -3.4865, -3.4865], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.18282920122146606\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-3.7471, -3.4092, -3.0400, -3.4741, -3.7471, -3.7471, -3.0400, -3.8329,\n",
      "        -2.7414, -3.2517], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6853, -3.6853, -3.2633, -3.4673, -3.6853, -3.6853, -3.2633, -3.7663,\n",
      "        -3.4673, -3.2633], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07189164310693741\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.5584, -3.5854, -3.5815, -2.8732], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.7617, -2.7272, -2.7246, -3.5110, -3.4486, -3.5113, -3.4489, -3.4486,\n",
      "        -3.5110, -3.7807], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7582, -3.4545, -3.4522, -3.5133, -3.4545, -3.5133, -3.5812, -3.4545,\n",
      "        -3.5133, -3.6743], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1087162047624588\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3354, -3.3973, -3.3080, -2.5909], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.3907, -3.0082, -2.9630, -3.4391, -3.6807, -3.2688, -3.4802, -3.7245,\n",
      "        -3.4391, -2.7107], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3318, -3.2456, -3.4247, -3.5660, -3.6667, -3.7074, -3.4978, -3.4247,\n",
      "        -3.5660, -3.4396], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11193076521158218\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-3.5064, -2.5859, -2.7070, -3.7055, -3.4020, -3.4373, -2.6987, -3.6683,\n",
      "        -3.3636, -3.4386], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5676, -3.3273, -3.4363, -3.7363, -3.4363, -3.4288, -3.4288, -3.6653,\n",
      "        -3.6653, -3.5533], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17248299717903137\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.5671, -3.5393, -3.5175, -2.8436], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.6958, -3.1905, -3.7525, -3.5393, -2.6987, -3.4083, -3.3693, -3.0583,\n",
      "        -2.5818, -3.2800], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7269, -3.2384, -3.6813, -3.5572, -3.4288, -3.5592, -3.6606, -3.5572,\n",
      "        -3.3236, -3.4288], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1470687836408615\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3791, -3.4025, -3.2826, -2.5800], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.6791, -3.6987, -3.5188, -2.6900, -3.4702, -3.2826, -2.5800, -3.3795,\n",
      "        -2.8329, -3.3356], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4112, -3.7177, -3.5496, -3.4210, -3.4603, -3.4210, -3.3220, -3.4210,\n",
      "        -3.3220, -3.3220], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1882590353488922\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4005, -3.4164, -3.2892, -2.5791], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.0135, -3.4743, -2.5791, -2.5791, -3.6742, -3.3794, -3.3794, -3.4164,\n",
      "        -3.6696, -3.6742], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6479, -3.5260, -3.3212, -3.3212, -3.6492, -3.4159, -3.4159, -3.5483,\n",
      "        -3.6630, -3.6492], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1527933031320572\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4265, -3.4358, -3.3020, -2.5825], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.5825, -3.5308, -3.4975, -3.3831, -3.4265, -2.6807, -2.5825, -3.4066,\n",
      "        -2.9421, -2.1835], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3242, -3.5255, -3.6384, -3.4126, -3.4042, -3.4126, -3.3242, -3.4477,\n",
      "        -3.4272, -2.5047], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.19974932074546814\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-3.6514, -2.6706, -3.5377, -3.5843, -3.3966, -3.7446, -2.9497, -3.7489,\n",
      "        -3.3371, -3.8191], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7739, -3.4035, -3.5191, -3.7739, -3.4119, -3.6547, -3.2384, -3.7243,\n",
      "        -3.6547, -3.7739], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07836945354938507\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4841, -3.4923, -3.3490, -2.5926], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.8321, -2.9098, -3.4923, -3.8226, -3.4142, -3.4251, -3.5297, -3.5531,\n",
      "        -3.6716, -3.7804], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3333, -3.5194, -3.5489, -3.6505, -3.4103, -3.4394, -3.4394, -3.5149,\n",
      "        -3.5194, -3.7292], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06912735104560852\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-2.1942, -3.2658, -3.3929, -3.8959, -3.5139, -2.5966, -3.8053, -3.5465,\n",
      "        -3.8053, -3.8327], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.4888, -3.2457, -3.6529, -3.7779, -3.5558, -3.3369, -3.7380, -3.5166,\n",
      "        -3.7380, -3.6529], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07608696073293686\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.7103, -3.6662, -3.5920, -2.8471], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.7933, -3.7009, -2.9497, -2.9745, -2.5061, -3.6427, -4.0063, -3.5350,\n",
      "        -3.4312, -3.3754], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6548, -3.6548, -3.2555, -3.4350, -2.4858, -3.7782, -3.6771, -3.5624,\n",
      "        -3.4129, -3.4129], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.045652344822883606\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-3.8276, -3.4406, -3.5599, -2.6813, -3.4359, -2.6850, -3.8439, -2.6813,\n",
      "        -2.6100, -3.8117], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6628, -3.6628, -3.4132, -3.4132, -3.4165, -3.4165, -3.7567, -3.4132,\n",
      "        -3.3490, -3.6966], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2271830290555954\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-3.7086, -3.5486, -3.7657, -2.6922, -2.9709, -3.8346, -3.4451, -3.6964,\n",
      "        -3.4361, -3.9733], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6738, -3.7185, -3.6362, -3.4230, -3.2742, -3.7185, -3.5842, -3.4421,\n",
      "        -3.4486, -3.7185], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08354631066322327\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.5494, -3.5790, -3.3957, -2.7063], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.8530, -3.7083, -2.7063, -3.9117, -2.7063, -3.5749, -2.7063, -3.8538,\n",
      "        -2.7048, -2.7063], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7429, -3.6906, -3.4357, -3.7864, -3.4357, -3.5400, -3.4357, -3.7700,\n",
      "        -3.4343, -3.4357], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.26964956521987915\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.5393, -3.5883, -3.3965, -2.7288], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.5865, -3.5865, -2.7288, -3.7053, -3.8358, -3.4277, -3.4219, -2.6545,\n",
      "        -3.8708, -2.8273], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6160, -3.6160, -3.4559, -3.7104, -3.7104, -3.4493, -3.4690, -3.3891,\n",
      "        -3.8064, -3.4559], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1487806737422943\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.5237, -3.5883, -3.3925, -2.7454], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.6748, -2.7668, -2.7454, -3.4262, -2.7454, -2.9270, -2.9270, -3.4401,\n",
      "        -3.6621, -3.8753], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4073, -3.6343, -3.4708, -3.4653, -3.4708, -3.4073, -3.4073, -3.6343,\n",
      "        -3.4901, -3.7988], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.28777751326560974\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.5138, -3.5908, -3.3942, -2.7669], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.4823, -2.6892, -2.7599, -3.5968, -2.7669, -3.6184, -2.8047, -3.4823,\n",
      "        -3.8729, -3.7067], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4902, -3.4203, -3.4839, -3.8295, -3.4902, -3.5242, -3.6564, -3.4902,\n",
      "        -3.8499, -3.7532], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.23732233047485352\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.5068, -3.5995, -3.4000, -2.7890], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.6373, -3.6983, -3.4333, -3.8589, -3.8461, -2.7802, -3.8069, -3.8176,\n",
      "        -3.8804, -2.7802], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.8557, -3.8031, -3.5021, -3.8731, -3.8031, -3.5021, -3.8031, -3.8557,\n",
      "        -3.8731, -3.5021], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1109507828950882\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-3.6153, -3.4785, -3.4098, -2.8088, -3.4445, -3.5224, -3.1953, -2.8000,\n",
      "        -2.8000, -3.8478], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6374, -3.6825, -3.4454, -3.5279, -3.5200, -3.6374, -3.6152, -3.5200,\n",
      "        -3.5200, -3.8112], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17939096689224243\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.5367, -3.6803, -3.5066, -2.9565], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.9138, -2.8297, -2.8297, -3.8598, -2.9918, -3.8537, -2.8297, -3.3412,\n",
      "        -3.8537, -3.4433], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9139, -3.5467, -3.5467, -3.9139, -3.4568, -3.8202, -3.5467, -3.3830,\n",
      "        -3.8202, -3.5453], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17759887874126434\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.5252, -3.6582, -3.4586, -2.8552], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.6827, -3.6627, -3.5535, -3.5360, -2.7401, -3.3642, -3.3642, -2.7401,\n",
      "        -1.7138, -3.5252], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6583, -3.6851, -3.6851, -3.7106, -3.4661, -3.4006, -3.4006, -3.4661,\n",
      "        10.0000, -3.5697], grad_fn=<AddBackward0>)\n",
      "LOSS: 13.832140922546387\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.5127, -3.6503, -3.4509, -2.7990], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.6058, -2.7883, -3.4865, -2.9297, -2.7990, -2.2390, -3.8174, -2.7883,\n",
      "        -3.6628, -3.6628], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6543, -3.5094, -3.5094, -3.5191, -3.5191, -2.4853, -3.8855, -3.5094,\n",
      "        -3.5090, -3.5090], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.20216019451618195\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.4990, -3.6350, -3.4391, -2.7455], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.7455, -3.1557, -3.7304, -3.5161, -3.7304, -3.5474, -3.0456, -2.7385,\n",
      "        -2.6179, -3.9115], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4709, -3.6246, -3.7214, -3.6061, -3.7214, -3.7411, -3.3135, -3.4646,\n",
      "        -3.3561, -3.8419], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1940767765045166\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.4922, -3.6264, -3.4323, -2.7010], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.5770, -2.5712, -3.9503, -2.7010, -3.4755, -3.9052, -3.9052, -2.5712,\n",
      "        -3.0058, -3.5770], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5648, -3.3140, -3.8135, -3.4309, -3.4279, -3.8058, -3.8058, -3.3140,\n",
      "        -3.2793, -3.5648], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17523443698883057\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.4856, -3.6119, -3.4256, -2.6675], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.8146, -3.5659, -2.6660, -3.7919, -2.6675, -3.6119, -3.4707, -3.9446,\n",
      "        -2.6675, -3.6074], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2819, -3.5545, -3.3994, -3.7950, -3.4007, -3.5545, -3.3994, -3.7950,\n",
      "        -3.4007, -3.4067], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.19027535617351532\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.4786, -3.5944, -3.4142, -2.6441], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.4434, -3.8685, -3.5814, -3.5707, -3.7609, -3.7609, -2.0827, -2.6441,\n",
      "        -2.6441, -3.8687], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3894, -3.7597, -3.4691, -3.5422, -3.6221, -3.6221, -2.3294, -3.3797,\n",
      "        -3.3797, -3.7597], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12216021865606308\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-3.6845, -3.4664, -3.5914, -3.4508, -3.9073, -2.6277, -2.4807, -3.4664,\n",
      "        -3.4508, -2.6277], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7774, -3.3649, -3.3777, -3.3677, -3.7774, -3.3649, -3.2326, -3.3649,\n",
      "        -3.3677, -3.3649], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17580115795135498\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.4517, -3.5385, -3.3861, -2.6214], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.8080, -3.7809, -3.0835, -3.8080, -3.6628, -3.5620, -3.8440, -3.6872,\n",
      "        -2.6211, -3.3449], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6544, -3.7752, -3.5523, -3.6544, -3.7752, -3.5338, -3.7445, -3.5946,\n",
      "        -3.3590, -3.3592], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08436381816864014\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.4353, -3.5014, -3.3699, -2.6191], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.5457, -3.7139, -2.7638, -3.8298, -3.4138, -3.4136, -3.2833, -3.6557,\n",
      "        -3.8340, -3.3699], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3739, -3.6550, -3.2100, -3.7457, -3.3739, -3.3555, -3.1955, -3.7457,\n",
      "        -3.7808, -3.2100], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.028835654258728027\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-2.7671, -2.4473, -3.4794, -3.2385, -2.4473, -2.8785, -2.6171, -3.5602,\n",
      "        -2.4473, -2.6442], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2026, -3.2026, -3.3798, -3.3554, -3.2026, -3.4390, -3.3554, -3.6589,\n",
      "        -3.2026, -3.3554], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.32992416620254517\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.3939, -3.4275, -3.3165, -2.6211], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.6211, -3.4783, -3.5763, -3.4984, -3.5157, -3.6875, -2.6638, -2.8142,\n",
      "        -3.5321, -2.0531], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3590, -3.5031, -3.6043, -3.5031, -3.6043, -3.7592, -3.3511, -3.5533,\n",
      "        -3.5533, -2.2533], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1618141382932663\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-3.4979, -2.8361, -3.3200, -3.7336, -2.6263, -3.3206, -2.4442, -2.6008,\n",
      "        -3.3023, -2.4442], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4212, -3.3636, -3.3407, -3.8118, -3.3636, -3.5154, -3.1998, -3.3407,\n",
      "        -3.1998, -3.1998], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2572159171104431\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.3735, -3.3835, -3.2804, -2.6254], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.6519, -3.0910, -3.3093, -2.8457, -3.5133, -3.7202, -3.3735, -3.6381,\n",
      "        -3.2804, -3.3834], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7819, -3.5661, -3.5262, -3.3629, -3.5746, -3.8246, -3.3629, -3.6841,\n",
      "        -3.2053, -3.5612], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06112374737858772\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-3.7275, -3.2596, -2.5855, -2.8587, -2.4563, -3.2020, -3.6668, -3.5352,\n",
      "        -2.6170, -2.4563], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.8349, -3.2107, -3.3270, -3.5716, -3.2107, -3.2101, -3.7992, -3.6905,\n",
      "        -3.3553, -3.2107], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.27968600392341614\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.2784, -3.3376, -3.1872, -2.4672], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.2479, -2.8768, -3.5474, -3.3395, -3.1872, -2.5814, -3.4635, -3.4813,\n",
      "        -3.2946, -3.2784], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2205, -3.5405, -3.5746, -3.6968, -3.3232, -3.3232, -3.5405, -3.6581,\n",
      "        -3.2160, -3.3511], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11871983110904694\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.2995, -3.3713, -3.2044, -2.4768], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.6038, -3.2434, -3.2044, -3.7730, -2.6038, -3.2044, -3.6589, -3.7278,\n",
      "        -2.9645, -3.3914], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3434, -3.2291, -3.3164, -3.8524, -3.3434, -3.3164, -3.8524, -3.8524,\n",
      "        -3.4424, -3.5746], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14405979216098785\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3294, -3.4195, -3.2361, -2.4874], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.1690, -3.2590, -1.9296, -3.7259, -2.5998, -2.5627, -3.7390, -3.2590,\n",
      "        -2.5627, -3.5526], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6182, -3.3064, -2.2273, -3.6924, -3.3398, -3.3064, -3.6841, -3.3064,\n",
      "        -3.3064, -3.5706], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.19532838463783264\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-3.7575, -3.6292, -3.2790, -3.3590, -3.2743, -3.2760, -2.5573, -2.5997,\n",
      "        -3.8325, -3.4572], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.8621, -3.6280, -3.2497, -3.3398, -3.3016, -3.2398, -3.3016, -3.3398,\n",
      "        -3.8629, -3.4342], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11172477900981903\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-2.8170, -3.5797, -3.0214, -3.6719, -3.4957, -2.6027, -2.5120, -3.8865,\n",
      "        -3.8959, -3.4483], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2608, -3.5706, -3.4327, -3.6515, -3.6912, -3.3425, -3.2608, -3.8785,\n",
      "        -3.8707, -3.5456], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15228548645973206\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.5939, -3.7072, -3.5461, -2.8587], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.7991, -2.9898, -2.6113, -3.8327, -1.3616, -2.6113, -3.3267, -3.3074,\n",
      "        -3.7268, -3.4809], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.8818, -3.2617, -3.3502, -3.7437, 10.0000, -3.3502, -3.2703, -3.3002,\n",
      "        -3.6908, -3.5595], grad_fn=<AddBackward0>)\n",
      "LOSS: 13.02778148651123\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.4101, -3.5390, -3.3177, -2.5526], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.8613, -3.3313, -2.4642, -2.5526, -2.9118, -2.4916, -3.3313, -3.2925,\n",
      "        -3.3177, -3.9607], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7055, -3.2425, -3.2178, -3.2973, -3.5155, -3.2425, -3.2425, -3.2425,\n",
      "        -3.2178, -3.8512], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.21152746677398682\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.3977, -3.5396, -3.3006, -2.5059], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.4850, -3.8363, -3.2742, -3.2742, -3.8774, -3.2742, -3.9732, -2.4181,\n",
      "        -1.9050, -2.5059], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4713, -3.3118, -3.1986, -3.1986, -3.8184, -3.1986, -3.8184, -3.1763,\n",
      "        -2.1149, -3.2553], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15004046261310577\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.3830, -3.5240, -3.2815, -2.4725], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.4049, -3.2508, -2.4725, -3.5530, -2.3839, -3.2815, -3.7303, -3.5331,\n",
      "        -3.3916, -3.3332], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.1644, -3.1644, -3.2253, -3.5535, -3.1455, -3.1455, -3.5535, -3.4200,\n",
      "        -3.1469, -3.1455], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.18886587023735046\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.3521, -3.5018, -3.2497, -2.4479], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.4943, -3.7068, -2.3638, -2.6543, -2.4479, -3.5018, -3.6498, -2.6596,\n",
      "        -3.6150, -3.9483], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3888, -3.5315, -3.1274, -3.2032, -3.2032, -3.3888, -3.5690, -3.1274,\n",
      "        -3.5315, -3.7850], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17683449387550354\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.3077, -3.4613, -3.2012, -2.4216], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.8105, -2.3487, -2.4216, -2.3571, -3.9214, -3.8492, -3.0415, -3.8302,\n",
      "        -3.0415, -2.4216], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7374, -3.1138, -3.1794, -3.1214, -3.7819, -3.7819, -3.5173, -3.6420,\n",
      "        -3.5173, -3.1794], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.28357020020484924\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.2694, -3.4195, -3.1590, -2.4091], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.1401, -3.5457, -3.7718, -3.8861, -3.1590, -2.4091, -3.7786, -3.6367,\n",
      "        -3.7928, -2.3495], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.1146, -3.5206, -3.6440, -3.7885, -3.1100, -3.1682, -3.7554, -3.5206,\n",
      "        -3.6440, -3.1146], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1227203831076622\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.2317, -3.3744, -3.1115, -2.4037], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.5055, -2.4037, -2.8068, -2.9458, -3.5376, -3.2779, -3.6717, -2.6644,\n",
      "        -3.0837, -2.3452], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5262, -3.1633, -3.1107, -3.2368, -3.3980, -3.1633, -3.2368, -3.1112,\n",
      "        -3.5016, -1.9836], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14813141524791718\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.1968, -3.3302, -3.0728, -2.4103], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.7756, -3.4579, -3.4089, -2.3539, -2.3539, -3.8373, -2.3476, -2.8237,\n",
      "        -3.7938, -2.6450], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4196, -3.3805, -3.4196, -3.1185, -3.1185, -3.8059, -3.1129, -3.1052,\n",
      "        -3.8169, -3.1693], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2531292736530304\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-2.3557, -3.3316, -3.4480, -3.6563, -2.4093, -2.7044, -3.3438, -2.8495,\n",
      "        -3.2562, -2.3557], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.1201, -3.5646, -3.5646, -3.6877, -3.1684, -3.1201, -3.3995, -3.1001,\n",
      "        -3.4340, -3.1201], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2084043025970459\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.1766, -3.3369, -3.0740, -2.3589], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.6286, -3.7253, -3.5032, -2.7974, -2.7246, -3.2285, -3.2285, -2.8743,\n",
      "        -3.8012, -3.8012], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7065, -3.8577, -3.5869, -3.4521, -3.1230, -3.3850, -3.3850, -3.0963,\n",
      "        -3.8701, -3.8701], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07257825136184692\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-3.4472, -2.4155, -2.4155, -3.1052, -2.8976, -2.7072, -3.0192, -3.6251,\n",
      "        -3.5233, -2.3541], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6079, -3.1739, -3.1739, -3.1187, -3.0881, -3.1739, -3.0881, -3.7221,\n",
      "        -3.6562, -3.1187], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.20469483733177185\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.1064, -3.2338, -3.0132, -2.3998], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.5268, -2.3998, -2.3998, -2.4120, -2.6419, -2.4120, -2.7523, -2.3516,\n",
      "        -3.8296, -3.6310], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6718, -3.1598, -3.1598, -3.1708, -3.1598, -3.1708, -3.1165, -3.1165,\n",
      "        -3.9228, -3.7375], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3333732485771179\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.1114, -3.2438, -3.0184, -2.4106], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.0904, -3.8569, -3.2438, -3.2793, -3.6541, -2.3512, -3.3851, -3.7064,\n",
      "        -3.0398, -2.4171], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.0753, -3.9514, -3.3876, -3.5759, -3.6595, -3.1160, -3.7552, -3.9077,\n",
      "        -3.1696, -3.1754], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14722320437431335\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.1232, -3.2726, -3.0319, -2.4175], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.4201, -2.4201, -2.9928, -3.5672, -2.3505, -3.4424, -3.7298, -3.0319,\n",
      "        -2.4175, -2.4175], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.1781, -3.1781, -3.0689, -3.5883, -3.1155, -3.5042, -3.9180, -3.1758,\n",
      "        -3.1758, -3.1758], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2950301170349121\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.1463, -3.3066, -3.0552, -2.4279], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.8469, -3.7108, -3.2557, -3.1463, -3.2660, -2.3530, -2.3530, -2.3530,\n",
      "        -3.6393, -3.6006], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7703, -3.7703, -3.3411, -3.1177, -3.3954, -3.1177, -3.1177, -3.1177,\n",
      "        -4.0069, -3.7229], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.19386674463748932\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-3.1883, -2.8386, -2.3575, -3.0890, -3.0829, -3.0226, -3.2931, -2.4357,\n",
      "        -2.4357, -2.3575], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.1917, -3.1217, -3.1217, -3.1921, -3.1921, -3.0612, -3.3557, -3.1921,\n",
      "        -3.1921, -3.1217], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.24206490814685822\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4499, -3.6030, -3.3180, -2.6715], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.2780, -3.9005, -3.9005, -3.4223, -3.3180, -2.6715, -3.7476, -3.4499,\n",
      "        -3.7764, -2.3648], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7548, -3.9502, -3.9502, -3.5786, -3.4044, -3.2021, -3.8001, -3.5786,\n",
      "        -4.0407, -3.1283], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12178635597229004\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-3.4193, -2.4512, -2.4512, -3.7098, -1.0481, -2.4512, -2.4512, -4.0241,\n",
      "        -2.4512, -2.0246], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4182, -3.2061, -3.2061, -3.7601, 10.0000, -3.2061, -3.2061, -3.9755,\n",
      "        -3.2061, -3.0575], grad_fn=<AddBackward0>)\n",
      "LOSS: 12.598145484924316\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.2638, -3.4417, -3.1724, -2.3811], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.3972, -2.3972, -2.8026, -4.1554, -3.3963, -3.4417, -3.2451, -2.3972,\n",
      "        -3.1584, -0.9768], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.1575, -3.1575, -3.1575, -3.9975, -3.3667, -3.3667, -3.1575, -3.1575,\n",
      "        -2.9831, 10.0000], grad_fn=<AddBackward0>)\n",
      "LOSS: 12.24210262298584\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.2418, -3.4358, -3.1600, -2.2699], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.8664, -3.1156, -3.7417, -1.9809, -3.4912, -3.5814, -3.6329, -3.1695,\n",
      "        -2.2949, -2.2949], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([10.0000, -2.8729, -3.6137, -2.8729, -3.4409, -3.4374, -3.4374, -3.0429,\n",
      "        -3.0654, -3.0654], grad_fn=<AddBackward0>)\n",
      "LOSS: 12.021459579467773\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.1882, -3.4168, -3.1207, -2.1183], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.9548, -2.1524, -2.1183, -3.4168, -2.3287, -2.1524, -2.1183, -3.3579,\n",
      "        -2.1183, -3.1207], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.7175, -2.9372, -2.9065, -3.1397, -3.1397, -2.9372, -2.9065, -3.1397,\n",
      "        -2.9065, -2.9065], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.39795738458633423\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-1.8945, -3.0595, -1.8945, -3.6628, -3.9278, -1.8945, -3.0595, -2.0386,\n",
      "        -3.1335, -1.8945], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.7050, -2.7050, -2.7050, -3.3590, -3.6275, -2.7050, -2.7050, -2.8348,\n",
      "        -2.7050, -2.7050], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.38792163133621216\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-1.8846, -1.8010, -3.0676, -3.5625, -1.8010, -2.7742, -2.9864, -3.4050,\n",
      "        -1.8439, -3.5868], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.6962, -2.6209, -2.7411, -3.2689, -2.6209, -2.4809, -2.6962, -3.0916,\n",
      "        -2.4809, -3.2689], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2971110939979553\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.9633, -3.2948, -2.8769, -1.7965], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.8769, -3.2948, -0.4142, -2.8769, -1.7260, -3.8405, -3.3032, -3.7429,\n",
      "        -2.0660, -1.8493], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.6168, -2.8516, 10.0000, -2.6168, -2.5534, -3.2001, -3.0216, -3.4988,\n",
      "        -2.8516, -2.6644], grad_fn=<AddBackward0>)\n",
      "LOSS: 11.130285263061523\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.8432, -3.2008, -2.7328, -1.6809], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.2008, -2.9833, -1.7427, -3.1230, -3.9036, -2.7390, -3.2612, -3.1783,\n",
      "        -2.1964, -3.9036], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.7505, -2.7505, -2.5684, -2.9366, -3.4259, -2.4637, -2.9760, -2.7505,\n",
      "        -2.9366, -3.4259], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.231782004237175\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.7049, -3.0704, -2.5690, -1.5894], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.6618, -1.6618, -1.9397, -3.1754, -1.5894, -3.4083, -3.4723, -3.3582,\n",
      "        -2.2808, -3.1754], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.4957, -2.4957, -2.6870, -3.0527, -2.4305, -3.3435, -2.9270, -2.7457,\n",
      "        -2.1888, -3.0527], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.33716070652008057\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-3.1987, -3.6373, -2.7245, -3.4866, -1.5212, -1.9313, -3.6373, -1.6075,\n",
      "        -1.5911, -1.5212], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.0429, -3.3587, -3.0132, -3.0132, -2.3690, -2.6283, -3.3587, -2.4468,\n",
      "        -2.1236, -2.3690], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3398451805114746\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.7509, -3.0842, -2.5447, -1.7492], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.5681, -1.4672, -2.5972, -1.7492, -1.4680, -2.2792, -3.4889, -2.1988,\n",
      "        -2.5141, -2.4131], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.4113, -2.3205, -2.9789, -2.3212, -2.3212, -2.3212, -3.3485, -2.0653,\n",
      "        -2.4113, -2.3212], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.26981422305107117\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-2.9483, -2.4978, -1.5388, -2.6208, -1.5388, -2.6462, -1.4181, -1.1351,\n",
      "        -2.1476, -1.4355], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.9893, -2.9408, -2.3849, -2.7247, -2.3849, -2.7247, -2.2763, -1.0297,\n",
      "        -2.8240, -2.2920], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.35852891206741333\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-1.9924, -1.5164, -2.8409, -1.3738, -1.9924, -2.1091, -3.0452, -1.5164,\n",
      "        -1.4088, -2.5605], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.3648, -2.3648, -2.9673, -2.2364, -2.3648, -1.9850, -3.3291, -2.3648,\n",
      "        -2.2679, -2.6891], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.33273568749427795\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-1.9759, -2.5205, -3.2279, -2.5650, -1.9759, -1.4844, -1.7988, -1.3370,\n",
      "        -3.1867, -2.5205], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2033, -2.4729, -3.2143, -2.8603, -2.2033, -2.3359, -1.9544, -2.2033,\n",
      "        -3.3128, -2.4729], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17110145092010498\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2211, -2.5295, -1.9722, -1.4546], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.4546, -1.4546, -2.7036, -2.4443,  0.0872, -2.4604, -3.0297, -2.9132,\n",
      "        -2.6172, -2.2211], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.3092, -2.3092, -2.6265, -2.8740, 10.0000, -2.4457, -2.9298, -2.9180,\n",
      "        -2.7639, -2.3092], grad_fn=<AddBackward0>)\n",
      "LOSS: 9.995321273803711\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1743, -2.4741, -1.9199, -1.3960], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.1355, -3.1068, -2.9874, -1.7022, -1.7476, -1.3093, -2.0952, -2.4028,\n",
      "        -2.3570, -1.5491], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.3942, -3.1709, -2.8905, -1.8714, -2.1784, -2.1784, -2.1784, -2.3942,\n",
      "        -2.7481, -2.1126], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15273654460906982\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1387, -2.4300, -1.8878, -1.3415], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.8753, -1.2524, -1.3415, -1.3415, -2.0619, -3.0611, -1.6724, -2.3525,\n",
      "        -1.3415, -1.6724], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2307, -2.1272, -2.2074, -2.2074, -2.6797, -3.1447, -1.8216, -2.3529,\n",
      "        -2.2074, -1.8216], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.357381671667099\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1211, -2.3999, -1.8705, -1.3004], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.8913, -1.3004, -1.6535, -1.1110, -2.0038, -1.2704, -1.2041, -1.8014,\n",
      "        -1.3004, -1.3004], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.1258, -2.1704, -1.7764, -1.9999, -1.9999, -1.7764, -2.0837, -1.7764,\n",
      "        -2.1704, -2.1704], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4161030650138855\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-1.0568, -2.5913, -1.2687, -1.2687, -1.9926, -1.0568, -1.1594, -1.2687,\n",
      "        -1.7652, -2.0704], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.9511, -2.7222, -2.1418, -2.1418, -1.9511, -1.9511, -2.0435, -2.1418,\n",
      "        -2.1418, -2.2813], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4873420298099518\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-1.1233, -1.0152, -2.6513, -1.0152, -1.6252,  0.3988, -0.7616, -2.8855,\n",
      "        -1.0152, -1.0152], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.0110, -1.9137, -2.5174, -1.9137, -1.6855, 10.0000, -0.6411, -3.1507,\n",
      "        -1.9137, -1.9137], grad_fn=<AddBackward0>)\n",
      "LOSS: 9.630624771118164\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2133, -2.5538, -1.9827, -1.0586], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.1924, -1.3371, -1.1875, -2.4604, -2.1698, -0.9539, -2.1924, -3.0073,\n",
      "        -2.5717, -2.5717], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.4519, -1.8586, -2.0687, -2.7237, -2.4519, -1.8586, -2.4519, -3.0507,\n",
      "        -2.6711, -2.6711], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2172040492296219\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-3.0190, -1.1313, -2.4853, -1.2882, -0.8850, -1.8701, -2.4853, -2.3062,\n",
      "        -3.0190, -2.3740], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.0109, -2.0182, -2.4547, -1.7965, -1.7965, -1.8920, -2.4547, -2.7499,\n",
      "        -3.0109, -2.4547], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.20815058052539825\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-2.4213, -2.0848, -1.8949, -3.0326, -0.9298, -1.0854, -0.8175, -2.4887,\n",
      "        -2.0848, -2.5987], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.3111, -2.1284, -2.4054, -2.9751, -1.8368, -1.9769, -1.7357, -2.4054,\n",
      "        -2.1284, -2.5840], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.274760901927948\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2237, -2.5790, -2.0065, -0.8814], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.8983, -2.6290, -2.3842, -2.8719, -2.8719, -1.6258, -2.1280, -1.8983,\n",
      "        -1.8725, -2.1936], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7933, -2.9483, -2.3574, -3.0398, -3.0398, -1.4570, -1.9477, -1.7933,\n",
      "        -2.3574, -2.2589], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.048152126371860504\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-0.8406, -2.8519, -1.8289, -0.8406, -0.7114, -0.8406, -1.0218, -3.0581,\n",
      "        -2.8911, -2.8328], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7566, -3.0290, -1.4151, -1.7566, -1.6402, -1.7566, -1.9196, -2.9142,\n",
      "        -3.0290, -2.5472], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4509557783603668\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.3886, -2.6340, -2.2027, -1.3275], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.6796, -2.1074, -2.4633, -2.8320, -2.6374, -2.8320, -1.7522, -2.9051,\n",
      "        -1.0071, -2.4633], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.6116, -2.5291, -2.2639, -2.5289, -2.5146, -2.5289, -1.6116, -3.0251,\n",
      "        -1.9064, -2.2639], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.21678335964679718\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2201, -2.5900, -2.0307, -0.8128], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.1884, -1.2844, -2.2484, -2.2201, -2.8084, -0.8128, -2.1325, -0.8128,\n",
      "        -0.6662, -2.9152], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.5996, -1.3702, -2.0696, -1.9113, -2.5384, -1.7315, -1.9113, -1.7315,\n",
      "        -1.5996, -2.9088], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2984999120235443\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1961, -2.5660, -2.0270, -0.8204], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.8204, -2.6722, -2.8927, -1.0442, -0.6568, -1.7005, -0.6568, -0.8204,\n",
      "        -2.1924, -2.3607], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7383, -2.7377, -3.0515, -1.9398, -1.5911, -2.2035, -1.5911, -1.7383,\n",
      "        -2.1559, -2.2441], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.45307546854019165\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1769, -2.5494, -2.0263, -0.8361], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.9541, -2.9397, -1.7269, -2.0263, -0.6575, -2.0263, -2.7480, -2.0263,\n",
      "        -2.8987, -2.9397], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7525, -2.5974, -1.5917, -1.5917, -1.5917, -1.5917, -2.5974, -1.5917,\n",
      "        -3.0727, -2.5974], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17857258021831512\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1403, -2.4963, -1.9779, -0.8650], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.5750, -0.8650, -1.9306, -1.7027, -0.6522, -2.1403, -2.8865, -2.8376,\n",
      "        -0.6996, -2.9809], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3769, -1.7785, -1.7785, -1.6297, -0.2062, -2.0229, -2.6648, -3.1011,\n",
      "        -1.6297, -3.0097], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2099398821592331\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1104, -2.4451, -1.9294, -0.9067], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.7925, -2.8262, -0.9067, -1.9001, -2.3835, -2.9390, -2.8262, -2.8261,\n",
      "        -2.8290, -1.5362], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.0541, -3.1304, -1.8160, -1.8160, -2.2176, -3.0541, -3.1304, -3.1304,\n",
      "        -2.7405, -1.4003], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12472233921289444\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.0970, -2.4179, -1.8882, -0.9433], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.6449, -2.1012, -1.6449, -2.1186, -0.9433, -1.9919, -2.9249, -0.9433,\n",
      "        -2.7798, -0.7886], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7097, -2.2900, -1.7097, -2.2370, -1.8490, -2.6691, -3.0774, -1.8490,\n",
      "        -3.0774, -1.7097], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3117472529411316\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.0933, -2.4087, -1.8631, -0.9771], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.8524, -0.9771, -0.9771, -2.1028, -2.4626, -0.9771, -1.2752, -2.3727,\n",
      "        -2.6292, -2.0537], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.8794, -1.8794, -1.8794, -2.1477, -2.2611, -1.8794, -2.1477, -2.2611,\n",
      "        -3.0822, -2.2621], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.35080498456954956\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1041, -2.4042, -1.8491, -1.0160], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.4300, -1.8434, -2.0667, -2.1041, -2.7981, -2.4011, -1.3137, -1.8491,\n",
      "        -1.6210, -1.3137], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4521, -1.9144, -2.3122, -2.1823, -3.0789, -2.6777, -2.1823, -1.7715,\n",
      "        -1.7715, -2.1823], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1764942705631256\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-2.3634, -2.4645, -2.1190, -1.0416, -1.6253, -1.3470, -2.0655, -0.8844,\n",
      "        -1.5358, -1.3470], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.3121, -2.3056, -2.2123, -1.9374, -1.7960, -2.2123, -2.2970, -1.7960,\n",
      "        -2.2123, -2.2123], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.37078428268432617\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1384, -2.4166, -1.8596, -1.0718], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.6538, -2.0350, -2.3226, -1.9020, -1.4562, -1.0718, -0.9174, -2.7598,\n",
      "        -0.5289, -1.6396], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.0580, -2.3106, -2.4155, -1.9646, -1.8257, -1.9646, -1.8257, -2.9639,\n",
      "        -0.1856, -1.8257], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2204643189907074\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-0.9287, -2.9660, -1.0914, -1.3661, -2.9660, -2.7699, -0.9287, -1.6602,\n",
      "        -1.3661, -1.4576], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.8358, -3.0379, -1.9822, -2.2295, -3.0379, -2.9877, -1.8358, -1.8358,\n",
      "        -2.2295, -1.4698], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.40190356969833374\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.3529, -2.5174, -2.0823, -1.4666], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.1902, -2.4215, -0.9450, -2.9559, -0.9450, -2.2029, -2.1796, -1.1137,\n",
      "        -2.8113, -2.4097], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2418, -3.0093, -1.8505, -3.1687, -1.8505, -2.2418, -2.3272, -2.0023,\n",
      "        -3.0093, -3.0143], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3251110315322876\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2367, -2.4922, -1.9435, -1.1306], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.3854, -2.0730, -1.4783, -2.8557, -1.4783, -1.9435, -1.9435, -1.6308,\n",
      "        -1.3854, -3.0350], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2469, -2.3775, -1.4604, -3.0105, -1.4604, -1.8657, -1.8657, -2.2469,\n",
      "        -2.2469, -3.0046], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.19941425323486328\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2787, -2.5286, -1.9697, -1.1434], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.3765, -2.5303,  0.9180, -3.1005, -1.3765, -1.1434, -3.1463, -1.4933,\n",
      "        -1.9697, -2.4280], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2389, -3.0108, 10.0000, -3.1856, -2.2389, -2.0290, -3.1856, -1.4600,\n",
      "        -1.8833, -2.4941], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.500716209411621\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-1.5198, -2.9448, -1.3390, -0.9654, -0.9654, -1.9675, -3.1257, -0.9654,\n",
      "        -2.1999, -1.5079], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4315, -2.9831, -2.2051, -1.8689, -1.8689, -2.0048, -2.9561, -1.8689,\n",
      "        -2.3014, -1.4315], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.32543817162513733\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3681, -2.5075, -1.9894, -1.3134], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.3134, -1.3134, -1.7543, -1.0960, -3.1817, -2.6647, -1.3134, -1.9894,\n",
      "        -3.1114, -1.3134], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.1821, -2.1821, -2.2787, -1.9864, -3.1647, -2.4825, -2.1821, -1.9864,\n",
      "        -2.9304, -2.1821], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4152323603630066\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.4103, -2.5460, -2.0193, -1.3144], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.6348, -2.6884, -2.4103, -2.0193, -1.9644, -1.3144, -2.0193, -2.2007,\n",
      "        -1.0914, -2.7309], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2451, -2.3505, -2.1830, -1.9823, -2.2451, -2.1830, -1.9823, -2.2560,\n",
      "        -1.9823, -2.3505], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.20949435234069824\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.4406, -2.5624, -2.0389, -1.3375], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.3375, -3.2113, -1.3955, -2.8402, -1.3375, -1.3375, -1.3375, -1.3375,\n",
      "        -2.3548, -0.9904], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2038, -2.9154, -1.9921, -2.9225, -2.2038, -2.2038, -2.2038, -2.2038,\n",
      "        -2.2559, -1.8913], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5023699998855591\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.4807, -2.5810, -2.0675, -1.3882], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.5764, -1.5567, -3.3232, -1.3882, -1.3882, -1.1061, -1.1061, -2.3398,\n",
      "        -1.8920, -3.3232], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4120, -1.4120, -3.2142, -2.2494, -2.2494, -1.9955, -1.9955, -2.4188,\n",
      "        -1.9203, -3.2142], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.31440865993499756\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.5205, -2.6033, -2.1033, -1.4431], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.6501, -2.8100, -3.2629, -1.5682, -1.1201, -1.1201, -3.1150, -0.4969,\n",
      "        -3.1150, -2.7866], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.3364, -2.9611, -3.2446, -1.4313, -2.0081, -2.0081, -2.9462,  0.0361,\n",
      "        -2.9462, -2.4483], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.21729862689971924\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.5508, -2.5966, -2.1229, -1.5152], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.2456, -1.8726, -2.2926, -2.8360, -3.0663, -1.1073, -3.0663, -3.2248,\n",
      "        -1.5152, -1.1073], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2818, -2.2818, -2.2818, -2.9765, -2.9837, -1.9966, -2.9837, -2.9837,\n",
      "        -2.3637, -1.9966], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.25618669390678406\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-2.2542, -2.3997, -1.1667, -2.7239, -2.3148, -1.9373, -1.1667, -2.2542,\n",
      "        -1.1667, -3.2051], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.9422, -2.5631, -2.0500, -2.6734, -2.2979, -2.0500, -2.0500, -2.9422,\n",
      "        -2.0500, -3.0288], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3360985517501831\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-2.8622, -2.8405, -2.4449, -1.9094], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.4603, -3.3189, -1.9094, -3.1796, -2.2340, -3.3189, -1.6426, -1.2445,\n",
      "        -1.9601, -1.5839], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.1056, -3.3749, -2.4783, -3.1002, -2.7907, -3.3749, -2.4783, -2.1201,\n",
      "        -2.1056, -1.5307], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.25515931844711304\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.6315, -2.5863, -2.1953, -1.6859], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.6859, -2.3686, -3.2196, -2.6874, -2.7474, -1.6859, -2.2461, -1.6859,\n",
      "        -2.8921, -3.3123], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.5173, -2.1591, -3.1705, -2.6659, -2.9719, -2.5173, -2.3477, -2.5173,\n",
      "        -2.7728, -3.4171], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.22064152359962463\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.6505, -2.5924, -2.2226, -1.7350], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.1458, -1.3369, -2.4870, -3.3136, -1.7350, -1.3040, -2.4673, -1.7350,\n",
      "        -1.6055, -2.4673], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2383, -2.2032, -2.9832, -3.4579, -2.5615, -2.1736, -2.5615, -2.5615,\n",
      "        -1.5933, -2.5615], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3166464865207672\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.6760, -2.6053, -2.2538, -1.7823], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.0055, -1.8436, -1.3860, -1.7823, -2.0949, -1.7823, -3.2612, -2.6944,\n",
      "        -1.3860, -2.9205], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2073, -2.2474, -2.2474, -2.6040, -2.2073, -2.6040, -3.3108, -2.8866,\n",
      "        -2.2474, -3.1035], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.31240183115005493\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.7098, -2.6294, -2.2990, -1.8318], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.5507, -1.8318, -1.4268, -0.7250, -2.1886, -1.3721, -3.2981, -3.2217,\n",
      "        -2.2990, -2.6294], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.7940, -2.6486, -2.2841,  0.0650, -2.7940, -2.2349, -3.3792, -3.3792,\n",
      "        -2.2841, -2.9233], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.33144351840019226\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.7508, -2.6728, -2.3528, -1.8684], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.3495, -1.8684, -1.4532, -1.8684, -3.3495, -2.4316, -3.1426, -1.4532,\n",
      "        -1.8684, -2.1624], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4228, -2.6816, -2.3079, -2.6816, -3.4228, -2.3079, -3.4228, -2.3079,\n",
      "        -2.6816, -2.6816], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.38187265396118164\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-2.3950, -2.5735, -3.0573, -2.6384, -3.4085, -1.4039, -2.4827, -2.7628,\n",
      "        -3.4085, -1.4039], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.9488, -2.7091, -3.1555, -2.7860, -3.4602, -2.2635, -2.4633, -2.9488,\n",
      "        -3.4602, -2.2635], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.187467560172081\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.0824, -3.0322, -2.7421, -2.2317], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.5189, -2.8985, -2.8608, -3.4773, -1.4200, -1.5189, -1.5189, -2.2317,\n",
      "        -1.9202, -2.7421], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.3670, -3.2023, -2.8117, -3.4898, -2.2780, -2.3670, -2.3670, -2.7282,\n",
      "        -2.7282, -2.8113], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3892926573753357\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.8526, -2.8186, -2.4810, -1.9374], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.0382, -3.1007, -3.5444, -1.4439, -1.9374, -2.8186, -3.2436, -2.5045,\n",
      "        -1.5599, -2.4359], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.0534, -3.1996, -3.5256, -2.2995, -2.7437, -3.0534, -3.5256, -2.4039,\n",
      "        -2.4039, -2.4919], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.22527086734771729\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.8953, -2.8757, -2.5250, -1.9582], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.8445, -3.6578, -2.3656, -1.4653, -1.9582, -1.7362, -2.5250, -1.9582,\n",
      "        -2.8601, -3.0846], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6728, -3.6728, -2.3187, -2.3187, -2.7624, -1.6492, -2.4410, -2.7624,\n",
      "        -2.8656, -3.0867], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.20684051513671875\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.9310, -2.9285, -2.5645, -1.9859], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.6750, -3.6750, -1.9859, -3.7246, -1.9859, -2.8668, -3.7246, -3.2209,\n",
      "        -2.8607, -3.0279], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5801, -3.5801, -2.7873, -3.7068, -2.7873, -2.9643, -3.7068, -3.2528,\n",
      "        -2.8948, -2.8528], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13455109298229218\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.9658, -2.9719, -2.6052, -2.0219], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.5266, -2.9048, -2.9074, -2.0219, -2.9121, -3.0395, -2.2583, -3.6058,\n",
      "        -1.5266, -1.6924], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.3740, -2.9265, -2.9720, -2.8197, -2.9265, -2.8755, -2.3740, -3.6166,\n",
      "        -2.3740, -2.5232], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2807890772819519\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-2.9514, -1.5649, -2.0626, -3.2985, -2.0626, -2.5036, -2.0626, -2.8393,\n",
      "        -3.7675, -2.0626], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.9740, -2.4084, -2.8564, -3.2019, -2.8564, -2.4084, -2.8564, -2.8564,\n",
      "        -3.6569, -2.8564], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3263014554977417\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.0411, -3.0594, -2.6923, -2.1173], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.4249, -1.7951, -2.8938, -3.8683, -2.8746, -2.5445, -3.4205, -2.2215,\n",
      "        -3.2925, -1.6135], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.9414, -2.6156, -2.6038, -3.8327, -2.9414, -2.4522, -3.7041, -2.6156,\n",
      "        -3.3498, -2.4522], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.19806604087352753\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-3.3833, -3.9177, -3.3692, -1.8297, -2.2618, -2.3559, -3.1495, -2.8876,\n",
      "        -1.6606, -2.3559], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3825, -3.8716, -3.2854, -2.6467, -2.6467, -2.4945, -2.9538, -2.9538,\n",
      "        -2.4945, -2.4945], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16014531254768372\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-1.7059, -1.7059, -3.0810, -3.1662, -1.7059, -3.6595, -3.2927, -3.4204,\n",
      "        -3.1238, -2.3941], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.5353, -2.5353, -3.0322, -2.9712, -2.5353, -3.4167, -3.3253, -3.4167,\n",
      "        -2.9872, -2.5353], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.22028502821922302\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.9522, -3.0698, -2.6129, -1.8900], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.3631, -2.6810, -2.2568, -2.8022, -3.3253, -3.3570, -2.9522, -2.9264,\n",
      "        -1.7568, -2.2568], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.7010, -2.7091, -3.0311, -2.7010, -3.3668, -3.0553, -3.0311, -2.9948,\n",
      "        -2.5811, -3.0311], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.21075189113616943\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.9643, -3.0866, -2.6294, -1.9218], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.4498, -3.1785, -3.1785, -1.8963, -2.9440, -3.3706, -3.2401, -3.4642,\n",
      "        -2.2463, -2.9440], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4098, -3.0775, -3.0775, -1.8082, -3.0217, -3.5088, -3.0217, -3.3884,\n",
      "        -1.8082, -3.0217], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.030627872794866562\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.9673, -3.0994, -2.6390, -1.9511], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.3553, -2.8567, -1.9238, -2.2731, -3.1803, -2.3553, -3.2837, -3.5605,\n",
      "        -1.8610, -2.3553], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.1198, -2.7560, -1.8532, -1.8532, -3.0373, -3.1198, -3.1233, -3.8791,\n",
      "        -2.6749, -3.1198], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.27548426389694214\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.9653, -3.1141, -2.6438, -1.9879], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.0455, -1.0063, -2.9186, -3.5868, -1.9153, -3.3943, -2.6438, -3.3489,\n",
      "        -1.9879, -2.8748], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.1338, -0.0196, -2.8203, -3.9106, -2.7238, -3.4865, -2.7238, -3.1688,\n",
      "        -2.7891, -2.7891], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2446129322052002\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.9648, -3.1348, -2.6515, -2.0183], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.9552, -2.4491, -3.5556, -2.5997, -2.0183, -1.9384, -2.0183, -2.7513,\n",
      "        -2.3036, -2.3854], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.7597, -3.2042, -3.9238, -3.0732, -2.8165, -1.9105, -2.8165, -3.5110,\n",
      "        -1.9105, -2.8506], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3800177276134491\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.9858, -3.1688, -2.6682, -2.0536], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.4842, -2.0536, -3.4286, -3.3141, -2.9858, -3.2666, -1.9934, -2.9326,\n",
      "        -2.4992, -2.4842], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2358, -2.8482, -3.6308, -3.5128, -3.2358, -3.6308, -2.7941, -3.0592,\n",
      "        -3.3694, -3.2358], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3451173007488251\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.0345, -3.2207, -2.6983, -2.0905], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.9403, -3.4577, -2.0281, -3.6305, -2.5895, -2.3266, -3.6462, -2.0905,\n",
      "        -2.0281, -3.6305], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.0515, -3.5053, -2.8252, -3.6388, -2.8252, -1.9582, -3.5596, -2.8814,\n",
      "        -2.8252, -3.6388], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.21102169156074524\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.0750, -3.2697, -2.7357, -2.1309], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.5478, -2.0922, -2.9592, -2.0679, -2.0679, -2.8000, -3.3068, -3.3068,\n",
      "        -2.5478, -2.9592], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2930, -2.8611, -3.0480, -2.8611, -2.8611, -2.8830, -3.6504, -3.6504,\n",
      "        -3.2930, -3.0480], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3218997120857239\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.1345, -3.3229, -2.7811, -2.1647], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.7755, -4.3020, -2.0849, -3.2391, -2.6704, -3.3628, -2.7811, -2.1647,\n",
      "        -2.5811, -4.3052], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3230, -4.2227, -2.8764, -3.4164, -2.8764, -3.3230, -2.8764, -2.9482,\n",
      "        -3.3230, -4.2227], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.21881571412086487\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.1979, -3.3710, -2.8385, -2.2010], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.9540, -2.2010, -2.2010, -3.5978, -3.3611, -3.2667, -2.2010,  0.9455,\n",
      "        -4.3679, -2.6014], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5095, -2.9809, -2.9809, -3.5095, -3.0751, -3.4296, -2.9809, 10.0000,\n",
      "        -4.2440, -3.3413], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.479584693908691\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.2266, -3.4005, -2.8658, -2.1875], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.4782, -3.7411, -2.0658, -2.7500, -2.5777, -2.0658, -2.1875, -3.5508,\n",
      "        -2.1875, -3.2266], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5967, -3.5967, -2.8593, -3.3199, -3.3199, -2.8593, -2.9687, -3.4750,\n",
      "        -2.9687, -3.3199], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.34048622846603394\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.2657, -3.4271, -2.8998, -2.1869], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.3648, -2.1869, -3.2657, -2.7489, -3.0782, -3.2967, -2.7489, -2.5455,\n",
      "        -2.5455, -3.4774], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.0321, -2.9682, -3.2909, -2.8399, -2.9682, -3.3658, -2.8399, -3.2909,\n",
      "        -3.2909, -3.2338], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.19258694350719452\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-3.3073, -3.3042, -3.5610, -2.7754, -2.0296, -3.1588, -2.8271, -2.0296,\n",
      "        -3.3073, -4.3742], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2728, -3.2369, -3.5444, -2.8267, -2.8267, -2.9198, -3.2369, -2.8267,\n",
      "        -3.2728, -4.2145], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15308691561222076\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-2.1406, -4.0482, -2.2176, -4.1159, -3.3788, -2.5171, -3.2328, -2.8005,\n",
      "        -3.3403,  1.2371], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.8207, -3.8113, -2.9959, -4.2238, -3.0275, -3.2654, -3.0275, -2.8207,\n",
      "        -3.0275, 10.0000], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.874781608581543\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3589, -3.4132, -2.9917, -2.2056], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.4508, -4.1313, -3.9886, -2.7894, -3.6446, -2.2056, -2.4896, -1.9689,\n",
      "        -2.7295, -3.9886], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2406, -3.7851, -3.7851, -2.7721, -3.5219, -2.9850, -3.2406, -2.7721,\n",
      "        -3.2406, -3.7851], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2340107411146164\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3621, -3.3741, -2.9853, -2.2113], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.1185, -3.9383, -2.4666, -3.3252, -3.9348, -3.2309, -2.2395, -2.4666,\n",
      "        -3.0406, -3.3621], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.8819, -3.7860, -3.2199, -3.0156, -4.2065, -3.0156, -1.8819, -3.2199,\n",
      "        -2.9430, -3.2199], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15879830718040466\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3479, -3.3419, -2.9461, -2.2234], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.8609, -2.4610, -3.0554, -2.2234, -2.9869, -3.8963, -2.2234, -3.2473,\n",
      "        -1.9021, -1.9021], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7991, -3.2149, -3.0010, -3.0010, -3.0010, -3.7991, -3.0010, -3.1956,\n",
      "        -2.7119, -2.7119], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.31084972620010376\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3389, -3.3129, -2.9164, -2.2515], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.2515, -3.8523, -2.5404, -2.4698, -2.2515, -1.8890, -3.6431, -2.2515,\n",
      "        -2.6736, -3.1416], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.0264, -3.8251, -3.0264, -3.2229, -3.0264, -2.7001, -3.5203, -3.0264,\n",
      "        -2.7001, -3.0355], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.32900431752204895\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3249, -3.2869, -2.8859, -2.2823], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.8963, -3.1038, -3.3349, -3.1814, -2.2481, -2.2823, -3.8152, -3.5336,\n",
      "        -3.6016, -1.8924], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.0541, -3.0573, -3.2442, -3.5025, -2.7031, -3.0541, -3.8633, -3.5583,\n",
      "        -3.8633, -2.7031], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16697195172309875\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-1.8818, -2.8667, -2.9954, -2.5203, -2.8667, -2.5203, -1.8818, -3.7808,\n",
      "        -3.7808, -3.7320], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.6937, -2.6937, -3.0768, -3.2683, -2.6937, -3.2683, -2.6937, -3.8994,\n",
      "        -3.8994, -3.8994], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2559678554534912\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.3258, -3.2178, -2.8455, -2.5545], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.3106, -2.5812, -3.0447, -3.5795, -3.7177, -3.7750, -1.8967, -2.0111,\n",
      "        -2.3330, -2.3239], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2990, -2.7070, -3.0997, -3.5781, -3.9271, -3.9271, -2.7070, -1.9085,\n",
      "        -1.9085, -3.0915], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1522596925497055\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.3258, -3.2275, -2.8243, -2.5794], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.9065, -3.0069, -2.6750, -3.9330, -2.9008, -2.5794, -2.5794, -2.5794,\n",
      "        -3.3258, -3.6338], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.7158, -3.0919, -3.0919, -4.2623, -3.1591, -3.3214, -3.3214, -3.3214,\n",
      "        -3.3214, -3.6524], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.26635652780532837\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.3363, -3.2717, -2.8141, -2.6027], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.4808, -2.6027, -2.5242, -3.5510, -2.6027, -3.0146, -2.9433, -3.5510,\n",
      "        -1.9154, -2.8895], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.6716, -3.3425, -3.1744, -3.6006, -3.3425, -3.0557, -3.1744, -3.6006,\n",
      "        -2.7239, -3.2718], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.37049806118011475\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.3610, -3.3145, -2.8131, -2.6358], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.0016, -3.4854, -2.6358, -2.6358, -2.9912, -2.3510, -2.3510, -2.9216,\n",
      "        -1.9373, -3.7987], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.9866, -3.6198, -3.3722, -3.3722, -3.3722, -3.1159, -3.1159, -3.0326,\n",
      "        -2.7436, -3.9530], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3104458153247833\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.3866, -3.3581, -2.8162, -2.6599], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.9644, -3.9370, -3.1209, -3.0062, -3.0306, -3.5353, -3.0568, -4.2150,\n",
      "        -2.8738, -3.3860], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.7680, -3.9654, -3.4319, -3.0101, -3.3939, -3.5864, -3.1477, -4.2604,\n",
      "        -3.4319, -3.4319], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12017695605754852\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.4052, -3.3979, -2.8187, -2.6641], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.5458, -2.0135, -4.0729, -3.4810, -2.8187, -4.2309, -3.0723, -3.9977,\n",
      "        -2.8680, -2.8187], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.7891, -2.0327, -4.2582, -3.4157, -3.1388, -4.2582, -3.1388, -3.9705,\n",
      "        -2.9844, -3.1388], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03225012868642807\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-2.3459, -2.3459, -3.3563, -2.5810, -2.6712, -2.1591, -4.1255, -2.9130,\n",
      "        -4.1255, -1.9924], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.1113, -3.1113, -3.4041, -2.7932, -3.4041, -2.0377, -4.2400, -2.9431,\n",
      "        -4.2400, -2.7932], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.24392613768577576\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.4898, -3.5275, -2.9140, -2.6778], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.6778, -2.9458, -2.0799, -4.0335, -4.0772, -2.9115, -2.6231, -3.5521,\n",
      "        -2.6778, -2.6778], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4100, -3.3358, -2.0469, -3.9499, -4.2190, -3.3613, -2.7949, -3.6513,\n",
      "        -3.4100, -3.4100], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.20303967595100403\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.5487, -3.5977, -2.9737, -2.6972], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.3083, -3.7235, -4.0955, -4.0955, -2.6683, -2.6972, -3.3236, -4.0955,\n",
      "        -2.6683, -1.9932], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.0775, -3.9469, -3.9469, -3.9469, -2.7939, -3.4275, -3.3273, -3.9469,\n",
      "        -2.7939, -2.7939], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.19137319922447205\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.6053, -3.6519, -3.0456, -2.7249], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.1805, -2.3030, -1.9962, -2.8829, -2.3030, -3.3975, -2.7249, -3.3793,\n",
      "        -3.4420, -1.9962], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4524, -3.0727, -2.7966, -2.7966, -3.0727, -3.3025, -3.4524, -3.0103,\n",
      "        -3.4524, -2.7966], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3222082853317261\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-3.3665, -4.1525, -2.7593, -3.3665, -3.6247, -2.5521, -2.7593, -4.1525,\n",
      "        -3.6378, -2.3189], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.8150, -3.9672, -3.4834, -2.8150, -3.7095, -3.0870, -3.4834, -3.9672,\n",
      "        -3.9062, -3.0870], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2680869400501251\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.7048, -3.6847, -3.1633, -2.8236], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.3479, -2.0740, -3.6063, -2.2434, -3.0171, -4.3142, -4.1265, -2.3479,\n",
      "        -3.3270, -3.3256], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.1131, -2.8666, -3.3512, -2.1445, -3.0647, -4.0128, -4.0128, -3.1131,\n",
      "        -3.1131, -2.8748], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2229154109954834\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.7332, -3.6517, -3.2165, -2.9084], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.1311, -3.7332, -2.9084, -2.4145, -3.6677, -2.9084, -2.8657, -4.0773,\n",
      "        -3.8107, -2.9084], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4036, -3.6176, -3.6176, -3.1730, -4.1857, -3.6176, -2.9327, -4.0889,\n",
      "        -3.8674, -3.6176], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.24479565024375916\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.7668, -3.6293, -3.2776, -3.0053], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.0281, -3.6227, -2.2181, -3.0207, -2.2181, -4.2673, -3.0053, -2.4833,\n",
      "        -3.4014, -3.3763], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.0586, -3.5089, -2.9963, -2.9963, -2.9963, -4.3865, -3.7047, -3.2350,\n",
      "        -3.0586, -3.2350], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.24316248297691345\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.7990, -3.6125, -3.3279, -3.1101], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.1855, -3.3104, -3.6676, -3.3958, -4.0074, -2.9390, -2.5662, -3.3279,\n",
      "        -4.1821, -2.5662], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4712, -3.2318, -3.9552, -3.3096, -4.2534, -3.0712, -3.3096, -3.3096,\n",
      "        -4.2534, -3.3096], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13665559887886047\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-3.1116, -2.6429, -3.5649, -2.6429, -4.1991, -3.0070, -2.6429, -2.6429,\n",
      "        -3.5649, -4.1991], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2787, -3.3786, -3.8754, -3.3786, -4.5335, -3.2550, -3.3786, -3.3786,\n",
      "        -3.8754, -4.5335], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2670871913433075\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-4.1769, -3.9216, -3.7540, -3.6494], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.7085, -3.6414, -4.2237, -3.2414, -3.0143, -3.7251, -2.7230, -3.0414,\n",
      "        -0.2927, -2.4036], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.0275, -3.9037, -4.3526, -3.3017, -3.4507, -4.1469, -3.4507, -3.3017,\n",
      "         0.8074, -2.4499], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.23687231540679932\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.9255, -3.7195, -3.4423, -3.2364], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.1965, -3.7705, -3.2364, -3.7195, -3.1965, -3.5056, -4.1495, -2.4306,\n",
      "        -2.4306, -3.2364], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3251, -4.1645, -3.9128, -4.3101, -3.3251, -3.5039, -4.3935, -3.1876,\n",
      "        -3.1876, -3.9128], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.26574456691741943\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.9988, -3.8154, -3.5009, -3.2601], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.4353, -2.4877, -2.8926, -2.4615, -2.8301, -4.4353, -3.8086, -3.2601,\n",
      "        -3.8086, -4.5191], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5935, -2.5260, -3.3294, -3.2153, -3.5471, -4.5935, -3.9341, -3.9341,\n",
      "        -3.9341, -4.5935], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.18159303069114685\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.0788, -3.9161, -3.5552, -3.2716], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.6748, -4.8246, -4.5528, -3.6024, -2.5339, -2.8787, -3.2716, -4.3508,\n",
      "        -4.0788, -3.2716], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9444, -4.6002, -4.6002, -3.3422, -2.5543, -3.5908, -3.9444, -4.4614,\n",
      "        -3.9444, -3.9444], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1636344939470291\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.1335, -4.0076, -3.5938, -3.2895], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.7617, -2.5715, -4.3226, -3.5964, -3.9970, -3.2895, -2.9348, -3.2895,\n",
      "        -3.4203, -3.2895], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.7622, -2.5856, -4.4991, -3.3145, -3.9605, -3.9605, -3.6413, -3.9605,\n",
      "        -3.2758, -3.9605], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.8353626132011414\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-2.9533, -3.4643, -3.2884, -3.2884, -3.2374, -2.5337, -2.9533, -2.5337,\n",
      "        -4.5336, -3.8901], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6580, -3.3748, -3.9595, -3.9595, -3.2964, -3.2804, -3.6580, -3.2804,\n",
      "        -4.5011, -4.1293], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.30786803364753723\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.2148, -4.1401, -3.6569, -3.3062], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.1084, -3.6569, -2.5588, -3.3062, -2.9875, -3.7360, -4.6066, -2.9875,\n",
      "        -4.4502, -3.8560], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9756, -3.6888, -3.3029, -3.9756, -3.6888, -4.3081, -4.5214, -3.6888,\n",
      "        -4.3081, -3.9075], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2361282855272293\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-3.3425, -2.5515, -3.5600, -4.8847, -3.4211, -3.5522, -4.4322, -4.2042,\n",
      "        -3.2665, -4.8168], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.0083, -3.3388, -3.3388, -4.6439, -3.4102, -3.4102, -4.5520, -4.3036,\n",
      "        -3.2964, -4.6439], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12452832609415054\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1827, -4.1894, -3.5980, -3.0806], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.9418, -3.9514, -3.3909, -4.2554, -3.3909, -3.6642, -3.5980, -2.4312,\n",
      "        -3.4449, -3.3267], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6901, -3.7821, -4.0518, -4.3051, -4.0518, -3.4437, -3.3568, -2.3383,\n",
      "        -3.4437, -3.7821], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1290939301252365\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-4.9757, -3.1296, -4.1650, -4.1967, -4.9757, -4.2065, -2.3960, -2.9976,\n",
      "        -3.2336, -2.6620], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7485, -3.8167, -4.4886, -4.1112, -4.7485, -4.0064, -2.3160, -2.3160,\n",
      "        -3.3958, -3.3958], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17630892992019653\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1770, -4.1864, -3.6171, -3.1907], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.1907, -3.3448, -2.7233, -4.1379, -3.2722, -3.9647, -3.1907, -2.7233,\n",
      "        -4.7093, -4.6736], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.8717, -3.4670, -3.4509, -4.0689, -3.5629, -3.8658, -3.8717, -3.4509,\n",
      "        -4.6790, -4.6790], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2101256400346756\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1658, -4.1771, -3.6345, -3.2521], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.6506, -3.6224, -3.2521, -3.6788, -4.2442, -2.3480, -3.8482, -3.2521,\n",
      "        -4.3276, -3.6224], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7351, -4.2602, -3.9269, -3.6111, -4.1136, -2.3255, -3.9269, -3.9269,\n",
      "        -4.2602, -4.2602], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17641741037368774\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1573, -4.1721, -3.6560, -3.3172], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.4977, -3.2748, -3.7139, -3.7346, -3.3172, -2.8541, -3.7139, -4.6874,\n",
      "        -4.3226, -3.8992], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6656, -3.5687, -4.3425, -3.9855, -3.9855, -3.5687, -4.3425, -4.7913,\n",
      "        -4.4197, -4.1920], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.20310445129871368\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1710, -4.1792, -3.6898, -3.3625], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.5319, -4.6951, -3.3625, -3.7966, -3.7643, -3.7249, -3.7966, -3.3195,\n",
      "        -4.1710, -3.5319], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7050, -4.8330, -4.0262, -4.4169, -4.0262, -3.7050, -4.4169, -3.6202,\n",
      "        -4.4169, -3.7050], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1509055495262146\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2118, -4.2014, -3.7458, -3.3939], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.4982, -5.0741, -3.3907, -4.2551, -4.0065, -3.8565, -2.9554, -4.8455,\n",
      "        -3.3939, -4.8455], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6813, -5.1258, -3.6599, -4.2270, -4.0194, -4.4709, -3.6599, -5.1258,\n",
      "        -4.0545, -5.1258], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1576860249042511\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-3.9063, -3.9063, -4.1457, -5.1026, -4.1948, -4.1642, -2.9881, -2.9881,\n",
      "        -4.4306, -4.0424], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5157, -4.5157, -4.0764, -5.1399, -4.2438, -4.2438, -3.6893, -3.6893,\n",
      "        -4.4537, -4.0319], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17415907979011536\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-3.6694, -3.9602, -5.1365, -3.8431, -4.9718, -5.1365, -4.2693, -3.9602,\n",
      "        -3.9602, -4.1931], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.1029, -4.5642, -5.1576, -4.5642, -4.7575, -5.1576, -4.3024, -4.5642,\n",
      "        -4.5642, -4.2611], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1854780912399292\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-3.9959, -4.2248, -4.7483, -4.2043, -5.1745, -3.4668, -4.7361, -4.5550,\n",
      "        -3.0848, -4.7087], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5963, -4.2830, -4.2830, -4.4990, -5.1811, -4.1202, -4.9699, -4.4990,\n",
      "        -3.7692, -4.5963], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16329920291900635\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.4394, -4.3326, -3.9730, -3.4951], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.3581, -3.6744, -3.6744, -4.2543, -4.0365, -5.2173, -4.2729, -3.6447,\n",
      "        -4.3326, -3.6947], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.1456, -3.7845, -3.7845, -4.5313, -4.6328, -5.2096, -4.3253, -3.8077,\n",
      "        -4.3865, -4.0978], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06964512169361115\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.4858, -4.3808, -4.0107, -3.5300], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.6637, -4.0680, -4.0680, -4.3384, -4.4026, -4.3934, -3.1052, -3.5300,\n",
      "        -3.5300, -4.0680], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5506, -4.6612, -4.6612, -4.3786, -4.3786, -4.1770, -3.7947, -4.1770,\n",
      "        -4.1770, -4.6612], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.24300968647003174\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5350, -4.4348, -4.0507, -3.5868], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.9773, -3.5868, -3.5868, -3.5868, -3.9773, -3.8559, -3.1318, -3.8559,\n",
      "        -3.9009, -5.0932], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9559, -4.2281, -4.2281, -4.2281, -3.9559, -4.2281, -3.8186, -4.2281,\n",
      "        -3.8581, -5.2732], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.20178186893463135\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5723, -4.4825, -4.0771, -3.6302], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.2154, -5.2664, -3.9387, -3.9300, -3.1687, -3.1687, -3.1687, -4.3768,\n",
      "        -3.9515, -4.5509], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.3915, -5.3065, -4.2672, -3.8939, -3.8518, -3.8518, -3.8518, -4.0186,\n",
      "        -4.0186, -5.0132], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2536248564720154\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5975, -4.5316, -4.0754, -3.6501], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.2100, -3.6501, -3.6501, -2.5125, -5.1201, -4.7779, -2.5125, -5.4245,\n",
      "        -4.1947, -4.1947], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.8890, -4.2851, -4.2851, -2.4342, -5.0937, -4.5389, -2.4342, -5.3287,\n",
      "        -4.7752, -4.7752], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.20207592844963074\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6246, -4.5683, -4.0766, -3.6880], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.6014, -4.6916, -4.2484, -3.8784, -3.2659, -5.4649, -5.1152, -4.2484,\n",
      "        -3.2659, -4.5683], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6115, -5.0990, -4.8236, -3.8740, -3.9393, -5.3625, -5.1353, -4.8236,\n",
      "        -3.9393, -4.6731], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1756545901298523\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-3.7237, -4.7945, -3.8215, -3.7237, -4.3029, -4.3029, -3.8580, -3.7237,\n",
      "        -4.5853, -4.3029], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3513, -5.1712, -3.9923, -4.3513, -4.8726, -4.8726, -3.8731, -4.3513,\n",
      "        -4.7640, -4.8726], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2358705997467041\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.4315, -4.2698, -3.8554, -3.3795], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.5827, -3.7687, -4.3655, -4.1113, -3.8541, -4.1113, -2.5867, -5.2001,\n",
      "        -5.4472, -4.9177], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4260, -4.3918, -4.9289, -4.0415, -3.8758, -4.0415, -2.5761, -5.1949,\n",
      "        -5.1654, -5.1654], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08814646303653717\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.4778, -4.3267, -3.8858, -3.4473], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.2088, -4.5248, -3.8858, -5.4729, -2.6116, -3.8464, -5.3026, -4.5248,\n",
      "        -3.8185, -4.7275], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.6254, -4.7947, -4.1025, -5.2043, -2.6254, -3.8879, -5.2285, -4.7947,\n",
      "        -4.4366, -4.8460], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10086943954229355\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5341, -4.3742, -3.9254, -3.4947], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.4932, -4.2255, -4.1930, -4.6028, -3.9254, -4.1930, -4.6028, -4.4932,\n",
      "        -4.8871, -4.3222], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0439, -4.8900, -4.3944, -4.4739, -4.1452, -4.3944, -4.4739, -5.0439,\n",
      "        -4.8289, -4.4739], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12370846420526505\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5903, -4.4176, -3.9739, -3.5379], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.9040, -3.9739, -4.3792, -4.3531, -5.3742, -5.2335, -3.9040, -4.5410,\n",
      "        -3.9040, -4.3070], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5136, -4.1841, -5.0869, -4.5136, -5.2686, -5.0869, -4.5136, -5.0869,\n",
      "        -4.5136, -4.9178], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.23894056677818298\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6400, -4.4552, -4.0301, -3.5845], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.4517, -5.4113, -3.8927, -5.2126, -5.7863, -4.6072, -4.3068, -5.0830,\n",
      "        -4.5684, -3.9552], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1116, -5.3000, -3.8683, -5.0065, -5.5990, -4.9066, -4.4934, -5.3000,\n",
      "        -5.1116, -4.5597], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13579195737838745\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6992, -4.4783, -4.0891, -3.6258], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.1856, -4.5683, -4.5683, -4.0891, -5.1856, -4.0018, -4.5683, -4.5160,\n",
      "        -2.0739, -3.6258], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0911, -5.1114, -5.1114, -4.2633, -5.0911, -4.6016, -5.1114, -3.8781,\n",
      "         0.2379, -4.2633], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.7450785636901855\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7386, -4.4876, -4.1110, -3.6502], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.0279, -4.0279, -4.5541, -4.5062, -3.7740, -5.6113, -4.9618, -3.7740,\n",
      "        -4.0279, -4.5541], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6251, -4.6251, -5.0987, -3.9054, -4.5623, -5.3356, -5.0987, -4.5623,\n",
      "        -4.6251, -5.0987], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.336162269115448\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7679, -4.4918, -4.1098, -3.6901], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.4017, -5.2414, -4.0856, -4.4529, -4.6516, -4.0856, -5.8168, -4.3763,\n",
      "        -4.6283, -4.4918], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6771, -5.1075, -4.6771, -3.9839, -4.9615, -4.6771, -5.6894, -4.4886,\n",
      "        -4.6771, -4.5583], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11450066417455673\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7887, -4.4956, -4.0940, -3.7417], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.3900, -4.1375, -4.0538, -4.5901, -4.5901, -4.3011, -4.5901, -4.2874,\n",
      "        -4.0940, -4.8798], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3871, -4.7237, -4.0897, -5.1153, -5.1153, -4.3675, -5.1153, -4.5779,\n",
      "        -4.3675, -4.5779], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1427147090435028\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8007, -4.4713, -4.0461, -3.8001], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.8001, -4.6542, -3.8001, -3.5888, -4.2176, -5.5925, -4.6542, -4.0461,\n",
      "        -4.8984, -5.7852], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4200, -5.0452, -4.4200, -4.2034, -4.7958, -5.3893, -5.0452, -4.4200,\n",
      "        -5.0309, -5.6858], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.19951224327087402\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8190, -4.4734, -4.0104, -3.8569], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.9928, -5.0381, -4.5214, -5.1966, -4.9308, -4.8354, -4.7152, -5.1350,\n",
      "        -4.1735, -4.7152], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6215, -5.2524, -4.7893, -4.9826, -4.9708, -4.7562, -4.9826, -5.2982,\n",
      "        -4.2745, -4.9826], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04891045391559601\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-5.0778, -4.2582, -4.7865, -4.8003, -4.9359, -5.9446, -4.3873, -5.7941,\n",
      "        -3.9910, -4.4484], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2096, -4.8324, -4.9175, -4.7409, -4.6738, -5.5566, -4.7409, -5.5566,\n",
      "        -4.3465, -4.8324], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10422690212726593\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9114, -4.9105, -4.1320, -4.4819], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.9891, -4.8673, -4.8911, -4.9121, -4.8673, -3.9492, -5.4439, -4.8673,\n",
      "        -4.4819, -3.9842], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5543, -4.8819, -4.6349, -4.6349, -4.8819, -4.5543, -5.5128, -4.8819,\n",
      "        -4.7188, -4.4137], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10739396512508392\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7004, -4.4093, -3.9486, -4.0785], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.7554, -5.2245, -5.1731, -5.2245, -4.7561, -4.0785, -5.3867, -4.0785,\n",
      "        -5.3920, -4.5845], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4830, -5.1598, -5.3272, -5.1598, -4.6195, -4.5537, -5.2140, -4.5537,\n",
      "        -5.1783, -4.7077], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0667349249124527\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6081, -4.3765, -3.9430, -4.2063], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.9475, -4.5822, -3.9297, -4.7157, -4.8071, -4.8162, -5.0512, -3.9297,\n",
      "        -3.9360, -4.2063], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5424, -4.8204, -4.5367, -4.6004, -4.6004, -4.6931, -4.8308, -4.5367,\n",
      "        -2.9341, -4.5487], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.23883643746376038\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5412, -4.3719, -3.9483, -4.3096], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.3096, -5.1348, -4.3096, -5.1175, -3.5067, -4.5412, -4.7823, -4.6882,\n",
      "        -4.7823, -4.5069], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5535, -5.1084, -4.5535, -4.8127, -4.5042, -4.6912, -4.6912, -4.5196,\n",
      "        -4.6912, -4.7835], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1351512372493744\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5028, -4.3805, -3.9496, -4.4109], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.8454, -5.1049, -5.1670, -4.8454, -2.4807, -4.3805, -5.8303, -5.5356,\n",
      "        -4.8454, -5.5752], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6720, -5.0779, -4.7813, -4.6720, -3.0709, -4.5026, -5.1346, -5.3766,\n",
      "        -4.6720, -5.0779], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1359495371580124\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.4685, -4.3461, -4.0644, -4.4624], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.4910, -4.2815, -5.7781, -3.8530, -4.8502, -4.8502, -0.4981, -4.6283,\n",
      "        -5.0369, -5.3971], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1814, -4.7514, -5.2429, -4.3200, -4.7514, -4.7514, -0.2299, -4.6699,\n",
      "        -5.1814, -5.3693], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09360544383525848\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.4404, -4.2785, -4.2344, -4.4933], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.6801, -4.5587, -4.2344, -4.2599, -4.3046, -5.2219, -1.6609, -4.9867,\n",
      "        -5.1505, -4.4933], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.1162, -4.7998, -4.8110, -4.8110, -4.8110, -5.5440, -3.1162, -5.3750,\n",
      "        -5.5440, -4.8110], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.37689220905303955\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-4.4113, -5.5582, -4.4785, -5.1540, -4.6322, -4.4262, -3.5892, -5.1380,\n",
      "        -4.8005, -4.3428], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9701, -5.4187, -4.9134, -5.5236, -4.9681, -5.4020, -3.0631, -5.0307,\n",
      "        -4.9134, -4.9681], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.24146679043769836\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.4834, -4.3405, -4.2071, -4.4752], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.4757, -4.3405, -4.4752, -4.4752, -4.7797, -4.1409, -4.5222, -4.5292,\n",
      "        -4.7797, -5.5024], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.0336, -4.7268, -4.7864, -4.7864, -4.8638, -4.7268, -4.9913, -4.8267,\n",
      "        -4.8638, -4.9913], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14655694365501404\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5748, -4.4446, -4.1204, -4.4946], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.7846, -4.7846, -4.5423, -4.5748, -3.5131, -5.1388, -4.3510, -4.1854,\n",
      "        -4.7846, -5.0852], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7909, -4.7909, -4.7624, -4.7909, -4.0230, -4.9159, -4.7909, -4.0230,\n",
      "        -4.7909, -5.2604], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06555381417274475\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6991, -4.5671, -4.0333, -4.5128], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.1474, -5.1474, -4.7630, -4.5128, -4.7630, -5.1474, -4.7630, -5.4580,\n",
      "        -3.5383, -4.7630], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8429, -4.8429, -4.7017, -4.6300, -4.7017, -4.8429, -4.7017, -4.8429,\n",
      "        -3.9248, -4.7017], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08347121626138687\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8104, -4.6708, -4.0303, -4.4956], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.7041, -5.5544, -4.0303, -4.9660, -4.5270, -4.7041, -5.2487, -3.9892,\n",
      "        -5.6142, -4.8694], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6878, -5.1105, -4.6273, -4.5933, -4.5903, -4.6878, -5.0771, -4.5903,\n",
      "        -5.0771, -4.8559], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1376200020313263\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9120, -4.7104, -4.0917, -4.4716], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.2673, -5.0915, -5.2390, -4.4997, -4.6297, -4.7104, -4.1649, -4.6297,\n",
      "        -5.1840, -5.6078], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1317, -4.9251, -5.0684, -4.7184, -4.7184, -4.6454, -3.6795, -4.7184,\n",
      "        -5.2449, -5.3442], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04518362507224083\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9815, -4.7231, -4.1387, -4.4576], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.1010, -4.1912, -4.9448, -5.0691, -5.0940, -5.0691, -4.1967, -5.5515,\n",
      "        -5.0691, -4.7439], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6909, -4.7248, -4.6369, -4.9796, -5.1753, -4.9796, -4.7770, -5.1753,\n",
      "        -4.9796, -5.2569], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1499718427658081\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0324, -4.7167, -4.1666, -4.4379], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.0503, -5.0503, -5.3106, -4.5119, -4.4379, -4.5119, -2.8277, -4.4723,\n",
      "        -4.5119, -4.1578], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0251, -5.0251, -5.0684, -4.7420, -4.7499, -4.7420, -2.9047, -4.7420,\n",
      "        -4.7420, -4.7499], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07454097270965576\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0475, -4.7044, -4.1590, -4.4292], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.6171, -4.2317, -5.0616, -4.1608, -4.2262, -4.4306, -5.1313, -2.6296,\n",
      "        -4.4976, -5.0886], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4086, -4.8085, -5.0574, -4.7431, -4.7431, -4.7448, -5.2162, -2.8986,\n",
      "        -4.7448, -5.0498], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1223200112581253\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-5.0481, -5.5863, -5.1828, -5.0916, -4.5156, -4.4141, -2.6264, -5.0916,\n",
      "        -5.3894, -5.0916], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0187, -5.1950, -5.0709, -5.0709, -4.7411, -4.6977, -2.9030, -5.0709,\n",
      "        -5.1950, -5.0709], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04133041203022003\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9735, -4.6486, -4.0821, -4.4033], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.1732, -5.1186, -5.0080, -5.4541, -4.0821, -5.6584, -4.8541, -4.8465,\n",
      "        -4.6486, -5.1186], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6739, -5.1030, -5.0080, -5.1030, -4.6739, -5.0080, -4.8057, -5.4041,\n",
      "        -4.8291, -5.1030], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1493666023015976\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8646, -4.6255, -4.0593, -4.3933], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.1958, -5.1469, -3.5214, -5.0140, -4.8754, -4.9082, -4.5657, -4.9082,\n",
      "        -5.4120, -5.1469], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2146, -5.1427, -3.4791, -5.0471, -4.8210, -4.8448, -4.7911, -4.8448,\n",
      "        -5.1427, -5.1427], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01376544963568449\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-4.6018, -5.1843, -5.4717, -4.6018, -5.1989, -0.6482, -5.2745, -4.6018,\n",
      "        -4.6018, -3.4609], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8294, -5.1834, -5.0884, -4.8294, -5.1278, -0.3680, -5.4262, -4.8294,\n",
      "        -4.8294, -2.9086], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07657404243946075\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0409, -4.8761, -4.3433, -4.5091], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.2203, -5.3832, -5.0077, -4.6607, -4.6347, -5.2203, -5.2203, -4.4024,\n",
      "        -4.6607, -4.3433], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1976, -5.2335, -5.1134, -4.8456, -4.8456, -5.1976, -5.1976, -4.6099,\n",
      "        -4.8456, -4.9090], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05110497027635574\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9890, -4.8767, -4.3387, -4.5785], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.6867, -5.2697, -4.4244, -4.7141, -5.2526, -4.6578, -5.0122, -5.2697,\n",
      "        -5.2697, -5.2697], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7863, -5.1920, -4.5820, -4.8399, -5.0605, -4.8399, -5.1186, -5.1920,\n",
      "        -5.1920, -5.1920], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.015613695606589317\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9434, -4.8833, -4.3423, -4.6347], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.8318, -4.7547, -4.8491, -2.6487, -5.1580, -5.3124, -5.0216, -4.5904,\n",
      "        -4.6647, -4.4425], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8462, -4.8391, -4.8221, -2.9800, -5.2145, -5.1983, -5.1314, -5.2398,\n",
      "        -4.8391, -4.5650], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.061325252056121826\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9108, -4.8966, -4.3527, -4.6806], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.3531, -3.9489, -4.4738, -3.9489, -5.1133, -4.7022, -4.7877, -4.4738,\n",
      "        -5.3567, -4.9108], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5431, -4.5540, -4.9174, -4.5540, -5.2126, -5.0201, -4.8397, -4.9174,\n",
      "        -5.2138, -4.7872], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13114216923713684\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8617, -4.9364, -4.3171, -4.6959], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.3950, -4.4925, -5.3877, -5.3267, -4.5358, -5.0636, -5.1311, -4.8415,\n",
      "        -5.3950, -4.8175], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2028, -4.5445, -5.1890, -5.2028, -4.8854, -5.1715, -5.2028, -5.1644,\n",
      "        -5.2028, -4.8182], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03746503219008446\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8358, -4.9793, -4.2729, -4.6902], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.9355, -2.6109, -4.8324, -4.9493, -5.4161, -5.2463, -3.5513, -4.5240,\n",
      "        -4.8324, -2.8291], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7615, -3.0113, -4.7944, -4.8000, -5.1890, -5.1716, -3.5462, -4.5390,\n",
      "        -4.7944, -3.0113], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.030638087540864944\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-5.4221, -3.9572, -4.9925, -5.1360, -5.1866, -5.1044, -2.8279, -4.9161,\n",
      "        -3.9572, -4.5436], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2082, -4.5615, -4.8134, -5.2082, -5.0917, -5.2040, -2.9979, -4.7875,\n",
      "        -4.5615, -4.5615], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08780349045991898\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.4605, -4.7158, -4.0018, -4.5608], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.0159, -5.0768, -5.3273, -4.6585, -4.2803, -4.5608, -3.9065, -5.4213,\n",
      "        -4.7170, -4.8252], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8397, -5.0955, -5.2453, -4.6017, -4.8523, -4.6017, -3.5503, -5.2453,\n",
      "        -4.8215, -4.8215], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.053898222744464874\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-5.4245, -4.0620, -5.4245, -5.3286, -2.6321, -4.5744, -4.7806, -5.4245,\n",
      "        -4.5744, -3.3688], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3026, -4.6558, -5.3026, -5.2335, -2.9586, -4.8802, -4.8575, -5.3026,\n",
      "        -4.8802, -3.5726], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0747334435582161\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7207, -4.9847, -4.3595, -4.7960], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.0915, -5.1778, -4.5849, -5.1726, -4.0915, -2.6679, -2.8652, -4.1545,\n",
      "        -5.4129, -5.1824], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9373, -5.3944, -4.7390, -5.1901, -4.9373, -2.9291, -2.9291, -4.7390,\n",
      "        -5.3944, -5.4146], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.19697114825248718\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6045, -4.8027, -4.1717, -4.5890], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.1717, -2.6582, -4.5890, -4.7769, -5.4052, -4.7954, -4.8002, -5.4052,\n",
      "        -5.4052, -4.7769], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7545, -2.8960, -4.7545, -4.9185, -5.4072, -4.9378, -5.4072, -5.4072,\n",
      "        -5.4072, -4.9185], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08524107187986374\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7240, -4.8351, -4.1586, -4.6032], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.3461, -4.7879, -4.7879, -4.3366, -5.0598, -2.6322, -5.3913, -3.5443,\n",
      "        -4.9329, -5.4868], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9115, -4.8990, -4.8990, -4.9029, -5.1544, -2.8694, -5.3706, -3.6680,\n",
      "        -4.8234, -5.4104], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07638446986675262\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8347, -4.8627, -4.1574, -4.6173], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.6398, -4.6173, -5.4910, -5.1796, -2.6172, -4.3167, -5.3806, -4.8012,\n",
      "        -5.5604, -4.8012], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8850, -4.7417, -5.2771, -5.2771, -2.8436, -4.8850, -5.3488, -4.8922,\n",
      "        -5.4034, -4.8922], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05473494529724121\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9274, -4.8788, -4.1574, -4.6390], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.6064, -5.5248, -4.3201, -4.7368, -4.3201, -4.6390, -5.0654, -4.2957,\n",
      "        -5.2796, -4.1574], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.8248, -5.3970, -4.7417, -4.9297, -4.7417, -4.7417, -5.1131, -4.8661,\n",
      "        -5.3237, -4.7417], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11381594836711884\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9969, -4.8764, -4.1561, -4.6430], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.5665, -5.2957, -4.6430, -4.2925, -4.1258, -4.3335, -4.6430, -5.6708,\n",
      "        -4.8622, -4.3523], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7133, -5.3169, -4.7405, -4.8632, -3.7875, -4.9002, -4.7405, -5.3169,\n",
      "        -4.9170, -4.7405], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1081313043832779\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0333, -4.8662, -4.1472, -4.6479], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.4561, -3.7256, -5.1953, -5.3716, -4.0922, -5.3716, -5.3716, -5.5421,\n",
      "        -5.0657, -4.9185], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2766, -3.8452, -5.1107, -5.3089, -3.8452, -5.3089, -5.3089, -5.4163,\n",
      "        -4.9382, -4.9510], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0159617867320776\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0628, -4.8334, -4.1553, -4.6478], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.7931, -5.0462, -4.9660, -4.6478, -4.9660, -4.8886, -5.5200, -4.9660,\n",
      "        -4.4036, -4.1553], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9972, -4.6654, -4.9972, -4.7398, -4.9972, -4.9972, -5.4421, -4.9972,\n",
      "        -4.8755, -4.7398], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0780273973941803\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0814, -4.8049, -4.1788, -4.6596], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.2819, -5.7607, -5.3680, -5.3680, -4.4135, -2.6418, -5.4974, -5.0113,\n",
      "        -5.0649, -4.1788], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0055, -5.3374, -5.3374, -5.3374, -4.9722, -2.7896, -5.4795, -5.0470,\n",
      "        -5.1578, -4.7609], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09403984248638153\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0702, -4.7783, -4.2303, -4.6701], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.2303, -5.1319, -5.0513, -5.3664, -4.8736, -4.2303, -5.1129, -5.3288,\n",
      "        -5.0289, -3.9365], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8073, -5.0505, -5.1209, -5.3862, -5.1209, -4.8073, -5.2113, -5.4078,\n",
      "        -5.0505, -3.9923], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07583485543727875\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0669, -4.7681, -4.2826, -4.6855], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.3137, -5.0849, -5.3785, -5.3785, -4.0127, -4.1408, -1.9790, -3.9571,\n",
      "        -5.0849, -5.3785], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4684, -5.1863, -5.4422, -5.4422, -4.0304, -4.0304, -0.4820, -4.0304,\n",
      "        -5.1863, -5.4422], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.23157048225402832\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0577, -4.7633, -4.3014, -4.6896], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.7273, -4.0585, -4.6896, -5.1070, -4.5872, -4.0585, -4.6863, -4.6896,\n",
      "        -5.4200, -4.7530], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4690, -4.0598, -4.8712, -5.2177, -4.7264, -4.0598, -4.8712, -4.8712,\n",
      "        -5.4690, -4.7264], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.020162781700491905\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0339, -4.7608, -4.3061, -4.7036], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.1417, -5.0339, -4.3916, -5.4059, -4.3061, -5.1417, -5.1554, -5.0849,\n",
      "        -5.4382, -4.7036], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2401, -5.2401, -4.9525, -5.4799, -4.8755, -5.2401, -5.1386, -5.2823,\n",
      "        -5.4799, -4.8755], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0776633471250534\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0337, -4.7773, -4.2963, -4.7381], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.7578, -4.5502, -4.9614, -5.1767, -5.3230, -5.6956, -4.2963, -4.7047,\n",
      "        -5.0870, -4.7047], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4900, -4.8667, -5.2342, -5.1262, -5.3429, -5.4653, -4.8667, -4.8667,\n",
      "        -5.2342, -4.8667], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07017160952091217\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9962, -4.7805, -4.2855, -4.7629], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.5970, -5.4895, -5.7239, -5.3877, -4.2855, -5.2218, -5.4706, -5.4706,\n",
      "        -5.4706, -5.1021], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3370, -5.6270, -5.4886, -5.4886, -4.8570, -5.2385, -5.4704, -5.4704,\n",
      "        -5.4704, -5.2569], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05028875917196274\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9579, -4.7984, -4.2826, -4.7816], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.2254, -4.7816, -5.5005, -5.2566, -3.5684, -5.2566, -4.2826, -4.7204,\n",
      "        -5.2566, -4.7204], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2176, -4.8544, -5.4805, -5.2483, -2.6120, -5.2483, -4.8544, -4.8544,\n",
      "        -5.2483, -4.8544], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12834861874580383\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9200, -4.8095, -4.2795, -4.7912], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.1383, -4.2795, -5.5185, -5.6010, -4.2795, -5.2900, -4.5781, -4.2795,\n",
      "        -4.9908, -5.5185], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2576, -4.8516, -5.4917, -5.4917, -4.8516, -5.2659, -4.6574, -4.8516,\n",
      "        -5.2659, -5.4917], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10918934643268585\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8889, -4.8343, -4.2921, -4.8072], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.2383, -5.5447, -5.0230, -4.8072, -5.5447, -5.1200, -4.2921, -4.2921,\n",
      "        -5.1355, -4.2921], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1661, -5.5207, -5.2870, -4.8629, -5.5207, -5.2706, -4.8629, -4.8629,\n",
      "        -4.6607, -4.8629], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13046357035636902\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8513, -4.8701, -4.3308, -4.8300], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.8300, -5.3313, -4.0978, -4.8045, -5.5778, -5.5765, -4.7135, -4.3308,\n",
      "        -5.2705,  0.3654], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8977, -5.3240, -4.1575, -4.8977, -5.5767, -5.6710, -5.2981, -4.8977,\n",
      "        -5.2107, 10.0000], grad_fn=<AddBackward0>)\n",
      "LOSS: 9.351785659790039\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7776, -4.8649, -4.2797, -4.7188], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.3544, -5.4570, -5.0464, -5.4933, -5.0464, -5.4933, -4.6169, -5.2408,\n",
      "        -5.3111, -5.2408], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.5226, -5.5417, -5.2799, -5.5417, -5.2799, -5.5417, -4.6581, -5.2799,\n",
      "        -5.2717, -5.2799], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0819115936756134\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7127, -4.8614, -4.2281, -4.6154], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.1478, -5.1478, -4.0234, -2.5560, -3.1752, -4.8738, -4.2281, -4.6928,\n",
      "        -5.4289, -5.1478], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2235, -5.2235, -3.8577, -2.4440, -2.4440, -4.6211, -4.8053, -4.8053,\n",
      "        -5.5136, -5.2235], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10087341070175171\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6438, -4.8549, -4.1724, -4.5276], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.3741, -4.1724, -5.0729, -5.0729, -5.0729, -4.6438, -4.5276, -4.6438,\n",
      "        -5.3122, -4.5276], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4779, -4.7551, -5.1673, -5.1673, -5.1673, -5.1673, -4.7551, -5.1673,\n",
      "        -5.4026, -4.7551], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10368414968252182\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-5.0950, -5.3446, -2.4828, -4.0958, -4.5893, -3.7322, -4.6663, -1.5177,\n",
      "        -5.0042, -3.7322], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3848, -5.3848, -2.3659, -4.6862, -4.4961, -3.5334, -5.0399, -0.0624,\n",
      "        -5.0399, -3.5334], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2794174253940582\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0817, -5.1610, -4.3188, -4.9276], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.9276, -4.4895, -4.9614, -3.7651, -3.9979, -4.7445, -5.3026, -4.4895,\n",
      "        -5.3026, -4.9229], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8870, -4.5981, -4.8870, -3.4019, -4.5981, -4.8210, -5.2540, -4.5981,\n",
      "        -5.2540, -5.2700], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0653987005352974\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7877, -4.9272, -3.8941, -4.4858], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.7196, -5.1451, -3.8646, -4.5782, -5.2701, -4.9223, -5.1814, -4.5887,\n",
      "        -4.8671, -5.2701], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7522, -4.6842, -3.3092, -4.6842, -5.1179, -4.7347, -5.1179, -4.6842,\n",
      "        -4.7347, -5.1179], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0645361840724945\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-3.3033, -5.2304, -5.2304, -4.4787, -4.8034, -1.3020, -5.0945, -4.4846,\n",
      "        -4.4846, -4.8034], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2432, -5.0309, -5.0309, -4.6331, -4.6331,  0.1705, -5.0309, -4.4488,\n",
      "        -4.4488, -4.6331], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.23400136828422546\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8931, -4.8845, -3.7939, -4.4460], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.7034, -5.4626, -4.7034, -4.6026, -5.3022, -5.4006, -4.7034, -5.3022,\n",
      "        -4.5342, -5.3320], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5557, -4.9591, -4.5557, -4.5903, -4.9591, -4.8884, -4.5557, -4.9591,\n",
      "        -4.1415, -4.8884], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11680327355861664\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8635, -4.8091, -3.8414, -4.3972], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.7433, -5.1638, -4.4225, -3.6001, -4.5894, -4.6893, -5.0657, -4.5894,\n",
      "        -2.1094, -5.1638], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5701, -4.9802, -4.5701, -3.0608, -4.5619, -4.5619, -5.0046, -4.5619,\n",
      "        -1.9651, -4.9802], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.045226823538541794\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-4.4890, -5.0675, -4.9800, -4.6913, -4.0574, -4.9258, -3.9857, -4.6888,\n",
      "        -4.9800, -4.4890], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5872, -4.8399, -5.0502, -4.7367, -4.6129, -5.0502, -4.5157, -4.7367,\n",
      "        -5.0502, -4.5872], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06901336461305618\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-4.8452, -4.9209, -4.2425, -4.4847, -4.8476, -3.7922, -4.4355, -4.4355,\n",
      "        -2.0681, -4.2425], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0650, -5.0650, -4.2513, -4.6253, -5.0362, -4.5398, -4.6004, -4.6004,\n",
      "        -1.8179, -4.2513], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08003740012645721\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7461, -4.4683, -3.8938, -4.3078], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.6122, -4.8889, -4.8889, -4.5125, -3.8938, -3.7602, -4.4263, -3.4145,\n",
      "        -4.8889, -4.4237], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7069, -5.0233, -5.0233, -4.5729, -4.5045, -4.3842, -4.5729, -2.9362,\n",
      "        -5.0233, -4.7069], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1159522533416748\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7159, -4.3617, -3.8467, -4.3187], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.8467, -4.3187, -4.4172, -1.9623, -4.0611, -5.1412, -4.4344, -3.8467,\n",
      "        -4.6423, -4.4344], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4620, -4.4620, -4.5352, -1.7525, -4.3618, -4.9610, -4.5352, -4.4620,\n",
      "        -4.9491, -4.5352], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10729964077472687\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6896, -4.2830, -3.8012, -4.3476], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.1012, -4.6896, -4.9079, -4.2889, -4.4526, -4.3806, -4.3726, -3.0481,\n",
      "        -3.2530, -3.8919], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4803, -4.4944, -4.9353, -4.6082, -4.4944, -4.5230, -4.4944, -2.9924,\n",
      "        -2.9924, -4.1931], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04831058531999588\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6546, -4.2161, -3.7385, -4.3814], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.9584, -0.3169, -3.9261, -3.7385, -4.4913, -4.3169, -3.7385, -4.8512,\n",
      "        -4.4913, -2.9584], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.0378,  0.3844, -4.3646, -4.3646, -4.4478, -4.4478, -4.3646, -4.7060,\n",
      "        -4.4478, -3.0378], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15228861570358276\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6194, -4.1588, -3.6760, -4.4103], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.4718, -4.9605, -3.6760, -4.0714, -4.5276, -2.9122, -4.9899, -3.1230,\n",
      "        -4.5276, -4.9899], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.0820, -4.8420, -4.3084, -4.1246, -4.4023, -3.0820, -4.8420, -3.0820,\n",
      "        -4.4023, -4.8420], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.067437544465065\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5869, -4.1130, -3.6296, -4.4259], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.7350, -3.8242, -4.1009, -4.4259, -4.9748, -5.0190, -3.7419, -4.0552,\n",
      "        -2.9210, -3.4371], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7253, -4.2344, -4.3814, -4.2666, -4.8131, -4.8131, -4.2666, -4.3814,\n",
      "        -3.1295, -3.1295], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08607521653175354\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-1.6899, -3.5780, -3.5780, -4.4161, -4.5980, -4.4161, -2.9039, -4.6916,\n",
      "        -4.5274, -5.0406], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7183, -4.2202, -4.2202, -4.2202, -4.7177, -4.2202, -3.1760, -4.7899,\n",
      "        -4.4094, -4.7899], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10772629827260971\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5194, -4.0478, -3.5612, -4.0911], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.5549, -3.7234, -5.0407, -4.5798, -4.5798, -4.5748, -3.5549, -4.3389,\n",
      "        -3.5549, -5.0407], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.1994, -4.1994, -4.7909, -4.3511, -4.3511, -4.3511, -4.1994, -4.7050,\n",
      "        -4.1994, -4.7909], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.18862520158290863\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5135, -4.0526, -3.5866, -4.0837], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.8874, -4.4603, -4.2457, -1.6529, -4.0526, -3.7713, -4.3439, -4.0526,\n",
      "        -3.5587, -5.0145], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2137, -4.3489, -4.3941, -1.6857, -4.2642, -4.2028, -4.2028, -4.2642,\n",
      "        -4.2028, -4.8212], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08899351209402084\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5199, -4.0889, -3.6217, -4.0705], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.9823, -4.2802, -2.9943, -4.5199, -4.8874, -2.9943, -4.5102, -4.0889,\n",
      "        -3.8278, -3.4340], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8586, -4.2053, -3.2066, -4.4501, -4.8586, -3.2066, -4.4450, -4.2744,\n",
      "        -4.2053, -3.2066], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03494972735643387\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5251, -4.1365, -3.6541, -4.0493], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.0493, -3.6541, -4.1890, -3.5556, -4.9432, -4.8108, -4.2041, -4.9432,\n",
      "        -4.1365, -2.4270], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.2001, -4.2886, -4.4120, -4.2001, -4.8923, -4.6884, -4.2001, -4.8923,\n",
      "        -4.2794, -1.6364], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15561683475971222\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-3.5528, -4.2163, -4.8995, -3.9465, -4.3665, -4.5845, -1.6564, -3.6481,\n",
      "        -4.8995, -4.1557], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.1975, -4.4951, -4.9248, -4.1244, -4.4310, -4.7946, -1.6312, -4.2833,\n",
      "        -4.9248, -4.4310], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10545597970485687\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3534, -4.0912, -3.6398, -3.8986], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.5386, -4.5671, -4.2627, -2.9760, -4.5671, -2.9760, -4.3637, -3.0136,\n",
      "        -0.1532, -4.5671], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.1847, -4.5850, -4.4334, -3.0799, -4.5850, -3.0799, -4.5850, -3.0799,\n",
      "         0.4013, -4.5850], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0829959288239479\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3465, -4.1424, -3.6258, -3.8515], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.0255, -4.3571, -4.5610, -4.8470, -4.8470, -4.3388, -4.1540, -4.5610,\n",
      "        -2.5010, -4.0255], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.1726, -4.6062, -4.6062, -4.9566, -4.9566, -4.2632, -4.4515, -4.6062,\n",
      "        -1.6269, -4.1726], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0991680845618248\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-4.2007, -4.0132, -3.8980, -3.5029, -4.0132, -4.2261, -4.0310, -4.7087,\n",
      "        -4.8448, -2.9330], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4733, -4.1526, -4.1125, -4.1526, -4.1526, -4.3516, -4.1526, -4.9475,\n",
      "        -4.9475, -3.0075], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06849817931652069\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3655, -4.1556, -3.4176, -4.0252], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.8521, -4.8521, -4.3735, -4.8902, -3.6967, -4.8521, -4.8521, -4.6413,\n",
      "        -4.8521, -4.3735], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9128, -4.9128, -4.3612, -4.9128, -4.3270, -4.9128, -4.9128, -4.7041,\n",
      "        -4.9128, -4.3612], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0420515313744545\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.9008, -3.1553, -2.8611, -2.2121], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.7943, -3.5013, -4.4201, -4.0290, -4.7823, -4.3041, -4.6138, -4.3310,\n",
      "        -4.6138, -4.3310], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7237, -4.1511, -4.3170, -4.0843, -4.3827, -4.5466, -4.5466, -4.5466,\n",
      "        -4.5466, -4.5466], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07614384591579437\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.4817, -3.1745, -1.5840, -0.7881], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.0966, -4.6211, -3.6499, -3.6499, -4.0479, -3.3449, -4.0479, -4.4410,\n",
      "        -2.8360, -4.6211], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8541, -4.5109, -4.2849, -4.2849, -4.0615, -2.9917, -4.0615, -4.2869,\n",
      "        -2.9917, -4.5109], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10626582056283951\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.8145, -1.8158, -0.0596,  1.5129], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.4516, -4.8007, -4.9215, -4.9215, -4.3894, -0.0596, -3.3909, -3.3909,\n",
      "        -3.3909, -0.8121], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.2723, -4.3640, -4.8503, -4.8503, -4.4910,  0.3616, -4.0518, -4.0518,\n",
      "        -4.0518,  0.3616], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.31087225675582886\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.0912, -4.9987, -4.2954, -4.9299], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.4007, -4.0280, -4.1953, -2.8427, -3.8685, -4.6026, -4.9114, -3.4007,\n",
      "        -3.4007, -4.8514], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.0606, -4.1077, -3.9866, -3.0072, -4.0606, -4.4817, -4.7060, -4.0606,\n",
      "        -4.0606, -4.8659], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14773988723754883\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8148, -4.7269, -3.8874, -4.5855], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.4750, -4.9356, -4.9356, -4.5855, -4.0861, -4.9356, -4.9356, -3.4316,\n",
      "        -3.8874, -4.9356], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.1275, -4.9042, -4.9042, -4.4986, -4.0884, -4.9042, -4.9042, -4.0884,\n",
      "        -4.0884, -4.9042], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09101126343011856\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-4.2198, -2.9049, -3.8693, -4.6609, -3.4696, -4.4797, -4.5713, -4.5706,\n",
      "        -4.5706, -3.3651], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4823, -3.0052, -4.3777, -4.7979, -4.1226, -4.3352, -4.1602, -4.5299,\n",
      "        -4.5299, -3.0052], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11054183542728424\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6124, -4.5306, -3.7916, -4.1940], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.0450, -2.2430, -4.3631, -4.5585, -4.6124, -3.5151, -3.5151,  1.4333,\n",
      "        -3.5151, -4.0996], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.2899, -1.6949, -4.3798, -4.5690, -4.5486, -4.1636, -4.1636, 10.0000,\n",
      "        -4.1636, -4.1636], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.507184028625488\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5617, -4.4697, -3.7982, -4.1245], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.8801, -3.5318, -4.4786, -4.4958, -4.2961, -4.2071, -4.0496, -4.4958,\n",
      "        -4.0417, -4.9215], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8866, -4.1786, -4.5794, -4.5794, -4.5794, -4.4424, -4.1786, -4.5794,\n",
      "        -4.0517, -4.8866], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05961325764656067\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-4.2671, -3.5399, -4.8464, -0.6381, -4.6524, -4.4837, -3.7780, -4.8279,\n",
      "        -4.0243, -4.4400], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4002, -4.1859, -5.0354,  0.4999, -4.8972, -4.5659, -4.0366, -4.8972,\n",
      "        -4.1859, -4.5659], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.19463320076465607\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2906, -4.2090, -3.5260, -3.9991], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.9766, -4.8208, -4.8208, -3.8081, -4.3854, -3.9183, -3.5260, -4.8208,\n",
      "        -3.8370, -4.3854], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0183, -5.0183, -5.0183, -3.9973, -4.5265, -4.1734, -4.1734, -5.0183,\n",
      "        -4.2032, -4.5265], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08125580102205276\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2976, -4.1944, -3.4793, -3.9886], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.6860, -3.4947, -4.1944, -1.3953, -4.4109, -3.9886, -4.6201, -4.5899,\n",
      "        -4.4548, -3.4793], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3174, -4.1452, -4.3174, -1.4347, -4.4651, -4.1313, -4.9698, -4.7819,\n",
      "        -4.4651, -4.1313], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14463293552398682\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3260, -4.2008, -3.4304, -3.9894], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.8343, -3.2018, -4.3619, -4.2008, -4.3619,  0.1879, -4.3619, -2.8044,\n",
      "        -3.9894, -4.7643], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9059, -2.7428, -4.4037, -4.2520, -4.4037,  0.6843, -4.4037, -2.7428,\n",
      "        -4.0874, -4.7599], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04835531488060951\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3498, -4.2078, -3.3659, -4.0021], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.3498, -3.8452, -4.3689, -4.2881, -4.7529, -4.5419, -4.4034, -4.4697,\n",
      "        -3.8562, -4.3498], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3277, -3.8020, -4.3277, -4.2321, -4.8258, -4.6340, -4.4706, -4.6340,\n",
      "        -4.1757, -4.3277], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.015502126887440681\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3750, -4.2184, -3.3037, -4.0136], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.8671, -4.8671, -3.6202, -4.3786, -2.6892, -2.9508, -4.7227, -4.3786,\n",
      "        -4.8671, -3.3037], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7487, -4.7487, -3.9733, -4.2582, -2.7439, -2.7439, -4.5158, -4.2582,\n",
      "        -4.7487, -3.9733], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07327310740947723\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-4.0737, -1.1595, -2.6641, -4.5739, -4.8676, -3.6356, -3.2674, -1.9406,\n",
      "        -4.8676, -4.3845], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6836, -1.2710, -2.7465, -4.2269, -4.7071, -4.0534, -3.9406, -1.2710,\n",
      "        -4.7071, -4.2269], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1444389522075653\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7149, -4.3771, -3.5838, -4.3792], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.0015, -2.9639, -3.2581, -2.9639, -4.5682, -4.0015, -4.5638, -1.8991,\n",
      "        -4.4783,  2.0040], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9323, -2.7092, -3.9323, -2.7092, -4.2254, -3.9323, -4.4931, -1.2493,\n",
      "        -4.4931, 10.0000], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.5074896812438965\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.4082, -4.1764, -3.2327, -3.9382], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.2327, -4.2042, -4.7949, -4.7050, -4.1576, -4.4317, -3.2327, -4.3442,\n",
      "        -4.2042, -4.0757], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9094, -4.1290, -4.6681, -4.6681, -4.0197, -4.4729, -3.9094, -4.2174,\n",
      "        -4.1290, -4.2174], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10015424340963364\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.4126, -4.1603, -3.2300, -3.8800], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.5828, -4.2981, -4.4498, -4.7466, -4.2981, -3.8589, -5.0461, -4.7466,\n",
      "        -4.5545, -4.6526], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9070, -4.2246, -4.2246, -4.6698, -4.2246, -3.6141, -4.6698, -4.6698,\n",
      "        -4.4748, -4.6698], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03865582495927811\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3830, -4.1390, -3.2471, -3.8066], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.3099, -3.9739, -3.6293, -4.1346, -4.3461, -3.6602, -1.0894, -3.2471,\n",
      "        -2.8806, -4.6781], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3912, -3.6225, -3.9224, -3.8987, -4.5075, -4.3912, -1.0808, -3.9224,\n",
      "        -2.4411, -4.6962], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14816048741340637\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3507, -4.0935, -3.2532, -3.7624], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.2415, -3.2532, -4.2415, -3.6721, -4.7719, -4.6376, -4.6376, -3.2532,\n",
      "        -4.2415, -4.2415], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3049, -3.9279, -4.3049, -3.9279, -4.5729, -4.7119, -4.7119, -3.9279,\n",
      "        -4.3049, -4.3049], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10425243526697159\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-4.7115, -3.7292, -4.1078, -3.7292, -4.6145, -3.2915, -3.6924, -4.0555,\n",
      "        -4.2572, -4.8296], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5942, -3.9323, -4.1784, -3.9323, -4.7293, -3.9624, -4.3805, -4.1784,\n",
      "        -4.3485, -4.5942], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11168617010116577\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6878, -4.2442, -3.7410, -4.3017], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.8418, -4.6201, -4.6201, -3.2691, -4.6201, -3.2461, -4.3425, -4.8556,\n",
      "        -4.2230, -3.5665], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6351, -4.7212, -4.7212, -3.9422, -4.7212, -3.9215, -4.5454, -4.7212,\n",
      "        -4.5454, -3.6351], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1150522232055664\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2270, -3.9945, -3.2283, -3.7509], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.3886, -3.7509, -3.2283, -3.2283, -4.3474, -4.3474, -4.6150, -4.6391,\n",
      "        -4.6391, -4.3474], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5774, -3.9055, -3.9055, -3.9055, -4.3730, -4.3730, -4.5238, -4.7052,\n",
      "        -4.7052, -4.3730], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09956648200750351\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1931, -3.9872, -3.2123, -3.7815], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.7238, -3.7815, -3.2197, -4.6742, -4.6742, -3.2103, -4.6742, -4.3987,\n",
      "        -4.2037, -3.9322], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.3558, -3.8911, -3.8977, -4.6903, -4.6903, -3.8893, -4.6903, -4.3734,\n",
      "        -4.4968, -3.8977], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1156693696975708\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1657, -3.9801, -3.1973, -3.8258], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.7382, -4.4555, -3.1973, -4.7174, -3.9894, -4.4555, -3.9801, -4.4002,\n",
      "        -3.8258, -4.6144], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6733, -4.3698, -3.8776, -4.6733, -4.0968, -4.3698, -3.8632, -4.5275,\n",
      "        -3.8776, -4.6733], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.053117237985134125\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-4.5023, -3.9772, -2.6955, -4.5023, -4.1082, -4.5023, -3.1894, -3.1879,\n",
      "        -4.2124, -4.7110], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3685, -3.8448, -2.4016, -4.3685, -4.3685, -4.3685, -3.8704, -3.8691,\n",
      "        -4.4478, -4.6618], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12111405283212662\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5916, -4.2975, -3.7417, -4.5354], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.2194, -3.5728, -0.9920, -2.8699, -4.4293, -2.5154, -4.1241, -4.7856,\n",
      "        -3.9696, -3.5516], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4357, -3.8685, -1.0045, -2.4330, -4.4947, -2.4330, -4.3675, -4.6567,\n",
      "        -3.8377, -3.5830], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04305584356188774\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0845, -3.9606, -3.1586, -3.9623], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.1586, -3.1586, -3.1586, -3.1586, -3.1357, -4.6945, -3.1586, -4.0845,\n",
      "        -3.1586, -4.6945], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.8427, -3.8427, -3.8427, -3.8427, -3.8221, -4.6269, -3.8427, -4.3386,\n",
      "        -3.8427, -4.6269], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3353210389614105\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0873, -3.9707, -3.1611, -4.0300], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.6913, -4.0873, -4.6056, -3.1611, -4.6056, -2.6785, -4.0300, -3.1353,\n",
      "        -3.9707, -4.6056], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.8450, -4.3222, -4.3222, -3.8450, -4.3222, -2.5548, -3.8450, -3.8218,\n",
      "        -3.8218, -4.3222], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13304153084754944\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0962, -3.9759, -3.1853, -4.0705], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.5440, -4.6069, -4.6069, -4.8987, -4.6069, -3.4528, -3.1853, -3.1853,\n",
      "        -3.1853, -0.9795], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.6011, -4.3283, -4.3283, -4.6370, -4.3283, -3.8479, -3.8668, -3.8668,\n",
      "        -3.8668, -1.0532], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.18593406677246094\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-4.6738, -4.7121, -3.7555, -3.6437, -3.9383, -4.0978, -4.5794, -2.8648,\n",
      "        -3.7561, -3.2456], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6937, -4.6937, -3.8968, -3.9089, -3.9089, -3.9210, -4.3667, -2.6293,\n",
      "        -3.9210, -3.9210], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0707307681441307\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5234, -4.3067, -3.7721, -4.5573], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.7418, -3.2649, -4.9028, -4.1183, -3.2901, -4.5234, -4.9028,  0.6111,\n",
      "        -4.2871, -4.1648], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.1119, -3.9384, -4.7401, -3.9611, -3.9611, -4.7401, -4.7401,  1.4736,\n",
      "        -4.4800, -4.4800], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2046068161725998\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1603, -4.0059, -3.3183, -4.1283], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.3183, -4.0133, -4.8912, -4.5372, -4.5372, -4.1283, -4.1283, -3.3183,\n",
      "        -2.9214, -3.7923], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9865, -4.1297, -4.7629, -4.4131, -4.4131, -3.9865, -3.9865, -3.9865,\n",
      "        -2.7054, -3.9865], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10783747583627701\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1955, -4.0229, -3.3504, -4.1183], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.1183, -3.3504, -4.5130, -3.8105, -3.3504, -4.8715, -3.3504, -4.0300,\n",
      "        -3.3369, -3.3504], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.0154, -4.0154, -4.4448, -4.0154, -4.0154, -4.7975, -4.0154, -4.1570,\n",
      "        -4.0033, -4.0154], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.229145810008049\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2314, -4.0461, -3.3910, -4.1129], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.6127, -2.9869, -4.6301, -3.3910, -4.7962, -4.4932, -4.8561, -0.0839,\n",
      "        -2.7158, -4.0461], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5978, -2.7768, -4.8390, -4.0519, -4.8390, -4.4827, -4.8390,  1.4803,\n",
      "        -2.7768, -4.0526], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2977456748485565\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2636, -4.0614, -3.4079, -4.0873], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.0214, -4.0873, -4.2793, -3.6196, -4.4594, -3.4205, -4.2636, -3.4205,\n",
      "        -3.4079, -4.8168], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.4761, -4.0671, -4.5007, -4.0619, -4.5007, -4.0785, -4.5007, -4.0785,\n",
      "        -4.0671, -4.8514], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3846863806247711\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3023, -4.0764, -3.4104, -4.0613], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.4082, -3.4292, -4.4082, -4.0583, -3.8772, -2.6268, -4.2339, -4.3804,\n",
      "        -3.4104, -4.7788], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4895, -4.0862, -4.4895, -4.5578, -4.0694, -2.8127, -4.4895, -4.3325,\n",
      "        -4.0694, -4.8483], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12726488709449768\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3376, -4.0910, -3.3956, -4.0500], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.3590, -4.0502, -3.4222, -4.3590, -4.3788, -4.2314, -4.7626, -3.7999,\n",
      "        -4.0910, -4.0500], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5260, -4.0203, -4.0800, -4.5260, -4.4616, -4.4616, -4.8270, -3.6964,\n",
      "        -4.0800, -4.0561], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05641769617795944\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-4.0470, -4.2452, -3.3724, -3.8015, -4.6406, -4.3570, -4.3570, -4.3570,\n",
      "        -3.3724, -4.1148], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.0352, -4.4213, -4.0352, -4.0352, -4.4910, -4.4213, -4.4213, -4.4213,\n",
      "        -4.0352, -4.0648], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10015536844730377\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7470, -4.4066, -3.7669, -4.3494], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.6577, -4.7603, -3.6637, -3.9253, -3.3515, -4.3494, -4.3494, -2.9678,\n",
      "        -4.5841, -4.1642], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4613, -4.7684, -4.0542, -4.2973, -4.0163, -4.3902, -4.3902, -2.8636,\n",
      "        -4.7684, -4.1716], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08197905868291855\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.4027, -4.1524, -3.3356, -4.0574], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.8056, -4.1524, -4.1926, -4.0574, -4.4179, -2.8002, -4.3044, -3.3356,\n",
      "        -4.7664, -4.4649], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6680, -4.0390, -4.1652, -4.0021, -4.2930, -2.8912, -4.3635, -4.0021,\n",
      "        -4.7430, -4.2930], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.18125344812870026\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3957, -4.1579, -3.3338, -4.0713], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.0713, -3.9243, -3.7018, -3.3338, -4.7726, -4.7015, -4.4649, -2.7582,\n",
      "        -4.0713, -3.9037], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.0004, -4.3071, -3.6615, -4.0004, -4.7318, -4.5319, -4.1706, -2.9202,\n",
      "        -4.0004, -4.0004], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07552136480808258\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3866, -4.1505, -3.3487, -4.0741], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.3398, -2.8016, -3.3487, -0.6400, -3.3976, -4.0741, -4.7710, -3.3487,\n",
      "        -3.3487, -4.7710], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3516, -2.9388, -4.0138, -0.6259, -4.0578, -4.0138, -4.7435, -4.0138,\n",
      "        -4.0138, -4.7435], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17874261736869812\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3862, -4.1535, -3.3774, -4.0819], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.6913, -4.6913, -4.4054, -4.3792, -3.0011, -3.0011, -4.3317, -3.3774,\n",
      "        -2.5060, -4.3880], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7658, -4.7658, -4.6342, -4.4432, -2.9518, -2.9518, -4.3658, -4.0397,\n",
      "        -2.9518, -4.3658], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07113957405090332\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3945, -4.1666, -3.3930, -4.0901], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.7629, -4.6894, -2.1618, -4.2602, -3.3930, -4.4059, -4.7629,  2.6516,\n",
      "        -4.3983, -3.3930], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7687, -4.7687, -0.5907, -4.2633, -4.0537, -4.3693, -4.7687, 10.0000,\n",
      "        -4.4389, -4.0537], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.735007286071777\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.4019, -4.1826, -3.3988, -4.0668], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.4503, -4.0668, -4.2972, -4.2972, -3.6585, -4.6647, -4.4331, -3.8598,\n",
      "         0.8171, -2.0276], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.1052, -4.0589, -4.3660, -4.3660, -3.9377, -4.6654, -4.4671, -3.7121,\n",
      "         1.4568, -0.5608], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3100382089614868\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.4048, -4.1923, -3.3935, -4.0372], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.6830, -3.3935,  2.7924, -4.2648, -4.3730, -4.0372, -3.3935, -3.7253,\n",
      "        -4.2648, -3.9008], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7406, -4.0542, 10.0000, -4.3548, -4.3548, -4.0542, -4.0542, -3.7144,\n",
      "        -4.3548, -4.0542], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.286661624908447\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.4047, -4.2048, -3.3751, -3.9913], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.3751, -4.4303, -3.9913, -4.2322, -4.6802, -3.4336, -3.3751, -3.3751,\n",
      "        -3.3751, -2.5248], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.0376, -4.4450, -4.0376, -4.3368, -4.6692, -4.0902, -4.0376, -4.0376,\n",
      "        -4.0376, -2.4526], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2205391228199005\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.4182, -4.2304, -3.3713, -3.9600], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.2110, -3.9600, -4.1819, -4.5980, -3.3713, -4.4350, -3.9600, -3.9600,\n",
      "        -4.8322, -4.0831], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3275, -4.0342, -3.8311, -4.6914, -4.0342, -4.4396, -4.0342, -4.0342,\n",
      "        -4.6914, -4.3757], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07066886126995087\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-4.7217, -4.6651, -4.5687, -4.1880, -3.3859, -4.2988, -1.2623, -4.6959,\n",
      "        -3.3782, -3.3782], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5145, -4.6775, -4.6775, -4.3199, -4.0881, -4.3199, -0.4875, -4.5145,\n",
      "        -4.0403, -4.0403], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.20760774612426758\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-1.0703, -3.4087, -4.5553, -4.1043, -4.3246, -4.5609, -3.3828, -3.1410,\n",
      "        -4.3292, -4.1747], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5021, -4.0655, -4.6570, -4.3500, -4.2423, -4.4292, -4.0445, -3.8269,\n",
      "        -4.2423, -4.3051], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17820337414741516\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.4349, -4.2559, -3.3842, -3.9906], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.3842, -4.5580, -4.5580, -4.1039, -4.1817, -4.1817, -3.3842, -4.5580,\n",
      "        -3.3842, -3.9906], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.0457, -4.6328, -4.6328, -4.3258, -4.2879, -4.2879, -4.0457, -4.6328,\n",
      "        -4.0457, -4.0457], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14046886563301086\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-3.3917, -4.8999, -4.2082, -4.5792, -4.5792, -4.0119, -3.3917, -4.2710,\n",
      "        -3.7490, -4.5792], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.0525, -4.6943, -4.2724, -4.6107, -4.6107, -4.2724, -4.0525, -4.2171,\n",
      "        -3.6851, -4.6107], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09976162016391754\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2638, -4.0659, -3.2881, -3.7213], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.4140, -4.2932, -4.8705, -4.2321, -4.2680, -4.5523, -4.1047, -3.4140,\n",
      "        -0.5694,  1.1698], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.0726, -4.2644, -4.6105, -4.2644, -3.9593, -4.4096, -4.0726, -4.0726,\n",
      "        -0.6211,  2.1769], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.20704936981201172\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-4.6593, -4.3633, -3.4537, -4.2309, -3.6425, -4.5745, -4.6593, -4.2572,\n",
      "        -3.4537, -4.2533], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6395, -4.4310, -4.1083, -4.2917, -4.1083, -4.6395, -4.6395, -4.2783,\n",
      "        -4.1083, -3.9617], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11728145182132721\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5256, -4.3101, -3.6752, -4.3159], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.0014, -2.4006, -4.3060, -4.3060, -4.6791, -4.3060, -4.6981, -4.6981,\n",
      "        -4.2073, -4.3060], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.0091, -1.3095, -4.3146, -4.3146, -4.6743, -4.3146, -4.6743, -4.6743,\n",
      "        -4.1383, -4.3146], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1196800023317337\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-4.3744, -4.2617, -4.3744, -3.7289, -4.4310, -3.7289, -3.5239, -4.3502,\n",
      "        -4.7511, -4.7511], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3560, -4.1715, -4.3560, -4.1715, -4.7943, -4.1715, -4.1715, -4.3560,\n",
      "        -4.7118, -4.7118], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09551918506622314\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2015, -3.9104, -3.3693, -3.8070], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.5356, -3.3693, -3.4671,  0.2263, -4.2734, -4.2734, -4.4618, -4.8055,\n",
      "        -4.8243, -4.4618], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.1821, -4.0324, -4.1204,  2.3185, -4.1821, -4.1821, -4.4164, -4.7565,\n",
      "        -4.8153, -4.4164], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5684942007064819\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1801, -3.8671, -3.3862, -3.7879], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.3637, -3.5347, -3.3862, -3.5347, -4.8118, -4.4918, -3.5347, -3.6027,\n",
      "        -0.0606, -3.5347], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4633, -4.1812, -4.0476, -4.1812, -4.7917, -4.4633, -4.1812, -3.7963,\n",
      "         2.3326, -4.1812], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.7885640859603882\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1519, -3.8434, -3.4086, -3.7933], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.5258, -3.4086, -4.1045, -4.2027, -3.5560, -3.5560, -4.8282, -4.2281,\n",
      "        -2.7968, -4.2281], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5126, -4.0678, -4.3768, -4.5724, -4.2004, -4.2004, -4.8337, -4.2004,\n",
      "        -3.8164, -4.2004], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2517181634902954\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1630, -3.8463, -3.4058, -3.7961], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.2624, -4.1105, -4.2213, -4.8416, -4.8416, -3.4058, -2.1133, -4.5486,\n",
      "        -3.7473, -0.2460], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5698, -4.3712, -4.1913, -4.8409, -4.8409, -4.0653, -1.2214, -4.5256,\n",
      "        -4.0653, -0.6822], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1685672104358673\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1680, -3.8466, -3.3941, -3.8053], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.8306, -3.8466, -4.8703, -4.1680, -4.8703, -4.9325, -4.2803, -3.9248,\n",
      "        -4.5837, -4.2690], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7571, -4.2419, -4.8421, -4.3702, -4.8421, -4.8421, -4.5529, -4.1801,\n",
      "        -4.5323, -4.5323], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.042371682822704315\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-4.2662, -3.3802, -3.9196, -1.7702, -4.4652, -3.5013, -4.9003, -4.0528,\n",
      "        -4.2207, -3.5013], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5276, -4.0422, -4.1511, -1.3824, -4.5190, -4.1511, -4.8396, -4.3359,\n",
      "        -4.1511, -4.1511], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16467607021331787\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.4964, -4.2186, -3.6980, -4.3804], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.6396, -4.1973, -4.9397, -3.5677, -3.4678, -3.4678, -4.9397, -4.6396,\n",
      "        -4.9397, -2.0583], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5226, -4.3282, -4.8427, -4.2109, -4.1210, -4.1210, -4.8427, -4.5226,\n",
      "        -4.8427, -1.4602], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16976317763328552\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2449, -3.9563, -3.3788, -3.8639], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.3494, -4.2875, -4.6625, -4.9708, -4.9708, -3.4661, -4.9708, -3.4661,\n",
      "        -4.4278, -4.6625], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3239, -4.4799, -4.5393, -4.8681, -4.8681, -4.1195, -4.8681, -4.1195,\n",
      "        -4.4799, -4.5393], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09561856091022491\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2721, -3.9917, -3.4024, -3.8766], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.9216,  1.5553, -2.5530, -2.0167, -4.3870, -3.4024, -4.6708, -3.4812,\n",
      "        -3.6250, -4.6708], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9040,  2.1938, -1.6424, -1.6424, -4.5668, -4.0621, -4.5668, -4.1331,\n",
      "        -3.6812, -4.5668], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.22945621609687805\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2994, -4.0051, -3.4368, -3.9170], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.6907, -3.5072, -4.6907, -0.4403, -4.2309, -3.5072, -1.9484, -3.5072,\n",
      "        -0.4403, -4.6907], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5948, -4.1564, -4.5948, -0.5896, -4.1564, -4.1564, -1.7699, -4.1564,\n",
      "        -0.5896, -4.5948], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13742955029010773\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-4.2536, -4.2536, -3.9501, -4.2536, -5.0385, -3.9344, -5.0385, -3.4832,\n",
      "        -4.7011, -3.5515], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.1964, -4.1964, -4.1964, -4.1964, -4.9974, -4.3633, -4.9974, -4.1348,\n",
      "        -4.6345, -4.1964], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11027821153402328\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.4701, -4.1667, -3.5810, -4.2633], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.9278, -4.1667, -3.7177, -3.9946, -4.2633, -3.5810, -5.0574, -4.7092,\n",
      "        -5.0574, -3.9946], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0408, -4.1789, -4.3459, -4.2229, -4.2229, -4.2229, -5.0408, -4.6665,\n",
      "        -5.0408, -4.2229], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0927823930978775\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.4980, -4.1715, -3.5924, -4.2663], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.1254, -4.3601, -4.7261, -5.0774, -3.5924, -4.5042, -4.7261, -4.8377,\n",
      "        -4.2532, -2.1471], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0707, -4.4704, -4.6920, -5.0707, -4.2332, -4.6920, -4.6920, -4.9257,\n",
      "        -4.3781, -2.0859], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04904770106077194\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5251, -4.1830, -3.5991, -4.2805], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.6240,  0.4462, -5.1031, -3.6240, -1.3846, -3.0860, -4.4811, -3.5991,\n",
      "        -3.5991, -3.6240], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.2616,  2.0628, -5.0887, -4.2616, -0.5985, -2.1787, -4.4841, -4.2392,\n",
      "        -4.2392, -4.2616], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6093727946281433\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5252, -4.1820, -3.5826, -4.2857], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.7516, -4.7516,  0.4443, -3.3599, -3.5826, -4.2857, -0.3253, -3.5826,\n",
      "        -4.7516, -4.2857], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6902, -4.6902,  2.0294, -3.7459, -4.2243, -4.2243, -0.5571, -4.2243,\n",
      "        -4.6902, -4.2243], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.35578516125679016\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5345, -4.1827, -3.5776, -4.2916], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.1275, -5.1275, -4.2214, -3.5776, -0.2792, -3.5776, -4.3045, -4.7526,\n",
      "        -4.5985, -4.2916], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0812, -5.0812, -4.2198, -4.2198, -0.5267, -4.2198, -4.3416, -4.6842,\n",
      "        -4.6842, -4.2198], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09091145545244217\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5481, -4.1887, -3.5864, -4.2986], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.3628, -4.7473, -4.7473, -4.5469, -3.5864, -5.1329, -4.7473, -4.2986,\n",
      "        -5.1329, -2.3628], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.4955, -4.6863, -4.6863, -4.6863, -4.2278, -5.0922, -4.6863, -4.2278,\n",
      "        -5.0922, -2.4955], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.048549238592386246\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5628, -4.1960, -3.6116, -4.2996], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.7295, -4.7295, -4.8445, -3.6116, -5.1334, -4.5628, -4.7295, -4.3735,\n",
      "        -3.7894, -5.1334], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7016, -4.7016, -4.9362, -4.2504, -5.1249, -4.7016, -4.7016, -4.5598,\n",
      "        -4.4104, -5.1249], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0858701765537262\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5956, -4.2106, -3.6440, -4.3096], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.3096, -4.8796, -3.9867, -4.1282, -4.8796, -5.1345, -3.9867, -3.7399,\n",
      "        -1.8284, -4.3592], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.2796, -4.9623, -4.3997, -4.2796, -4.9623, -5.1578, -4.3997, -3.6826,\n",
      "        -0.4443, -4.3997], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.22998368740081787\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6306, -4.2227, -3.6701, -4.3099], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.6701, -4.1461, -4.7743, -4.6900, -3.6701, -4.2503, -4.6900, -3.6701,\n",
      "        -5.1239, -4.6306], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3031, -4.3031, -4.9089, -4.7315, -4.3031, -4.3771, -4.7315, -4.3031,\n",
      "        -5.1854, -4.7315], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12782998383045197\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6761, -4.2509, -3.6944, -4.3214], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.5831, -4.6793, -4.6793, -0.2217, -2.1550, -3.6944, -4.6793, -4.2509,\n",
      "        -4.6793, -5.2546], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6704, -4.7424, -4.7424, -0.4836, -2.6511, -4.3249, -4.7424, -4.4234,\n",
      "        -4.7424, -4.9934], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08337376266717911\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-4.1680, -3.8089, -3.8733, -3.5990, -5.1164, -3.7196, -4.3350, -3.7196,\n",
      "        -3.8089, -3.7196], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3477, -4.4280, -4.4860, -3.6599, -5.2214, -4.3477, -4.3477, -4.3477,\n",
      "        -4.4280, -4.3477], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.23724007606506348\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-4.7534, -3.6191, -5.1211, -5.0238, -3.7553, -3.7716, -4.3242, -4.7860,\n",
      "        -4.3559, -3.7553], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7743, -3.6589, -5.2488, -5.0223, -4.3798, -3.6589, -4.3944, -4.7440,\n",
      "        -4.3798, -4.3798], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08182109892368317\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7962, -4.4011, -3.7901, -4.3844], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.9745, -3.7901, -3.7901, -5.5058, -4.3844, -4.6778, -4.6778, -4.6778,\n",
      "        -4.6778, -5.1315], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2710, -4.4111, -4.4111, -5.2710, -4.4111, -4.7922, -4.7922, -4.7922,\n",
      "        -4.7922, -5.2710], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09867607057094574\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-4.7039, -3.8108, -0.2837, -5.1516, -4.7039, -4.7039, -5.1516, -4.7039,\n",
      "        -4.5649, -2.9453], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7938, -4.4297, -0.5604, -5.2656, -4.7938, -4.7938, -5.2656, -4.7938,\n",
      "        -4.6279, -2.6133], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06321299076080322\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8573, -4.5061, -3.8084, -4.4806], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.8226, -5.1883, -3.8185, -5.2507, -5.1883, -4.7438, -3.8084, -5.1883,\n",
      "        -4.4806, -4.4806], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7896, -5.2390, -4.4367, -5.2390, -5.2390, -4.7752, -4.4275, -5.2390,\n",
      "        -4.4275, -4.4275], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0781034529209137\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8848, -4.5557, -3.8104, -4.5303], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.8936, -5.2275,  0.4374, -3.8032, -4.7835, -3.8104, -4.7835, -5.2275,\n",
      "        -4.7835, -3.8032], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5043, -5.2193,  1.7119, -4.4228, -4.7612, -4.4293, -4.7612, -5.2193,\n",
      "        -4.7613, -4.4228], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3150188624858856\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9020, -4.5942, -3.8140, -4.5599], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.9197, -5.2455, -3.8140, -4.8023, -5.2455, -4.2086, -4.2086, -4.5599,\n",
      "        -4.8303, -5.2785], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5661, -5.2041, -4.4326, -4.7505, -5.2041, -4.4131, -4.4131, -4.4326,\n",
      "        -4.7877, -5.0048], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06904418021440506\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9012, -4.6130, -3.8466, -4.5819], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.5819, -4.9417, -4.6678, -3.8466, -3.7028, -3.8466, -4.9417, -3.8466,\n",
      "        -4.6604, -5.2547], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4619, -4.7706, -4.3299, -4.4619, -3.5688, -4.4619, -4.7706, -4.4619,\n",
      "        -4.5896, -5.2234], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13470588624477386\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9038, -4.6135, -3.9124, -4.5994], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.9038, -4.9127, -4.2516, -4.8630, -4.2899, -3.9124, -4.2516, -5.2542,\n",
      "        -5.2542, -3.6822], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8264, -4.6369, -4.5211, -4.8609, -4.4583, -4.5211, -4.5211, -5.2759,\n",
      "        -5.2759, -3.5933], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06351800262928009\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8888, -4.5885, -3.9751, -4.6015], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.3418, -0.2506, -4.8463, -4.5885, -2.0999, -5.2566, -4.8571, -4.8463,\n",
      "        -4.8888, -4.8463], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5776, -0.5049, -4.9076, -4.5021, -2.6720, -5.3429, -4.9195, -4.9076,\n",
      "        -4.9076, -4.9076], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04780537635087967\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8695, -4.5733, -4.0199, -4.5926], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.4270, -4.0199, -4.8756, -4.7693, -4.0199, -5.2581, -4.8884, -5.2581,\n",
      "        -2.1770, -4.6235], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6179, -4.6179, -4.9843, -4.7566, -4.6179, -5.3996, -4.9843, -5.3996,\n",
      "        -2.6605, -4.4882], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10649530589580536\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8527, -4.5619, -4.0578, -4.5870], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.2699, -5.3568, -4.0578, -5.0871, -4.4385, -4.3252,  0.5983, -4.5870,\n",
      "        -3.9688, -4.9095], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4530, -5.2911, -4.6520, -5.1608, -3.6751, -4.6520,  1.6402, -4.6520,\n",
      "        -4.5719, -5.0550], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.25605902075767517\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8178, -4.5126, -4.0764, -4.5729], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.5729, -0.2307, -4.9386, -4.5729, -3.9987, -5.2805, -4.9899, -4.5511,\n",
      "        -4.5126, -4.0764], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6687, -0.4113, -5.1089, -4.6687, -4.5989, -5.4909, -5.1089, -4.8018,\n",
      "        -4.5989, -4.6687], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09197582304477692\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7955, -4.4888, -4.0819, -4.5776], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.0819, -5.3108, -4.0819, -4.5810, -4.0819, -5.3331, -4.7955, -4.9761,\n",
      "        -4.9761, -4.9761], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6737, -5.5114, -4.6737, -5.1997, -4.6737, -5.3529, -5.1386, -5.1386,\n",
      "        -5.1386, -5.1386], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1671215146780014\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8120, -4.4849, -4.0636, -4.6233], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.0636, -5.3710, -5.3710, -2.0598, -4.0636, -3.9831, -5.3710, -5.3710,\n",
      "        -5.1273, -2.0598], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6573, -5.4861, -5.4861, -2.5872, -4.6573, -4.5848, -5.4861, -5.4861,\n",
      "        -5.3285, -2.5872], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17166435718536377\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8443, -4.5134, -4.0377, -4.6688], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.0377, -1.7393, -4.0377, -4.3019, -3.9222, -5.4320, -3.9489, -5.4320,\n",
      "        -4.8349, -2.5263], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6339, -0.3094, -4.6339, -3.6968, -3.6968, -5.4510, -4.5540, -5.4510,\n",
      "        -4.7819, -2.5654], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.35436469316482544\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-4.3938, -5.1202, -3.9757, -4.2567, -4.3938, -5.4709, -4.0331, -4.3859,\n",
      "        -4.0331, -4.3771], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5040, -5.0683, -4.5782, -3.7097, -4.5040, -5.4312, -4.6298, -4.5387,\n",
      "        -4.6298, -4.5782], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1466555893421173\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9029, -4.5229, -4.0428, -4.7440], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.5187, -5.1508, -4.3828, -4.5851, -3.9273, -2.0591, -4.0428, -5.1508,\n",
      "        -3.3932, -5.5213], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4887, -5.0643, -4.4887, -4.7793, -4.5346, -2.4098, -4.6386, -5.0643,\n",
      "        -3.7435, -5.4251], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10433731973171234\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9377, -4.5482, -4.0496, -4.7730], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.6081, -4.0496, -5.1684, -3.9225, -3.6104, -4.3949, -4.0496, -2.3189,\n",
      "        -4.9089, -3.9225], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7815, -4.6447, -5.0567, -4.5303, -4.4772, -4.4721, -4.6447, -2.3404,\n",
      "        -5.0567, -4.5303], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.22690212726593018\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9778, -4.5876, -4.0466, -4.8171], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.0466, -4.8171, -5.5829, -4.0466, -3.1125, -2.0803, -4.0466, -3.7118,\n",
      "        -4.1228, -5.0599], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6419, -4.6419, -5.4018, -4.6419, -2.2884, -2.2884, -4.6419, -4.3997,\n",
      "        -3.8012, -4.9393], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.24403128027915955\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0057, -4.6114, -4.0353, -4.8714], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.8714, -3.8830, -4.0353, -4.4240, -5.6250, -4.0353, -4.8714, -4.0353,\n",
      "        -5.6250, -4.0353], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6318, -4.4947, -4.6318, -4.4176, -5.3749, -4.6318, -4.6318, -4.6318,\n",
      "        -5.3749, -4.6318], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2037336826324463\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0355, -4.6367, -4.0671, -4.9073], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.0671, -5.2473, -5.2473, -5.6471, -2.2120, -4.0671, -5.6471, -5.2391,\n",
      "        -5.2473, -4.0671], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6604, -5.0119, -5.0119, -5.3902, -2.2595, -4.6604, -5.3902, -5.1786,\n",
      "        -5.0119, -4.6604], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13600197434425354\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0638, -4.6633, -4.1443, -4.9220], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.6333, -5.2271, -5.2290, -4.5466, -4.9220, -5.2290, -5.6333,  0.6676,\n",
      "        -4.6942, -5.0638], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4495, -5.2337, -5.0626, -4.4708, -4.7298, -5.0626, -5.4495,  1.5597,\n",
      "        -4.8261, -5.0626], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09789852797985077\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0775, -4.6805, -4.2271, -4.9018], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.5797, -4.2271, -4.2271, -4.9018, -5.0202, -3.5833, -4.2271, -4.2271,\n",
      "        -4.8145, -5.0775], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8044, -4.8044, -4.8044, -4.8044, -5.1217, -4.2250, -4.8044, -4.8044,\n",
      "        -5.0128, -5.1217], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1856338083744049\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.1082, -4.7025, -4.3147, -4.8881], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.7983, -5.5446, -5.5446, -4.8881, -4.8881, -4.8373, -4.7169, -4.8881,\n",
      "        -4.8881, -5.1344], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3387, -5.5953, -5.5953, -4.8832, -4.8832, -5.0752, -4.9667, -4.8832,\n",
      "        -4.8832, -5.1880], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03383350744843483\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-5.5168, -5.1026, -4.3757, -5.1026, -5.2515, -4.8801, -4.4946, -5.5168,\n",
      "        -4.3757, -4.3757], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6454, -5.2295, -4.9382, -5.2295, -5.4182, -4.9382, -4.9382, -5.6454,\n",
      "        -4.9382, -4.9382], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12422088533639908\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.1676, -4.7655, -4.4002, -4.8882], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.5242, -4.1950, -4.4002, -5.1876,  0.7654, -4.4002, -4.4961, -5.4628,\n",
      "        -5.4870, -5.5242], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6688, -4.7755, -4.9602, -5.2428,  1.5267, -4.9602, -4.6385, -5.4270,\n",
      "        -5.6688, -5.6688], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16430765390396118\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.1998, -4.8014, -4.4094, -4.8966], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.3251, -4.5224, -4.4094, -4.4094, -4.4094, -4.5477, -5.5287, -5.5287,\n",
      "        -4.2164, -4.4874], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4189, -4.6347, -4.9685, -4.9685, -4.9685, -4.9685, -5.6708, -5.6708,\n",
      "        -4.7948, -4.6347], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1532706916332245\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.2280, -4.8417, -4.3977, -4.9144], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.3977, -4.8417, -5.5573, -4.3962, -4.9144, -5.5573, -1.1948, -4.8579,\n",
      "        -5.1151, -4.5324], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9580, -4.8113, -5.6609, -4.8494, -4.9580, -5.6609, -0.2556, -5.0336,\n",
      "        -5.2275, -4.8113], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15468427538871765\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.2572, -4.8869, -4.3666, -4.9445], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.2642, -4.3666, -5.1413, -4.3666, -4.2233, -4.2233, -5.1211, -5.1413,\n",
      "        -4.6567, -4.6095], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3100, -4.9300, -5.1910, -4.9300, -4.8010, -4.8010, -5.0434, -5.1910,\n",
      "        -4.9300, -4.5964], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.24816444516181946\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3094, -4.9242, -4.3193, -4.9714], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.3193, -4.6121, -4.2491, -5.0452, -5.1781, -5.1781, -4.4935, -4.9714,\n",
      "        -5.1674, -4.9714], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8874, -4.8874, -4.2681, -5.1509, -5.1509, -5.1509, -4.7416, -4.8874,\n",
      "        -5.0032, -4.8874], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05141022801399231\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3516, -4.9469, -4.2767, -4.9930], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.2159, -4.2767, -4.9469, -5.3516, -5.2159, -4.5784, -5.6966, -4.9930,\n",
      "        -5.6810, -4.3790], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1206, -4.8491, -4.7716, -5.1206, -5.1206, -4.8491, -5.5512, -4.8491,\n",
      "        -5.5512, -4.5697], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05981297418475151\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3491, -4.9413, -4.2561, -4.9838], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.3946, -4.2561, -4.2561, -2.1702, -5.6230, -4.9838, -4.2160, -4.9838,\n",
      "        -4.9838, -4.2561], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5624, -4.8305, -4.8305, -2.0331, -5.2401, -4.8305, -4.7944, -4.8305,\n",
      "        -4.8305, -4.8305], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15883326530456543\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3287, -4.9379, -4.2638, -4.9731], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.2740, -1.1442, -2.1867, -4.4699, -4.9731, -4.2638, -4.2645, -5.3651,\n",
      "        -3.6084, -5.4764], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1705, -0.3108, -2.0298, -4.8380, -4.8375, -4.8375, -4.8380, -4.9896,\n",
      "        -2.0298, -5.2629], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4220608174800873\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.2718, -4.9266, -4.2475, -4.9627], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.1707, -4.2475, -5.2951, -5.2951, -4.9627, -5.0093, -5.5985, -4.9627,\n",
      "        -4.9266, -4.4414], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1801, -4.8227, -5.1801, -5.1801, -4.8227, -4.9973, -5.2679, -4.8227,\n",
      "        -4.8521, -4.6470], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.055382102727890015\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.2160, -4.8932, -4.2588, -4.9398], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.3243, -5.3004, -4.9398, -4.8972, -5.7451, -4.9398, -5.8664, -5.7451,\n",
      "        -5.7451, -4.7622], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8919, -5.2156, -4.8329, -4.8329, -5.6276, -4.8329, -5.6276, -5.6276,\n",
      "        -5.6276, -4.6815], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04612326994538307\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.1507, -4.8582, -4.3001, -4.9046], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.7479, -4.9162, -5.2332, -4.3001, -5.2410, -4.3001, -5.2883, -5.7156,\n",
      "        -5.2883, -4.3001], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7383, -3.9573, -5.1461, -4.8701, -5.1461, -4.8701, -5.2790, -5.6831,\n",
      "        -5.2790, -4.8701], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.19121721386909485\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0910, -4.7843, -4.3755, -4.8808], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.2784, -5.1926, -5.6885, -2.3492, -5.0174, -5.2784, -4.5489, -4.3755,\n",
      "        -5.2972, -4.3755], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3687, -5.3687, -5.7675, -2.0969, -5.2003, -5.3687, -4.8205, -4.9379,\n",
      "        -5.3687, -4.9379], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0862286388874054\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-4.4280, -5.2797, -4.4280, -4.8692, -5.2797, -5.1436, -4.9917, -5.5462,\n",
      "        -4.4280, -4.9177], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9852, -5.4259, -4.9852, -4.9852, -5.4259, -5.1574, -5.2669, -5.8262,\n",
      "        -4.9852, -4.9852], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11465100198984146\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.5409, -5.1175, -4.9640, -5.3068], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.4559, -5.4298, -5.6829, -4.4968, -5.0218, -4.8755,  2.6312, -5.4637,\n",
      "        -5.0980, -5.6829], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0103, -5.5882, -5.8457, -4.8973, -5.4676, -5.0103, 10.0000, -5.8457,\n",
      "        -5.3746, -5.8457], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.528427600860596\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0387, -4.6883, -4.4546, -4.9013], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.3222, -4.4546, -4.4546, -4.4546, -5.4755, -3.6170, -4.4546, -4.4546,\n",
      "        -4.0871, -4.9536], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4583, -5.0091, -5.0091, -5.0091, -5.6125, -2.1625, -5.0091, -5.0091,\n",
      "        -4.1459, -5.0091], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3696916699409485\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0664, -4.6692, -4.4218, -4.9398], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.1242, -4.9371, -5.3578, -3.5303, -4.4218, -5.3578, -4.6692, -4.4974,\n",
      "        -5.3578, -5.7065], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3431, -4.8537, -5.4157, -2.2315, -4.9796, -5.4157, -5.0476, -5.0476,\n",
      "        -5.4157, -5.7422], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.25101110339164734\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.1106, -4.6706, -4.3513, -4.9978], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.3513, -4.6800, -5.4229, -4.3513, -4.3513, -5.7451, -4.9978, -5.4065,\n",
      "        -4.9978, -5.7451], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9161, -4.0786, -5.5327, -4.9161, -4.9161, -5.6610, -4.9161, -5.3291,\n",
      "        -4.9161, -5.6610], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13645103573799133\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.1605, -4.6499, -4.3194, -5.0496], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.7713, -4.3194, -3.2793, -4.3194, -4.7557, -5.7713, -4.3194, -4.4784,\n",
      "        -4.9948, -5.7713], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5975, -4.8874, -2.4570, -4.8874, -4.8874, -5.5975, -4.8874, -4.6756,\n",
      "        -5.2137, -5.5975], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.18390662968158722\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.2069, -4.6500, -4.2809, -5.0937], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.8429, -5.4874, -5.0337, -4.2809, -4.7639, -4.2809, -5.4465, -4.8935,\n",
      "        -5.7864, -5.3777], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1612, -5.2354, -5.2354, -4.8528, -4.7220, -4.8528, -5.2354, -5.1227,\n",
      "        -5.5303, -5.1227], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10892526805400848\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.2383, -4.6621, -4.2667, -5.1272], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.5124, -4.6621, -5.7938, -4.8940, -4.2415, -5.2383, -4.8940, -5.7133,\n",
      "        -4.2415, -5.0013], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2128, -4.8174, -5.4948, -5.0998, -4.8174, -5.2128, -5.0998, -5.4948,\n",
      "        -4.8174, -4.8400], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10255948454141617\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.2615, -4.7046, -4.2769, -5.1500], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.5087, -5.5087, -4.2769, -5.1808, -4.7046, -5.0130, -5.1500, -4.2769,\n",
      "        -5.1500, -4.2769], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2033, -5.2033, -4.8492, -5.4529, -4.7954, -5.0953, -4.8492, -4.8492,\n",
      "        -4.8492, -4.8492], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1439170390367508\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.2853, -4.7543, -4.3146, -5.1534], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.0432, -5.7527, -5.7638, -2.9602, -4.2198, -5.7638, -3.5206, -2.9602,\n",
      "        -5.4830, -4.3146], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0983, -5.4785, -5.5055, -2.8515, -4.7978, -5.5055, -3.6642, -2.8515,\n",
      "        -5.2205, -4.8832], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09821326285600662\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.2914, -4.8066, -4.3751, -5.1434], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.4425, -5.3995, -2.9340, -4.2488, -4.3751, -5.1434, -5.4425, -4.3751,\n",
      "        -0.2593, -5.7193], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2616, -5.0145, -2.9132, -4.8239, -4.9376, -4.9376, -5.2616, -4.9376,\n",
      "        -0.3933, -5.5478], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12673993408679962\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.2989, -4.8557, -4.4704, -5.1144], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.9052, -3.4846, -4.4704, -4.4704, -4.4704, -5.3807, -4.8500, -4.4704,\n",
      "        -5.3807, -4.4704], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.9560, -3.6434, -5.0233, -5.0233, -5.0233, -5.3366, -5.0233, -5.0233,\n",
      "        -5.3366, -5.0233], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15905675292015076\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3143, -4.9124, -4.5692, -5.0974], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.2265, -5.3295, -5.5996, -5.3143, -5.1277, -4.5692, -4.3864, -5.2229,\n",
      "        -5.3143, -5.7086], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4125, -5.4125, -5.7038, -5.4125, -5.7006, -5.1123, -4.9478, -5.2481,\n",
      "        -5.4125, -5.7006], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10105852782726288\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3502, -4.9734, -4.6451, -5.1086], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.6451, -5.6969, -3.6982, -4.6451, -5.6496, -5.5777, -5.5777, -2.9370,\n",
      "        -4.6451, -4.9468], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1806, -5.7382, -4.3284, -5.1806, -5.7471, -5.7471, -5.7471, -3.0451,\n",
      "        -5.1806, -5.1806], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1392337828874588\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3950, -5.0128, -4.7291, -5.1303], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.0771, -5.3583, -0.3515, -5.3413, -5.6730, -5.0128, -5.6730, -5.5757,\n",
      "        -4.5228, -4.7102], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3867, -5.5123, -0.3532, -5.3867, -5.7612, -5.0420, -5.7612, -5.7612,\n",
      "        -5.1160, -4.3865], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06290961802005768\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.4400, -5.0355, -4.7887, -5.1705], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.7887, -5.3549, -5.2991, -5.0489, -4.7887, -5.0744, -5.1705, -4.7887,\n",
      "        -5.5967,  3.1362], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3098, -5.4132, -5.5440, -5.3098, -5.3098, -5.4132, -5.3098, -5.3098,\n",
      "        -5.7625, 10.0000], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.821965217590332\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.4794, -5.0635, -4.8184, -5.2042], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.0635, -5.4262, -5.1416, -5.6778, -5.3187, -5.0797, -5.6235, -4.5198,\n",
      "        -2.4307, -5.3187], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0679, -5.5600, -5.4192, -5.7694, -5.5600, -5.4192, -5.7694, -5.0679,\n",
      "        -3.1795, -5.5600], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12176080048084259\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.5227, -5.1207, -4.8133, -5.2371], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.2371, -2.4187, -4.8183, -5.3388, -5.3388, -4.8027, -5.7254, -5.1094,\n",
      "        -3.0037, -5.3388], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3320, -0.2701, -5.0435, -5.5415, -5.5415, -4.8439, -5.8116, -5.3822,\n",
      "        -3.1769, -5.5415], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4912969470024109\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.5587, -5.1867, -4.7746, -5.2609], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.3066, -5.3506, -4.8045, -5.2609, -4.7746, -3.0088, -5.2609, -5.1867,\n",
      "        -4.7641, -3.4650], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2835, -5.4911, -5.2972, -5.2972, -5.2972, -3.0820, -5.2972, -4.9846,\n",
      "        -4.4262, -3.5137], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0701422318816185\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.5729, -5.2076, -4.7183, -5.2788], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.3762, -5.6837, -5.5729, -4.1346, -4.3781, -4.9343, -4.7183, -4.7183,\n",
      "        -4.3781, -2.5954], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4409, -5.7640, -5.4409, -4.9403, -4.9403, -5.2464, -5.2464, -5.2464,\n",
      "        -4.9403, -3.0045], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.21321401000022888\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-4.6474, -5.7190, -5.4289, -5.7610, -4.6474, -5.7610, -4.8083, -5.7190,\n",
      "        -4.6474, -4.8863], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1827, -5.7052, -5.3977, -5.7052, -5.1827, -5.7052, -4.7148, -5.7052,\n",
      "        -5.1827, -5.1827], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09636817127466202\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.5640, -5.2399, -4.5990, -5.3165], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.5990, -5.4195, -4.8519, -5.4879, -5.8475, -4.5990, -5.4879, -4.5990,\n",
      "        -4.5990, -4.5990], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1391, -5.1445, -5.1391, -5.3838, -5.6706, -5.1391, -5.3838, -5.1391,\n",
      "        -5.1391, -5.1391], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1669597178697586\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.5501, -5.2402, -4.5818, -5.3393], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.8384, -4.8585, -4.5818, -4.1136, -4.3568, -4.1136, -5.5317, -5.8006,\n",
      "         1.6786, -5.7610], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6738, -4.6820, -5.1236, -3.3247, -4.8232, -3.3247, -5.3840, -5.6738,\n",
      "         2.3175, -5.5630], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.22995030879974365\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.4705, -5.2296, -4.6127, -5.3627], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.6127, -4.6127, -4.2640, -4.2802, -5.2296, -5.3627, -5.5679, -4.6127,\n",
      "        -4.6127, -4.8932], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1514, -5.1514, -4.8376, -3.3594, -4.8376, -5.1514, -5.4245, -5.1514,\n",
      "        -5.1514, -5.1514], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2623195946216583\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-5.3721, -5.8832, -5.3841, -4.6725, -5.1497, -4.6725, -5.0339, -0.5088,\n",
      "        -5.8832, -2.8868], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2053, -5.8061, -5.5008, -5.2053, -4.9021, -5.2053, -5.6015, -0.4689,\n",
      "        -5.8061, -2.6559], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10594264417886734\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3427, -5.0603, -4.7243, -5.3761], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.7408, -2.8194, -2.8194, -3.1932, -4.9628, -4.6632, -4.7243, -4.7243,\n",
      "        -4.7408, -5.9146], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8414, -2.6039, -2.6039, -4.6396, -5.2518, -4.6396, -5.2518, -5.2518,\n",
      "        -4.8414, -5.8001], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2859117388725281\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3022, -4.9777, -4.7214, -5.4035], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.9656, -4.6799, -5.4035, -5.6292, -4.7214, -4.9711, -5.4593, -4.7214,\n",
      "        -4.0332, -5.9656], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.7383, -4.8590, -5.2493, -5.7383, -5.2493, -5.2532, -5.5794, -5.2493,\n",
      "        -4.6298, -5.7383], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1178307980298996\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.2766, -4.9452, -4.7132, -5.4188], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.7132, -5.9851, -5.7143, -5.4188, -5.4350, -5.9851, -5.7143, -4.7132,\n",
      "        -4.7132, -4.0188], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2419, -5.7160, -5.5848, -5.2419, -5.5848, -5.7160, -5.5848, -5.2419,\n",
      "        -5.2419, -4.6169], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1428331881761551\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.2598, -4.9456, -4.7311, -5.4169], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.9456, -5.4084, -5.2203, -4.9368, -2.6034, -5.1124, -2.6034, -4.9592,\n",
      "        -5.4169, -4.7311], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0465, -5.6983, -5.1547, -5.1547, -2.5455, -5.2580, -2.5455, -5.2662,\n",
      "        -5.2580, -5.2580], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.057098835706710815\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-0.6384, -5.1976, -4.7088, -5.9846, -5.7343, -5.7343, -5.2256, -4.7195,\n",
      "        -0.6384, -3.5913], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6492, -5.1552, -4.9027, -5.7527, -5.5974, -5.5974, -5.3821, -5.2476,\n",
      "        -0.6492, -3.7915], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.047430120408535004\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.2358, -5.0133, -4.7100, -5.4201], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.7100, -4.7100, -5.7302, -3.4063, -5.9330, -5.0133, -3.1477, -5.0977,\n",
      "        -3.9953, -5.4200], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2390, -5.2390, -5.5880, -3.8329, -5.8023, -5.0627, -2.5073, -5.2390,\n",
      "        -4.5958, -5.2390], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16047875583171844\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.2405, -5.0649, -4.6911, -5.4120], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.6911, -5.7296, -4.1455, -5.4120, -5.1364, -4.6911, -5.9695, -2.5294,\n",
      "        -5.3999, -4.6911], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2220, -5.5761, -3.8443, -5.2220, -5.2655, -5.2220, -5.8640, -2.5246,\n",
      "        -5.2655, -5.2220], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10418196767568588\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.2493, -5.1069, -4.7015, -5.4016], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.9555, -5.9555, -3.7638, -4.7015, -3.1952, -5.1069, -3.1952, -4.7015,\n",
      "        -3.4191, -4.9602], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.8872, -5.8872, -3.8756, -5.2314, -2.5362, -5.0838, -2.5362, -5.2314,\n",
      "        -3.8756, -4.9322], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16614416241645813\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.2731, -5.1466, -4.6780, -5.4045], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.7228, -5.7228, -5.5063, -4.6780, -4.6780, -4.6780, -5.9567, -5.1466,\n",
      "        -5.4146, -3.1641], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5647, -5.5647, -5.5959, -5.2102, -5.2102, -5.2102, -5.8665, -5.0755,\n",
      "        -5.3732, -2.6155], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1223544329404831\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3043, -5.1800, -4.6669, -5.4099], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.6583, -2.8855, -2.6328, -4.6669, -4.6669, -5.4099, -4.6669, -5.9560,\n",
      "        -5.3955, -4.6669], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.8559, -2.7090, -2.7090, -5.2002, -5.2002, -5.2002, -5.2002, -5.8559,\n",
      "        -5.5498, -5.2002], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1291472166776657\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-5.3482, -4.6789, -5.4196, -5.6091, -5.7194, -5.3482, -5.4196, -5.2487,\n",
      "        -5.1661, -4.5461], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2847, -5.2110, -5.2110, -5.5535, -5.5535, -5.2847, -5.2110, -5.2110,\n",
      "        -5.3799, -5.0915], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07534606754779816\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.4095, -5.2376, -4.7104, -5.4123], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.5984, -4.7104, -4.5162, -5.9336, -5.0865, -5.7033, -4.5881, -5.7033,\n",
      "        -5.7033,  3.8958], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6840, -5.2393, -5.1293, -5.8875, -5.2393, -5.5779, -5.1293, -5.5779,\n",
      "        -5.5779, 10.0000], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.8290321826934814\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.4462, -5.2516, -4.7148, -5.3794], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.9059, -5.1525, -5.4462, -5.4462, -0.7325, -5.6734, -5.5143, -3.0976,\n",
      "        -5.3794, -4.7148], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.9004, -5.3314, -5.5896, -5.5896, -0.7770, -5.5896, -5.4094, -2.8934,\n",
      "        -5.2433, -5.2433], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04327261820435524\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.5186, -5.2698, -4.7236, -5.3598], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.5161, -5.7243, -5.0941, -3.7768, -3.0429, -2.8479, -5.2279, -3.8737,\n",
      "        -5.0778, -5.2698], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4128, -5.7077, -5.2512, -3.7656, -2.9242, -2.9242, -5.3327, -3.7656,\n",
      "        -4.9871, -5.1496], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.010102884843945503\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.5761, -5.2592, -4.7367, -5.3355], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.4712, -5.3355, -4.7367, -2.8931, -5.8679, -5.6007, -4.7367, -4.7367,\n",
      "        -4.7367, -5.6007], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5947, -5.2630, -5.2630, -2.9531, -5.9241, -5.5947, -5.2630, -5.2630,\n",
      "        -5.2630, -5.5947], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11354263126850128\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.6424, -5.2583, -4.7687, -5.3221], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.7687, -5.6424,  0.3739, -5.8449, -5.8455, -4.6651, -4.7687, -4.7687,\n",
      "        -5.8449, -5.8624], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2918, -5.6138,  2.8471, -5.9588, -5.9588, -5.1985, -5.2918, -5.2918,\n",
      "        -5.9588, -5.7589], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.7272981405258179\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.6718, -5.2406, -4.7754, -5.2755], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.7754, -5.5007, -3.0722, -3.5232, -2.9659, -5.3025, -4.0520, -4.7754,\n",
      "        -4.7754, -3.7150], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2979, -5.4617, -2.9729, -4.6494, -2.9729, -5.4617, -3.7418, -5.2979,\n",
      "        -5.2979, -3.7418], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.22210459411144257\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.6926, -5.2270, -4.7750, -5.2605], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.7750, -5.4676, -4.6615, -4.7750, -5.4139, -4.6615, -4.7750, -4.7750,\n",
      "        -4.7750, -4.7750], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2975, -5.5907, -5.1953, -5.2975, -5.4456, -5.1953, -5.2975, -5.2975,\n",
      "        -5.2975, -5.2975], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.22241654992103577\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.7315, -5.2310, -4.8032, -5.2704], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.0032, -4.8032, -5.2704,  2.1072, -5.2704, -5.4465, -4.8032, -4.8134,\n",
      "        -4.6703, -4.6703], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6028, -5.3229, -5.3229,  2.9839, -5.3229, -5.5913, -5.3229, -5.0682,\n",
      "        -5.2033, -5.2033], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2327822893857956\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.7776, -5.2421, -4.8397, -5.2973], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.0388, -5.7623, -6.0191, -4.8397, -4.8397, -5.4055, -5.4411, -4.8397,\n",
      "        -5.4411, -2.2199], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.9979, -5.9769, -5.9769, -5.3557, -5.3557, -5.5986, -5.5986, -5.3557,\n",
      "        -5.5986, -0.2418], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4848237931728363\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.8234, -5.2534, -4.8635, -5.3337], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.4736, -4.6835, -4.8635, -4.8635, -5.2534, -5.3057, -5.1599, -5.7615,\n",
      "        -4.9475, -0.3996], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3502, -5.2151, -5.3771, -5.3771, -5.2151, -5.3502, -5.3771, -5.9667,\n",
      "        -4.5729, -0.2395], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10843183100223541\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.8537, -5.2432, -4.8853, -5.3733], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-6.0126, -4.8853, -6.0126, -5.0781, -5.8537, -5.0362, -2.0974, -4.0900,\n",
      "        -5.8483, -5.7754], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.9517, -5.3968, -5.9517, -5.3968, -5.5703, -5.0482, -0.2438, -5.0482,\n",
      "        -5.9517, -5.9517], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.48466676473617554\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.8689, -5.2093, -4.8625, -5.3752], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.4408, -4.8625, -6.0388, -2.7685, -5.7715, -4.9006, -5.2889, -4.8625,\n",
      "        -4.8625, -5.7715], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5507, -5.3763, -5.9156, -2.7660, -5.9156, -5.2106, -5.4220, -5.3763,\n",
      "        -5.3763, -5.9156], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09744741767644882\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-5.2687, -4.8419, -5.4044, -5.8181, -4.8419, -5.4044, -4.8419, -2.6637,\n",
      "        -4.8419, -5.2385], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3254, -5.3578, -5.3578, -5.7705, -5.3578, -5.3578, -5.3578, -2.6694,\n",
      "        -5.3578, -5.3254], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10816220194101334\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.0493, -5.2508, -5.0204, -5.4948], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.8145, -1.2570, -5.4388, -2.5762, -5.4948, -4.6130, -5.8145, -0.4066,\n",
      "        -4.6130, -4.8406], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.8685, -0.4064, -5.3565, -2.5831, -5.5183, -5.1517, -5.8685, -0.4064,\n",
      "        -5.1517, -5.3565], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15833435952663422\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.9139, -5.1636, -4.8510, -5.4815], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.5294, -4.8407, -5.5294, -5.3872, -5.2230, -4.8510, -5.5294, -1.6823,\n",
      "        -5.5258, -4.8242], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5176, -5.1480, -5.5176, -5.7595, -5.3126, -5.3659, -5.5176, -0.4693,\n",
      "        -5.4632, -4.4786], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.21013322472572327\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.9153, -5.1616, -4.8607, -5.5141], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.3083, -4.8607, -5.5141, -4.8607, -5.8738, -5.0186, -5.0186, -4.8690,\n",
      "        -2.6400, -2.6400], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3047, -5.3746, -5.3746, -5.3746, -5.8495, -5.3746, -5.3746, -4.8212,\n",
      "        -2.3987, -2.3987], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0920560210943222\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.8992, -5.1322, -4.8673, -5.5283], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.6028, -5.9062, -4.8673, -5.5283, -4.8673, -3.2313, -2.3327, -5.9062,\n",
      "        -4.8673, -4.8673], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5525, -5.8389, -5.3806, -5.3806, -5.3806, -2.3130, -2.3130, -5.8389,\n",
      "        -5.3806, -5.3806], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.19307063519954681\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.8941, -5.1169, -4.8726, -5.5536], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.1579, -5.5066, -4.8726, -5.1169, -5.6538, -4.8726, -5.0859, -5.5536,\n",
      "        -4.8726, -4.2676], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3153, -5.3864, -5.3854, -5.1158, -5.5773, -5.3854, -5.3854, -5.3854,\n",
      "        -5.3854, -4.8063], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12420197576284409\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.9017, -5.1108, -4.8696, -5.5640], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.8210, -5.1587, -4.8696, -5.3166, -5.9663, -5.4721, -5.9663, -6.0457,\n",
      "        -4.8011, -5.7055], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0877, -5.3145, -5.3826, -5.4863, -5.8469, -5.6084, -5.8469, -5.8469,\n",
      "        -4.7670, -5.6084], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04846574738621712\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-4.8568, -4.8568, -5.1332, -4.7900, -4.8568, -4.8568, -5.5733, -4.7490,\n",
      "        -5.5733, -5.9871], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3711, -5.3711, -5.3711, -4.7464, -5.3711, -5.3711, -5.3711, -4.4853,\n",
      "        -5.3711, -5.8948], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12763521075248718\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.1489, -5.3543, -5.1871, -5.7761], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.7761, -4.8032, -4.8750, -4.8750, -5.9671, -4.8750, -5.5693, -5.9960,\n",
      "        -4.8750, -4.8750], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6684, -4.7564, -5.3875, -5.3875, -5.9310, -5.3875, -5.3875, -5.9310,\n",
      "        -5.3875, -5.3875], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1365690976381302\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.9056, -5.1930, -4.9220, -5.5698], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.8281, -5.4186, -4.9220, -5.4071, -6.1781, -6.0670, -5.8004, -5.9764,\n",
      "        -5.8004, -4.9220], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9799, -5.5301, -5.4298, -5.4727, -5.9730, -5.9730, -5.7335, -5.7767,\n",
      "        -5.7335, -5.4298], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06553100794553757\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.8864, -5.2529, -4.9728, -5.5700], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.9438, -5.9508, -5.8864, -5.3132, -1.2456, -5.8095, -4.5868, -4.9728,\n",
      "        -4.9728, -6.0030], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.8205, -6.0467, -5.7922, -5.3975, -1.0401, -5.7922, -5.1281, -5.4756,\n",
      "        -5.4756, -6.0467], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08832764625549316\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.8557, -5.3055, -5.0309, -5.5735], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.3055, -5.3639, -5.0309, -5.6765, -6.0078, -5.3729, -5.9030, -5.0309,\n",
      "        -5.8241, -4.6226], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1603, -5.4434, -5.5278, -5.8754, -6.1088, -5.4434, -5.8754, -5.5278,\n",
      "        -5.8603, -5.1603], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08672048151493073\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.8283, -5.3605, -5.0837, -5.5822], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.9637, -5.5822, -5.1847, -5.7584, -5.0837, -5.0837, -5.8453, -5.7584,\n",
      "        -1.5881, -5.8453], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9009, -5.5753, -5.5753, -5.9235, -5.5753, -5.5753, -5.9235, -5.9235,\n",
      "        -1.1382, -5.9235], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09092439711093903\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.7849, -5.4012, -5.1139, -5.5978], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.1139, -5.1750, -5.8729, -5.1139, -5.1139, -1.8047, -5.5978, -6.0803,\n",
      "        -6.0803, -5.5978], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6025, -5.2303, -5.9632, -5.6025, -5.6025, -1.1947, -5.6025, -6.2449,\n",
      "        -6.2449, -5.6025], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11538244783878326\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.7611, -5.4493, -5.1484, -5.6447], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.6447, -5.5552, -5.4209, -5.9281, -4.0482, -6.1590, -6.1590, -5.9281,\n",
      "        -5.0134, -5.6447], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6336, -5.6336, -5.2218, -5.9997, -4.1428, -6.3012, -6.3012, -5.9997,\n",
      "        -4.9695, -5.6336], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.010756335221230984\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.7368, -5.4814, -5.1688, -5.6949], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.9932, -5.6949, -6.2457, -5.0362, -4.2971, -1.0583, -5.1688, -5.0362,\n",
      "        -6.2457, -5.1688], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.0261, -5.6519, -6.3432, -4.9950, -4.1615, -1.3692, -5.6519, -4.9950,\n",
      "        -6.3432, -5.6519], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06072145700454712\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-5.6182, -5.7479, -2.2250, -2.2250, -5.8255, -4.2675, -6.0534, -5.7195,\n",
      "        -5.9645, -5.2002], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6802, -5.6802, -2.0956, -2.0956, -6.0563, -5.0275, -6.0563, -6.0563,\n",
      "        -6.3681, -5.6802], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11796067655086517\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.7769, -5.5402, -5.1950, -5.8135], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.7737, -2.1999, -6.1599, -5.8135, -5.8135, -2.1999, -6.4198, -4.7737,\n",
      "        -6.1033, -5.7975], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2964, -2.1152, -6.3869, -5.6755, -5.6755, -2.1152, -6.3869, -5.2964,\n",
      "        -6.0280, -5.7326], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06612776964902878\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.8277, -5.5676, -5.1964, -5.8689], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.8277, -5.8689, -5.1964, -6.1469, -1.1760, -5.7414, -5.5676, -5.1964,\n",
      "        -6.4897, -5.8689], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.0091, -5.6768, -5.6768, -6.0091, -1.5819, -5.9832, -5.2951, -5.6768,\n",
      "        -6.3826, -5.6768], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08960594981908798\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.9010, -5.5771, -5.2242, -5.8997], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.2242, -6.1630, -6.0051, -6.5325, -5.2242, -5.6383, -6.1527, -5.2242,\n",
      "        -5.2242, -5.2242], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.7018, -6.0135, -6.0135, -6.4045, -5.7018, -5.5540, -6.4045, -5.7018,\n",
      "        -5.7018, -5.7018], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12497502565383911\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.9981, -5.5927, -5.2756, -5.9350], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.8486, -6.1821, -6.1821, -6.1821, -5.2756, -5.2756, -6.1821, -5.0107,\n",
      "        -5.2756, -6.0359], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3638, -6.0406, -6.0406, -6.0406, -5.7481, -5.7481, -6.0406, -4.9374,\n",
      "        -5.7481, -6.0406], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10204306989908218\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.0942, -5.6095, -5.3611, -5.9621], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.3611, -6.1827, -6.3540, -5.9621, -6.5648, -5.6095, -5.7876, -6.1827,\n",
      "        -5.9621, -5.6627], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.8250, -6.0964, -6.4927, -5.8250, -6.4927, -5.4294, -5.6428, -6.0964,\n",
      "        -5.8250, -5.8250], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03719251975417137\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.1705, -5.6042, -5.4554, -5.9614], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 4.2522, -2.4150, -4.3410, -6.4329, -5.4554, -5.4554, -5.9614, -2.2412,\n",
      "        -6.1737, -5.5714], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([10.0000, -2.1519, -4.9069, -6.5574, -5.9099, -5.9099, -5.9099, -2.1519,\n",
      "        -6.1775, -5.9099], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.397995710372925\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-5.8191, -1.4029, -5.9448, -6.1671, -5.5135, -5.9203, -6.2463, -1.4029,\n",
      "        -5.8191, -5.7794], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.9622, -1.7115, -5.9622, -6.2288, -5.9622, -5.7722, -6.5311, -1.7115,\n",
      "        -5.9622, -6.0144], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05951680988073349\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-5.5801, -5.5801, -5.9259, -5.9259, -5.9259, -4.8035, -6.1752, -4.9348,\n",
      "        -6.1752, -4.4599], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.9721, -5.9721, -5.9721, -5.9721, -5.9721, -5.6206, -6.1953, -5.5273,\n",
      "        -6.1953, -4.8847], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15139272809028625\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.9198, -5.0054, -5.2883, -5.6555], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.2190, -5.6723, -2.0564, -5.6723, -6.0957, -6.2190, -6.5809, -5.6723,\n",
      "        -2.3005, -5.6723], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.0723, -5.8439, -1.6722, -5.8439, -6.0723, -6.0723, -6.3354, -5.8439,\n",
      "        -2.0647, -5.8439], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.042475342750549316\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.1294, -4.1548, -4.6958, -4.9301], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.8580, -5.7951, -6.5980, -5.7951, -5.9930, -5.7951, -5.7951, -5.7951,\n",
      "        -5.7951, -6.5980], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3911, -5.7196, -6.1950, -5.7196, -5.7196, -5.7196, -5.7196, -5.7196,\n",
      "        -5.7196, -6.1950], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06517772376537323\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0136, -4.0570, -4.1658, -4.5596], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.8435, -5.9707, -5.4558, -6.1604, -6.5557, -5.8435, -4.5596, -5.9707,\n",
      "        -6.0904, -5.8435], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6926, -5.6926, -5.3789, -5.7577, -6.1616, -5.6926, -4.7434, -5.6926,\n",
      "        -5.9361, -5.6926], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06038929894566536\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0434, -4.1242, -4.1953, -4.5431], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.1953, -6.1895, -5.2689, -6.1895, -6.0187, -5.4782, -1.5502, -6.1895,\n",
      "        -5.8966, -6.7293], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.9969, -6.0076, -5.4390, -6.0076, -6.0076, -5.4390, -1.6100, -6.0076,\n",
      "        -5.7420, -6.2158], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5254267454147339\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9734, -4.1796, -4.1007, -4.4619], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-6.3881, -6.3881, -5.3727, -2.3975, -5.7527, -6.3881, -1.4917, -5.8241,\n",
      "        -5.7527, -4.7004], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.3167, -6.3167, -5.5362, -2.0749, -5.8355, -6.3167, -1.5905, -5.8355,\n",
      "        -5.8355, -5.4119], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06759421527385712\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4854, -2.3694, -2.4378, -1.2904], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-6.1023, -5.7842, -6.1023, -6.1023, -5.9794, -5.6884, -5.6884, -6.7034,\n",
      "        -6.1023, -5.8023], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.2022, -5.9001, -6.2022, -6.2022, -6.2221, -5.9001, -5.9001, -6.3815,\n",
      "        -6.2022, -5.9695], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03334617242217064\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.0382, -3.1774, -1.5063, -0.6580], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.7725, -5.7725, -5.6579, -6.0794, -3.5068, -6.0794, -6.0794, -5.6746,\n",
      "        -5.6579, -5.6579], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.9370, -5.9370, -5.9370, -6.2398, -4.5674, -6.2398, -6.2398, -5.6348,\n",
      "        -5.9370, -5.9370], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14913594722747803\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 1.8407, -1.4809,  2.6593,  4.8458], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-6.3224, -5.6542, -4.8927, -5.8039, -5.6542, -2.5537, -6.0326, -5.8039,\n",
      "        -6.0912, -5.6542], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.4169, -5.9079, -5.2138, -5.9079, -5.9079, -2.3472, -5.9079, -5.9079,\n",
      "        -6.2051, -5.9079], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.039785925298929214\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-6.5176, -5.9630, -6.3330, -6.3548], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.4670, -4.3053, -5.8650, -5.2204, -6.3198, -2.6228, -6.3548, -5.4670,\n",
      "        -6.1155, -5.6803], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.9031, -4.9739, -5.8574, -5.3125, -6.1390, -2.4554, -6.3667, -5.9031,\n",
      "        -6.1390, -5.8574], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09286164492368698\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-6.4516, -5.7494, -6.0585, -6.3976], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.0911, -5.0270, -5.9880, -6.4516, -5.9319, -6.3995, -6.3995, -4.0048,\n",
      "        -5.6741, -5.6741], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2515, -4.9404, -5.8183, -6.1745, -5.8183, -6.3355, -6.3355, -4.4285,\n",
      "        -5.8183, -5.8183], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03809962794184685\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-6.0843, -5.3687, -5.5688, -6.0065], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.1841, -2.4497, -6.4326, -5.0155, -5.9943, -5.2714, -6.4326, -2.7048,\n",
      "        -5.0697, -6.4326], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.0799, -2.6600, -6.3344, -4.9323, -5.8115, -5.5140, -6.3344, -2.6600,\n",
      "        -5.5140, -6.3344], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.038258057087659836\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-5.9189, -5.4111, -5.3447, -5.8373], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-6.4481, -5.6611, -6.4481, -6.0339, -5.6611, -6.4481, -5.8736, -4.9964,\n",
      "        -5.6611, -6.4481], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.3369, -5.8093, -6.3369, -5.8093, -5.8093, -6.3369, -5.7163, -4.9396,\n",
      "        -5.8093, -6.3369], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01937643252313137\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.4674, -4.8319, -4.7684, -5.2864], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-6.0481, -6.0481, -6.1924, -5.6722, -5.9031, -5.2194, -5.6722, -5.6722,\n",
      "        -2.5013, -6.3625], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.8256, -5.8256, -6.0782, -5.8256, -5.8256, -5.4910, -5.8256, -5.8256,\n",
      "        -2.7978, -6.1959], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03780682757496834\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-5.7694, -4.6378, -4.8640, -2.8196, -5.7093, -5.7093, -5.2270, -6.2449,\n",
      "        -5.7093, -4.8640], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.7429, -5.3291, -5.3291, -2.8346, -5.8532, -5.8532, -5.4974, -6.0975,\n",
      "        -5.8532, -5.3291], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1068347841501236\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.1342, -4.5053, -4.7464, -5.1396], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.2131, -5.2601, -5.5420, -6.3747, -6.2343, -6.3747, -6.2343, -5.7168,\n",
      "        -5.7168, -5.7168], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5453, -5.2362, -5.2362, -6.4676, -6.1674, -6.4676, -6.1674, -5.9200,\n",
      "        -5.9200, -5.9200], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.035456232726573944\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.4583, -4.0154, -3.6806, -4.2515], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.7500, -5.7500, -5.7500, -5.7500, -6.3513, -5.7500, -5.0697, -5.7500,\n",
      "        -6.2147, -4.9479], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.9529, -5.9529, -5.9529, -5.9529, -6.5025, -5.9529, -5.0895, -5.9529,\n",
      "        -6.5025, -5.2877], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04687528312206268\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.9240, -2.6632, -2.9299, -2.1319], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.9299, -5.9967, -5.8351, -1.5619, -5.8351, -2.6632, -6.1391, -0.8035,\n",
      "        -5.8351, -6.3466], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.9187, -5.9325, -5.9325, -1.7232, -5.9325, -2.9187, -6.1817,  3.4612,\n",
      "        -5.9325, -6.4769], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.8331044912338257\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8407, -3.0622, -1.4266, -0.5768], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-6.0448, -5.4531, -5.8741, -5.9913, -5.9913, -5.8149, -6.0448, -3.6950,\n",
      "        -6.0448, -5.8741], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.1073, -5.8557, -5.8637, -5.8637, -5.8637, -6.1073, -6.1073, -2.8668,\n",
      "        -6.1073, -5.8637], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09779872000217438\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 1.8261, -1.5102,  2.7140,  4.9110], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.9740, -6.1723, -5.9700, -5.9372, -5.2448, -6.0824, -6.2722, -5.8665,\n",
      "        -5.3422, -6.2764], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.8080, -6.3375, -6.0388, -5.8080, -5.3892, -6.0388, -6.3375, -5.8080,\n",
      "        -5.3892, -6.0388], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01654106378555298\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.9963, -5.8642, -6.2149, -6.0994], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.9090, -6.0994, -5.9090, -2.9229, -2.6215, -5.7248, -5.8944, -4.8215,\n",
      "        -5.8472, -5.8472], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.9870, -6.2778, -5.9870, -2.8583, -2.8583, -6.1570, -5.7690, -4.9409,\n",
      "        -5.7690, -5.7690], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03332626074552536\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-5.9992, -5.6575, -6.0067, -6.1694], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.8220, -5.8625, -6.1034, -5.8584, -0.0726, -6.1531, -6.0424, -5.2773,\n",
      "        -5.8625, -5.8220], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.7213, -5.9267, -6.2081, -5.7213,  3.3843, -5.9267, -6.2081, -5.7131,\n",
      "        -5.9267, -5.7213], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.22772216796875\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-5.5076, -5.0855, -5.2707, -5.4734], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.7686, -4.4126, -5.4734, -4.6294, -2.8303, -4.5325, -5.9113, -5.7167,\n",
      "        -5.7686, -5.9485], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6740, -4.0369, -6.0290, -5.0793, -2.7977, -4.9713, -6.1267, -5.6740,\n",
      "        -5.6740, -6.0290], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09184069186449051\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-5.3587, -5.1358, -4.9650, -5.2707], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.5978, -4.2142, -5.6403, -5.6403, -5.7023, -4.2571, -5.6779, -5.7023,\n",
      "        -5.8255, -5.8939], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6102, -3.9332, -5.6102, -5.6102, -5.7974, -3.9332, -5.6102, -5.7974,\n",
      "        -6.0242, -5.9273], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.024910910055041313\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7830, -4.5138, -4.1019, -4.3926], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.6197, -4.9361, -5.1944, -5.6498, -4.3926, -5.4529, -3.1737, -5.8471,\n",
      "        -5.6197, -5.9147], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5307, -5.3541, -5.0739, -5.7038, -4.8911, -5.8114, -2.7413, -5.8114,\n",
      "        -5.5307, -5.7038], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08177598565816879\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.9794, -3.5277, -3.0525, -3.5857], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.0802, -0.2399, -5.0476, -5.7213, -5.3936, -5.6136, -5.1092, -4.4763,\n",
      "        -5.5603, -3.5277], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7472, -0.2721, -5.0321, -5.8542, -5.7359, -5.6646, -5.4226, -4.5601,\n",
      "        -5.4898, -3.7472], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04080096259713173\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.7776, -2.4813, -2.5291, -1.9375], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.9149, -5.1988, -4.3096, -5.5160, -5.2424, -5.5933, -5.1988, -2.5291,\n",
      "        -1.9375, -5.1988], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9813, -5.4368, -4.4431, -5.4368, -5.3300, -5.6132, -5.4368, -2.7438,\n",
      "        -0.1370, -5.4368], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.34943413734436035\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.6832, -2.5006,  0.0084,  1.0459], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.3374, -5.1239, -5.6852, -5.1239, -5.1239, -3.7324, -4.8438, -2.4397,\n",
      "        -4.8438, -5.1239], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5387, -5.3650, -5.7243, -5.3650, -5.3650, -3.5982, -4.9738, -2.6709,\n",
      "        -4.9738, -5.3650], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03797326982021332\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 1.7485, -1.5342,  2.6709,  4.6137], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.1100, -4.0827, -5.6677, -5.1100, -5.1100,  4.6137, -5.1100, -5.3108,\n",
      "        -3.6731, -4.0827], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2690, -4.4471, -5.6224, -5.2690, -5.2690, 10.0000, -5.2690, -5.4340,\n",
      "        -3.5717, -4.4471], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.9406042098999023\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.6574, -4.9854, -5.3362, -5.6569], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.6569, -5.2896, -5.1408, -5.1571, -5.6569, -4.5600, -5.0864, -5.7250,\n",
      "        -5.1571, -5.6569], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4868, -5.2929, -5.1400, -5.1400, -5.4868, -4.9198, -4.9198, -5.2929,\n",
      "        -5.1400, -5.4868], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04312463104724884\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-5.4445, -4.6138, -5.0278, -5.5391], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.0097, -4.5237, -5.6543, -5.6217, -5.1674, -4.0344, -5.4638, -5.1474,\n",
      "        -4.5237, -5.1674], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9928, -4.8822, -5.2262, -5.4135, -5.0613, -4.2624, -5.2262, -5.0613,\n",
      "        -4.8822, -5.0613], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0622306689620018\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-5.2382, -4.5347, -4.7164, -5.1955], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.8710, -3.8869, -5.3911, -5.1095, -5.1095, -4.4329, -5.3911, -5.1095,\n",
      "        -5.1095, -5.1095], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7947, -4.3134, -5.2479, -5.0531, -5.0531, -4.6350, -5.2479, -5.0531,\n",
      "        -5.0531, -5.0531], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02854977175593376\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.7548, -4.5393, -4.1170, -4.5994], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.0326, -5.3124, -4.7571,  0.3713, -5.2371, -5.2371, -5.0468, -3.9588,\n",
      "        -5.3124, -5.5048], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0613, -5.2814, -5.0306,  0.4390, -5.0613, -5.0613, -5.0613, -4.2698,\n",
      "        -5.2814, -5.4329], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.024604499340057373\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1576, -4.1553, -3.4342, -3.7578], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.9628, -4.9628, -5.4158, -4.9628, -4.8484, -4.9628, -4.9628, -4.9628,\n",
      "        -4.9628, -3.2753], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0765, -5.0765, -5.4662, -5.0765, -5.0577, -5.0765, -5.0765, -5.0765,\n",
      "        -5.0765, -3.4412], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.016432616859674454\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.5434, -3.2781, -2.6992, -3.1654], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.2555, -4.9578, -5.3533, -2.6992, -2.6992, -5.1759,  0.4914, -4.9578,\n",
      "        -4.9578, -4.9578], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4293, -5.0612, -5.4654, -2.0964, -2.0964, -5.3381,  0.6110, -5.0612,\n",
      "        -5.0612, -5.0612], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0852997750043869\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.2437, -2.1840, -2.0941, -1.2210], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.9520, -5.1792, -4.9520, -2.0941, -5.1531, -4.8222, -4.0546, -4.6257,\n",
      "        -5.3356, -5.3356], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0214, -5.3214, -5.0214, -2.0989, -5.3214, -5.0147, -4.2454, -5.0214,\n",
      "        -5.4399, -5.4399], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.031009316444396973\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.0999, -2.3210,  0.5703,  1.8867], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.3654, -5.3654, -5.1464, -4.3631, -4.9779, -4.4516, -1.2451, -3.3989,\n",
      "        -5.3654, -5.1464], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3747, -5.3747, -5.2578, -4.6006, -4.9372, -4.5331,  0.6980, -4.0868,\n",
      "        -5.3747, -5.2578], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4338606297969818\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 1.8813, -1.6636,  2.9229,  5.0546], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.4172, -5.3651, -5.0149, -4.9861, -4.9861, -4.6471, -4.9861, -5.0121,\n",
      "        -4.9861, -5.2060], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2990, -5.2990, -4.8550, -4.8550, -4.8550, -4.8607, -4.8550, -4.9358,\n",
      "        -4.8550, -5.1824], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.016472645103931427\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.1690, -4.7419, -5.1671, -5.3455], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.9490, -5.1338, -5.3455, -5.3455, -4.9490, -3.6240, -4.9490, -5.1671,\n",
      "        -4.9490, -4.9490], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8159, -5.1488, -5.2677, -5.2677, -4.8159, -3.9086, -4.8159, -5.1488,\n",
      "        -4.8159, -4.8159], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01821747049689293\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-4.9339, -4.3296, -4.7159, -5.1805], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-3.7846, -3.4108, -4.1185, -3.3458, -4.7406, -4.8609, -4.7406, -4.8609,\n",
      "        -4.8609, -4.2298], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9230, -3.1974, -4.9867, -3.9230, -4.8966, -4.8068, -4.8966, -4.8068,\n",
      "        -4.8068, -4.5273], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1297607719898224\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-4.9418, -4.3817, -4.4270, -4.9421], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.9837, -5.3127, -4.9837, -4.2039, -3.6944, -4.3172, -4.7738, -5.1142,\n",
      "        -3.6944, -4.7738], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7835, -5.2517, -4.7835, -4.5089, -4.0400, -4.3200, -4.7835, -5.1240,\n",
      "        -4.0400, -4.7835], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04160524159669876\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.4569, -4.3173, -3.6870, -4.2645], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.3071, -4.7136, -5.3071, -5.0805, -4.3665, -4.2169, -3.8701, -0.1519,\n",
      "        -5.0805, -5.0805], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2381, -4.7676, -5.2381, -5.1038, -3.6585, -4.4868, -3.7675,  0.6614,\n",
      "        -5.1038, -5.1038], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1260111778974533\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.9313, -3.8238, -3.0237, -3.5159], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.2694, -4.1205, -3.8238, -4.9417, -4.6676, -4.1969, -4.9417, -3.8395,\n",
      "        -4.3033, -5.1136], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3241, -3.6514, -3.7214, -4.7394, -4.7394, -4.4556, -4.7394, -3.9782,\n",
      "        -4.3241, -5.0708], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04089218005537987\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4469, -3.0093, -2.3798, -3.0153], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.2764, -4.6391, -4.6391, -5.2915, -4.6391, -3.0846, -2.3798, -4.6391,\n",
      "        -4.6391, -1.6719], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3409, -4.7001, -4.7001, -5.1736, -4.7001, -3.1418, -1.7922, -4.7001,\n",
      "        -4.7001,  0.6134], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5608018636703491\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.1076, -1.8550, -1.7436, -0.9282], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.6458, -4.6458, -4.6458, -3.9391, -4.6458, -5.1073, -4.2413, -4.7117,\n",
      "        -3.6559, -3.7030], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6648, -4.6648, -4.6648, -4.3750, -4.6648, -4.9958, -4.3909, -4.6648,\n",
      "        -3.8989, -3.7135], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02875734306871891\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.2816, -2.1132,  0.4991,  1.5390], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.6578, -5.2445, -4.6578, -4.2676, -5.3631, -5.1987,  0.4991, -3.7683,\n",
      "        -5.1383, -4.2676], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6287, -5.1100, -4.6287, -4.3641, -5.1100, -5.1100,  0.3851, -3.7434,\n",
      "        -4.9583, -4.3641], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.015635669231414795\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 1.8939, -1.3708,  3.0119,  4.9652], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.9381, -4.6646, -5.2604, -5.1405, -4.6646, -1.7822, -4.9673, -4.8627,\n",
      "        -4.6646, -5.1405], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9450, -4.6146, -5.1012, -4.9450, -4.6146, -1.9004, -4.6146, -4.5367,\n",
      "        -4.6146, -4.9450], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03539290279150009\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.2347, -4.6018, -4.9142, -5.3148], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.7903, -4.6388, -3.5606, -4.6018, -5.3148, -5.3148, -4.7002, -3.6291,\n",
      "        -4.9227, -4.6388], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9808, -4.6458, -4.5633, -4.6857, -5.1416, -5.1416, -4.6458, -3.7756,\n",
      "        -4.6458, -4.6458], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12098802626132965\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-4.9542, -4.1105, -4.5182, -5.1517], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.3405, -3.1940, -4.6093, -2.3676, -5.2767, -4.0686, -4.6093, -3.7704,\n",
      "        -5.2767, -4.6093], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.1603, -3.1309, -4.6617, -1.9114, -5.1670, -4.3995, -4.6617, -3.7823,\n",
      "        -5.1670, -4.6617], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03865579143166542\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-4.7436, -4.0551, -4.0571, -4.6649], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.6490, -4.5906, -2.3456, -4.5906, -4.5906, -4.6581, -1.0397, -1.8120,\n",
      "        -4.6581, -4.5906], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6848, -4.6848, -1.9357, -4.6848, -4.6848, -4.7150,  0.0976, -1.9357,\n",
      "        -4.7150, -4.6848], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1520031988620758\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.6013, -4.2657, -3.7618, -4.4880], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.5880, -4.5880, -5.1889, -5.3810, -2.3182, -4.8489, -2.3182, -4.6692,\n",
      "         4.8530, -3.6252], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6915, -4.6915, -5.2022, -5.2022, -1.9299, -4.6915, -1.9299, -4.7124,\n",
      "        10.0000, -3.7851], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.6899030208587646\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0598, -3.6303, -3.0535, -3.7239], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.5534, -4.1340, -4.2815, -4.8739, -4.5886, -4.9600, -4.5534, -4.8116,\n",
      "        -5.1366, -4.5534], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7072, -4.5848, -4.3544, -4.7206, -4.7072, -5.0304, -4.7072, -4.7072,\n",
      "        -5.2276, -4.7072], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03412231057882309\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-1.7859, -4.8379, -3.6991, -5.0968, -1.7859, -4.1570, -4.2464, -4.5380,\n",
      "        -4.9254, -5.0968], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.9014, -4.7413, -3.7194, -5.2551, -1.9014, -4.5276, -4.4214, -4.7240,\n",
      "        -5.0439, -5.2551], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.030326392501592636\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.1436, -1.7752, -1.7868, -0.9943], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.7927, -4.5383, -5.0874, -4.5383, -1.7752, -0.9943, -4.9070, -5.0334,\n",
      "        -4.9070, -4.1777], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7304, -4.7304, -5.2580, -4.7304, -1.8949, -0.0539, -5.0448, -5.2580,\n",
      "        -5.0448, -4.4689], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1178852766752243\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.5950, -1.9885,  0.1312,  1.0024], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.2313, -4.5388, -4.8066, -5.0819, -4.1949, -4.2313, -5.0198, -5.0819,\n",
      "        -4.2313, -4.8985], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4087, -4.7301, -4.7754, -5.2397, -4.3962, -4.4087, -5.2397, -5.2397,\n",
      "        -4.4087, -5.0363], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02896951511502266\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 1.9840, -0.9579,  3.1175,  5.1553], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.9110, -4.9110, -4.5495, -4.2782, -4.2331, -4.5495, -4.8181, -4.5495,\n",
      "        -4.2331, -4.5495], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0098, -5.0098, -4.7135, -4.2777, -4.3862, -4.7135, -4.7135, -4.7135,\n",
      "        -4.3862, -4.7135], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01849982514977455\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.0434, -4.6996, -4.7322, -5.1309], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.6180, -2.0312,  5.1930, -1.6832, -3.7065, -4.0771, -0.6647, -5.1309,\n",
      "        -4.7137, -4.6180], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6694, -1.8112, 10.0000, -1.8112, -3.8243, -4.3359, -0.1947, -5.2297,\n",
      "        -4.7471, -4.6694], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.3489527702331543\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-4.8505, -4.1307, -4.4921, -5.1431], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-3.6527, -4.5626, -4.9630, -4.6751, -4.9630, -4.6751, -4.6722, -4.9300,\n",
      "        -4.9630, -4.6751], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7831, -4.6334, -4.9011, -4.6334, -4.9011, -4.6334, -4.9011, -4.6334,\n",
      "        -4.9011, -4.6334], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.017906997352838516\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-4.2713, -3.6948, -3.5772, -4.1552], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.6828, -5.0460,  0.8914, -2.7149, -5.0460, -3.6729, -5.1569, -3.6583,\n",
      "        -4.1442, -4.7101], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8614, -5.1658,  3.9019, -2.7755, -5.1658, -3.6819, -5.1658, -3.6819,\n",
      "        -4.2562, -4.6091], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.9150732755661011\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.4410, -3.6056, -3.8165, -4.3657], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.1189, -4.7057, -4.7057, -4.7057, -3.5526, -4.5335, -4.9152, -4.5335,\n",
      "        -5.1189, -4.7057], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1106, -4.5558, -4.5558, -4.5558, -3.6992, -4.5558, -4.5558, -4.5558,\n",
      "        -5.1106, -4.5558], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.024175971746444702\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.9153, -3.4024, -2.8839, -3.5401], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.0489, -3.5258, -2.6261, -5.0611, -4.6465, -4.5474,  1.1193, -4.6465,\n",
      "        -4.2761, -4.6894], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.2282, -3.6732, -2.6629, -5.0927, -4.5353, -4.6477,  4.0523, -4.5353,\n",
      "        -4.1732, -4.7562], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.870872974395752\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.0272, -2.5900, -1.7384, -2.3877], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.0395, -1.7384, -1.5118, -4.1859, -4.9734, -4.7026, -4.5310, -4.5351,\n",
      "        -4.7026, -4.7080], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9301, -1.4765, -1.4765, -4.1495, -5.0779, -4.5129, -4.6356, -4.5129,\n",
      "        -4.5129, -4.7215], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.017763441428542137\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.8715, -1.4890, -1.4340, -0.4307], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.5902, -4.4175, -4.5902, -4.6112, -4.5641, -2.5632, -4.5872, -4.0939,\n",
      "        -4.0014, -4.4175], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6954, -4.4965, -4.6954, -4.3052, -4.4965, -2.4567, -4.4965, -4.1314,\n",
      "        -4.0445, -4.4965], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.015563331544399261\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.1865, -1.7090,  0.5829,  1.6938], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.5856, -4.7780, -4.3240, -4.3240, -4.2144, -4.8814, -4.7113, -4.3240,\n",
      "        -4.4460, -4.4953], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.3676, -4.9915, -4.4833, -4.4833, -4.2989, -4.6717, -4.6208, -4.4833,\n",
      "        -4.4833, -4.6717], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.026111628860235214\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 2.2506, -0.8372,  3.4293,  5.7865], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.3182, -3.7582, -4.7414, -4.4871, -0.2769, -4.1258, -4.0429,  0.6878,\n",
      "        -1.4392, -4.7414], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.2492, -3.6157, -4.9496, -4.6003,  0.6549, -4.4523, -4.2765,  0.6549,\n",
      "        -1.2492, -4.9496], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11911233514547348\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.7752, -4.4390, -4.3276, -4.6639], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.8768, -4.4124, -4.2024, -4.2024, -3.2568, -3.8768, -4.2024, -4.7242,\n",
      "        -3.9044,  0.7633], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.0266, -4.6061, -4.4061, -4.4061, -3.5303, -4.0266, -4.4061, -4.8949,\n",
      "        -3.8669,  0.7460], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.031268347054719925\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8184, -3.9314, -4.1371, -4.4232], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.5661, -4.2610, -4.2122, -3.4932, -4.3233, -2.8084, -4.2122, -4.2610,\n",
      "        -3.1985, -3.2832], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.1175, -4.3279, -4.3279, -3.8714, -4.1699, -4.0676, -4.3279, -4.3279,\n",
      "        -3.1906, -3.4669], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.20229282975196838\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3754, -3.4128, -3.8826, -4.2712], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.3627, -4.6316, -4.6316,  5.8260, -4.4695, -4.2861, -4.6829, -3.8199,\n",
      "        -4.6316, -3.8075], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4379, -4.7940, -4.7940, 10.0000, -4.4379, -4.2193, -4.7940, -4.0715,\n",
      "        -4.7940, -3.6428], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.7615158557891846\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0792, -3.1570, -3.5796, -3.9694], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.0891, -3.4468, -4.3497, -4.3497, -4.3497, -4.4297, -4.5132, -4.2701,\n",
      "        -2.9713, -4.2897], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.9382, -3.7485, -4.1021, -4.1021, -4.1021, -4.3296, -4.3296, -3.9603,\n",
      "        -3.8413, -4.1021], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.22619494795799255\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4858, -2.9288, -2.5986, -3.0418], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.9554, -4.3390, -4.6686, -4.3390, -4.3390, -4.2509, -3.4725, -4.3390,\n",
      "        -4.3390, -3.9554], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6605, -4.0164, -4.5801, -4.0164, -4.0164, -4.0164, -3.9272, -4.0164,\n",
      "        -4.0164, -3.6605], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09639649093151093\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.5337, -2.1743, -1.3227, -1.7733], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.5875, -1.3227, -4.6149, -4.6149, -4.1527, -4.1527, -2.5466, -1.3227,\n",
      "        -4.6149, -4.0947], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5813, -0.9073, -4.5813, -4.5813, -4.0083, -4.0083, -2.1904, -0.9073,\n",
      "        -4.5813, -4.0083], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05245063826441765\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.4251, -1.0524, -1.0215,  0.1770], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.6413, -2.4611, -3.7023, -4.4623, -3.9362, -3.9362, -3.9308, -4.1293,\n",
      "        -4.5541, -3.9308], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6871, -2.0743, -3.6651, -4.2878, -4.0086, -4.0086, -4.0086, -4.2878,\n",
      "        -4.5933, -4.0086], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.023282695561647415\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.3295, -1.3323,  1.0875,  2.4399], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.7680,  0.2361,  1.0875, -4.4286, -3.7680, -0.9417, -4.0775, -3.5951,\n",
      "        -3.6536, -4.4286], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9948,  1.1959,  1.1959, -4.2882, -3.9948, -0.7875, -4.2882, -3.6567,\n",
      "        -3.8740, -4.2882], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11959201097488403\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 2.4297, -0.6794,  3.6489,  6.2973], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.3072, -4.0725, -1.0068, -3.9354, -4.0438, -3.6704, -3.4319, -3.5116,\n",
      "        -3.6434, -3.6434], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.9284, -4.2720, -0.7053, -3.9730, -3.9730, -3.8578, -3.4615, -3.6388,\n",
      "        -3.9730, -3.9730], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05501057952642441\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.3952, -3.9106, -3.9033, -4.4502], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.3629, -4.0283, -2.2931, -3.5847, -2.2931, -3.5910, -3.5847, -4.4502,\n",
      "        -3.5847, -3.5847], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.2319, -4.2319, -1.9040, -3.9239, -1.9040, -3.8127, -3.9239, -4.5130,\n",
      "        -3.9239, -3.9239], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08749966323375702\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5648, -3.5068, -3.9205, -4.3677], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 1.1156, -3.6469,  0.3659, -3.6412, -4.0542,  0.4334, -2.2252, -3.4905,\n",
      "        -3.6412, -3.5068], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.2284, -3.4169,  1.2284, -3.8509, -4.1562,  1.2284, -1.9537, -3.5166,\n",
      "        -3.8509, -3.7400], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16688981652259827\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.9791, -2.9559, -3.5355, -4.0916], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.0871, -3.0813, -3.0813, -1.1407, -3.7333,  6.3796, -3.5352, -3.1957,\n",
      "        -3.7333, -3.7333], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.0767, -3.4390, -3.4390, -0.5451, -3.7732, 10.0000, -3.4390, -3.4548,\n",
      "        -3.7732, -3.7732], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.3799092769622803\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.5476, -2.6587, -3.1233, -3.6672], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-3.8097, -3.3539, -2.6824, -3.7987, -3.1236, -4.3885, -3.4991, -4.3885,\n",
      "        -2.3701, -3.3539], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7255, -3.6009, -3.1331, -3.7255, -3.4477, -4.0185, -3.4477, -4.0185,\n",
      "        -2.0406, -3.6009], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08276525139808655\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.1601, -2.6663, -2.3083, -2.9456], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.8093, -3.8093, -3.9644, -3.8030, -4.4282, -3.8093, -3.0454, -3.8093,\n",
      "        -3.8093, -3.8030], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7271, -3.7271, -4.0229, -3.7271, -4.2696, -3.7271, -3.3770, -3.7271,\n",
      "        -3.7271, -3.7271], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01838177628815174\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2402, -1.9640, -1.1310, -1.6262], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.7953, -3.9192, -2.6300, -3.4568, -2.6300, -4.3562, -4.3562, -3.8014,\n",
      "        -4.0190, -1.1310], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7355, -4.0362, -3.0218, -3.3269, -3.0218, -4.2716, -4.2716, -4.0005,\n",
      "        -4.0362, -0.2771], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11245113611221313\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.7403, -0.7077, -0.4998,  0.8578], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.2797, -4.1066, -3.8321,  3.8139, -3.3133, -3.7369, -4.1853, -3.9801,\n",
      "         2.5311, -3.3785], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.2959, -4.0195, -3.7627,  5.0982, -3.3344, -3.7627, -4.0693, -4.0693,\n",
      "         5.0982, -3.3344], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.8276316523551941\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.6369, -1.1746,  1.2393,  2.6440], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.2603, -3.6434, -3.6434, -3.7214, -4.0675, -4.1653, -3.6434, -3.2603,\n",
      "        -3.7117, -3.6434], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3269, -3.7777, -3.7777, -3.7777, -4.0867, -4.3077, -3.7777, -3.3269,\n",
      "        -4.0867, -3.7777], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.024550404399633408\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 2.9229, -0.5621,  4.0474,  6.8387], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.9759, -3.2031, -3.5987, -3.8630, -3.6232, -3.9789, -1.1496, -3.9759,\n",
      "        -3.2031, -3.6815], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.0663, -3.2935, -3.7625, -4.0663, -3.7625, -4.0067, -0.1210, -4.0663,\n",
      "        -3.2935, -4.0663], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1327088624238968\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.8792, -3.6126, -3.7445, -4.1847], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-3.6685, -3.7445, -3.8260, -3.6685, -3.6685,  2.9525, -3.6685, -3.2773,\n",
      "        -1.7934, -3.6685], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7114, -3.9994, -3.9994, -3.7114, -3.7114,  5.1547, -3.7114, -3.5547,\n",
      "        -1.7666, -3.7114], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5031936168670654\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.9538, -3.2113, -3.9532, -4.6423], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-3.7506, -3.9449, -3.7130, -4.2879, -4.2879, -3.6733, -2.9649, -3.9449,\n",
      "        -2.7553, -3.7506], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6553, -3.9164, -3.6553, -4.2061, -4.2061, -3.6553, -3.0597, -3.9164,\n",
      "        -3.2816, -3.6553], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0322762094438076\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-3.3384, -2.7334, -3.0554, -3.5688], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-3.9225, -3.8008, -3.7535, -3.8008, -2.4173,  1.5133, -3.6646, -0.4098,\n",
      "        -3.5364, -3.1733], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.8585, -3.6220, -3.6220, -3.6220, -2.5713,  1.5310, -3.6220, -0.3020,\n",
      "        -3.8559, -3.4600], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.030708346515893936\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-2.8959, -2.8943, -2.2441, -2.9250], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.7399, -3.8200, -3.8013, -3.8681, -3.8013, -3.1424, -2.9012, -3.8013,\n",
      "        -3.9893, -3.6251], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.5550, -3.8281, -3.6111, -3.8281, -3.6111, -3.4464, -3.1052, -3.6111,\n",
      "        -3.4464, -3.6111], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12035336345434189\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.4170, -2.3892, -1.5874, -2.1995], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.2815, -1.5874, -3.1796, -2.3892, -3.1509, -3.7352, -3.7352, -3.3957,\n",
      "        -4.3879, -3.5412], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4551, -1.6835, -3.1045, -2.4287, -3.4726, -3.6308, -3.6308, -3.4726,\n",
      "        -4.2259, -3.6308], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02119947038590908\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8053, -1.7883, -0.6980, -1.2855], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 2.8682, -2.9198, -3.7656, -3.6606, -1.8053,  2.8682, -3.1373, -3.1154,\n",
      "        -3.6606, -4.3765], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 5.0331, -2.8774, -3.8590, -3.6526, -2.3460,  5.0331, -3.1092, -3.1092,\n",
      "        -3.6526, -4.2652], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.9690345525741577\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3577, -0.5172, -0.1873,  0.8455], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.6621, -3.5318, -3.5318, -1.9933, -3.6621, -3.6418,  1.8720, -3.5318,\n",
      "        -4.2936, -3.5318], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.8581, -3.6496, -3.6496, -3.2252, -3.8581, -3.8696,  1.7190, -3.6496,\n",
      "        -4.2776, -3.6496], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17255347967147827\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 1.4315, -0.8178,  1.9936,  3.1222], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.4797, -3.4797, -3.4797, -3.4797, -3.4797, -1.7414, -3.3225, -4.2843,\n",
      "        -3.2763, -2.0412], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6069, -3.6069, -3.6069, -3.6069, -3.6069, -1.5275, -3.6069, -4.2452,\n",
      "        -3.0425, -3.1595], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1514388918876648\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([ 6.5011, -4.3189, -2.5968, -2.3662, -3.4999, -2.8091,  6.5011, -3.4999,\n",
      "        -2.7716, -2.9834], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([10.0000, -4.1718, -2.6954, -2.6954, -3.5281, -2.9708, 10.0000, -3.5281,\n",
      "        -2.6954, -3.3371], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.478339672088623\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 1.6323, -0.6892,  2.1941,  3.3435], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.2947, -3.4782, -3.7226, -2.3247,  2.1941,  0.8959, -3.7371, -2.6758,\n",
      "        -2.6758, -3.4782], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.1008, -3.4538, -3.6716, -2.9552,  2.0092,  2.0092, -3.6716, -2.6248,\n",
      "        -2.6248, -3.4538], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1721983104944229\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 3.7518, -0.2013,  4.6112,  6.7041], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.2005, -2.8635, -4.3096, -3.6942, -3.6942, -4.2540, -4.2540, -3.3297,\n",
      "        -4.2540, -3.7127], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.4056, -3.1732, -4.0254, -3.5991, -3.5991, -4.0254, -4.0254, -3.3789,\n",
      "        -4.0254, -3.5991], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04089804366230965\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.8614, -3.3323, -3.5922, -4.1321], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-3.8420, -3.4052, -3.4052, -3.4052, -3.7001,  3.5859, -4.1321, -3.2878,\n",
      "        -3.1749, -4.1321], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5782, -3.3468, -3.3468, -3.3468, -3.5782,  5.1173, -3.9991, -3.3468,\n",
      "        -3.1909, -3.9991], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.24789564311504364\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.6630, -2.8197, -3.3206, -4.1035], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-3.3768, -2.5803, -3.3768, -3.5885, -3.5885, -3.9554, -3.8274, -3.3138,\n",
      "        -3.1011, -3.4550], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3222, -2.7561, -3.3222, -3.5809, -3.5809, -3.9857, -3.9857, -3.3222,\n",
      "        -3.3222, -3.5809], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.012779409065842628\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-3.0486, -2.3054, -2.4593, -3.0737], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 3.8977, -0.4175, -2.2510, -0.1719, -3.1181,  0.2883,  1.3486, -1.4596,\n",
      "        -1.4959, -3.5371], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 5.2433,  0.2137, -2.4344,  0.2137, -3.1379,  0.2137,  2.5079, -2.4572,\n",
      "        -1.3758, -3.5763], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4752611219882965\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-2.4581, -2.4606, -1.4387, -2.2051], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.1572, -2.4942, -3.4818, -3.4818, -3.5369, -3.6606, -3.6606, -0.3433,\n",
      "        -3.5775, -2.4606], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2448, -2.6842, -3.5551, -3.5551, -3.5551, -3.9159, -3.9159,  0.3235,\n",
      "        -3.5551, -2.2948], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06576857715845108\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.7409, -2.1608, -1.5339, -2.5580], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.5210, -3.0852, -3.5473, -3.4665, -3.5779, -3.4665, -2.8993, -2.7087,\n",
      "        -3.4665, -3.5779], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.6364, -3.2004, -3.4379, -3.5279, -3.8285, -3.5279, -3.2004, -2.9833,\n",
      "        -3.5279, -3.8285], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.034153420478105545\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.5603, -1.3492, -0.1543, -0.8426], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.1570, -3.0220, -1.3910, -3.5537, -3.0220, -3.5537, -3.1570, -2.1034,\n",
      "         1.5703, -2.5006], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3941, -3.1453, -2.3479, -3.7608, -3.1453, -3.7608, -3.3941, -2.3039,\n",
      "         2.7969, -2.5934], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.26977142691612244\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.8207,  0.0602,  0.5832,  1.6658], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.5712, -3.5712, -3.5712, -3.0907, -2.9865, -3.5048, -2.0651, -1.5101,\n",
      "        -2.9865, -2.9865], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7291, -3.7291, -3.7291, -3.0918, -3.0918, -3.4489, -2.2928, -2.2928,\n",
      "        -3.0918, -3.0918], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07757335901260376\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 2.3320, -0.2970,  2.9681,  4.2863], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.9442, -2.9442, -2.9442, -3.8786, -2.9442, -2.9442, -3.6133, -1.6082,\n",
      "        -2.9442, -3.1132], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.0475, -3.0475, -3.0475, -3.6821, -3.0475, -3.0475, -3.6821, -2.1914,\n",
      "        -3.0475, -3.0475], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04518143832683563\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.0883, 0.0208, 4.9919, 7.0223], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 4.3103, -2.3479, -2.9406, -3.1565, -2.9406, -2.9406,  0.0208, -3.5778,\n",
      "        -1.3021,  4.3103], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 5.3201, -2.3178, -3.0046, -3.0046, -3.0046, -3.0046,  2.8792, -3.3768,\n",
      "        -0.9956,  5.3201], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.038059115409851\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-2.9572,  1.8379, -2.9572, -3.6983,  0.2615, -3.5897, -1.5700, -3.0077,\n",
      "        -2.9572, -2.9521], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.9417,  2.8484, -2.9417, -3.6569,  0.6541, -3.3355, -2.1190, -2.9417,\n",
      "        -2.9417, -3.3355], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16951541602611542\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.4702, -2.3487, -2.6967, -3.6839], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.9951, -3.2437, -2.8571, -3.2437, -2.9951, -3.0764, -3.2437, -1.8010,\n",
      "        -3.7902, -2.0117], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.8865, -2.8865, -3.1138, -2.8865, -2.8865, -2.8865, -2.8865, -2.4030,\n",
      "        -3.6849, -2.0595], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08841215819120407\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-2.9553, -1.8072, -2.0201, -2.8525], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-3.6069, -2.4706, -2.9191, -2.9191, -2.9191, -2.9191, -1.7587, -2.9191,\n",
      "        -3.5505, -3.6069], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2812, -2.3700, -2.8808, -2.8808, -2.8808, -2.8808, -1.8744, -2.8808,\n",
      "        -3.2812, -3.2812], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03154579550027847\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-2.7569, -2.3078, -1.4795, -2.7042], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.1022, -2.7904, -3.0704, -1.5498, -2.7904, -3.4893, -3.7852, -2.4135,\n",
      "        -3.7104, -0.8636], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.3948, -2.8920, -2.8920, -2.0438, -2.8920, -3.3136, -3.6287, -2.3948,\n",
      "        -3.6287, -0.9502], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.045206181704998016\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.3861, -1.7484, -0.7800, -2.1347], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.4184, -2.6989, -2.1175, -3.4184,  4.1694, -2.6989,  2.1642, -3.4184,\n",
      "        -2.6989, -2.6989], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3529, -2.9058, -2.4311, -3.3529,  5.1208, -2.9058,  2.7525, -3.3529,\n",
      "        -2.9058, -2.9058], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1533544957637787\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.5204, -1.0834,  0.1123, -0.8171], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.7240,  0.1123, -2.3566, -1.4331, -2.6768,  1.1885, -2.3177, -2.8932,\n",
      "        -2.0531, -3.3392], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6039,  1.0493, -2.4461, -1.6495, -2.9111,  1.0493, -2.2898, -3.3736,\n",
      "        -2.3430, -3.3736], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13383381068706512\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3279,  0.6263,  1.2097,  2.3128], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.0479, -3.7447, -2.6952, -1.6026, -2.8002, -3.2832, -3.7447, -2.6952,\n",
      "        -3.2832, -3.6446], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.8669, -3.6612, -2.8980, -2.0099, -2.7373, -3.3607, -3.6612, -2.8980,\n",
      "        -3.3607, -3.6612], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03111320175230503\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.3637, 0.2375, 3.0188, 4.1384], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.7509,  0.2375, -2.3450, -2.8560, -2.7509, -1.9794, -2.7509, -2.2333,\n",
      "        -3.7678, -3.7678], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.8799,  1.1045, -2.3643, -2.8799, -2.8799, -2.2996, -2.8799, -2.3643,\n",
      "        -3.7379, -3.7379], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09240464121103287\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.0219, 0.8357, 4.9080, 6.6262], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.8427, -1.8251, -2.8175, -2.1862, -2.8427, -2.4516, -0.7347, -3.8169,\n",
      "        -2.8935, -2.6914], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.8539, -1.9387, -2.8539, -2.7491, -2.8539, -2.7491, -0.8632, -3.8330,\n",
      "        -2.8539, -2.4219], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0510893277823925\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.7190, -3.1723, -3.2786, -3.8836], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 2.9628, -3.2786, -3.2786,  2.2382, -2.9549, -3.8836, -2.2665, -2.9466,\n",
      "        -2.9466, -2.9549], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 2.6646, -3.2582, -3.2582,  2.6646, -2.8136, -3.8551, -2.5018, -2.8136,\n",
      "        -2.8136, -2.8136], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04031096026301384\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.1076, -2.2576, -2.6062, -3.3858], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.9752, -3.0120, -3.9172, -3.0120, -2.9507, -3.0120,  1.0697, -0.8489,\n",
      "         0.1165, -0.3381], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 2.6373, -2.7873, -3.8455, -2.7873, -2.7873, -2.7873,  1.0033, -0.8952,\n",
      "         1.0033, -0.8952], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4049113392829895\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-2.7984, -1.8282, -2.1994, -2.8693], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.8209, -2.9378, -3.0039,  0.4657, -1.8117, -3.2599,  0.1777, -3.2831,\n",
      "         1.0827, -3.0039], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.8401, -2.7721, -2.7721,  0.9207, -2.2216, -3.2225,  0.9207, -3.2225,\n",
      "         2.5626, -2.7721], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.32578644156455994\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-2.7105, -2.2277, -1.7278, -2.7999], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.9275, -2.9614, -2.9614, -2.9614, -3.2829, -2.2277,  1.9931, -4.0062,\n",
      "         1.9931, -2.1729], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.7938, -2.7660, -2.7660, -2.7660, -3.2301, -2.5550,  2.4506, -3.8798,\n",
      "         2.4506, -2.2267], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06796447932720184\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2842, -1.5394, -0.8066, -2.1994], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.2777, -1.8616,  0.3658, -3.2777, -4.0210, -3.1495, -1.9707,  0.3658,\n",
      "        -3.2777, -4.0210], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2579, -2.5136,  0.7363, -3.2579, -3.9275, -3.2579, -2.4349,  0.7363,\n",
      "        -3.2579, -3.9275], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09455665946006775\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.1631, -0.6720,  0.5041, -0.4771], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.5489, -4.0207, -2.7717, -3.3260, -1.5688, -2.7717, -2.0111, -2.9413,\n",
      "        -0.7403, -3.3256], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9931, -3.9931, -2.8100, -3.0978, -1.6663, -2.8100, -2.4697, -3.0978,\n",
      "        -0.5463, -3.3027], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0535600371658802\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.6608,  0.9370,  0.8839,  1.7796], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 1.7796, -2.8279, -2.6102, -2.0588, -3.4385,  0.6529, -3.9919, -2.6884,\n",
      "        -2.6884, -2.2934], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 2.1577, -2.8529, -2.7275, -2.4862, -3.8916,  0.6017, -3.8916, -2.8529,\n",
      "        -2.8529, -2.3238], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06129832938313484\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.9671, 0.6738, 2.7218, 3.4110], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.7069, -2.6371, -2.6371, -3.2890, -3.9512, -1.4644, -3.1642, -3.3829,\n",
      "        -3.9512, -1.7058], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3314, -2.9017, -2.9017, -3.3888, -3.7762, -1.5197, -3.3888, -3.1542,\n",
      "        -3.7762, -1.8698], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.048491064459085464\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.8270, 1.7069, 4.8796, 6.1182], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.6473, -3.9128, -3.9128, -0.5473, -3.9128, -0.5473, -2.6511, -3.0156,\n",
      "         1.6804, -2.6511], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.1899, -3.7141, -3.7141, -0.2769, -3.7141, -0.2769, -2.9270, -3.3951,\n",
      "         1.9865, -2.9270], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0863940492272377\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.6909, -3.3206, -3.0396, -3.8820], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.9438, -3.2744, -3.8820, -2.4032, -3.5429, -3.3140, -2.7311, -2.1494,\n",
      "        -2.7311, -3.8820], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.9345, -3.3744, -3.7357, -2.8317, -3.7357, -3.3744, -2.9345, -2.4394,\n",
      "        -2.9345, -3.7357], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04440874606370926\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.6215, -2.6066, -2.7611, -3.3197], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.8477, -3.0167, -3.3197,  0.8074, -2.8477, -1.4128, -2.2117, -2.8477,\n",
      "        -2.7611, -3.3197], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.9488, -2.9488, -3.3459,  0.4516, -2.9488, -1.5092, -2.2304, -2.9488,\n",
      "        -2.9488, -3.3459], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.020810505375266075\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.1041, -1.8964, -2.3476, -3.2662], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.1730, -3.8168, -2.4863, -2.9750,  1.5913, -3.6355, -3.8168, -2.6822,\n",
      "        -2.9750, -2.9750], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2733, -3.8120, -2.3652, -2.9536,  1.7624, -3.8120, -3.8120, -2.7067,\n",
      "        -2.9536, -2.9536], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.008717160671949387\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.5143, -1.3775, -1.8158, -2.5919], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.7604, -3.7767, -3.3567, -0.4176, -2.5634, -3.0748, -2.5889, -3.1352,\n",
      "        -3.7767, -3.3567], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.4227, -3.8303, -3.3070, -0.3156, -2.6935, -2.9624, -2.2978, -2.9624,\n",
      "        -3.8303, -3.3070], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02792816422879696\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2306, -1.4131, -0.6378, -2.2144], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.1603, -2.4388, -3.1603, -3.1261,  0.7268, -3.7342, -3.1603,  2.9414,\n",
      "        -2.1245, -3.7342], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.9745, -2.7794, -2.9745, -2.6867,  0.4282, -3.8366, -2.9745,  4.3256,\n",
      "        -2.3287, -3.8366], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2480432540178299\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9900, -0.4023,  0.7044, -0.3494], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.1872,  1.7002, -3.1872, -2.5089, -3.0005, -1.8917, -3.1872, -1.3828,\n",
      "         0.7044, -3.1872], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.9925,  1.6329, -2.9925, -2.3045, -2.9925, -2.2445, -2.9925, -1.5740,\n",
      "         0.4523, -2.9925], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.042266279458999634\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.7127,  1.1951,  0.7134,  1.6547], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.3112, -2.6159, -3.6280, -3.1281,  0.7134,  2.3461, -3.1579, -1.4201,\n",
      "        -2.1573,  1.7008], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3104, -2.7464, -3.8020, -3.0290,  0.4892,  1.6246, -3.0290, -1.5458,\n",
      "        -2.2984,  1.6246], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06859935075044632\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7004, 0.8251, 2.3283, 2.9155], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.1207,  2.9155,  3.7395, -3.0629,  0.7309, -3.1207, -1.4764, -3.0919,\n",
      "        -3.1207, -3.3158], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.0687,  4.2519,  4.2519, -3.3973,  0.5282, -3.0687, -1.8003, -3.0687,\n",
      "        -3.0687, -3.3973], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2321358472108841\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.7472, 2.0997, 4.5780, 5.8023], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.2845, -2.2052, -0.6391, -3.0882, -2.9513,  0.6755, -3.5384, -2.3298,\n",
      "        -2.0589, -2.7914], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4207, -2.3417, -0.3920, -3.0968, -3.0968,  0.5669, -3.7428, -2.3417,\n",
      "        -2.2355, -3.0968], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02976284921169281\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.3734, -3.2453, -3.0357, -3.5251], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.6503, -3.2453, -3.2531, -3.0536,  1.2507, -0.5448,  0.6503, -3.0357,\n",
      "        -3.0536, -2.0802], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.6000, -3.4600, -3.4353, -3.1037,  0.6000, -0.4147,  0.6000, -3.4353,\n",
      "        -3.1037, -2.3234], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07485900819301605\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.5301, -2.6959, -2.9544, -3.2740], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 2.9708, -2.9942, -0.5418, -3.0425, -1.3384, -1.3384, -3.8099, -1.4643,\n",
      "        -2.9544, -3.2740], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.1675, -3.0991, -0.4514, -3.0991, -1.4978, -1.4978, -3.7711, -1.4978,\n",
      "        -3.0991, -3.4263], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15521851181983948\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.0296, -2.0633, -2.5087, -3.2803], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-3.0369, -2.9685, -0.4907, -1.4490, -1.5017,  3.0060, -3.2755, -2.7829,\n",
      "        -2.7829,  0.7510], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.0793, -3.0793, -0.4876, -1.5003, -1.7993,  4.1350, -3.4116, -2.8569,\n",
      "        -2.8569,  0.6731], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14154914021492004\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2969, -1.4264, -1.8024, -2.4765], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.4968, -3.0335,  0.5356, -2.7467, -3.2742, -3.0335, -3.3658,  1.8980,\n",
      "        -3.0335, -3.2742], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7796, -3.0534,  0.7082, -2.8335, -3.3842, -3.0534, -3.8550,  1.7533,\n",
      "        -3.0534, -3.3842], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04029638692736626\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9798, -1.3914, -0.5456, -2.0447], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.0222, -2.6171, -2.1125, -0.8407, -3.0222, -2.2544, -2.4660, -0.5456,\n",
      "        -3.1922, -3.0222], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.0290, -2.8288, -2.2273, -0.5353, -3.0290, -2.3501, -2.8091, -0.5353,\n",
      "        -3.3554, -3.0290], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.030499562621116638\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.0026, -0.4664,  0.4818, -0.5033], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.6500, -3.0269, -2.3315, -2.9379, -2.0883, -2.9379, -3.0269, -3.6500,\n",
      "        -3.0269, -3.6500], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9072, -3.0007, -2.3395, -3.0007, -2.2473, -3.0007, -3.0007, -3.9072,\n",
      "        -3.0007, -3.9072], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.023385265842080116\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4018,  1.3712,  0.7281,  1.9286], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 5.5503, -0.4549, -2.0474, -2.3436, -3.3553, -2.6713, -2.9641, -3.0323,\n",
      "         0.4498, -3.2125], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([10.0000, -0.5952, -2.2623, -2.3179, -3.2688, -2.7166, -2.9618, -2.9618,\n",
      "         0.7357, -3.2688], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.9966106414794922\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.0646, 1.1172, 2.4303, 3.2551], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.9977, -3.7805, -3.7805, -2.3238, -2.8492, -3.7805, -2.9977, -2.9977,\n",
      "         2.4303, -1.8525], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.9216, -3.8409, -3.8409, -2.2981, -2.9216, -3.8409, -2.9216, -2.9216,\n",
      "         1.9296, -2.2422], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04367171972990036\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.8281, 2.2102, 4.2680, 5.6771], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.6134, -2.9319, -1.3341, -0.4756, -2.9473, -3.8285, -2.9473, -2.8606,\n",
      "         3.3346, -2.9473], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.6390, -2.8895, -1.4281, -0.5622, -2.8895, -3.8217, -2.8895, -2.8895,\n",
      "         4.1094, -2.8895], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06300336122512817\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.6080, -3.1177, -3.2719, -3.8634], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.2650, -3.1177, -2.8871, -3.3700,  3.4248, -2.8871, -1.2289, -1.9509,\n",
      "         5.7285, -2.3934], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2817, -3.3355, -2.8587, -3.1541,  4.1556, -2.8587, -1.3896, -2.1599,\n",
      "        10.0000, -2.6673], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.9020404815673828\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.4020, -2.5822, -2.9574, -3.9624], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-3.1188, -0.4231, -2.3806, -2.7929, -2.7929, -2.3764, -1.9338, -2.8224,\n",
      "        -0.3648, -3.3329], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3240, -1.5693, -2.2770, -2.8285, -2.8285, -2.6454, -2.0921, -2.8285,\n",
      "        -0.4862, -3.1388], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.151903435587883\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-2.7341, -1.7577, -2.0467, -2.9380], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.7690, -2.7125, -3.3143, -1.1441, -2.8242, -3.3143, -3.3143, -3.8831,\n",
      "        -2.7125, -2.7125], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.8041, -2.8041, -3.1346, -1.2543, -2.8041, -3.1346, -3.1346, -3.8173,\n",
      "        -2.8041, -2.8041], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.014026761054992676\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-2.2345, -1.9257, -1.0910, -2.3899], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.7152, -3.2697, -2.6525, -2.7152, -3.2697, -2.6525, -2.6525, -0.2190,\n",
      "        -1.0910, -2.6525], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.7911, -3.1389, -2.7911, -2.7911, -3.1389, -2.7911, -2.7911, -0.4659,\n",
      "        -1.1971, -2.7911], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.019482651725411415\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8196, -1.1413, -0.1849, -1.7767], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.6851, -2.6365, -1.9755,  1.5589, -3.8878, -2.7435, -3.2257, -2.6851,\n",
      "        -2.7764, -1.7476], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.7779, -2.7779, -2.2592,  1.0975, -3.8519, -2.7779, -3.1374, -2.7779,\n",
      "        -2.7779, -1.9615], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03866692632436752\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9928, -0.3995,  0.6000, -0.4178], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.1830,  1.5648, -2.3243, -0.1529, -0.3995, -2.6390,  4.4899, -1.7544,\n",
      "        -3.1938, -3.8873], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.1473,  1.1559, -2.5817, -0.4600, -0.4600, -2.7830,  4.6503, -2.0095,\n",
      "        -3.3028, -3.8744], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04562319070100784\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.1278,  1.5610,  1.0056,  2.4598], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.6512, -2.6734, -2.8146, -3.1464, -2.7049, -2.6512, -2.0784, -3.1464,\n",
      "        -0.4043, -3.7180], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.7900, -2.7900, -3.3019, -3.1595, -2.7900, -2.7900, -2.2462, -3.1595,\n",
      "        -1.3595, -3.8953], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12691530585289001\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.6293, 1.4633, 2.9220, 4.1861], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.6940, -3.9388, -2.6940, -1.1133,  6.4214,  0.5679, -1.3272, -2.6940,\n",
      "        -1.0435,  0.5679], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.7684, -3.8853, -2.7684, -1.1353, 10.0000,  1.2480, -1.2862, -2.7684,\n",
      "        -1.1353,  1.2480], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.3761560916900635\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.2771, 2.4382, 4.6312, 6.5626], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.6990, -2.6990, -2.6990, -2.7425,  0.6159, -0.1071, -1.0713, -3.3490,\n",
      "        -3.1774, -1.9014], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.7346, -2.7346, -2.7346, -2.5143,  1.2705, -0.4457, -1.0964, -3.1987,\n",
      "        -3.1987, -1.9146], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0622808039188385\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.8012, -3.1605, -3.2415, -3.9810], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.6771, -2.6942, -2.6942, -2.6942, -3.9810, -3.0854,  0.6989, -2.6942,\n",
      "        -0.9733, -3.1355], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.8510, -2.7096, -2.7096, -2.7096, -3.8444, -3.0928,  1.2696, -2.7096,\n",
      "        -1.0581, -3.0928], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03845148906111717\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.2828, -2.3369, -2.6078, -3.7623], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.7502, -2.6854,  6.7986, -2.6854, -0.9303,  2.4987, -2.6386, -3.1440,\n",
      "        -0.0208, -3.1498], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.6868, -2.6868, 10.0000, -2.6868, -1.0188,  3.0257, -2.6868, -3.1032,\n",
      "        -0.2810, -3.0743], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.0615556240081787\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-2.7576, -1.6683, -1.9190, -2.8775], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.6518, -2.5956, -0.8790, -1.8343, -2.5956, -1.9952, -3.9821, -1.0026,\n",
      "        -2.6518,  6.9830], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.6515, -2.6515, -0.9711, -1.7911, -2.6515, -2.0753, -3.8108, -0.9711,\n",
      "        -2.6515, 10.0000], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.915554404258728\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-2.1631, -1.8052, -0.8277, -2.1965], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.6060, -2.6060, -0.7883, -3.4808, -2.6060, -0.9654, -1.9803, -0.7883,\n",
      "         7.2328, -3.4808], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.6058, -2.6058, -0.9118, -3.7865, -2.6058, -0.9118, -2.0379, -0.9118,\n",
      "        10.0000, -3.7865], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.7881098985671997\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.6964, -0.8980,  0.1903, -1.4414], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.7160, -1.7160, -2.5484, -2.5484, -3.0095, -3.0095, -3.7690, -1.7523,\n",
      "         0.1459, -2.5636], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.9768, -1.9768, -2.5444, -2.5444, -3.0102, -3.0102, -3.7205, -1.6867,\n",
      "        -0.9064, -2.5444], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12503933906555176\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.6542, -0.0207,  1.2503,  0.2085], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.4824,  3.5038, -2.9885, -2.5255, -2.5255, -0.8346, -2.0625, -2.5275,\n",
      "        -3.7947, -2.5255], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6550,  3.6059, -2.9786, -2.4994, -2.4994, -0.7672, -2.3802, -2.4994,\n",
      "        -3.6550, -2.4994], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.016813868656754494\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.0647,  1.7254,  1.2570,  2.8805], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.5011, -2.5011, -2.5011, -2.5011, -2.5011, -1.4085,  1.3068, -2.5053,\n",
      "        -2.5011, -2.9673], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.4519, -2.4519, -2.4519, -2.4519, -2.4519, -1.5198,  1.5924, -2.4519,\n",
      "        -2.4519, -2.9310], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.011270830407738686\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1810, 1.8258, 3.6757, 5.3997], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.9357, -2.4516, -2.9357, -3.6839, -2.9357,  0.3188, -0.7182, -3.7421,\n",
      "        -2.9357, -1.6503], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.8900, -2.4152, -2.8900, -3.5222, -2.8900, -0.6369, -0.6457, -3.5222,\n",
      "        -2.8900, -1.5968], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10057777166366577\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.1316, 2.9574, 5.7963, 8.4764], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.2786, -2.0575, -2.4103, -2.4103, -2.4103, -2.9022,  0.4496, -0.4503,\n",
      "        -2.9022, -0.6649], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5393, -2.1926, -2.3837, -2.3837, -2.3837, -2.8517,  0.2564, -0.5953,\n",
      "        -2.8517, -0.5953], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0757652074098587\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.6864, -2.6914, -2.7112, -3.5834], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.5932, -3.5834, -1.5961, -2.7818, -3.5834, -2.6914, -2.8884, -2.3757,\n",
      "         1.3652, -2.3757], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.6235, -3.4223, -1.5639, -2.8184, -3.4223, -2.6235, -2.8184, -2.3546,\n",
      "         1.7094, -2.3546], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01840578392148018\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.1869, -1.7633, -2.1045, -3.3147], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.8606,  5.6943, -2.8606, -2.2424, -2.8606, -1.2311, -2.6320,  0.0384,\n",
      "        -3.5113, -3.5113], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.7965,  6.9199, -2.7965, -1.5422, -2.7965, -1.3153, -2.7965,  0.2244,\n",
      "        -3.3688, -3.3688], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2114018201828003\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-2.7981, -1.1899, -1.6187, -2.5250], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.5831, -2.3233, -2.3233,  1.3420, -0.0892,  8.9221, -2.3233, -2.7201,\n",
      "        -0.0892, -2.3233], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.5450, -2.3043, -2.3043,  1.7508,  0.2078, 10.0000, -2.3043, -2.7582,\n",
      "         0.2078, -2.3043], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15097567439079285\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-2.1508, -1.5617, -0.6200, -2.0608], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.2512, -3.0095, -2.7170, -2.7170, -2.3120, -2.7170, -2.3037, -0.4962,\n",
      "         1.3868,  0.5415], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5127, -2.7216, -2.7216, -2.7216, -2.2670, -2.7216, -2.2670, -0.1484,\n",
      "         1.7871,  0.1961], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.055518776178359985\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.5129, -0.4774,  0.5705, -1.0228], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.6695, -2.6695, -2.5095, -2.5095, -1.9139, -3.1956, -0.0161, -3.2589,\n",
      "        -2.2431, -2.2450], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.7225, -2.7225, -2.4986, -2.4986, -2.0304, -3.2585, -0.0938, -3.2585,\n",
      "        -2.2498, -2.2498], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0029490513261407614\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.7806,  0.0427,  1.3460, -0.0489], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.1808,  0.0559, -2.1979, -2.1979,  1.3460, -1.9141, -2.1878, -2.1979,\n",
      "        -0.4612,  1.4712], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2240, -0.0417, -2.2283, -2.2283,  1.8330, -2.0134, -2.2283, -2.2283,\n",
      "        -0.4676,  1.8330], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03937188535928726\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.1172, 1.9546, 1.5059, 3.1224], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.1294, -3.1856, -1.8314,  3.1224,  5.7050, -2.1491, -3.1294, -2.1491,\n",
      "         9.4334, -3.1294], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.1708, -3.1708, -2.2060,  4.5786,  7.4901, -2.2060, -3.1708, -2.2060,\n",
      "        10.0000, -3.1708], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5780156254768372\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.7673, 2.1395, 4.2771, 6.2189], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.0480, -2.6345,  6.2189,  9.5002, -2.1077, -2.1077, -3.0768, -1.8851,\n",
      "         6.2189, -3.0768], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4924, -2.6966,  7.5502, 10.0000, -2.1469, -2.1469, -3.1130, -1.9466,\n",
      "         7.5502, -3.1130], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4005137085914612\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0784, 3.3362, 6.5968, 9.5451], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.1415, -2.0050, -1.8137,  9.5451,  1.4737, -0.4574, -2.0050,  0.1099,\n",
      "         2.1858, -1.5556], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3377, -2.0979, -2.0979, 10.0000,  1.9461, -0.3377, -2.0979,  0.3264,\n",
      "         1.9461, -1.9317], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08267021179199219\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.8834, -2.2767, -2.1619, -2.9605], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.9605, -1.9195, -2.9605, -1.9195,  6.4117, -2.9605, -1.9912, -1.9195,\n",
      "        -2.8834,  0.8110], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.9458, -2.0321, -2.9458, -2.0321,  7.6402, -2.9458, -2.0321, -2.0321,\n",
      "        -2.9458,  0.3947], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17266890406608582\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.0733, -1.7858, -2.0904, -2.5725], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.9480, -1.8757, -2.9084, -1.8757, -1.7858, -0.2794, -1.8757, -1.4944,\n",
      "        -1.8757, -1.8757], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.9564, -1.9564, -2.8900, -1.9564, -1.8045, -0.2448, -1.9564, -1.8407,\n",
      "        -1.9564, -1.9564], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01544399093836546\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2584, -0.8242, -1.3516, -2.4403], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.8743, -1.4653, -1.2585, -1.8743, -1.0191,  0.8409, -0.2348, -3.0282,\n",
      "        -1.8743,  0.2681], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.8748, -1.7662, -1.2113, -1.8748, -1.2113,  0.5018, -0.2432, -2.8614,\n",
      "        -1.8748,  0.5018], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.032721683382987976\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3853,  0.0380, -0.5152, -1.1683], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.8878, -2.8483, -2.2872, -1.1758, -1.8878, -1.8878,  3.4605, -2.0558,\n",
      "         0.8001, -1.9076], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7989, -2.8502, -2.4908, -1.1801, -1.7989, -1.7989,  4.9986, -2.3072,\n",
      "         0.5357, -1.7989], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2575760781764984\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.0091, -0.1760,  0.7704, -0.9799], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.4707, -1.8762, -2.8136, -1.6568, -2.0611, -1.1021, -2.0611, -1.8762,\n",
      "        -1.8762, -1.8762], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.4279, -1.7366, -2.8070, -1.6033, -2.4279, -1.1328, -2.4279, -1.7366,\n",
      "        -1.7366, -1.7366], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03526964783668518\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.1844,  0.4775,  1.8198,  0.2490], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.8109,  6.7444, -2.2076, -2.8535, -2.3685, -0.1092, -1.8547,  0.4775,\n",
      "        -2.3685, -1.2702], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.6787,  7.7246, -2.3377, -2.7952, -2.3377, -0.3380, -1.6787,  0.6379,\n",
      "        -2.3377, -1.0412], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11620241403579712\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.2431, 2.4441, 2.3076, 3.8739], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.8126,  6.8038, -1.8126, -1.8126, -0.7054, -0.5953, -2.8661,  1.8769,\n",
      "        -1.8126,  3.8739], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.6349,  7.7246, -1.6349, -1.6349, -1.0148, -0.8467, -2.7907,  2.4865,\n",
      "        -1.6349,  5.1234], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3071584701538086\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.8001, 2.4809, 4.8861, 6.8461], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.1483, -1.4135, -1.6862,  0.8553, -2.8380, -1.6862, -2.1096, -1.6862,\n",
      "        -1.2068, -3.1614], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2134, -1.4345, -1.6329,  0.5538, -2.8148, -1.6329, -2.2134, -1.6329,\n",
      "        -0.9983, -2.8148], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.027904832735657692\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.0272, 3.4827, 6.9632, 9.6542], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 2.5421, -0.9552,  9.6542, -1.5002,  9.6542, -1.9799, -0.9552,  0.8413,\n",
      "        -1.5592,  4.2183], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 2.7965, -0.9104, 10.0000, -1.6428, 10.0000, -2.1876, -0.9104,  0.9058,\n",
      "        -1.6428,  5.1940], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13343513011932373\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.6245, -2.0949, -2.1117, -2.8026], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.0067, -2.1117, -1.8649, -1.4037, -0.4109,  2.2264, -1.4549, -1.8356,\n",
      "        -1.0145, -2.0049], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9780, -2.1341, -2.1341, -1.3698, -0.8399,  2.9911, -1.6258, -2.1341,\n",
      "        -0.9780, -2.1341], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09800932556390762\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-2.3527, -1.3698, -1.6354, -2.8979], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.3804, -1.3738, -1.7834, -1.8285,  0.0551, -1.4223, -1.3804, -1.9588,\n",
      "        -1.3738,  0.7916], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.6109, -1.6109, -2.0620, -1.4693,  0.6713, -1.6109, -1.6109, -2.0620,\n",
      "        -1.6109,  1.1404], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0972864031791687\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-1.5996, -0.2926, -0.6588, -1.6063], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.3911,  6.9237, -1.4029, -1.4029,  4.6402, -1.4029, -0.8438,  0.3673,\n",
      "        -1.7761,  0.3578], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.5514,  7.6700, -1.5514, -1.5514,  5.2314, -1.5514, -0.8463, -0.0967,\n",
      "        -1.9529, -0.0967], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14513704180717468\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-0.8481, -0.6565,  0.3783, -1.1006], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.4497, -1.5068, -1.1821, -1.6071, -2.9048,  0.4200, -1.4497, -2.6885,\n",
      "        -1.1821,  6.9207], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4778, -1.4778, -1.1855, -2.0638, -2.7540, -0.0351, -1.4778, -2.7540,\n",
      "        -1.1855,  7.6362], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09571270644664764\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.7014,  0.2254,  1.1730, -0.6549], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.3849, -1.4817, -1.8145, -1.8145, -1.4817, -0.6549,  1.1730, -0.8408,\n",
      "        -1.8145, -1.4817], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4095, -1.4095, -1.7567, -1.7567, -1.4095, -0.5912,  1.3681, -1.2951,\n",
      "        -1.7567, -1.4095], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.027483850717544556\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5706, 1.1611, 2.7102, 1.1350], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 3.0152, -1.8381, -1.4490, -1.4822, -2.9534, -1.9979, -1.4822, -2.2264,\n",
      "        -0.5142, -2.9534], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.4338, -1.7088, -1.2405, -1.3619, -2.6427, -1.7088, -1.3619, -1.8835,\n",
      "        -0.6075, -2.6427], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06673877686262131\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.3199, 3.0372, 3.4382, 5.0082], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.4033, -1.3532, -1.4033, -1.4033, -1.4033, -1.8322, -1.6294, -1.4033,\n",
      "        -1.3532, -0.4125], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3657, -1.3657, -1.3657, -1.3657, -1.3657, -1.7391, -1.1650, -1.3657,\n",
      "        -1.3657, -0.6144], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02725156582891941\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.2125, 2.7230, 5.2640, 6.9434], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 3.6585, -1.3098, -0.4275, -0.8759, -1.9260,  0.3568, -1.8206, -1.8206,\n",
      "         9.4489, -0.4275], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 5.2490, -1.3881, -0.5643, -1.2910, -1.8805,  0.4206, -1.7883, -1.7883,\n",
      "        10.0000, -0.5643], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.30576473474502563\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.3835, 3.7741, 7.2302, 9.4197], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 5.1347, -1.4738, -1.2524, -0.2727,  1.4184,  6.9082, -2.7653,  0.3676,\n",
      "         1.2518,  9.4197], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 5.2174, -1.3793, -1.3793, -0.4166, -0.5431,  7.4777, -2.7233,  0.5027,\n",
      "         1.6904, 10.0000], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4773377776145935\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3407, -1.8733, -1.8932, -2.7487], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.2375, -1.9158, -1.8844, -1.6668, -1.2375, -1.2375, -1.7011, -1.2375,\n",
      "         6.8687,  5.1307], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3128, -1.7912, -1.7912, -1.7912, -1.3128, -1.3128, -1.7854, -1.3128,\n",
      "         7.4439,  5.1818], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.040298812091350555\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.6408, -0.7801, -0.9707, -2.3306], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.9841, -1.9174,  0.4725,  2.9950,  5.2448,  0.6701, -1.8884, -1.9174,\n",
      "        -1.3463, -1.5132], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.8325, -1.7503,  0.5450,  3.6161,  5.1563,  0.5450, -1.7503, -1.7503,\n",
      "        -1.2462, -1.2462], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05937211960554123\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-1.4252, -0.1850, -0.6205, -1.8729], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.9142, -2.6710, -1.6409,  1.5316,  0.6848, -1.9142, -1.5704, -2.6710,\n",
      "        -0.2265,  2.0284], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7290, -2.5875, -1.7290,  1.7423,  0.5860, -1.7290, -1.6441, -2.5875,\n",
      "        -0.1809,  1.7423], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.023374173790216446\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-0.3931, -0.3258,  0.7024, -0.9753], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.8653, -1.2252, -1.2252, -2.1351, -1.2252,  6.7937, -1.8653, -1.2252,\n",
      "        -0.1798, -1.2252], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7142, -1.1760, -1.1760, -2.5624, -1.1760,  7.3343, -1.7142, -1.1760,\n",
      "        -0.5167, -1.1760], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06459809094667435\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.0828,  0.8236,  1.8578, -0.1773], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 1.7094,  0.7265, -1.1936, -2.4755, -0.3936,  6.7846,  0.5861, -1.1936,\n",
      "        -2.1403, -1.1936], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.8604,  0.6720, -1.1407, -2.5035, -0.3462,  7.2970,  0.6720, -1.1407,\n",
      "        -2.5035, -1.1407], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04390425235033035\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.1380, 1.8011, 3.2182, 1.4789], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 1.8714,  6.7750, -1.2655, -0.2299,  0.6513, -0.2299, -0.7439,  6.7750,\n",
      "        -1.8154,  0.7321], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.8964,  7.2478, -1.0936, -0.3411,  0.6842, -0.3411, -0.9140,  7.2478,\n",
      "        -1.6695,  0.6842], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05555722862482071\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.6890, 3.6307, 3.6993, 5.0341], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 6.7830,  5.0341,  1.2523, -1.2149, -1.5971, -2.2795,  3.6993, -2.2795,\n",
      "        -1.1290, -1.5147], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 7.1920,  5.1047,  0.7906, -1.0735, -1.6587, -2.3484,  3.5307, -2.3484,\n",
      "        -1.0735, -1.6587], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04709701985120773\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.3804, 3.4410, 5.3565, 6.7937], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.1983, -1.1088, -1.4074, -1.4406, -1.1088, -1.7135, -1.4074, -2.2214,\n",
      "        -2.2214, -1.1088], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3321, -1.0611, -1.0611, -1.4552, -1.0611, -1.6298, -1.0611, -2.2965,\n",
      "        -2.2965, -1.0611], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05465490370988846\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.4256, 4.6037, 7.1762, 8.9942], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 1.9823,  3.3674, -2.1307, -1.8784, -2.1307,  1.9823, -1.6241,  3.7104,\n",
      "        -1.0455, -0.1965], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 2.0306,  3.5290, -2.2474, -1.4640, -2.2474,  2.0306, -1.6327,  3.5290,\n",
      "        -1.0761, -0.2968], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.027367163449525833\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.9088, -1.4162, -1.2845, -2.0405], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.9808,  0.8127, -1.4162, -1.5348, -1.5348,  0.8709,  0.2863, -2.0405,\n",
      "        -0.9808,  3.7229], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.0983,  0.8115, -1.5156, -1.6431, -1.6431,  0.8115,  0.2103, -2.1560,\n",
      "        -1.0983,  3.5433], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01158648356795311\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.0018, -0.7070, -0.9330, -1.4741], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.7070, -1.8811, -0.9495, -1.9753, -0.9495, -0.9495, -1.4741, -0.3903,\n",
      "        -0.9495, -0.1222], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.8670, -2.0948, -1.1100, -2.0948, -1.1100, -1.1100, -1.6363, -1.1100,\n",
      "        -1.1100,  0.2293], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08563389629125595\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3163,  0.2013, -0.2164, -1.5757], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.9255, -1.4714, -1.9010, -0.9255, -0.9180, -0.9255,  0.2773, -1.3204,\n",
      "         1.6950,  0.8985], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.0274, -1.5859, -2.0647, -1.0274, -1.0274, -1.0274,  0.2387, -1.5189,\n",
      "         0.7726,  0.8408], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09781167656183243\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.5831,  1.0367,  0.4007, -0.5566], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.2845,  2.1596, -1.2034, -1.2180, -1.2034,  6.8419, -2.1026, -0.9917,\n",
      "        -1.2231, -2.0154], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.0669,  2.0960, -1.5172, -0.9199, -1.5172,  6.9295, -2.0831, -0.9562,\n",
      "        -1.4526, -2.0831], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04800059273838997\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.0534,  1.2003,  1.9510, -0.3782], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.1530,  8.7398,  6.8157,  0.0940,  1.1535,  0.2012,  0.7853, -2.0990,\n",
      "        -1.2091, -1.2091], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.0631, 10.0000,  6.8658,  0.3077,  0.7559,  0.3077,  0.7559, -2.0631,\n",
      "        -0.8189, -0.8189], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.21205425262451172\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.0884, 2.2198, 3.3533, 1.4241], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 1.2041, -0.9244, -1.7174,  1.9864, -1.0535, -0.9244,  0.7785, -0.4218,\n",
      "        -0.9244,  3.7061], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.7878, -0.7483, -1.3797,  2.0180, -0.6748, -0.7483,  0.7878, -0.6748,\n",
      "        -0.7483,  3.4452], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06569844484329224\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.6430, 3.9504, 3.7460, 4.9930], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.2887, -0.8312, -2.1874,  8.8402, -0.8312, -0.9450,  6.9328, -0.9450,\n",
      "        -0.8312, -1.2712], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.2868, -0.7401, -2.1440, 10.0000, -0.7401, -0.7401,  6.9562, -0.7401,\n",
      "        -0.7401, -1.3881], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14700505137443542\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.5210, 3.8380, 5.6088, 7.0381], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 7.0381, -0.0749, -0.5385, -1.3561,  0.1163, -2.1559,  8.9603, -1.3561,\n",
      "        -0.8622,  3.9356], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 7.0643, -0.1946, -0.1946, -1.4620, -0.1946, -2.2205, 10.0000, -1.4620,\n",
      "        -0.7607,  3.5826], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14723345637321472\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.4615, 4.9127, 7.3544, 9.1331], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 9.1331, -2.1433, -0.5855, -1.3366,  5.2387,  0.2328,  2.0846, -2.1433,\n",
      "         0.2328, -0.3152], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([10.0000, -2.2010, -0.7905, -1.5016,  5.4613,  0.2165,  2.1624, -2.2010,\n",
      "         0.2165, -0.7939], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11128608137369156\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2284, -1.5568, -1.3185, -2.1396], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.2595, -0.4904,  0.5522, -0.5519, -1.1453, -0.4904,  0.3232, -0.6201,\n",
      "         5.7798,  5.7798], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4920, -0.7877,  0.8754, -0.9796, -1.4920, -0.7877,  0.2095, -0.7877,\n",
      "         5.5934,  5.5934], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07490058243274689\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9249, -0.4922, -0.5858, -1.0814], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.4828,  1.1317,  1.2837,  0.2722, -0.4828, -0.6438, -0.9253, -0.5482,\n",
      "         1.0273, -1.0814], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.7550,  1.2851,  1.2851,  0.2281, -0.7550, -0.9392, -0.7331, -0.9392,\n",
      "         1.2851, -1.4430], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06479031592607498\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2318,  0.3688,  0.0458, -1.2242], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.3214, -0.6055,  9.6505,  1.0724,  1.3453, -0.5378,  1.0724, -2.2111,\n",
      "         5.6090, -0.5338], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2859, -0.6972, 10.0000,  1.2770,  1.2770, -0.6972,  1.2770, -2.2859,\n",
      "         5.8104, -0.6972], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03184739500284195\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.6176,  1.1598,  0.5218, -0.2868], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.2964, -2.2638, -0.2868, -0.6162, -1.1953,  1.1598,  0.0182, -1.5185,\n",
      "        -0.6162, -2.2638], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5850, -2.3262, -0.5850, -0.6198, -1.2668,  1.2317,  0.3379, -1.2668,\n",
      "        -0.6198, -2.3262], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03558875620365143\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.2701, 1.4715, 2.4487, 0.3845], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.3082, -1.1026,  0.5272,  0.0962,  0.0619,  4.0112, -1.1026,  0.2645,\n",
      "        -0.5507,  0.4377], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2650, -1.2005,  1.0428, -0.0498,  0.3817,  4.2000, -1.2005,  0.3817,\n",
      "        -0.5487, -0.0498], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06974674761295319\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.1049, 2.2875, 3.5339, 1.7325], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.3715,  0.6692,  0.5309,  1.4836,  2.2875, -2.3444, -1.1460, -0.6368,\n",
      "         1.2367, -1.1460], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.1853,  0.1130,  0.3754,  1.2431,  2.1805, -2.2508, -1.1853, -0.5222,\n",
      "         1.2431, -1.1853], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04625288024544716\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.9569, 4.1613, 4.1717, 5.9088], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.5909, -0.5909,  0.3820, -0.5909, -0.6247,  2.2649,  5.9088,  0.1401,\n",
      "        -0.4273, -1.1992], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5297, -0.5297,  0.1226, -0.5297, -0.5297,  2.2258,  6.1047,  0.1245,\n",
      "        -0.5297, -1.2137], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.013845423236489296\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.7592, 4.0971, 5.8896, 7.9441], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.5316, -1.4375, -0.4491,  1.3519, -1.3670, -0.5728, -2.4027, -0.5316,\n",
      "        -0.5316, -1.3670], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5370, -1.4822, -0.5370,  1.3366, -1.2353, -0.5370, -2.2303, -0.5370,\n",
      "        -0.5370, -1.2353], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.007577876560389996\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.7759,  5.2686,  7.6822, 10.2797], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 7.9897,  0.4739, -0.5737,  6.0074, -0.4688, -1.4498, -1.2936,  0.4642,\n",
      "         6.0074, -0.5660], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 8.2517,  0.2955, -0.5822,  6.1908, -0.5484, -1.5076, -1.2708,  0.3068,\n",
      "         6.1908, -0.5484], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.020304415374994278\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2476, -1.4682, -1.1788, -2.3794], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 2.1971,  0.4469, -0.4828, -0.4150, -1.2795, -2.3793,  5.8008, -0.4150,\n",
      "        -1.4682, -0.4828], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 2.3595,  0.3985, -0.5602, -0.5602, -1.3060, -2.0609,  6.2252, -0.5602,\n",
      "        -1.4563, -0.5602], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03652142733335495\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9645, -0.3659, -0.4958, -1.3661], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.4060, -1.1373, -1.2575, -0.5476, -0.5476,  1.3936,  0.5206,  4.1871,\n",
      "        -0.4923,  0.4432], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5515, -1.3293, -1.3293, -0.5515, -0.5515,  1.0115,  0.2543,  4.5170,\n",
      "        -0.3503,  0.4527], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04092803969979286\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.7628,  0.7506,  0.6951, -0.8224], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.3998,  1.3566,  1.3566, -0.5584, -2.3464, -0.4339,  4.4822,  1.3836,\n",
      "         1.7082, 10.4202], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3199,  1.4001,  1.4001, -0.5306, -2.0323, -0.5306,  4.5439,  1.0379,\n",
      "         1.0379, 10.0000], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08682061731815338\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4913,  1.4037,  0.8411, -0.3132], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.1840,  3.6761,  0.4056,  0.4056,  2.6123, -2.3451, -0.4878, -0.5832,\n",
      "        -2.1964, -2.3451], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3057,  4.5446,  0.2347,  0.2347,  2.3085, -2.0656, -0.5051, -0.5051,\n",
      "        -2.0656, -2.0656], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10994352400302887\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.1893, 1.3397, 2.5358, 0.0717], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.2580,  0.5907,  0.5907, -0.5587, -0.5587, -1.2580, -1.3619, -0.5587,\n",
      "        -2.3184,  0.2724], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.2751,  0.2371,  0.2371, -0.4684, -0.4684, -1.2751, -1.5056, -0.4684,\n",
      "        -2.1322,  0.4524], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0362832248210907\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.1502, 2.2526, 3.6521, 1.5711], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.5393, -1.4739,  0.5846,  1.3394, -0.6502, -1.4739,  5.9342,  3.1956,\n",
      "         5.9342, -0.6502], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5184, -1.2586,  0.2557,  1.2192, -0.4739, -1.2586,  6.2229,  2.2869,\n",
      "         6.2229, -0.4739], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12704607844352722\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1005, 4.3214, 4.3874, 6.0030], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.4472, -0.0128,  6.0030,  0.2480, -1.4472, -2.2627, -0.6731, -2.0553,\n",
      "         0.8512, -0.6731], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.2673,  0.3661,  6.2031,  0.2657, -1.2673, -2.2251, -0.5167, -2.2251,\n",
      "         0.3661, -0.5167], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0563199408352375\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.8357, 4.1863, 6.0380, 7.9915], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.6539, -0.6942, -1.4295, 10.3676, -0.6942,  0.2594, -0.6942, -0.6036,\n",
      "        -0.7235,  6.0380], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5772, -0.5772, -1.3059, 10.0000, -0.5772,  0.2557, -0.5772, -0.2818,\n",
      "        -0.5772,  6.1924], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.034608155488967896\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.8818,  5.3754,  7.9442, 10.3426], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.3165,  7.9442, -1.3165, -1.3165,  1.7140,  0.3672, -2.0864, -0.9992,\n",
      "         7.9672, -1.4287], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3830,  8.3083, -1.3830, -1.3830,  1.3070,  0.2256, -2.2063, -0.3733,\n",
      "         8.3083, -1.3830], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08562270551919937\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1952, -1.5770, -1.2998, -2.0304], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 2.2443, -0.6159,  1.1543, -0.6032,  2.2443,  0.4073, -0.6159, -0.6260,\n",
      "        -2.0304,  0.1125], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 2.6145, -0.7784,  1.3870, -0.7784,  2.6145,  0.2034, -0.7784, -0.7784,\n",
      "        -2.1698,  0.2802], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05239791423082352\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.0432, -0.5864, -0.6627, -1.2910], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 4.0526,  2.2658, -0.6083,  1.7337,  1.4075, -1.2910,  0.2167,  1.4075,\n",
      "        -0.6409, -0.6409], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.2893,  2.6474, -0.8296,  1.4143,  1.4143, -1.5278,  0.5603,  1.4143,\n",
      "        -0.8296, -0.8296], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.059810537844896317\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.8560,  0.4324,  0.5097, -0.9308], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.7480, -2.1353, -0.7210, -1.9994, -1.9994,  4.4795, -0.7210,  4.1960,\n",
      "         5.8132,  1.2145], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.8474, -2.2543, -0.8474, -2.2543, -2.2543,  4.2319, -0.8474,  4.2319,\n",
      "         6.0913,  1.4345], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.037427403032779694\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.8203,  1.4430,  0.4084, -0.5018], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 1.4551, -0.8047, -0.4821, -1.4525,  0.3218, -1.3900,  0.2389, -1.5890,\n",
      "        -0.5992, -1.3900], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.4407, -0.8436, -0.6204, -1.5393,  0.3675, -1.5393,  0.3096, -1.4339,\n",
      "        -0.6204, -1.5393], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.010459504090249538\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.0953,  2.6415,  1.2907,  0.7732], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.8965, -0.8965, -0.8965,  0.1969,  5.7112, -2.0689, -1.5246, -1.4670,\n",
      "        -0.8965,  2.7099], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.8227, -0.8227, -0.8227,  0.3508,  6.0206, -2.3721, -1.5161, -1.5161,\n",
      "        -0.8227,  2.6373], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02408883348107338\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.3861, 2.5021, 4.0455, 1.7474], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 2.5021, -0.8205,  0.3964,  4.0455,  0.0932, -0.9357,  5.6790,  0.0932,\n",
      "         0.3688, -0.9357], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 2.6410, -0.8020,  0.1956,  4.1111,  0.1956, -0.8020,  5.9823,  0.1956,\n",
      "         0.3877, -0.8020], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.021339500322937965\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.8010, 4.3306, 4.3720, 5.6674], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.9486, -0.9486,  7.7144, -0.9486,  1.2815,  1.5503,  0.4095,  1.2815,\n",
      "        -1.0526, -1.4725], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.7885, -0.7885,  8.0472, -0.7885,  1.4740,  1.4740,  0.1534,  1.4740,\n",
      "        -0.7885, -1.5222], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04054563492536545\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.7734, 4.1997, 6.2436, 7.6827], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.9054, -1.6287, -1.6287,  2.5821, 10.0148,  0.3947, -1.4889,  0.2859,\n",
      "        -0.9054,  1.5369], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.8178, -1.5043, -1.5043,  2.6933, 10.0000,  0.3832, -1.5263,  0.4236,\n",
      "        -0.8178,  1.5187], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.007972018793225288\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.8632, 5.4446, 8.2295, 9.9830], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.6238,  1.5715, -0.8448, -0.8701,  1.7671, -1.2171, -0.6247,  2.8564,\n",
      "        -1.6238,  1.4919], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.5287,  1.4502, -0.8581, -0.8581,  1.4502, -1.5287, -0.7075,  2.7380,\n",
      "        -1.5287,  1.5708], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02577768824994564\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.0777, -1.5536, -1.4195, -2.2130], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.7998,  2.6136, -0.4731,  2.8594,  9.9466, -1.6058, -0.5877,  9.9466,\n",
      "        -1.4195, -1.5536], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.8896,  2.7655, -0.8896,  2.7655, 10.0000, -1.5289, -0.6930, 10.0000,\n",
      "        -1.5289, -1.4551], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.025782659649848938\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.0303, -0.5625, -0.8316, -1.5941], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.5941,  9.9159,  1.5321, -1.3479,  4.2033,  0.2897, -0.7563,  2.6447,\n",
      "        -2.2440,  1.4870], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.5063, 10.0000,  1.6113, -1.5063,  4.0856,  0.3513, -0.8800,  2.7830,\n",
      "        -2.2601,  1.6113], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0113909263163805\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.8868,  0.2929,  0.3350, -1.3588], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.2670,  4.5285,  9.8952, -2.3954,  4.2010,  7.5801,  4.3870,  4.2010,\n",
      "        -1.5663, -0.8946], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2547,  4.0824, 10.0000, -2.2547,  4.0824,  7.9057,  4.0824,  4.0824,\n",
      "        -1.4625, -0.8634], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.046876225620508194\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.6976,  1.5482,  0.4503, -0.5819], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.5258, -1.4247,  0.1581, -0.7493, -0.7493, -2.2707, -0.7493, -2.2707,\n",
      "        -0.7493,  1.5214], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.7227, -1.4340,  0.3934, -0.8577, -0.8577, -2.2822, -0.8577, -2.2822,\n",
      "        -0.8577,  1.5767], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.014451961033046246\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.1233, 2.8719, 1.4778, 0.8556], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 7.5903, -0.7790, -1.4641, -1.4624, -2.2804, -1.4624, -1.4780,  1.5422,\n",
      "        -1.4624, -0.7790], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 7.9031, -0.8248, -1.3891, -1.5231, -2.3302, -1.5231, -1.3891,  1.5332,\n",
      "        -1.5231, -0.8248], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.012917721644043922\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.5288, 2.7688, 4.0900, 1.7049], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.2797,  4.2757,  9.8875, -0.7998,  1.4757,  7.6003,  4.0900, -2.2797,\n",
      "         1.4757,  2.7794], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.3463,  4.1833, 10.0000, -0.7962,  1.5014,  7.8988,  4.1833, -2.3463,\n",
      "         1.5014,  2.6810], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.013886133208870888\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.8893, 4.4742, 4.4360, 5.7898], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 4.0702,  2.9333, -1.4726,  6.2348,  0.3337,  4.2959, -0.4936, -0.8159,\n",
      "         5.7898, -0.3516], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.2108,  2.6632, -1.5298,  5.8549,  0.3261,  4.2108, -0.6996, -0.7665,\n",
      "         5.8549, -0.6909], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04118721932172775\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.7858, 4.3047, 6.2137, 7.6320], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.4660, -1.3780,  2.7140, -0.5734, -2.3101, -0.8274, -1.3758, -0.6767,\n",
      "        -0.8274, -0.8335], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.5161, -1.3082,  2.6625, -0.6838, -2.3194, -0.7465, -1.3082, -0.7465,\n",
      "        -0.7465, -0.7465], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0052404869347810745\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.8215, 5.5442, 8.0777, 9.8854], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.3286,  4.0755,  1.5888, -0.8162,  0.3857, -2.3286, -2.3286,  7.6465,\n",
      "         1.4523, -0.7011], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.3321,  4.2619,  1.4215, -0.7337,  0.4300, -2.3321, -2.3321,  7.8968,\n",
      "         1.4215, -0.7337], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01361820101737976\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2065, -1.4986, -1.5780, -2.3477], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 7.6611, -2.3477, -0.7885, -0.7885,  0.2232, -1.5780, -0.7885, -0.7279,\n",
      "         2.8704, -0.7885], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 7.8917, -2.3488, -0.7252, -0.7252,  0.3492, -1.3271, -0.7252, -0.7252,\n",
      "         2.6932, -0.7252], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01793300360441208\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.5447, -0.6159, -0.6393, -2.4255], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.7214, -0.7214, -0.8485,  1.4610, -2.3335, -0.7214,  1.4610, -1.3132,\n",
      "        -2.3335, -0.7214], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.7374, -0.7374, -1.5543,  1.6290, -2.3628, -0.7374,  1.6290, -1.3710,\n",
      "        -2.3628, -0.7374], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05606260150671005\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.9748,  0.3626,  0.1850, -1.2472], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.5615, -0.6732,  0.2715,  2.7612,  2.7612, -0.4156, -0.6732, -0.7327,\n",
      "         7.6966, -2.3272], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6736, -0.7172,  0.3184,  2.7760,  2.7760, -0.6605, -0.7172, -0.7172,\n",
      "         7.8798, -2.3174], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01129499264061451\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.0334,  0.3381,  1.5596, -0.5177], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.3381,  1.5312,  1.6557, -0.6348, -2.2493, -0.9040, -0.6348, -2.3216,\n",
      "         1.5596, -0.6348], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.4036,  1.5073,  1.5073, -0.7009, -2.2797, -0.6552, -0.7009, -2.2797,\n",
      "         1.5073, -0.7009], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.010730916634202003\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5393, 1.6597, 2.8110, 0.2689], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.6113, -0.6113, -1.3936,  0.9250, -1.3906, -1.3936, -1.3936, -0.6113,\n",
      "         2.8110, -0.6851], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6923, -0.6923, -1.4141,  0.3258, -1.4141, -1.4141, -1.4141, -0.6923,\n",
      "         2.8328, -0.6923], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03811245411634445\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.5777, 2.9264, 4.2704, 1.8444], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.4429, -0.6135, -0.7168, -1.4458, -0.7168, -0.6135, -1.4305,  1.5712,\n",
      "        -0.8954, -2.3309], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6186, -0.6627, -0.6627, -1.3986, -0.6627, -0.6627, -1.4116,  1.5386,\n",
      "        -0.6583, -2.2449], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.010881644673645496\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.8576, 4.4526, 4.4626, 5.8851], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 4.2791, -2.3452,  9.8237, -2.3112, -2.1182, -0.6204,  5.8851, -0.4238,\n",
      "        -2.3313,  2.8620], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.2966, -2.2421, 10.0000, -2.2421, -2.2421, -0.6447,  5.9691, -0.5933,\n",
      "        -2.2421,  2.8511], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.010657746344804764\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.8041, 4.3304, 6.2867, 7.7564], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.6309, -0.6309, -2.3387, -1.4755, -0.6309,  0.4325,  0.3798,  0.4889,\n",
      "        -0.7569,  7.7564], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6270, -0.6270, -2.2428, -1.4272, -0.6270,  0.4408,  0.3950,  0.3950,\n",
      "        -0.6270,  7.8417], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.004484754055738449\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.7216, 5.4629, 8.0510, 9.8250], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 5.9060, -0.6389, -1.4282,  4.3025, -0.7042, -1.4749,  2.8390, -0.6389,\n",
      "         4.4582,  2.8390], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 5.9942, -0.6245, -1.4376,  4.3154, -0.6245, -1.4376,  2.8722, -0.6245,\n",
      "         4.3154,  2.8722], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0038773014675825834\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-2.2775,  7.7871, -0.6386,  5.9329, -2.2775, -0.6386,  0.3816,  6.3004,\n",
      "         2.8066, -1.4625], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2386,  7.8467, -0.6280,  6.0084, -2.2386, -0.6280,  0.4047,  6.0084,\n",
      "         2.8860, -1.4528], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.010469472967088223\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1870, -0.5151, -0.7783, -1.4438], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.6481, -0.6481, -0.6629,  0.5333, -0.5334,  4.3148,  0.5295,  0.5333,\n",
      "        -0.6629, -0.6481], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6286, -0.6286, -0.6286,  0.5071, -0.5235,  4.3712,  0.3998,  0.5071,\n",
      "        -0.6286, -0.6286], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0024965472985059023\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2605,  0.2806,  0.1705, -1.5548], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.2144,  0.5232,  5.9980, -0.6466,  0.5608, -1.2605, -2.2144,  4.4567,\n",
      "         8.0009,  0.2806], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2393,  0.4112,  6.0434, -0.6359,  0.5203, -1.3583, -2.2393,  4.3982,\n",
      "         7.8634,  0.3532], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00547398766502738\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.5977,  1.4883,  0.7560, -0.4165], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.4074, -2.1876, -0.6107, -0.6473, -0.5531, -0.6473, -2.1876,  0.3867,\n",
      "         1.7029,  2.8741], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4978, -2.2360, -0.6417, -0.6417, -0.7650, -0.6417, -2.2360,  0.4268,\n",
      "         1.4849,  2.8900], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.010813894681632519\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5303, 1.7203, 2.8797, 0.4104], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.4245, -1.4442, -2.1793, -1.4442, -1.4197, -0.5988, -1.4442, -0.6488,\n",
      "         2.8797, -0.6488], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3516, -1.5239, -2.2337, -1.5239, -1.5239, -0.6474, -1.5239, -0.6474,\n",
      "         2.8895, -0.6474], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.004065030254423618\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.5719, 2.9712, 4.3196, 1.9734], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 6.0708,  2.9712,  4.3196, -1.4314, -0.3682,  7.8709, -2.1779, -0.6659,\n",
      "        -1.3642, -0.6659], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 6.0838,  2.8877,  4.4637, -1.5200, -0.6471,  7.8870, -2.2277, -0.6471,\n",
      "        -1.5200, -0.6471], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.014125647023320198\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.8611, 4.4399, 4.4197, 6.0656], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.6372,  4.4399, -0.6276,  7.8745, -0.6692, -0.6692, -0.8133, -0.8133,\n",
      "        -0.8133, -1.4545], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.5546,  4.4590, -0.6227,  7.8851, -0.6227, -0.6227, -0.6227, -0.6227,\n",
      "        -0.6227, -1.4960], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.012228870764374733\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.7741, 4.3296, 6.2139, 7.8776], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.4593,  2.9922, -0.6602, -0.6602, -0.7309, -2.2374,  6.2139, -0.6602,\n",
      "        -0.2330, -0.6602], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.5000,  2.9205, -0.6383, -0.6383, -0.6383, -2.2107,  6.0898, -0.6383,\n",
      "         0.3411, -0.6383], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.036303695291280746\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.6639, 5.3956, 7.8867, 9.8673], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 7.8807, -0.6449, -1.3829, -2.2195, -2.2560, -0.6449,  1.7104,  0.5080,\n",
      "         0.6303,  2.8804], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 7.8806, -0.6796, -1.2989, -2.1774, -2.1774, -0.6796,  1.6645,  0.5393,\n",
      "         0.5310,  2.9510], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.003534585703164339\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.4221, -1.4273, -1.2978, -2.2711], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 7.8834, -1.6350, -1.3558, -0.6494,  9.8652,  7.8834, -1.3558,  0.3195,\n",
      "        -0.4845, -0.4547], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 7.8787, -1.2877, -1.4361, -0.7124, 10.0000,  7.8787, -1.4361,  0.5153,\n",
      "        -0.7734, -0.7124], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.034384988248348236\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1080, -0.4700, -0.5476, -1.3593], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.3240, -0.6494, -0.6494, -1.1002, -1.3054, -2.2975,  6.2038, -0.6494,\n",
      "         0.5895, -1.1002], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.4924, -0.7084, -0.7084, -1.2927, -1.4230, -2.1748,  6.0967, -0.7084,\n",
      "         0.5546, -1.2927], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.015449151396751404\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2874,  0.2936,  0.2261, -1.6439], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.7499, -2.3356, -1.4169, -0.6544, -2.3356,  0.2936, -1.3670,  7.8860,\n",
      "         2.9802, -0.3449], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6864, -2.1956, -1.2735, -0.6864, -2.1956,  0.2133, -1.3955,  7.8784,\n",
      "         2.9639, -0.5682], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01222423929721117\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.6639,  1.3428,  0.6508, -0.6650], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.7518, -0.6747,  9.8676, -0.6747,  0.2117,  1.7654,  6.1598, -0.6747,\n",
      "         6.0616,  0.4714], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6619, -0.6619, 10.0000, -0.6619,  0.4584,  1.6657,  6.1007, -0.6619,\n",
      "         6.1007,  0.5318], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.010559623129665852\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6714, 1.7697, 2.9618, 0.3870], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.3805,  0.3428, -0.3805, -1.4928,  9.8800, -1.3489,  2.9618, -0.6737,\n",
      "        -1.3489,  1.7697], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6914,  0.1979, -0.6914, -1.2950, 10.0000, -1.3424,  2.9509, -0.6463,\n",
      "        -1.3424,  1.6656], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.027978133410215378\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7710, 2.9611, 4.3961, 1.9601], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 4.4149,  0.3672,  2.7364,  6.0894, -1.3702, -0.6680,  1.3122,  0.1185,\n",
      "        -0.6680,  4.4149], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.4805,  0.1810,  2.9565,  6.1216, -1.3745, -0.6502,  1.6721,  0.5302,\n",
      "        -0.6502,  4.4805], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.039240334182977676\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1056, 4.4104, 4.3943, 6.0971], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.7144, -0.6829, -1.3915, -2.3050, -0.6829, -0.7144, -2.2191,  0.4523,\n",
      "         0.3950, -0.6829], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6421, -0.6421, -1.3910, -2.2378, -0.6421, -0.6421, -2.2378,  0.5042,\n",
      "         0.3788, -0.6421], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0023255501873791218\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.9641, 4.2977, 6.0833, 7.9256], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.6839,  0.4680, -0.6839,  7.9256, -0.4586,  2.8930,  0.4433, -0.6839,\n",
      "         0.3998, -0.6839], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6402,  0.4809, -0.6402,  7.9251, -0.6307,  2.9088,  0.4809, -0.6402,\n",
      "         0.3581, -0.6402], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0040804678574204445\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.9058, 5.3629, 7.6677, 9.9265], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.8219, -0.6757, -0.6757, -1.3913, -0.6595,  2.9428,  4.3691,  1.3348,\n",
      "        -0.6757,  4.3691], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6456, -0.6456, -0.6456, -1.3245, -0.6456,  2.8921,  4.5089,  1.4819,\n",
      "        -0.6456,  4.5089], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.010173086076974869\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.0466, -1.4072, -1.4205, -2.2386], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 4.3888,  1.6240, -0.6542,  9.9332,  7.9369, -0.6542, -1.4316, -0.6542,\n",
      "         0.4379,  2.8634], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.5150,  1.5771, -0.6658, 10.0000,  7.9399, -0.6658, -1.4851, -0.6658,\n",
      "         0.4616,  2.8953], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.002744056051596999\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.1579, -0.4163, -0.3484, -2.1336], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.6340, -0.6340, -0.6340, -0.6340,  2.9146, -0.7982,  4.3331,  1.6673,\n",
      "         9.9402, -0.6340], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6799, -0.6799, -0.6799, -0.6799,  2.8998, -1.3135,  4.5170,  1.5799,\n",
      "        10.0000, -0.6799], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03213895112276077\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.8032,  0.4419,  0.4293, -1.3799], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.6113, -0.6113, -0.4017, -0.6113, -0.6192, -1.4700, -1.4700, -0.4017,\n",
      "         0.3565,  7.9449], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6866, -0.6866, -0.6445, -0.6866, -0.6023, -1.5380, -1.5380, -0.6445,\n",
      "         0.2964,  7.9520], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.014806963503360748\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.2704,  1.4095,  0.7635, -0.5430], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.6352, -0.6352, -0.6352, -2.0977,  0.3564, -2.2038,  7.9460,  0.3564,\n",
      "        -0.6228, -0.6228], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6858, -0.6858, -0.6858, -2.2353,  0.2939, -2.2353,  7.9556,  0.2939,\n",
      "        -0.6858, -0.6858], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0043456172570586205\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.9311, 1.6615, 2.8898, 0.3151], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 1.6285, -1.5577, -1.5577, -0.6170, -0.6652,  1.6615, -2.2067,  0.3664,\n",
      "        -1.5577, -1.3801], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.6008, -1.5544, -1.5544, -0.6702, -0.6702,  1.6008, -2.2261,  0.2982,\n",
      "        -1.5544, -1.2478], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.002983373822644353\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.0044, 2.9255, 4.3797, 1.9181], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.5037, -0.6311, -1.1600, -0.6600, -1.6069,  0.4705, -0.6311,  6.0613,\n",
      "        -1.6069, -2.2015], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.4538, -0.6552, -1.5546, -0.6552, -1.5546,  0.3164, -0.6552,  6.1467,\n",
      "        -1.5546, -2.2074], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01958708092570305\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2766, 4.4154, 4.4662, 6.0485], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 2.8143, -1.6228,  9.9514,  0.4685, -1.9026, -2.1937,  0.3717, -0.6714,\n",
      "         4.3802, -1.9615], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 2.9422, -1.5258, 10.0000,  0.4444, -2.1632, -2.1632,  0.3534, -0.6461,\n",
      "         4.4436, -2.1632], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01432151161134243\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0432, 4.3114, 6.1131, 7.9391], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.2992, -0.7115, -1.6480, -0.5444, -0.7350,  6.0339, -0.7350,  9.9557,\n",
      "         1.5586, -1.5133], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4899, -0.6269, -1.4899, -0.5316, -0.6269,  6.1452, -0.6269, 10.0000,\n",
      "         1.5846, -1.4899], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.010764829814434052\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.9658, 5.4030, 7.7395, 9.9667], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.7478, -1.2756,  0.5465, -0.5031, -0.7403, -0.7437, -0.7437, -0.7437,\n",
      "         0.6505,  7.9444], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6182, -1.3162,  0.4295, -0.5072, -0.6182, -0.6182, -0.6182, -0.6182,\n",
      "         0.4295,  7.9700], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.014386603608727455\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.0889, -1.1051, -1.2468, -2.0758], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 1.6393, -0.6943,  4.5039, -1.1051, -1.6052, -1.6052, -1.6052,  0.5575,\n",
      "        -0.7259, -1.6052], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.6104, -0.6386,  4.4446, -1.2924, -1.4428, -1.4428, -1.4428,  0.4753,\n",
      "        -0.6386, -1.4428], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01624160259962082\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3480, -0.4290, -0.2665, -2.1956], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.5119, -0.6888, -0.7157, -1.1783,  0.6144, -0.6888, -0.6081, -0.2665,\n",
      "         4.4486,  7.9683], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4655, -0.6873, -0.6873, -1.4655,  0.5336, -0.6873, -0.6873, -0.5464,\n",
      "         4.4718,  7.9980], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.017800070345401764\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.1366,  0.4947,  0.4235, -1.4384], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.9875,  6.1023, -1.4203, -0.6792, -0.6886, -0.5530,  7.9820, -1.9875,\n",
      "        -1.4203, -0.6792], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.0537,  6.1838, -1.4628, -0.7168, -0.7168, -0.7168,  8.0124, -2.0537,\n",
      "        -1.4628, -0.7168], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.005041657947003841\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3732,  1.6139,  0.9641, -0.3559], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 1.7598, -0.5345,  0.5829, -0.4908, -0.6812, -0.6812,  6.1202, -2.1856,\n",
      "         1.7598, -0.6692], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.6775, -0.7260,  0.5838, -0.4754, -0.7260, -0.7260,  6.1914, -2.0652,\n",
      "         1.6775, -0.7260], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0077250078320503235\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6820, 1.6927, 2.9777, 0.3976], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 6.1406, -0.5515, -0.6947,  0.2734,  4.5784,  0.3701, -0.6473, -2.1394,\n",
      "        -1.3333,  4.5784], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 6.1969, -0.7237, -0.7237,  0.4625,  4.5265,  0.3376, -0.7237, -2.0791,\n",
      "        -1.4526,  4.5265], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.009949829429388046\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8449, 2.9595, 4.4706, 2.0355], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.3250, -2.0081, -0.7177, -0.5969, -0.7177,  1.7268, -0.7177, 10.0417,\n",
      "        -1.3250, -2.0081], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4561, -2.0929, -0.7135, -0.7135, -0.7135,  1.6695, -0.7135, 10.0000,\n",
      "        -1.4561, -2.0929], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0067374869249761105\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1242, 4.4666, 4.5461, 6.1661], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 4.4529, -2.0572, -0.4882, -0.7441,  4.4666, -1.3892, -0.6669,  0.3150,\n",
      "        -1.3610, -0.7441], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.5495, -2.0877, -0.4727, -0.6872,  4.5495, -1.4489, -0.6872,  0.3544,\n",
      "        -1.4489, -0.6872], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0037080110050737858\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.9215, 4.3138, 6.1369, 7.9855], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.8222,  1.7425,  0.5500,  6.1561,  0.3787,  0.3787, -1.4040,  7.9855,\n",
      "        -0.7596,  0.2918], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6592,  1.6470,  0.4970,  6.1869,  0.3692,  0.3692, -1.4361,  8.0317,\n",
      "        -0.6592,  0.3692], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.005887252744287252\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.8819,  5.4029,  7.8073, 10.0317], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.7641,  1.6599, -0.7641,  1.7502, -0.4671, -1.2095,  1.7502, 10.0317,\n",
      "        -1.3961,  0.3865], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6522,  1.6531, -0.6522,  1.6531, -0.4579, -1.3718,  1.6531, 10.0000,\n",
      "        -1.4378,  0.3804], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.007309935986995697\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.6001, -1.2521, -1.5413, -2.2225], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.2521, -1.4049,  3.0978,  1.7360, -0.7397, -0.7397, -1.4687, -2.2225,\n",
      "         1.6445, -1.5413], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3498, -1.4581,  3.0265,  1.6797, -0.6623, -0.6623, -1.4581, -2.1269,\n",
      "         1.6797, -1.4581], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.005000312812626362\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.6123, -0.5008, -0.3406, -2.1740], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.3643, -0.6951, -1.4917,  0.4291, -2.2605, -0.6142,  6.1530, -1.3246,\n",
      "        -0.6951, -2.2605], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.3615, -0.6828, -1.4925,  0.4591, -2.1778, -0.6828,  6.1735, -1.3065,\n",
      "        -0.6828, -2.1778], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0020339114125818014\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-2.2769, -0.6541, -0.6027, -0.5884, -0.6027,  6.1582, -1.5053,  4.5991,\n",
      "        -0.6541, -1.5348], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2284, -0.7057, -0.7057, -0.5507, -0.7057,  6.1712, -1.5296,  4.5423,\n",
      "        -0.7057, -1.5296], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0034338929690420628\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.5025,  1.5777,  0.9706, -0.3449], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.4765, -0.5446, -1.5227, -0.6099, -1.5227, -2.2942, -1.5227, -0.7496,\n",
      "        -1.4061,  6.1616], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.4200, -0.5554, -1.5548, -0.7222, -1.5548, -2.2655, -1.5548, -0.7222,\n",
      "        -1.2387,  6.1678], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.004864509217441082\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6383, 1.6544, 3.1043, 0.4532], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.6280,  4.5869,  7.9592,  6.2101,  0.4642, -0.6283,  1.6544,  5.3425,\n",
      "         4.5869, -2.3025], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.7298,  4.5454,  8.0132,  6.1633,  0.4160, -0.5823,  1.7939,  6.1633,\n",
      "         4.5454, -2.2751], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07171645760536194\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8196, 2.9523, 4.5826, 2.0413], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.1566, -2.3138,  6.1531, -0.6407, -0.6407, -0.6407, -0.6190, -1.5441,\n",
      "        -1.5760,  1.6765], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2676, -2.2676,  6.1350, -0.7199, -0.7199, -0.7199, -0.5804, -1.5571,\n",
      "        -1.5571,  1.7930], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00491294264793396\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.0593, 4.5060, 4.6002, 6.1416], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.4974, -1.5656, -0.6834, -1.5656,  3.0874, -0.5367, -0.6752, -2.3189,\n",
      "        -2.3189, -0.6752], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.5335, -1.5335, -0.7005, -1.5335,  3.1098, -0.5339, -0.7005, -2.2403,\n",
      "        -2.2403, -0.7005], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0017808213597163558\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.8433, 4.3594, 6.1414, 7.8739], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.4375,  6.1345,  4.5850, -0.7076,  1.6419, -1.6160,  6.1345, -0.7076,\n",
      "         1.7042, -0.7056], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.2951,  6.0865,  4.5211, -0.6865,  1.7684, -1.5185,  6.0865, -0.6865,\n",
      "         1.7684, -0.6865], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0059854621067643166\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.8543, 5.5678, 7.8604, 9.9664], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.4939, -1.3294, -0.7383,  0.5099,  0.3006,  3.0313, -1.6196, -0.5327,\n",
      "        -0.7383, -1.6196], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.5011, -1.3347, -0.6743,  0.5162,  0.4159,  3.0781, -1.5063, -0.5080,\n",
      "        -0.6743, -1.5063], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0050056930631399155\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.7100, -1.3253, -1.5778, -2.2451], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.7508, -1.5993,  3.0292, -0.7508, -0.7508, -2.2451, -1.5778, -1.7903,\n",
      "         1.6811, -0.9042], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6725, -1.5055,  3.0676, -0.6725, -0.6725, -2.1927, -1.5055, -1.5055,\n",
      "         1.7271, -0.6725], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.017352620139718056\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.6825, -0.5854, -0.3909, -2.2337], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.5307, -0.5854, -0.7173,  7.8312, -0.7173,  0.4739, -2.6564, -0.7173,\n",
      "         1.6735, -0.7173], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.5370, -0.5336, -0.6916,  7.9637, -0.6916,  0.5061, -2.2164, -0.6916,\n",
      "         1.7302, -0.6916], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02207338437438011\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3251,  0.4370,  0.3656, -1.4581], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 7.8239, -0.6742,  6.1498, -2.1290, -1.4021, -2.1290, -0.6742,  4.5341,\n",
      "         4.5341,  3.0392], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 7.9597, -0.7222,  6.0415, -2.2619, -1.3361, -2.2619, -0.7222,  4.5348,\n",
      "         4.5348,  3.0807], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.007618519477546215\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3495,  1.6486,  1.0257, -0.2677], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.6473,  3.0064, -2.0996,  3.0397, -1.5078, -1.5078,  0.4406, -0.6626,\n",
      "         0.4406, -0.6771], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.7374,  3.0812, -2.2825,  3.0812, -1.6094, -1.6094,  0.5661, -0.7374,\n",
      "         0.5661, -0.6226], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.010959768667817116\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7502, 1.6979, 3.0255, 0.4179], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.6454, -0.6454, -0.6454,  0.3018,  9.9342, -0.6454,  1.6703, -0.6454,\n",
      "        -0.6991,  4.5471], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.7284, -0.7284, -0.7284,  0.3790, 10.0000, -0.7284,  1.7229, -0.7284,\n",
      "        -0.7284,  4.5163], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.004934238735586405\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.9234, 3.0518, 4.4798, 1.9893], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.6522,  7.8028,  3.0518,  2.9929, -1.2024,  4.4798, -1.4324, -0.8270,\n",
      "         9.9263, -1.3763], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5709,  7.9337,  3.0318,  3.0318, -1.3805,  4.5034, -1.5758, -0.7031,\n",
      "        10.0000, -1.3805], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.009928942658007145\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1767, 4.6361, 4.4938, 6.1024], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.2886, -1.5195,  3.1767, -0.7058, -0.6949, -0.6190, -0.6949, -0.8223,\n",
      "        -0.6949,  0.3510], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2154, -1.5388,  3.0099, -0.6831, -0.6831, -0.5544, -0.6831, -0.6831,\n",
      "        -0.6831,  0.4111], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.006164964288473129\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.9060, 4.5315, 6.0315, 7.8036], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.3174, -0.7223, -0.7223, -0.7050, -0.7050,  1.7642,  6.0887,  6.0315,\n",
      "        -2.1475,  0.5420], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2061, -0.6771, -0.6771, -0.6771, -0.6771,  1.6740,  6.0233,  6.0233,\n",
      "        -2.2061,  0.5161], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.003459113882854581\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.9404, 5.7958, 7.7525, 9.9174], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 4.4727, -0.7029, -0.7029,  7.7525, -0.7029,  0.4945, -1.4483, -2.1630,\n",
      "        -2.1759, -2.1630], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.4665, -0.6795, -0.6795,  7.9257, -0.6795,  0.5233, -1.4878, -2.2105,\n",
      "        -2.2105, -2.2105], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.003975520376116037\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2744, -1.3440, -1.4466, -2.1899], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.4660,  6.0549, -0.7429,  6.0549, -1.4466,  1.7816,  3.1050, -0.6956,\n",
      "        -0.6956, -0.5513], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4646,  6.0242, -0.6778,  6.0242, -1.4646,  1.6957,  3.0218, -0.6778,\n",
      "        -0.6778, -0.5533], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0021370844915509224\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3796, -0.5402, -0.3322, -2.2432], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 4.4858,  6.0697,  5.9144, -1.4415,  1.7787, -2.2148, -0.6810,  1.7973,\n",
      "        -1.4415, -0.7485], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.4320,  6.0242,  6.0242, -1.4499,  1.7176, -2.2171, -0.6817,  1.7176,\n",
      "        -1.4499, -0.6817], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0031708453316241503\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.1353,  0.4840,  0.3987, -1.5227], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 3.0464, -0.6645,  9.8985,  0.5589, -2.2367,  9.8985, -2.2367,  0.4446,\n",
      "        -1.4385,  0.4446], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.0504, -0.6896, 10.0000,  0.5598, -2.2311, 10.0000, -2.2311,  0.3976,\n",
      "        -1.4419,  0.3976], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0025724314618855715\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.1917,  1.7600,  1.0779, -0.2803], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 5.8311, -0.4780, -2.2522, -2.2522,  6.0163, -0.6529,  3.0958,  0.3418,\n",
      "        -1.4219, -0.5293], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 6.0302, -0.5667, -2.2391, -2.2391,  6.0302, -0.6924,  3.0612,  0.3921,\n",
      "        -1.4302, -0.5748], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.005548956338316202\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.8832, 1.8150, 3.0887, 0.4047], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.3435,  9.9080, -1.4349,  9.9080, -0.6430,  7.8104, -0.7428, -1.4136,\n",
      "         1.9634, -1.3842], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.3874, 10.0000, -1.4229, 10.0000, -0.6908,  7.9172, -0.6908, -1.4229,\n",
      "         1.7886, -1.2416], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.008637170307338238\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.9752, 3.1092, 4.5235, 1.9512], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 3.1155,  1.8349, -1.4054, -1.3648, -1.4054, -0.5125, -1.4054, -2.2643,\n",
      "         4.5235, -0.6364], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.0712,  1.8040, -1.4041, -1.2439, -1.4041, -0.5650, -1.4041, -2.2283,\n",
      "         4.4043, -0.6764], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0037418946158140898\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1365, 4.6949, 4.4985, 6.0102], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.6402, -0.6402, -0.7237,  0.3781, -0.6402, -1.4242, -0.4245,  7.8247,\n",
      "         0.3781,  9.9282], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6597, -0.6597, -0.6597,  0.3978, -0.6597, -1.3821, -0.5348,  7.9353,\n",
      "         0.3978, 10.0000], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0037352540530264378\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.9751, 4.6550, 6.1356, 7.8374], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.6124, -2.2306, -0.6403,  7.8374,  6.0187, -0.6403,  0.6124,  0.4255,\n",
      "        -0.5895, -1.3902], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.5699, -2.1759, -0.6416,  7.9473,  6.0537, -0.6416,  0.5699,  0.4016,\n",
      "        -0.6416, -1.3715], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0023558116517961025\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([8.0188, 5.9548, 7.8814, 9.9527], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.5044, -1.3858, -1.3858, -0.4095, -0.6817, -2.2073, -0.5994, -0.6382,\n",
      "        -1.3858, -1.3870], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5285, -1.3685, -1.3685, -0.5059, -0.6256, -2.1586, -0.6256, -0.6256,\n",
      "        -1.3685, -1.3685], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.001747584668919444\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1032, -1.2788, -1.4159, -2.1774], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 1.7927, -0.5133, -0.6319, -1.3823, -0.3562, -0.6319, -0.6319, -2.1774,\n",
      "        -0.6319,  0.6213], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.7976, -0.5261, -0.6140, -1.3796, -0.4970, -0.6140, -0.6140, -2.1509,\n",
      "        -0.6140,  0.5632], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.002537116874009371\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3168, -0.5312, -0.3880, -2.2549], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.6249,  1.5557, -1.3168,  4.4770, -0.6214,  0.6203,  0.5742, -2.1484,\n",
      "         6.0581, -0.6249], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6041,  1.8542, -1.3492,  4.4523, -0.6041,  0.5603,  0.6135, -2.1455,\n",
      "         6.0894, -0.6041], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.009804810397326946\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9996,  0.5917,  0.4558, -1.4042], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.4517, -0.6098,  0.3981, -0.6098, -0.5896, -1.3695,  7.8894,  7.8894,\n",
      "        -2.1180, -0.6802], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.4109, -0.5935,  0.4109, -0.5935, -0.5935, -1.3985,  7.9874,  7.9874,\n",
      "        -2.1392, -0.4674], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.006814944092184305\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.1290,  1.7824,  1.0517, -0.2450], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.5038,  0.4650,  0.4538,  4.5028,  4.4723, -0.5935, -0.5935,  0.4268,\n",
      "        -1.3652, -0.5935], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.6042,  0.4252,  0.4252,  4.4809,  4.4809, -0.5916, -0.5916,  0.4252,\n",
      "        -1.4135, -0.5916], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0015380332479253411\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.0187, 1.8967, 3.1336, 0.5265], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.5415, -0.5821,  4.4708, -0.5821, -1.2876,  4.5072,  1.8201,  4.5072,\n",
      "         7.9152, -0.5821], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5915, -0.5915,  4.4942, -0.5915, -1.3986,  4.4942,  1.8336,  4.4942,\n",
      "         7.9985, -0.5915], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0023117319215089083\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.0026, 3.0926, 4.4719, 1.9298], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.5719, -0.5719,  3.2206, -0.5719,  1.7577, -1.3661, -0.5719, -0.5285,\n",
      "        -0.5719, -2.0553], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5907, -0.5907,  3.0247, -0.5907,  1.8310, -1.4399, -0.5907, -0.5907,\n",
      "        -0.5907, -2.1437], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.006265235133469105\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2048, 4.7399, 4.5089, 6.1092], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 1.7705, -0.5696,  7.9348, -0.5294, -0.5294, -0.5969,  0.5893, -2.0572,\n",
      "        -0.6546, -2.0572], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.8396, -0.5827,  8.0019, -0.5827, -0.5827, -0.5410,  0.5796, -2.1328,\n",
      "        -0.5827, -2.1328], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0034929022658616304\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0584, 4.7462, 6.2139, 7.9413], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 3.1073, -0.5721,  3.1617, -2.0776,  0.6268, -0.5721, -1.4005,  3.1426,\n",
      "         0.5113, -0.5721], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.0428, -0.5741,  3.0428, -2.1153,  0.6107, -0.5741, -1.4286,  3.0428,\n",
      "         0.6107, -0.5741], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.004060368984937668\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([8.0631, 6.0130, 7.9378, 9.9956], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.3233, -0.5665,  0.6106,  0.4781, -0.5757, -0.5757, -0.5757,  7.9456,\n",
      "        -1.2263,  6.0890], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5388, -0.5697,  0.5830,  0.5088, -0.5697, -0.5697, -0.5697,  7.9961,\n",
      "        -1.4034,  6.1510], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.008602896705269814\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.9530, -1.2311, -1.3669, -2.1469], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.5707, -0.5707,  0.5565, -2.1469, -0.5707,  0.5565,  6.0779, -0.5707,\n",
      "        -0.5707, -1.4488], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5662, -0.5662,  0.5199, -2.1080, -0.5662,  0.5199,  6.1509, -0.5662,\n",
      "        -0.5662, -1.4189], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0010537944035604596\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.2593, -0.5503, -0.4091, -2.3323], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.5664,  6.0743,  3.1716, -0.5664, -2.1800,  4.7569,  4.5508, -0.5503,\n",
      "        -0.6029,  0.5337], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5635,  6.1502,  3.0958, -0.5635, -2.1131,  4.4668,  4.4668, -0.5197,\n",
      "        -0.5635,  0.6311], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.011914956383407116\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9035,  0.6122,  0.4947, -1.4337], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.5651, -2.1928,  7.9507, -2.1928, -0.5651,  6.1013, -1.2100, -1.2100,\n",
      "        -0.5651, -1.3779], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5663, -2.1249,  7.9851, -2.1249, -0.5663,  6.1556, -1.4235, -1.4235,\n",
      "        -0.5663, -1.4235], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.010658225044608116\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.1146,  1.7548,  1.0426, -0.3240], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 6.1312,  7.9581,  9.9875,  0.6215, -0.5944, -0.6196,  9.9875,  1.7153,\n",
      "        -0.5306, -0.6290], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 6.1623,  7.9887, 10.0000,  0.5793, -0.5741, -0.5741, 10.0000,  1.8034,\n",
      "        -0.4986, -0.5741], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0018277069320902228\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.0215, 1.9012, 3.1518, 0.4776], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 1.8106,  7.9666,  3.1117, -0.5828, -0.6159,  0.4623, -1.4116,  1.7590,\n",
      "        -0.5828,  4.5815], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.8006,  7.9937,  3.1234, -0.5839, -0.5839,  0.5566, -1.3657,  1.8366,\n",
      "        -0.5839,  4.5465], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00202481122687459\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.0469, 3.1449, 4.5856, 1.9674], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.8230,  0.6323, -0.6308,  3.1109, -0.6308,  0.4599, -2.1845, -2.1845,\n",
      "         0.5998, -0.6539], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4741,  0.5925, -0.5861,  3.1270, -0.5861,  0.5665, -2.1138, -2.1138,\n",
      "         0.5665, -0.5861], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01546142715960741\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2035, 4.7435, 4.5794, 6.2302], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.6253, -2.1570, -0.6253, -0.3712, 10.0047,  3.1361, -0.6253, -0.6398,\n",
      "        -0.6253, -2.1570], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5966, -2.1296, -0.5966, -0.4456, 10.0000,  3.1345, -0.5966, -0.5966,\n",
      "        -0.5966, -2.1296], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0012234628666192293\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0586, 4.7394, 6.2724, 7.9943], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.5647, -0.5102,  1.7209, -1.3672,  3.2258,  4.5956,  3.1362,  0.5647,\n",
      "        -0.2531,  1.7685], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.5916, -0.6112,  1.7752, -1.3412,  3.1450,  4.6379,  3.1450,  0.5916,\n",
      "        -0.4615,  1.8226], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0070054791867733\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.0335,  5.9679,  7.9729, 10.0124], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.0997, -1.7741, -0.4960, -0.5941,  6.2843, -1.9333,  0.5434, -1.3615,\n",
      "         7.9990, -0.4960], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.1576, -2.1576, -0.6109, -0.6109,  6.1991, -2.1576,  0.5927, -1.3420,\n",
      "         8.0112, -0.6109], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.023759763687849045\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([ 1.7492, -0.5934, -1.2208, -2.0671, -1.3476, -0.5934, -1.3123, -0.3706,\n",
      "        -0.4770,  8.0052], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.8038, -0.5738, -1.2913, -2.0988, -1.3336, -0.5738, -1.3336, -0.4183,\n",
      "        -0.4550,  8.0126], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0013168428558856249\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.0069, -1.1708, -1.3416, -2.0459], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.5867,  1.8795, -0.5867, 10.0146, -1.3416, -1.3528, -0.5867,  4.5919,\n",
      "        -1.3528, -0.5606], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5434,  1.7904, -0.5434, 10.0000, -1.3328, -1.3328, -0.5434,  4.6636,\n",
      "        -1.3328, -0.5434], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0020108665339648724\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.2437, -0.4083, -0.2966, -2.1648], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.3859, -0.5643, -2.1648,  0.6088, -0.6333,  0.5368, -0.5391, -0.5643,\n",
      "         1.7791, -0.5643], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3406, -0.5228, -2.0220,  0.5693, -0.5228,  0.6013, -0.5228, -0.5228,\n",
      "         1.7888, -0.5228], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.004591024946421385\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9562,  0.7182,  0.5735, -1.3001], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 1.8734, -1.3952,  1.7708, -1.3001, -0.5272, -0.5135, -0.5272, -0.5272,\n",
      "        -0.6327, -2.0020], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.8000, -1.3582,  1.8000, -1.3582, -0.5214, -0.5214, -0.5214, -0.5214,\n",
      "        -0.5214, -2.0188], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.002381251659244299\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.2099,  1.7623,  1.0593, -0.3217], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 6.3115, -0.4907,  0.6502, -2.0886, -0.4907, -1.3905,  6.3186,  4.6449,\n",
      "        -1.3654, -0.6103], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 6.2226, -0.5276,  0.6111, -2.0191, -0.5276, -1.3684,  6.2226,  4.6803,\n",
      "        -1.3684, -0.5276], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0034804940223693848\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.8845, 1.8411, 3.1432, 0.4642], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 1.7503, -0.4630,  4.6817,  1.7579,  6.3149, -0.3720,  4.6817, -1.3768,\n",
      "         0.5868, -0.4630], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.8288, -0.5409,  4.6834,  1.7677,  6.2304, -0.4235,  4.6834, -1.3788,\n",
      "         0.6356, -0.5409], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0030582735780626535\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.0232, 3.1536, 4.6910, 2.0483], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.5869,  0.5447, 10.0233, -1.9597,  3.1420,  0.7249,  1.8301, -1.3968,\n",
      "        -1.3674, -2.1207], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.6460,  0.5770, 10.0000, -2.0292,  3.2219,  0.5770,  1.8278, -1.3800,\n",
      "        -1.3800, -2.0292], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.004698409698903561\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.0817, 4.7062, 4.6691, 6.3041], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.4976, 10.0234,  6.3041, -1.9893, -0.0943, -0.4456,  6.3041, -0.4456,\n",
      "         8.0485,  1.8681], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.5780, 10.0000,  6.2437, -2.0327, -0.3511, -0.5522,  6.2437, -0.5522,\n",
      "         8.0211,  1.7599], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.011733362451195717\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.9782, 4.7497, 6.3374, 8.0529], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.6688,  1.7818, -0.3389,  8.0529, -0.4669, -0.4669,  0.6831,  6.2843,\n",
      "         0.4433,  1.7818], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.6388,  1.8118, -0.3981,  8.0191, -0.5396, -0.5396,  0.5865,  6.2476,\n",
      "         0.6036,  1.8118], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0054277521558105946\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.9073,  5.9598,  7.9899, 10.0226], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.5016, -0.4866,  3.0330, -1.3687,  6.2673,  4.6585,  0.6229,  4.7600,\n",
      "        -1.9635,  4.6585], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5259, -0.5259,  3.1926, -1.3531,  6.2531,  4.6406,  0.6169,  4.6406,\n",
      "        -1.9983,  4.6406], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.004422636702656746\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1866, -1.0919, -1.2828, -1.9660], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.3683,  1.7811,  3.0751, -1.3683, -0.5390, -0.4925, -0.5390, 10.0256,\n",
      "        -0.5390,  3.0751], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3389,  1.7804,  3.1622, -1.3389, -0.5141, -0.5141, -0.5141, 10.0000,\n",
      "        -0.5141,  3.1622], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0019889543764293194\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3058, -0.3191, -0.2371, -2.0286], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.5639, -0.5639, 10.0275, -0.5639,  3.1679, 10.0275, -0.3202, -1.9643,\n",
      "        -1.9643,  3.0402], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5050, -0.5050, 10.0000, -0.5050,  3.1323, 10.0000, -0.2663, -1.9709,\n",
      "        -1.9709,  3.1323], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0024649440310895443\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9820,  0.8218,  0.6105, -1.2113], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.3137, -0.5734, -0.5734, -1.9623, -1.4735, -1.9623, -1.9623, -1.2113,\n",
      "        -1.3550,  4.7644], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3253, -0.5030, -0.5030, -1.9682, -1.3253, -1.9682, -1.9682, -1.3253,\n",
      "        -1.3253,  4.6453], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00601935014128685\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.2328,  1.8305,  1.0157, -0.3061], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.8129, -1.3485, -0.5611, 10.0264,  8.0741, -1.9623, -0.5634, -1.3485,\n",
      "        -1.9623, -1.3164], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.6475, -1.3311, -0.5019, 10.0000,  8.0237, -1.9743, -0.2684, -1.3311,\n",
      "        -1.9743, -1.3311], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.012226416729390621\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.9040, 1.8660, 3.0629, 0.5096], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 7.8953,  0.7160,  3.0629, -0.5454,  3.0629, -1.1084, -1.3414, -1.1084,\n",
      "        -0.5454,  1.8660], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 8.0214,  0.5586,  3.0677, -0.5133,  3.0677, -1.2505, -1.3516, -1.2505,\n",
      "        -0.5133,  1.7566], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.009527917020022869\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.0044, 3.0650, 4.5266, 1.9702], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.6847, -0.4417, -0.4699, -0.5055, -1.1821,  6.3015, -0.5055,  0.6654,\n",
      "        -1.9937, -1.3381], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4011, -0.3714, -0.3714, -0.5446, -1.2148,  6.2593, -0.5446,  0.5922,\n",
      "        -2.0639, -1.3975], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01148044504225254\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2322, 4.6321, 4.6248, 6.3021], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 8.0574, -0.4693,  0.5021,  3.1179, -0.4433,  6.3021, -2.0353,  4.6248,\n",
      "         0.5690,  4.5351], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 8.0063, -0.5808,  0.6300,  3.0816, -0.4562,  6.2517, -2.1357,  4.6719,\n",
      "         0.5457,  4.6719], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0066963667050004005\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1254, 4.6518, 6.3156, 8.0433], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.3490, -2.0935,  3.1362, -0.1698,  9.9921, -2.0360, -0.3248,  6.3156,\n",
      "        -0.4471, -0.4471], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4839, -2.1856,  3.0929, -0.5007, 10.0000, -2.1856, -0.5007,  6.2389,\n",
      "        -0.6085, -0.6085], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.024941453710198402\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([8.1157, 5.8043, 7.9746, 9.9773], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 8.0290, -0.5652,  8.0290, -1.3783, -0.4438, -0.4438,  2.9615,  0.4785,\n",
      "        -0.4438,  3.1454], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 7.9796, -0.6242,  7.9796, -1.5034, -0.6242, -0.6242,  3.0995,  0.5484,\n",
      "        -0.6242,  3.0995], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.014770573005080223\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.0003, -1.3686, -1.3597, -2.2217], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 8.0126,  0.5619, -2.2217,  9.9642,  4.5387, -2.2217, -0.5367, -0.4887,\n",
      "        -0.4887, -2.2217], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 7.9677,  0.5195, -2.2237, 10.0000,  4.6048, -2.2237, -0.5020, -0.6190,\n",
      "        -0.6190, -2.2237], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.004468020051717758\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9675, -0.5512, -0.6477, -1.4675], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.4675, -0.5523,  9.9529, -2.2845, -0.5635,  6.1965,  2.1731, -1.4675,\n",
      "         6.2886, -1.4675], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4961, -0.6068, 10.0000, -2.2273, -0.6068,  6.1968,  1.7907, -1.4961,\n",
      "         6.1968, -1.4961], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.016747120767831802\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.7810,  0.5509,  0.5176, -1.5052], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.3512,  0.5509,  4.4989,  4.4989,  6.1714,  0.6392,  1.7021,  4.5848,\n",
      "         0.5176, -2.3336], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3030,  0.5736,  4.5543,  4.5543,  6.1889,  0.5417,  1.6564,  4.5543,\n",
      "         0.5417, -2.2161], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.003620285540819168\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.0759,  1.7623,  1.0562, -0.3970], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.5487, -0.6320,  9.9464,  3.1139, -1.5224, -0.6413, -2.3611, -1.4683,\n",
      "        -0.5343, -1.2973], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4694, -0.5695, 10.0000,  3.0347, -1.4694, -0.5695, -2.2011, -1.4694,\n",
      "        -0.5206, -1.4694], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.008271503262221813\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.1748, 1.7889, 3.1187, 0.4512], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.6837, -1.4804, -2.3561,  0.5853, -2.3561,  4.5433,  0.5976,  0.4512,\n",
      "         6.1348, -0.6837], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5638, -1.4589, -2.1946,  0.6338, -2.1946,  4.5213,  0.5592,  0.5919,\n",
      "         6.1814, -0.5638], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.010770199820399284\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.0739, 2.9721, 4.4789, 1.8223], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.7007, -1.4834, -0.6484, -2.2992,  6.1450,  7.9866, -0.7869,  3.1402,\n",
      "         1.7909, -0.7007], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5729, -1.4709, -0.5729, -2.2073,  6.1879,  7.9682, -1.3855,  3.0310,\n",
      "         1.8262, -0.5729], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04206765443086624\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2737, 4.5694, 4.5500, 6.1646], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.4081, -0.7151, -2.2245,  0.5058, -0.6793, -1.5179,  1.7796,  1.7213,\n",
      "        -0.7151, -0.6793], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4941, -0.5904, -2.2297,  0.5382, -0.5904, -1.4941,  1.8605,  1.8605,\n",
      "        -0.5904, -0.5904], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.008185097947716713\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0766, 4.6213, 6.2646, 8.0098], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 3.2101, -0.6535,  6.1843, -2.1616,  2.9365, -0.5895, -1.3973, -0.3283,\n",
      "         4.5290, -0.6153], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.0761, -0.6218,  6.2088, -2.2174,  3.0761, -0.5508, -1.2707, -0.5365,\n",
      "         4.5659, -0.6218], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.010444415733218193\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.0314,  5.8097,  7.9620, 10.0078], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.5985, -1.4356,  4.5437, -2.1128,  0.5254,  1.9295, -1.4356, -0.6288,\n",
      "         4.5508, -0.6110], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6419, -1.4980,  4.5739, -2.1653,  0.7365,  1.9051, -1.4980, -0.6419,\n",
      "         4.5739, -0.6419], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.006015906110405922\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.0835, -1.3900, -1.2548, -2.0885], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 4.5484, 10.0127, -0.6163,  4.6180, -2.0629, -0.6147, 10.0127,  0.3945,\n",
      "        -2.0885, -0.6147], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.5722, 10.0000, -0.6449,  4.5722, -2.1293, -0.6449, 10.0000,  0.5158,\n",
      "        -2.1293, -0.6449], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0026411942671984434\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.0865, -0.5176, -0.5379, -1.4226], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.2647,  3.2463,  2.9728,  4.5745, 10.0155,  3.2463,  6.1890, -0.5971,\n",
      "        -0.5971,  0.5495], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.1604,  3.1170,  3.1170,  4.5701, 10.0000,  3.1170,  6.2137, -0.6374,\n",
      "        -0.6374,  0.7620], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01143618579953909\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.1070,  0.4739,  0.5493, -1.4926], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.6263, -0.6059, 10.0159,  1.6874,  1.7594,  1.7776, -0.6059,  4.6311,\n",
      "         4.5906, -0.6059], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6214, -0.6214, 10.0000,  1.8962,  1.6789,  1.8962, -0.6214,  4.5669,\n",
      "         4.5669, -0.6214], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.006983195431530476\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.5237,  1.7155,  0.6321, -0.5544], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.6310,  1.9092, -0.4490,  0.6164, -1.5211,  4.5572, -0.6317,  8.0092,\n",
      "         1.7223, -0.6310], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5988,  1.8478, -0.5303,  0.7183, -1.4041,  4.5673, -0.5988,  8.0151,\n",
      "         1.8478, -0.5988], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00535163190215826\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4210, 3.0302, 1.7114, 0.8898], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 6.1862,  0.6478, -2.0144,  6.2002, -0.5775, -1.5268, -1.2073, 10.0172,\n",
      "         0.5537,  1.7606], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 6.2058,  0.6840, -2.0866,  6.2058, -0.5758, -1.3843, -1.3843, 10.0000,\n",
      "         0.5845,  1.7994], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.006134187802672386\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8317, 3.1070, 4.5183, 1.9525], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.5670, -1.4433, -0.6118, -0.6118, -1.4433, -2.0237,  4.6444, -2.0237,\n",
      "        -0.6541, -0.6541], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5484, -1.3634, -0.5484, -0.5484, -1.3634, -2.1113,  4.5668, -2.1113,\n",
      "        -0.5484, -0.5484], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.006484766956418753\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1064, 4.6537, 4.5028, 6.1929], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.5847,  6.1929, -0.6165, -0.6431, -2.0355,  1.7579, -0.6431, -0.4076,\n",
      "        -2.0355, -0.2475], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5322,  6.2013, -0.5322, -0.5322, -2.1072,  1.7598, -0.5322, -0.5202,\n",
      "        -2.1072, -0.5202], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.013193638995289803\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.9143, 4.6797, 6.1387, 8.0019], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 8.0019, -0.3925, -1.4301, -0.5842, -1.4301, -0.6164, -0.6164,  4.5000,\n",
      "         1.7659, -2.0793], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 8.0190, -0.4849, -1.3328, -0.4747, -1.3328, -0.5178, -0.5178,  4.5786,\n",
      "         1.7719, -2.0993], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00658098328858614\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.8767,  5.9163,  7.8083, 10.0238], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.5723,  4.5101,  6.2059, -0.3534,  0.3810,  0.5712,  1.7841, -1.4053,\n",
      "        -0.5723, -0.5723], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5174,  4.5853,  6.2027, -0.4616,  0.6480,  0.5894,  1.6763, -1.3286,\n",
      "        -0.5174, -0.5174], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01155651081353426\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1860, -1.2597, -1.2563, -2.0841], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.3796,  8.0002, -1.3796,  6.1460,  6.2068, -0.5176, -0.4967,  1.8025,\n",
      "         2.9749, -0.5176], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3381,  8.0200, -1.3381,  6.2002,  6.2002, -0.5289, -0.5289,  1.6774,\n",
      "         3.0757, -0.5289], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.003393057733774185\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1135, -0.3890, -0.4856, -1.3499], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.3499, -0.4637, -0.4637, -1.3878,  0.6763, -0.4637,  0.6230, -1.3878,\n",
      "        -2.0894, -1.3499], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3501, -0.5427, -0.5427, -1.3501,  0.5982, -0.5427,  0.5664, -1.3501,\n",
      "        -2.1133, -1.3501], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.003145144786685705\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.1825,  0.5175,  0.5312, -1.5160], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.4613,  1.7297, -0.4270,  3.1479,  4.5606,  1.7325, -0.4270, -0.4270,\n",
      "        10.0174,  6.2081], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.5593,  1.7661, -0.5503,  3.1046,  4.5872,  1.7049, -0.5503, -0.5503,\n",
      "        10.0000,  6.1953], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.006040174979716539\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.5115,  1.7253,  0.6405, -0.5163], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.2353, -2.1115, -2.1115, -0.4193,  6.1669, -2.1054,  0.5053, -0.5163,\n",
      "         1.7515, -2.1115], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3693, -2.1118, -2.1118, -0.5533,  6.1908, -2.1118,  0.5306, -0.5533,\n",
      "         1.7058, -2.1118], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.004061736632138491\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5099, 3.0811, 1.7716, 0.9833], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.4277, -0.5025, -1.2984, -0.4277, -1.2984,  2.9998, -0.4277,  6.1947,\n",
      "        10.0061, -0.4277], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5470, -0.5372, -1.3598, -0.5470, -1.3598,  3.1070, -0.5470,  6.1853,\n",
      "        10.0000, -0.5470], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.007732342928647995\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8723, 3.1637, 4.5440, 2.0192], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.2975,  2.9840, -0.4696,  2.9840,  9.9985,  0.5035,  6.1814, -0.4696,\n",
      "        -1.3251,  0.5982], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3343,  3.0896, -0.5268,  3.0896, 10.0000,  0.5053,  6.1785, -0.5268,\n",
      "        -1.3343,  0.5664], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0031300366390496492\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1365, 4.6511, 4.5064, 6.1681], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.2990,  0.7405, -0.5143, -0.2611, -0.4450, -0.3379,  9.9914,  6.1681,\n",
      "         6.1255,  1.6264], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3041,  0.4638, -0.5025, -0.5065, -0.5025, -0.5065, 10.0000,  6.1721,\n",
      "         6.1721,  1.6806], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01739032194018364\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.9412, 4.6629, 6.1135, 7.9608], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 1.6714, -0.5575,  0.5723, -0.3153,  6.1538, -1.2945,  7.9608, -0.5575,\n",
      "         7.9608, -0.5575], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.6754, -0.4849,  0.5956, -0.4721,  6.1647, -1.2838,  7.9851, -0.4849,\n",
      "         7.9851, -0.4849], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.004236062057316303\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.9253, 5.9198, 7.7714, 9.9759], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 6.1419, -2.2613,  2.9691, -1.3269,  1.6699, -0.4704, -0.5681,  2.9691,\n",
      "         1.5980,  7.9547], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 6.1592, -2.1556,  3.0261, -1.2761,  1.6722, -0.4681, -0.4681,  3.0261,\n",
      "         1.6722,  7.9783], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0036618702579289675\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2729, -1.2821, -1.4983, -2.2668], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.6100,  0.5051, -1.3236,  9.9721, -0.6100,  6.1003, -0.6100,  1.6643,\n",
      "        -2.2668, -2.2668], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4623,  0.6168, -1.2709, 10.0000, -0.4623,  6.1574, -0.4623,  1.6751,\n",
      "        -2.1539, -2.1539], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.011029904708266258\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4326, -0.5059, -0.4435, -2.2805], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 4.4579, -0.4520,  4.4579, -0.5888,  0.7242, -0.5593, -1.3343, -1.3068,\n",
      "        -0.5888,  1.6250], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.5336, -0.4759,  4.5336, -0.4759,  0.4625, -0.4759, -1.2901, -1.3992,\n",
      "        -0.4759,  1.6986], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.012889666482806206\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.0790,  0.6493,  0.5558, -1.4101], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 3.0483, -1.3047, -1.3641,  0.5992, -1.3641, -2.2216, -0.5331,  6.1478,\n",
      "        -0.5331,  0.5992], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.0431, -1.3284, -1.3770,  0.4410, -1.3770, -2.2277, -0.5087,  6.1590,\n",
      "        -0.5087,  0.4410], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.005234058015048504\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4712,  1.5867,  0.7625, -0.6795], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.2188, -1.2624,  1.9740, -1.2624, -2.2188, -2.2188, -1.2624,  0.6229,\n",
      "        -1.2624, -0.4539], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2586, -1.3566,  1.8000, -1.3566, -2.2586, -2.2586, -1.3566,  0.5723,\n",
      "        -1.3566, -0.5348], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.007965978235006332\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.8404, 1.8486, 3.0914, 0.5337], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.4349, -0.4404, -0.4404, -1.4344, -0.4404,  3.0914, -1.4344,  6.1116,\n",
      "        -2.3356,  0.5098], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5412, -0.5412, -0.5412, -1.3419, -0.5412,  3.0818, -1.3419,  6.1401,\n",
      "        -2.2333,  0.5705], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.007390986196696758\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8616, 3.1149, 4.5322, 1.9003], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.2617, -0.6193,  1.8540, -0.3972,  4.5322,  1.8228,  4.5322, -0.4358,\n",
      "        -1.3395, -1.4220], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3618, -0.3851,  1.8174, -0.3916,  4.4788,  1.7780,  4.4788, -0.5280,\n",
      "        -1.3618, -1.3575], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00871244166046381\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1609, 4.6291, 4.5661, 6.0720], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.5446,  7.9084, -1.3699, -0.4783,  1.8248,  6.0720,  1.7760, -0.4517,\n",
      "         0.7123,  3.0739], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.5984,  7.9246, -1.3377, -0.5098,  1.7665,  6.1175,  1.8364, -0.5098,\n",
      "         0.4656,  3.0636], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00786581076681614\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.9820, 4.6518, 6.2071, 7.8974], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.2390,  3.0610, -0.7244, -1.3603, -0.4696,  1.8837,  1.8310, -0.4700,\n",
      "        -0.4376, -0.4709], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2243,  3.0479, -0.3431, -1.3939, -0.4916,  1.7549,  1.7549, -0.4916,\n",
      "        -0.3431, -0.3730], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01887509599328041\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.9562, 5.8868, 7.9030, 9.9015], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.2979, -0.4749,  4.6420, -0.4749, -1.3638, -0.4749, -1.4063, -0.4749,\n",
      "         0.6460,  0.6401], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3252, -0.4924,  4.4588, -0.4924, -1.3904, -0.4924, -1.3252, -0.4924,\n",
      "         0.6128,  0.6128], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0044653150252997875\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-2.2980, -2.2980, -0.5108,  7.8968, -1.3747,  0.5583, -0.4872,  1.8450,\n",
      "        -0.5108,  1.7907], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2372, -2.2372, -0.4975,  7.9131, -1.3896,  0.6090, -0.4975,  1.7590,\n",
      "        -0.4975,  1.7590], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.001930544152855873\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2454, -1.3904, -1.3993, -2.2787], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 7.9006, -2.2787,  4.4769,  0.7240, -0.4936,  0.6296, -0.4936, -1.3904,\n",
      "        -0.5046,  9.9068], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 7.9161, -2.2513,  4.4905,  0.6644, -0.5029,  0.5691, -0.5029, -1.3861,\n",
      "        -0.5029, 10.0000], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0017261089524254203\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4723, -0.5907, -0.4238, -2.3389], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 3.1638, -2.2563,  9.9152, -0.5202,  6.2247,  0.5816,  1.8569, -1.3847,\n",
      "        -0.4975,  0.5922], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.0271, -2.2570, 10.0000, -0.5094,  6.1174,  0.6713,  1.7750, -1.3650,\n",
      "        -0.5094,  0.5871], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0052817524410784245\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.1403,  0.5586,  0.5462, -1.5150], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 3.0951,  9.9310, -1.3710, -0.5991, -1.3710,  0.5381,  0.5586,  1.8481,\n",
      "         7.9211, -0.4953], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.0291, 10.0000, -1.3786, -0.4956, -1.3786,  0.5842,  0.6030,  1.7856,\n",
      "         7.9379, -0.5157], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.002862163120880723\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.2943,  1.7963,  1.0266, -0.3636], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.5092,  6.2279,  7.9360, -0.5031, -0.4753, -1.2346, -0.5031, -0.5092,\n",
      "         0.5277, -2.2393], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5157,  6.1424,  7.9542, -0.5157, -0.5195, -1.3741, -0.5157, -0.5157,\n",
      "         0.5769, -2.2612], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0032342406921088696\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7771, 1.8007, 3.1159, 0.5224], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.5096, -2.1980, -2.1980, -1.3577,  4.5902,  6.1875, -0.5096,  1.8097,\n",
      "        -1.3577, -1.3577], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5137, -2.2583, -2.2583, -1.3979,  4.5688,  6.1550, -0.5137,  1.8043,\n",
      "        -1.3979, -1.3979], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0013701139250770211\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7123, 3.0132, 4.4899, 1.8365], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 3.1278,  0.5181,  9.9775, -0.4476,  4.5600, -0.5133, -2.2018,  0.5181,\n",
      "         1.8168, -0.5082], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.0409,  0.5604, 10.0000, -0.5337,  4.5778, -0.5096, -2.2533,  0.5604,\n",
      "         1.8150, -0.5096], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0022017904557287693\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1423, 4.5520, 4.5990, 6.2059], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.3786, -2.2097,  1.8309,  0.6064, -1.3907, -0.4993, -0.4993, -1.4487,\n",
      "        -0.4679,  1.7904], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4167, -2.2408,  1.8269,  0.6368, -1.4167, -0.5110, -0.5110, -1.3069,\n",
      "        -0.5110,  1.8269], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00275764474645257\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.9222, 4.5955, 6.2285, 7.9787], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.2557, -0.4759, -0.5026,  7.9787, -1.4110, -0.5473,  1.8485,  6.2285,\n",
      "         6.2104,  0.6372], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2348, -0.5023, -0.5023,  7.9988, -1.4135, -0.5783,  1.8300,  6.1809,\n",
      "         6.1809,  0.5434], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0014773535076528788\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.9024,  5.8348,  7.9275, 10.0074], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.5235, -1.3711,  3.1442,  4.5984,  3.1442, -1.4310, -2.2231, -0.5108,\n",
      "        -2.1413,  5.9181], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.6777, -1.4096,  3.0574,  4.5908,  3.0574, -1.4096, -2.2340, -0.4937,\n",
      "        -2.2340,  6.1885], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.012297387234866619\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2573, -1.4098, -1.3643, -2.2247], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 4.5170,  0.4392,  7.9891, -1.4404, -0.5160, -2.2247, -1.4513, -2.2247,\n",
      "         0.6120,  3.1447], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.5904,  0.6788,  8.0093, -1.3018, -0.4906, -2.2279, -1.4131, -2.2279,\n",
      "         0.5347,  3.0653], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.009684993885457516\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1540, -0.4686, -0.5460, -1.4648], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.2241, 10.0117, -1.3513, -2.2241, -0.4686,  6.2083,  0.5851, -1.4648,\n",
      "        10.0117, -1.4092], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2162, 10.0000, -1.4217, -2.2162, -0.4967,  6.1913,  0.6098, -1.4217,\n",
      "        10.0000, -1.2964], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.002162726130336523\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.1568,  0.5108,  0.5616, -1.5423], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 4.5862, -2.2196, -1.3973,  6.2223, -2.1509, 10.0121, -2.2196, -0.5181,\n",
      "        -2.2288,  4.5916], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.5836, -2.2146, -1.3077,  6.1923, -2.2146, 10.0000, -2.2146, -0.4889,\n",
      "        -2.2146,  4.5836], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0014300808543339372\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.5794,  1.6830,  0.5788, -0.5686], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.2014,  7.9933, 10.0131,  7.9933, -1.4800, -0.5638, 10.0131, -1.5392,\n",
      "        -2.2014,  1.7693], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2038,  8.0117, 10.0000,  8.0117, -1.4184, -0.4831, 10.0000, -1.4184,\n",
      "        -2.2038,  1.8226], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0028795700054615736\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5874, 3.1043, 1.8767, 1.0795], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.1765, -1.4337,  3.1333,  0.5693, -0.3717,  0.5735, -0.4729, -0.5245,\n",
      "        -0.5245, -0.5413], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.1869, -1.3345,  3.1158,  0.5071, -0.4783,  0.5071, -0.4783, -0.4876,\n",
      "        -0.4876, -0.5601], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.003303315956145525\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7840, 3.0335, 4.5841, 1.9084], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.1561,  0.5684,  7.9995,  0.5592, -1.3519,  0.5672, -2.1561, -0.5287,\n",
      "        -1.4749, -0.5287], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.1753,  0.5930,  8.0129,  0.5008, -1.3564,  0.5008, -2.1753, -0.4967,\n",
      "        -1.4351, -0.4967], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0012985080247744918\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1950, 4.5110, 4.5738, 6.2050], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.5323,  6.2050,  3.1224, -0.5115,  1.7882,  6.2050, -2.1401, -0.4166,\n",
      "        -0.5115,  1.6627], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5089,  6.2017,  3.1348, -0.5089,  1.8102,  6.2017, -2.1642, -0.4409,\n",
      "        -0.5089,  1.7830], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.001686571165919304\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0365, 4.5745, 6.2185, 8.0043], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.5643, -1.4681, -1.3479, -0.5941, 10.0144, -1.4681, -0.5326, -0.5326,\n",
      "        -1.2804,  0.3295], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.4981, -1.4530, -1.4530, -0.5305, 10.0000, -1.4530, -0.5186, -0.5186,\n",
      "        -1.4530,  0.5932], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01198909617960453\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.0530,  5.7938,  7.8878, 10.0117], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.4849, -2.1331, -0.5592, -0.5256,  6.2017, -0.5592, -0.5256,  6.2069,\n",
      "         8.0039,  0.5494], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4223, -2.1804, -0.5219, -0.5219,  6.2035, -0.5219, -0.5219,  6.2035,\n",
      "         8.0105,  0.6017], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0011758996406570077\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1096, -1.3342, -1.3356, -2.1417], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.4350, -0.5340, -1.3342, -1.3912,  1.7786, -0.5038, -1.3153, -0.5771,\n",
      "        -0.5771,  6.0687], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4534, -0.5239, -1.4534, -1.4168,  1.7705, -0.4225, -1.4534, -0.5239,\n",
      "        -0.5239,  6.2032], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.006484703626483679\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4055, -0.5866, -0.4868, -2.3009], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 1.0614,  4.5967,  3.0749,  0.5663,  6.1887,  3.0566,  6.1887,  7.9969,\n",
      "         0.6489, -1.4208], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.5183,  4.5699,  3.1371,  0.5520,  6.1972,  3.1371,  6.1972,  8.0002,\n",
      "         0.5183, -1.4081], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03236119821667671\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9750,  0.5752,  0.6313, -1.4135], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.4257,  0.5805, -0.5677, -2.1998, -0.5677, -1.4257, -0.5835, -0.5835,\n",
      "        -0.5835,  4.5845], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3858,  0.5378, -0.5268, -2.2083, -0.5268, -1.3858, -0.5268, -0.5268,\n",
      "        -0.5268,  4.5484], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0019375222036615014\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4492,  1.7592,  0.5560, -0.5446], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 4.5244, -0.5741, -2.1000, -0.5558,  0.5236, -1.3168, -1.3168, -2.2233,\n",
      "         1.7738,  6.1510], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.5359, -0.5287, -2.2163, -0.5956,  0.5833, -1.3759, -1.3759, -2.2163,\n",
      "         1.7538,  6.1742], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0028804747853428125\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5975, 3.0285, 1.7147, 0.8679], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 3.0821, -1.4035, -1.4035, -0.5676,  3.0596, -0.5676, -0.5676, -1.4035,\n",
      "        -0.3933, -0.5676], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.1158, -1.3540, -1.3540, -0.5270,  3.1158, -0.5270, -0.5270, -1.3540,\n",
      "        -0.4414, -0.5270], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0020513187628239393\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.9468, 3.0820, 4.5727, 1.8784], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 1.8056, -0.5457,  4.5727,  3.0709, -2.2547,  0.6314,  0.6314, -2.1311,\n",
      "        -1.3771,  7.9562], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.7091, -0.5332,  4.5211,  3.1155, -2.2242,  0.6250,  0.6250, -2.2242,\n",
      "        -1.3459,  7.9582], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.002477172063663602\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.3282, 4.5187, 4.5342, 6.1348], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.5350,  4.5663,  4.5342, -0.5275,  0.4779, -0.5275,  6.1348,  1.7810,\n",
      "         6.1348, -2.2604], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5916,  4.5213,  4.5213, -0.5384,  0.5883, -0.5384,  6.1572,  1.7730,\n",
      "         6.1572, -2.2312], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0019741381984204054\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2025, 4.5884, 6.1865, 7.9517], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.5108,  3.0969, -0.5582, -1.4636, -0.5419, -0.5582, -2.2563, -2.2563,\n",
      "        -1.3170,  9.9485], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5477,  3.1032, -0.5477, -1.3802, -0.5833, -0.5477, -2.2364, -2.2364,\n",
      "        -1.3360, 10.0000], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0014086782466620207\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([8.2255, 5.7909, 7.8420, 9.9533], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.5004,  9.9533, -0.5004,  0.4239, -2.2450, -0.5004, -0.5419, -0.3711,\n",
      "        -0.5004, -2.2450], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5568, 10.0000, -0.5568,  0.5489, -2.2378, -0.5568, -0.5568, -0.4177,\n",
      "        -0.5568, -2.2378], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0033062822185456753\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([ 4.5220,  9.9600, -0.5108, -2.2354,  1.8034,  4.5220, -0.5108,  2.9644,\n",
      "         0.4447,  0.6401], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.5594, 10.0000, -0.5554, -2.2528,  1.7976,  4.5594, -0.5554,  3.0811,\n",
      "         0.5305,  0.6650], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0030320719815790653\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3633, -0.5436, -0.4625, -2.2773], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 6.1857,  1.6675,  0.5823,  6.1593,  2.9742, -0.5296, -2.2293, -0.5296,\n",
      "        -1.2318, -0.5278], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 6.1663,  1.7898,  0.5690,  6.1663,  3.0568, -0.5440, -2.2406, -0.5440,\n",
      "        -1.3035, -0.5440], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0028328322805464268\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.0079,  0.6044,  0.6146, -1.3754], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.5504, -0.5504, -0.5504, -0.5283, -0.5504, -0.5504, -0.5504,  1.6442,\n",
      "        -0.4291,  9.9705], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5281, -0.5281, -0.5281, -0.5281, -0.5281, -0.5281, -0.5281,  1.7737,\n",
      "        -0.5281, 10.0000], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0030446196906268597\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3692,  1.9230,  0.6024, -0.4202], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.5120,  1.7732,  0.6024,  7.9680,  0.6235,  0.6235, -0.5580,  3.1001,\n",
      "        -1.4554, -0.5580], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5003,  1.7552,  0.7307,  7.9786,  0.5958,  0.5958, -0.5100,  3.0034,\n",
      "        -1.2629, -0.5100], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0069553302600979805\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5063, 3.0066, 1.5565, 0.7079], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.5071, -1.3811, -0.3033, -0.5392, -0.5392, -0.5392, -1.2156, -2.2767,\n",
      "        -1.4249,  0.6209], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5078, -1.4420, -0.4406, -0.5078, -0.5078, -0.5078, -1.2729, -2.1978,\n",
      "        -1.2729,  0.7380], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.007183869369328022\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.9019, 3.0773, 4.4466, 1.8361], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.5010, -0.5010,  4.4466,  0.6135,  3.3841, -0.3861,  3.0823, -1.3577,\n",
      "         0.6210,  1.7610], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5174, -0.5174,  4.5892,  0.5849,  3.0019, -0.5174,  3.0019, -1.3100,\n",
      "         0.5175,  1.7740], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02046079933643341\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.3332, 4.5597, 4.5327, 6.1890], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.2596, -2.1468, -0.4747, -0.5335, -2.1468,  6.1890, -0.4501,  0.6953,\n",
      "        -0.4747, -1.2596], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3456, -2.1723, -0.5187, -0.4343, -2.1723,  6.1804, -0.5187,  0.7263,\n",
      "        -0.5187, -1.3456], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0035533071495592594\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2077, 4.6225, 6.1834, 7.9755], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 4.5304, -0.4151,  1.7723, -0.4151, -0.4771, -0.4151,  4.5304, -2.1442,\n",
      "        -2.1442, -1.3061], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.5446, -0.5110,  1.8215, -0.5110, -0.5110, -0.5110,  4.5446, -2.1275,\n",
      "        -2.1275, -1.3658], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.003567961510270834\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([8.2013, 5.8442, 7.8548, 9.9835], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 4.5566,  6.1298, -2.1454, -2.1454,  8.2013,  1.7975,  1.8377, -0.4077,\n",
      "         0.5585,  0.4567], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.5168,  6.1741, -2.1015, -2.1015,  7.9851,  1.8278,  1.8278, -0.4953,\n",
      "         0.5892,  0.6178], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.008968162350356579\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2392, -1.2734, -1.2115, -2.1332], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.3956, -2.1332,  7.9742,  3.1367,  9.9903, -0.4973, -2.2309, -0.4154,\n",
      "        -0.3395,  3.1367], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3784, -2.0903,  7.9912,  3.1123, 10.0000, -0.4784, -2.0903, -0.4784,\n",
      "        -0.3404,  3.1123], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0027809799648821354\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2143, -0.4270, -0.5012, -1.4199], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.7220, -0.4332, -1.4131, -0.4332, -0.4332,  3.1213, -2.2508, -0.4186,\n",
      "        -1.4116,  0.5982], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.7524, -0.4616, -1.3006, -0.4616, -0.4616,  3.1192, -2.1034, -0.4616,\n",
      "        -1.3843,  0.7524], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.006413648370653391\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.0473,  0.6730,  0.7050, -1.3493], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.6730, -1.4461, -0.4198, -1.3493, -0.4504, -1.2369,  7.9829, -0.5487,\n",
      "         4.5772,  3.0984], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.6162, -1.3871, -0.4187, -1.3871, -0.4328, -1.3063,  8.0043, -0.4328,\n",
      "         4.4906,  3.1195], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0035087503492832184\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4475,  1.9440,  0.6828, -0.4434], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 6.1019, -0.4628,  1.8543, -0.4628, -0.5703, -0.3338,  6.1019, -1.2709,\n",
      "         6.1019, -0.3998], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 6.1894, -0.4186,  1.7748, -0.4186, -0.4186, -0.3780,  6.1894, -1.3908,\n",
      "         6.1894, -0.4248], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.007319048047065735\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.3780, 3.0265, 1.5991, 0.6694], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.3849, -1.4341, 10.0191, -0.3920,  3.0789,  6.1234, -0.4468, -1.2447,\n",
      "        -2.2105, 10.0191], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3853, -1.3853, 10.0000, -0.4221,  3.1251,  6.1899, -0.4162, -1.2924,\n",
      "        -2.1202, 10.0000], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.002194735687226057\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8735, 3.1583, 4.5910, 1.9400], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.4703, -0.4703, 10.0238, -0.4664, -0.4703, -1.3255,  1.9230,  4.5640,\n",
      "         7.9873,  1.9230], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4222, -0.4222, 10.0000, -0.4222, -0.4222, -1.3894,  1.7185,  4.5344,\n",
      "         8.0214,  1.7185], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.009921153075993061\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.0825, 4.5562, 4.4907, 6.1734], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 4.6012, -0.4622,  6.1734, -0.4543, -0.4622,  4.6012,  4.6012, -1.3871,\n",
      "         7.9856,  4.6012], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.5561, -0.4352,  6.1870, -0.3684, -0.4352,  4.5561,  4.5561, -1.3984,\n",
      "         8.0236,  4.5561], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0018733736360445619\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0267, 4.6177, 6.1579, 7.9875], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 1.7671, -0.4588, -1.3646, -0.4189,  1.8250,  0.6924, -0.4588,  7.9875,\n",
      "        -2.0218,  0.6139], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.7722, -0.4504, -1.4090, -0.3990,  1.7722,  0.6708, -0.4504,  8.0281,\n",
      "        -2.1845,  0.6425], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.003471349598839879\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.9920,  5.8468,  7.8338, 10.0328], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.4400, -0.4603, -1.3534, -1.3483, -2.0894, -2.0894,  0.6075, -0.4513,\n",
      "         7.9876, -0.4513], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4606, -0.3828, -1.4143, -1.2536, -2.1755, -2.1755,  0.6427, -0.4606,\n",
      "         8.0295, -0.4606], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0037118694745004177\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-2.0943, -0.4460,  1.7614, -0.5045, -0.4393, -0.4393, -0.4473, -0.4473,\n",
      "        -1.3409, -2.0943], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.1646, -0.3958,  1.7699, -0.4587, -0.3851, -0.3851, -0.4587, -0.4587,\n",
      "        -1.2609, -2.1646], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.002711803885176778\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.0710, -1.3176, -1.2829, -2.1044], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.5815, -0.4505, -0.4692,  1.8869, -2.0710,  7.9837, -1.3545,  1.8114,\n",
      "        -0.4505,  1.7912], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.5954, -0.4502, -0.4502,  1.7634, -2.1546,  8.0258, -1.3770,  1.7634,\n",
      "        -0.4502,  1.7634], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0028146603144705296\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.0166, -0.3937, -0.4485, -1.3562], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.4468, -0.3937, -1.3562, -1.3562, -0.3039, 10.0271, -1.2962, -0.3039,\n",
      "         1.7815,  0.6189], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4448, -0.4247, -1.3544, -1.3544, -0.4247, 10.0000, -1.2735, -0.4247,\n",
      "         1.7736,  0.6319], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.003163157496601343\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9904,  0.5969,  0.6497, -1.4259], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 4.5556, -0.4823, -1.3589, -0.4442, -0.5446,  1.8097,  7.9826,  4.6177,\n",
      "         4.5587,  7.9826], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.6633, -0.4419, -1.3364, -0.4419, -0.3915,  1.7614,  8.0216,  4.6633,\n",
      "         4.6633,  8.0216], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.005558884236961603\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4664,  1.8044,  0.6174, -0.5652], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 4.6031, -1.2638, -2.1307,  0.6529, -1.2638, -0.3571,  3.1232,  1.8044,\n",
      "         0.5693, -0.4379], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.6574, -1.3156, -2.1062,  0.6238, -1.3156, -0.4044,  3.1061,  1.7702,\n",
      "         0.6238, -0.4397], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0016429604729637504\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4705, 3.0843, 1.6982, 0.7545], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.4287, 10.0041, -0.5012,  6.1682, -0.3579,  4.5693, -0.4287, -1.4338,\n",
      "        -1.2459, -1.3669], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4439, 10.0000, -0.4439,  6.1760, -0.3878,  4.6490, -0.4439, -1.3186,\n",
      "        -1.3221, -1.3186], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0032483034301549196\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8628, 3.1160, 4.5843, 1.8845], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.6122, -0.3766,  6.2650, -1.3709, -1.1991, -1.9608,  1.8270, -0.4149,\n",
      "        -0.4149, -0.4149], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.6090, -0.4521,  6.1713, -1.3278, -1.3278, -2.0792,  1.8259, -0.4521,\n",
      "        -0.4521, -0.4521], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.005107471719384193\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1146, 4.6035, 4.5960, 6.2461], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.3829, -0.4153,  4.6035,  6.2461, -0.4153,  3.1480, -0.4153,  3.1480,\n",
      "         7.9650,  1.8129], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3306, -0.4501,  4.6215,  6.1685, -0.4501,  3.1308, -0.4501,  3.1308,\n",
      "         7.9864,  1.8332], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0014163674786686897\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.9870, 4.6335, 6.1922, 7.9641], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.4244,  6.2230,  7.9641,  6.2230, -0.4244,  4.5926,  4.6039, -0.3808,\n",
      "        -0.4244, -2.1804], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4473,  6.1677,  7.9787,  6.1677, -0.4473,  4.6007,  4.6007, -0.3321,\n",
      "        -0.4473, -2.0881], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0018853072542697191\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.9025, 5.8549, 7.8445, 9.9692], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 4.5914,  7.9663, -0.4696, -1.2876,  0.4918, -1.2876,  6.1952, -0.4414,\n",
      "         1.7980, -0.3679], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.5757,  7.9723, -0.3331, -1.3311,  0.6182, -1.3311,  6.1697, -0.4430,\n",
      "         1.8282, -0.3331], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.004147356376051903\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.0260, -1.2798, -1.2456, -2.1915], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.1915,  1.7978, -0.4694, -0.4694,  7.9676, -1.2958,  0.6466,  4.5840,\n",
      "        -2.1915,  1.7880], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.1210,  1.8150, -0.4420, -0.4420,  7.9654, -1.3243,  0.6180,  4.5518,\n",
      "        -2.1210,  1.8150], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0015141373733058572\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9878, -0.3520, -0.4436, -1.3897], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.0261, -2.0261,  4.5727, -0.4474,  3.1409, -1.3055,  1.8006,  1.8461,\n",
      "        -1.2737, -1.2694], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.1425, -2.1425,  4.5356, -0.3764,  3.1154, -1.3168,  1.7975,  1.7975,\n",
      "        -1.4027, -1.3168], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00555440504103899\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9338,  0.6377,  0.6752, -1.4007], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 6.1366, -0.5146, -0.5146, -1.2865,  7.9750, -0.5146, -0.5146, -0.4463,\n",
      "        -2.1892, -0.9338], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 6.1775, -0.4475, -0.4475, -1.3149,  7.9584, -0.4475, -0.4475, -0.4475,\n",
      "        -2.1578, -1.3984], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02376418374478817\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4776,  1.7658,  0.5873, -0.6362], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 4.5801,  7.9813,  3.1166,  1.7424, -2.1927, -2.1927, -1.3704,  3.1166,\n",
      "        -1.4382,  6.1301], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.5171,  7.9589,  3.1221,  1.8308, -2.1472, -2.1472, -1.3176,  3.1221,\n",
      "        -1.3176,  6.1832], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0036659161560237408\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5036, 3.1420, 1.7702, 0.8043], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.0850,  0.6050, -1.4479, -0.4818, -1.3966, -1.3510, -0.4204, -2.1765,\n",
      "         3.1420,  0.6050], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.1375,  0.5777, -1.3334, -0.4733, -1.3334, -1.3334, -0.4647, -2.1375,\n",
      "         3.1293,  0.5777], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.002537766005843878\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7831, 3.0999, 4.5960, 1.8831], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.4208, -0.4208, -0.4630, -1.3388, -0.4630, -1.3388, -0.4630, -1.3388,\n",
      "        -0.4630, -0.4630], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4546, -0.4546, -0.4895, -1.3648, -0.4895, -1.3648, -0.4895, -1.3648,\n",
      "        -0.4895, -0.4895], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0007821419276297092\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.9470, 4.5569, 4.5595, 6.1642], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 1.7242,  3.0853,  0.5227, -1.3804,  0.6354,  9.9715,  1.7242, -0.4766,\n",
      "        -0.4766,  6.1642], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.8173,  3.1425,  0.5657, -1.2018,  0.5518, 10.0000,  1.8173, -0.5036,\n",
      "        -0.5036,  6.2065], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.006535144057124853\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.9240, 4.6401, 6.2289, 8.0144], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.4649, -0.4525,  6.1758,  8.0144, -1.2265,  0.5162, -0.4187, -1.3293,\n",
      "        -1.3293, -0.2114], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5023, -0.5023,  6.2130,  7.9798, -1.3710,  0.5637, -0.4363, -1.3710,\n",
      "        -1.3710, -0.4441], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00874983612447977\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.8144, 5.8424, 7.8678, 9.9820], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 9.9820, -1.3222, -0.4578,  3.1312,  1.8557,  9.9820, -0.4578,  4.6411,\n",
      "        -1.3222, -1.3317], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([10.0000, -1.3693, -0.5008,  3.1371,  1.8393, 10.0000, -0.5008,  4.5670,\n",
      "        -1.3693, -1.2021], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0031389729119837284\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3226, -1.3636, -1.2481, -2.0800], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.2481, -2.0800, -2.0800, -1.3209, -2.0800,  0.5334,  3.0840,  1.7310,\n",
      "        -2.0800, -0.4694], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3719, -2.1233, -2.1233, -1.3719, -2.1233,  0.5579,  3.1323,  1.8359,\n",
      "        -2.1233, -0.4985], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0040193526074290276\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2580, -0.4322, -0.4043, -1.3234], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 8.0208, -0.4795,  9.9904, -2.3568,  3.1373,  1.7390,  0.5826,  0.5635,\n",
      "        -1.3234,  4.5842], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 7.9914, -0.4929, 10.0000, -2.1434,  3.1258,  1.8269,  0.6584,  0.5993,\n",
      "        -1.3639,  4.5840], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.006321213208138943\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.5275,  0.5771, -0.4980, -0.4811], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 1.7917, -1.3281, -0.4091,  3.1164, -1.3630, -1.3281, -2.1316, -2.1316,\n",
      "        -1.3281, -0.4980], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.8048, -1.3682, -0.4806,  3.1122, -1.2557, -1.3682, -2.1761, -2.1761,\n",
      "        -1.3682, -0.4806], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0025899354368448257\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.5435,  1.8133,  0.6526, -0.5095], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.1603, -0.5107, -0.5107, -0.4919,  0.5910, -1.3512, -1.3512, -0.5107,\n",
      "         1.8016, -2.3709], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.1993, -0.4614, -0.4614, -0.4614,  0.6016, -1.3567, -1.3567, -0.4614,\n",
      "         1.7843, -2.1993], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.003966227173805237\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4570, 3.1606, 1.7871, 0.8586], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.3548, -0.4236,  6.2072,  0.6464,  9.9921, -2.3551, -2.2343, -0.4968,\n",
      "         0.5934,  1.7715], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3471, -0.4483,  6.2113,  0.5944, 10.0000, -2.2031, -2.2031, -0.4483,\n",
      "         0.6433,  1.7681], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0032408875413239002\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7070, 3.1118, 4.5398, 1.8817], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 1.7607,  4.5398, -0.5060,  1.7854, -2.2070,  6.2107,  0.6516,  0.5997,\n",
      "         1.7962, -0.5060], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.7581,  4.5896, -0.4441,  1.8410, -2.2130,  6.2091,  0.6436,  0.6436,\n",
      "         1.7581, -0.4441], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0016734125092625618\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.9635, 4.6009, 4.5665, 6.2112], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.5557,  3.0670,  3.0670,  1.7635,  4.6009,  9.9923, -0.3586,  1.8189,\n",
      "         9.9923,  0.6221], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.6111,  3.0932,  3.0932,  1.7603,  4.5901, 10.0000, -0.4401,  1.8328,\n",
      "        10.0000,  0.6370], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.001175873214378953\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.9221, 4.6373, 6.2041, 8.0073], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.2360,  0.6518, -0.4609, -1.3817, -0.3652, -0.4812, -0.4609, -0.3652,\n",
      "        -0.4609,  1.7674], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2219,  0.6301, -0.4481, -1.3286, -0.4134, -0.4481, -0.4481, -0.4134,\n",
      "        -0.4481,  1.7698], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0009735381463542581\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.8423, 5.8373, 7.8544, 9.9974], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.6820, -0.4404, -0.4404, -2.2402,  9.9974, -0.4404,  8.0077, -0.4700,\n",
      "        -0.4700, -0.4404], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.6256, -0.4490, -0.4490, -2.2178, 10.0000, -0.4490,  7.9977, -0.4490,\n",
      "        -0.4490, -0.4490], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0004973126342520118\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1701, -1.3989, -1.3549, -2.2413], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 6.2242, -0.4267, -0.4267,  0.6945, -0.4267,  1.8053,  6.2128,  1.7702,\n",
      "         3.0877, -0.4571], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 6.2076, -0.4484, -0.4484,  0.6248, -0.4484,  1.8104,  6.2076,  1.7789,\n",
      "         3.1057, -0.4484], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0007070224964991212\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.0531, -0.4265, -0.3731, -1.4147], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.6541, -0.4224,  6.2266,  3.0852, -0.4224,  0.6365,  1.7700,  1.7700,\n",
      "         0.8445,  0.5752], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.5810, -0.4447,  6.2084,  3.1022, -0.4447,  0.5930,  1.7767,  1.7767,\n",
      "         0.5810,  0.5810], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.007843692786991596\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3126,  0.6195, -0.4129, -0.4396], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.4396,  3.0804, -1.4181, -0.4396,  3.1177, -2.1344,  0.7006, -2.2447,\n",
      "        -2.2448, -1.4181], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4424,  3.1045, -1.3284, -0.4424,  3.1045, -2.2230,  0.6282, -2.2230,\n",
      "        -2.2230, -1.3284], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0030926153995096684\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4096,  1.8130,  0.6985, -0.4796], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 4.5992, -1.4063,  1.8126,  8.0177, -1.4063,  0.6510, -0.4116,  0.7293,\n",
      "        -0.5537, -0.4096], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.6106, -1.3284,  1.7648,  8.0131, -1.3284,  0.6313, -0.4380,  0.6317,\n",
      "        -0.3771, -0.3436], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.006068744696676731\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5412, 3.1263, 1.8133, 0.8797], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 3.0497, -0.5427, -0.4310,  0.7010,  4.5419, -2.2210, -1.3866,  4.5419,\n",
      "         4.5419,  1.8099], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.0877, -0.4138, -0.4261,  0.6481,  4.6155, -2.2480, -1.4551,  4.6155,\n",
      "         4.6155,  1.7447], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.004681441001594067\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7326, 3.1005, 4.5406, 1.9100], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.4022,  3.1368,  6.2251,  8.0224, -2.2219, -0.4477, -0.4477,  0.6728,\n",
      "         0.6536, 10.0211], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4118,  3.0866,  6.2201,  8.0190, -2.2367, -0.4118, -0.4118,  0.6680,\n",
      "         0.6680, 10.0000], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0006129201501607895\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.9657, 4.5836, 4.5501, 6.2108], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.2251, -1.3692, 10.0201, -0.3844,  1.8021,  0.6326,  0.7622, -1.3692,\n",
      "        -1.4415, -1.3657], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2292, -1.3459, 10.0000, -0.3962,  1.7316,  0.6219,  0.6219, -1.3459,\n",
      "        -1.3459, -1.4450], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.004182162694633007\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.8884, 4.6300, 6.2052, 8.0208], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.6706, -1.3640,  4.5546, -0.4551, -2.2089,  4.5546,  0.6811, -0.3870,\n",
      "         0.6180,  6.1983], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.6371, -1.3483,  4.5785, -0.3960, -2.2289,  4.5785,  0.6352, -0.3965,\n",
      "         0.6352,  6.2187], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0009309243177995086\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.7834,  5.8361,  7.8697, 10.0177], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([10.0177,  6.1853, -0.4471, -1.4442,  0.6035, -1.3261,  1.8352,  0.8793,\n",
      "         3.1257,  4.5702], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([10.0000,  6.2172, -0.3942, -1.3536,  0.6517, -1.3536,  1.7385,  0.6982,\n",
      "         3.1132,  4.5668], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.005778655409812927\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1766, -1.3618, -1.4291, -2.2114], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.3618, -2.1889, -1.3210,  4.5853,  1.7098, -2.1889,  4.5474, 10.0121,\n",
      "        -0.4361, -1.3618], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4064, -2.2256, -1.4064,  4.5539,  1.7504, -2.2256,  4.5539, 10.0000,\n",
      "        -0.3914, -1.3595], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0016819050069898367\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3542, -0.4318, -0.4936, -2.1924], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 3.1293, -1.1054, -0.3724, 10.0040, -2.1113, 10.0040,  3.0637, -0.4261,\n",
      "        10.0040, -1.3709], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.1369, -1.3886, -0.3865, 10.0000, -2.2151, 10.0000,  3.1369, -0.3865,\n",
      "        10.0000, -1.3616], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.009828509762883186\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.7765,  0.5942,  0.4704, -1.3892], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.4179,  6.1391, -0.3741,  4.6013, -0.4451,  7.9976, -0.3917,  7.7477,\n",
      "         6.2282,  3.1352], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3707,  6.1979, -0.3707,  4.5252, -0.3707,  7.9964, -0.3943,  7.9964,\n",
      "         6.1979,  3.1412], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00798368826508522\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.4750,  0.7272,  1.8493, -0.3308], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.3359, -1.3995,  4.6024,  4.6024,  7.9838, -0.3777, -0.3777, -1.3995,\n",
      "        -1.3723, -1.3995], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4625, -1.3451,  4.5145,  4.5145,  7.9822, -0.3633, -0.3633, -1.3451,\n",
      "        -1.3023, -1.3451], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.004568625707179308\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6526, 1.8394, 3.0901, 0.5639], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.7289,  9.9719,  3.0901, -1.3883, -0.3701,  0.7486,  0.7486, -0.4007,\n",
      "        -0.4007, -2.2228], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.6632, 10.0000,  3.1312, -1.3480, -0.3647,  0.7765,  0.7765, -0.3647,\n",
      "        -0.3647, -2.1423], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.001908110803924501\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6702, 3.1411, 4.5856, 1.9015], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.3711,  9.9671,  0.4527,  1.7545,  3.1411, -0.4012,  4.5856, -0.3806,\n",
      "         0.5859,  4.5856], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3611, 10.0000,  0.6859,  1.8114,  3.1270, -0.4197,  4.5330, -0.3740,\n",
      "         0.6729,  4.5330], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.007250919006764889\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.9275, 4.5821, 4.6012, 6.1711], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 3.1203,  0.7932,  3.1242,  1.8515, -0.3866,  1.8515, -0.3526, -0.3866,\n",
      "        -0.4189,  9.9682], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.1233,  0.7742,  3.1233,  1.8118, -0.4394,  1.8118, -0.3854, -0.4394,\n",
      "        -0.4394, 10.0000], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0011589727364480495\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.9130, 4.6035, 6.2382, 7.9745], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.2832,  7.9745,  6.1926, -1.3320,  2.9485,  0.6735, -1.3320,  6.1926,\n",
      "         0.6115,  3.1427], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.2861,  7.9740,  6.1770, -1.3431,  3.1184,  0.7725, -1.3431,  6.1770,\n",
      "         0.6462,  3.1184], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.004120268858969212\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.8778, 5.8081, 7.9140, 9.9728], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.3685, -0.3591,  1.8247, -0.3129, -2.1635, -0.3075, -1.3177,  4.5601,\n",
      "         3.0841,  4.5601], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3957, -0.4571,  1.8374, -0.3957, -2.1156, -0.3957, -1.3317,  4.5943,\n",
      "         3.1041,  4.5943], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0030360445380210876\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1494, -1.2760, -1.2282, -2.0628], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.3056, -0.3056, -0.3056,  3.0759,  0.6169, -1.2282,  9.9717,  0.6111,\n",
      "        -0.3056,  3.1571], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3884, -0.3884, -0.3884,  3.0900,  0.6166, -1.3302, 10.0000,  0.6166,\n",
      "        -0.3884,  3.0900], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00433284230530262\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1745, -0.4321, -0.3780, -1.3053], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.3780,  7.9732,  9.9715,  0.4913,  1.7941, -0.3640, -0.3285,  7.9732,\n",
      "         3.1423, -2.0653], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3745,  7.9743, 10.0000,  0.6620,  1.8281, -0.4354, -0.3745,  7.9743,\n",
      "         3.0697, -2.1247], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.004712226800620556\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3426,  0.7129, -0.3578, -0.3143], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 1.7980, -2.1466,  4.4980,  0.6798, -0.4089, -0.3578, -0.4089,  1.8543,\n",
      "         0.6058,  0.5504], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.8037, -2.1246,  4.6201,  0.6336, -0.4156, -0.3584, -0.4156,  1.8037,\n",
      "         0.6182,  0.7803], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.007321608252823353\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3753,  1.9677,  0.7493, -0.3746], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 1.8411,  1.9677, -0.4123,  7.9690,  7.9690, -0.3661, -1.2938,  1.9677,\n",
      "         3.1111, -0.3661], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.7814,  1.7814, -0.3935,  7.9725,  7.9725, -0.3510, -1.3507,  1.7814,\n",
      "         3.0482, -0.3510], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.008095575496554375\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4804, 3.1009, 1.7317, 0.7294], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.2851,  7.9641,  0.7178,  7.9641,  4.4992,  3.1009, -0.3838,  1.9387,\n",
      "        -2.0897,  3.1009], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3454,  7.9685,  0.7448,  7.9685,  4.6156,  3.0493, -0.3497,  1.7908,\n",
      "        -2.1242,  3.0493], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.004751558415591717\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7080, 3.0917, 4.5194, 1.8301], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.2948,  6.2256, -0.3728, -1.2948, -0.3728, -0.3633, -0.4222, -0.3728,\n",
      "         9.9578, -2.1127], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3269,  6.1614, -0.3547, -1.3269, -0.3547, -0.3547, -0.3719, -0.3547,\n",
      "        10.0000, -2.1355], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0012070810189470649\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1459, 4.6030, 4.6502, 6.2081], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.1659, -2.1393, -2.1393,  9.9529, -0.3663,  1.8668, -0.3663,  1.8281,\n",
      "        -0.3609, -1.2988], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.1487, -2.1487, -2.1487, 10.0000, -0.3621,  1.7987, -0.3621,  1.8021,\n",
      "        -0.3621, -1.3043], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0008067990420386195\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0710, 4.6041, 6.2369, 7.9507], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.3745, -2.1672,  0.6587,  0.7000,  7.9507, -0.3588, -1.3025, -0.3588,\n",
      "         1.8161,  3.0915], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3700, -2.1552,  0.6487,  0.6487,  7.9563, -0.3700, -1.2825, -0.3700,\n",
      "         1.8095,  3.1064], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0003847691114060581\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([8.0648, 5.8102, 7.9259, 9.9503], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 3.0923,  0.5843,  9.9503, -1.3040, -2.1906,  7.9259, -0.4156,  0.5843,\n",
      "        -0.3891, -2.1906], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.1220,  0.6421, 10.0000, -1.2660, -2.1569,  7.9553, -0.3672,  0.6421,\n",
      "        -0.3792, -2.1569], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00170408864505589\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.0894, -1.3156, -1.2951, -2.2025], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.3633, -1.2948,  0.6954, -0.3755, -1.3173, -0.4720,  0.5114, -0.4139,\n",
      "        -0.3633, -0.3964], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3855, -1.2556,  0.6013, -0.3671, -1.2479, -0.3671,  0.5566, -0.3671,\n",
      "        -0.3855, -0.3855], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.003159894375130534\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.0110, -0.4148, -0.2787, -1.2768], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.3303, -2.2074,  6.1730,  1.7630,  9.9559, -0.4148,  1.7882, -0.6324,\n",
      "         6.1730,  0.6030], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.2581, -2.1807,  6.1572,  1.8159, 10.0000, -0.3828,  1.8078, -0.3942,\n",
      "         6.1572,  0.6284], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.006997963879257441\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3058,  0.6506, -0.3955, -0.4036], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 3.1155,  6.1786, -0.3955, -2.1933, -1.3409, -1.2485, -0.3955, -1.3218,\n",
      "         1.8007,  0.7099], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.1433,  6.1655, -0.4144, -2.1896, -1.2738, -1.2447, -0.4144, -1.2447,\n",
      "         1.8039,  0.6206], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00201164442114532\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4048,  1.7584,  0.6316, -0.5745], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.5745, -0.4048, -0.4078, -0.4078,  9.9765,  0.6316, -1.3143,  3.1159,\n",
      "         0.6316,  9.9765], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4336, -0.4083, -0.4336, -0.4336, 10.0000,  0.5826, -1.2416,  3.1493,\n",
      "         0.5826, 10.0000], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0033525624312460423\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6778, 3.1291, 1.8544, 0.8095], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.3359,  3.1170,  1.6737, -1.2096, -0.4381, -0.3753,  0.6974,  4.6597,\n",
      "         4.5583, -0.4455], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3075,  3.1523,  1.8053, -1.2452, -0.4566, -0.4171,  0.5063,  4.5732,\n",
      "         4.5732, -0.4171], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.006774353329092264\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8895, 3.0986, 4.6007, 1.8959], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 7.9935, -0.4765, -1.3220,  3.2602,  4.6400,  3.0980,  4.6007,  0.6429,\n",
      "        -0.3476, -0.4765], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 8.0029, -0.4688, -1.2635,  3.1406,  4.5810,  3.1406,  4.5810,  0.5998,\n",
      "        -0.3827, -0.4688], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0026703495532274246\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2480, 4.5618, 4.6137, 6.2081], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.3103, -0.5100, -0.5100, -0.5100, -1.1882,  4.5894,  0.6358,  0.6269,\n",
      "         4.5894,  0.5812], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.2869, -0.4769, -0.4769, -0.4769, -1.2869,  4.5873,  0.6112,  0.5418,\n",
      "         4.5873,  0.6112], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0022345136385411024\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1756, 4.6032, 6.2292, 8.0159], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.2872, -0.5282, -0.5282,  3.1589, -0.5282,  0.5717,  8.0159, -1.1893,\n",
      "         3.0731,  0.5717], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3281, -0.4855, -0.4855,  3.1244, -0.4855,  0.6178,  8.0260, -1.3025,\n",
      "         3.1244,  0.6178], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0028150458820164204\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.1936,  5.8199,  7.9094, 10.0384], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.6661, -2.0178, -0.5230, 10.0384, -0.5230, -2.0178, -1.1971, -2.0178,\n",
      "        10.0384,  8.0244], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.6025, -2.1541, -0.4921, 10.0000, -0.4921, -2.1541, -1.3042, -2.1541,\n",
      "        10.0000,  8.0346], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.007615589536726475\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.9074, -1.2661, -1.2825, -2.0611], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.4312,  4.5883,  8.0211, -1.3618, -0.3701, -0.4879,  6.1984, -1.2347,\n",
      "        -1.3212, -0.3345], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4915,  4.5785,  8.0309, -1.3011, -0.4022, -0.4546,  6.2190, -1.3011,\n",
      "        -1.3011, -0.4915], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.003950140438973904\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.1612, -0.3613, -0.4912, -2.2191], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 4.6094,  3.0878, -0.5009, -0.5009, 10.0294,  3.0878, -1.2883,  4.5855,\n",
      "        -1.2929, -1.2470], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.5640,  3.1270, -0.4768, -0.4768, 10.0000,  3.1270, -1.3252,  4.5640,\n",
      "        -1.3252, -1.3252], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0016129299765452743\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4777,  0.6682,  0.5752, -1.3182], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 1.8044, 10.0241, -0.3215,  0.5926, -2.1526, -0.4459, -1.2420,  1.7942,\n",
      "        -1.2420,  8.0115], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.8548, 10.0000, -0.4491,  0.6240, -2.1178, -0.4491, -1.3175,  1.7856,\n",
      "        -1.3175,  8.0217], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0033206294756382704\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.7882,  0.6464,  1.8099, -0.3829], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 1.8175,  4.5750,  6.2283, -0.1193, -1.9334, -2.2031,  1.8055, -0.4782,\n",
      "        -1.3912, -1.3912], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.7916,  4.5456,  6.2065, -0.3970, -2.1223, -2.1223,  1.7916, -0.4401,\n",
      "        -1.3575, -1.3575], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01252767164260149\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.8367, 1.8289, 3.1076, 0.5699], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.4264,  1.8280,  8.0094, -0.4618, -0.4618, -0.4618, -0.4618,  1.8289,\n",
      "        -0.4618, -2.2349], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3757,  1.7968,  8.0177, -0.4330, -0.4330, -0.4330, -0.4330,  1.7968,\n",
      "        -0.4330, -2.1108], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.002418891992419958\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8221, 3.1202, 4.5639, 1.8692], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.4402, -1.3220,  0.5795, 10.0233, -0.4402,  6.1641, 10.0233, -0.4402,\n",
      "        -1.4465,  0.8057], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4222, -1.3839,  0.6245, 10.0000, -0.4222,  6.2132, 10.0000, -0.4222,\n",
      "        -1.3839,  0.6387], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0042135813273489475\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.0944, 4.5883, 4.5789, 6.1730], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 3.0944, -1.4541,  4.5883,  6.1730, -0.4133, -1.3355, 10.0260, -2.1289,\n",
      "        -2.2520, -0.4179], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.1126, -1.3761,  4.5557,  6.2170, -0.4171, -1.3761, 10.0000, -2.1118,\n",
      "        -2.1118, -0.4171], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0031697750091552734\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0867, 4.6046, 6.2342, 8.0238], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.2453, 10.0297,  0.7028,  3.1831,  6.1922,  6.1922, -1.3293, -0.3910,\n",
      "        -0.3910,  3.1494], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.2460, 10.0000,  0.6955,  3.1151,  6.2214,  6.2214, -1.3685, -0.4176,\n",
      "        -0.4176,  3.1151], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.001138459425419569\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.0604,  5.8145,  7.9178, 10.0310], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.3745, -0.4541, -2.2390,  1.9090,  3.1578,  3.1025, -0.3745, -1.2580,\n",
      "         0.6142, -0.2726], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4193, -0.4061, -2.1322,  1.8420,  3.1216,  3.1216, -0.4193, -1.2453,\n",
      "         0.5992, -0.3346], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0028101704083383083\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2414, -1.2690, -1.3329, -2.2234], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.4562,  0.6432,  8.0281,  7.9188, -1.3329, -0.3962,  1.9197, -0.3816,\n",
      "        10.0332,  0.6317], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4095,  0.5906,  8.0299,  8.0299, -1.3566, -0.4211,  1.8455, -0.4211,\n",
      "        10.0000,  0.5906], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0028326876927167177\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4092, -0.2816, -0.3811, -2.1200], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.4007, -0.3941, -1.3722, -0.3676, -0.3676,  4.5863, -2.2169,  0.7516,\n",
      "        -1.4007, -1.3256], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3547, -0.4233, -1.3547, -0.4233, -0.4233,  4.6217, -2.1511,  0.7286,\n",
      "        -1.3547, -1.2535], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.002290308242663741\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.7450,  0.7538,  0.6609, -1.1816], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.3821, -0.3784,  4.5880, -0.3784, -0.3784, -1.3821, -0.2373, -2.1999,\n",
      "         0.6395, -2.2829], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3655, -0.4245,  4.6328, -0.4245, -0.4245, -1.3655, -0.3216, -2.1613,\n",
      "         0.5849, -2.1613], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0035306396894156933\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.5174,  0.6878,  1.8973, -0.2362], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.3561,  3.1616, -1.3561, -0.4013, 10.0200, -0.4013, -0.4013,  0.6878,\n",
      "        -0.4013, -0.4013], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3736,  3.1281, -1.3736, -0.4284, 10.0000, -0.4284, -0.4284,  0.7076,\n",
      "        -0.4284, -0.4284], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0006201762007549405\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5589, 1.8186, 3.1486, 0.6732], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 6.2663, -1.3531,  3.1486, -2.1895, -1.3366, -0.4336, -0.4336, -0.4336,\n",
      "         4.5825,  3.1486], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 6.2149, -1.2761,  3.1243, -2.1907, -1.3878, -0.4288, -0.4288, -0.4288,\n",
      "         4.6397,  3.1243], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.001569990417920053\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6211, 3.1107, 4.5875, 1.9582], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 8.0122, -1.3199, -0.3785,  6.2596,  4.5800, -2.1908,  4.5668,  8.0122,\n",
      "        -1.3199, -0.4583], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 8.0049, -1.3926, -0.4321,  6.2109,  4.6336, -2.2058,  4.6336,  8.0049,\n",
      "        -1.3926, -0.4321], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.002417495707049966\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.9558, 4.5674, 4.5766, 6.2388], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 1.7624, -2.2062, -1.4961, -0.4353,  1.8101,  9.9943,  3.1349, -1.1226,\n",
      "         1.9579,  4.5974], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.8214, -2.2165, -1.2963, -0.4231,  1.8214, 10.0000,  3.1377, -1.2963,\n",
      "         1.8452,  4.6149], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00869756005704403\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.9705, 4.5934, 6.2437, 7.9908], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 6.2122, -1.4724, -0.3248, -1.4724, -1.3328,  1.7998, -0.4894, -2.2303,\n",
      "        -1.3328,  3.1243], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 6.1917, -1.2923, -0.2945, -1.2923, -1.3835,  1.8529, -0.4321, -2.2228,\n",
      "        -1.3835,  3.1400], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.007770826108753681\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.9723, 5.8139, 7.9489, 9.9588], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.2526,  4.6074,  4.6074, -1.4766, -2.3072, -0.4252, -0.3365,  0.6329,\n",
      "        -0.6621,  1.7679], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2435,  4.5688,  4.5688, -1.3826, -2.2435, -0.4407, -0.3082,  0.5855,\n",
      "        -0.4304,  1.8517], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.007994437590241432\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2414, -1.4183, -1.4968, -2.2683], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.4183, -0.4936,  6.1738,  6.1738,  1.7683, -0.4936,  9.9421, -1.4968,\n",
      "        -0.4936,  4.5536], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3231, -0.4535,  6.1656,  6.1656,  1.8401, -0.4535, 10.0000, -1.3914,\n",
      "        -0.4535,  4.5564], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0033652689307928085\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.2196, -0.3842, -0.4180, -2.1325], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 9.9317, -1.3934,  4.6088, -0.4638, -0.4808,  0.5915,  0.5611,  0.6046,\n",
      "         0.6756,  0.5611], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([10.0000, -1.3974,  4.5445, -0.4646, -0.4646,  0.5943,  0.5600,  0.6464,\n",
      "         0.6464,  0.5600], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.001167868496850133\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5778,  0.6925,  0.5990, -1.2860], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.5122,  4.6069, -0.4692, -0.4061, -2.2507,  9.9286, -0.4061,  3.1285,\n",
      "         7.9497,  6.1563], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5089,  4.5407, -0.4733, -0.3768, -2.3055, 10.0000, -0.3768,  3.1462,\n",
      "         7.9357,  6.1547], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0014746271772310138\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.7222,  0.5761,  1.8288, -0.3789], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.4066,  0.6725,  4.5983,  1.8288, -1.2830, -0.4656, -0.4613,  0.5088,\n",
      "         3.1204,  1.7817], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5160,  0.6459,  4.5433,  1.7897, -1.4115, -0.4780, -0.4780,  0.5443,\n",
      "         3.1385,  1.8084], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.003646926488727331\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7686, 1.7044, 3.1006, 0.5267], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 7.9532,  7.9532,  9.9364, -0.5363, -0.4569,  3.1006,  6.1667, -2.2398,\n",
      "         6.1668,  7.9532], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 7.9428,  7.9428, 10.0000, -0.5068, -0.4861,  3.1315,  6.1579, -2.2850,\n",
      "         6.1579,  7.9428], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0009250285802409053\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8093, 3.0449, 4.5826, 1.8042], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.4852,  1.7900,  6.1735, -0.5373, -2.2378,  3.1036, -2.2378,  1.8240,\n",
      "         1.6993,  9.9451], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4911,  1.7984,  6.1621, -0.5003, -2.2690,  3.1244, -2.2690,  1.7984,\n",
      "         1.7933, 10.0000], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0016486013773828745\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1509, 4.5464, 4.6187, 6.1786], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.4389, -0.4316,  0.6349, -0.4530, -0.4958, -0.4530,  7.9640, -1.4732,\n",
      "        -0.4530, -0.4530], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3827, -0.4286,  0.6508, -0.4902, -0.4902, -0.4902,  7.9602, -1.3827,\n",
      "        -0.4902, -0.4902], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.001721037901006639\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1158, 4.5992, 6.2796, 7.9707], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.5043,  1.8144, -0.4706, -1.9093,  1.8144, -0.5043,  9.9669,  0.6709,\n",
      "        -0.4706, -0.4706], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4850,  1.8033, -0.4850, -2.2595,  1.8033, -0.4850, 10.0000,  0.6329,\n",
      "        -0.4850, -0.4850], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.012677629478275776\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([8.0960, 5.8310, 7.9723, 9.9801], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.4777, -0.4132,  1.7251, -0.5093,  1.8189, -1.3843, -0.4777,  0.6399,\n",
      "         4.5412, -0.4777], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4787, -0.4241,  1.7838, -0.4787,  1.7838, -1.3935, -0.4787,  0.6370,\n",
      "         4.5732, -0.4787], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0006861662259325385\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.9444, -1.4199, -1.3733, -2.2341], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.5118,  1.7401, -1.4430, -1.4430, -1.4430, -0.3913,  4.5316,  7.9868,\n",
      "        -0.4845,  0.6762], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4717,  1.7838, -1.3934, -1.3934, -1.3934, -0.4717,  4.5768,  7.9925,\n",
      "        -0.4717,  0.6579], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0019936992321163416\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9887, -0.4786, -0.4434, -1.4332], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.5596,  3.1420, -0.4911, -1.3708,  4.6047, -0.4911,  3.0884, -0.4911,\n",
      "        -1.4332,  1.7504], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.6683,  3.0694, -0.4581, -1.3990,  4.5782, -0.4581,  3.0694, -0.4581,\n",
      "        -1.3990,  1.7796], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0024234591983258724\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2843,  0.6119, -0.4854, -0.4910], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 1.7959, -1.2843,  0.5488, -2.2191,  0.6618, -0.4654,  1.7608, -2.2191,\n",
      "         6.1996, 10.0128], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.8343, -1.3935,  0.5847, -2.2260,  0.6703, -0.4815,  1.7807, -2.2260,\n",
      "         6.2014, 10.0000], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0015675763133913279\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3528,  1.8573,  0.6635, -0.3980], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.4162,  8.0085, -1.3863,  4.5278,  8.0085,  0.6639,  4.6025, -2.0051,\n",
      "        -0.3725,  1.7786], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4464,  8.0198, -1.2735,  4.5808,  8.0198,  0.6264,  4.5808, -2.2118,\n",
      "        -0.4025,  1.7859], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0062248678877949715\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5753, 3.1636, 1.8133, 0.8010], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.4714, -1.3781,  1.7824,  0.5395,  3.1051,  8.0153, -1.3934, -0.4714,\n",
      "         3.1051, -0.4714], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4376, -1.3670,  1.7945,  0.6042,  3.0836,  8.0269, -1.3670, -0.4376,\n",
      "         3.0836, -0.4376], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0009634837624616921\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7419, 3.0958, 4.5516, 1.7940], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 8.0225,  8.0225,  4.6559, -0.4536,  3.1684, -0.4536,  1.7876,  3.1684,\n",
      "         8.0225,  3.1684], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 8.0337,  8.0337,  4.5777, -0.4331,  3.0964, -0.4331,  1.8037,  3.0964,\n",
      "         8.0337,  3.0964], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0023110373876988888\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.0425, 4.6116, 4.6304, 6.2045], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([10.0462, -2.1155, -0.2576,  0.6809,  3.1551, -0.4289,  6.2045, -2.1155,\n",
      "        -0.4289, -0.4289], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([10.0000, -2.0797, -0.3758,  0.6797,  3.1246, -0.4362,  6.2276, -2.0797,\n",
      "        -0.4362, -0.4362], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0020307046361267567\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0531, 4.6457, 6.3139, 8.0362], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.0818,  8.0301,  0.5553, -1.2887, -0.4113, 10.0517, -0.4522, -1.3738,\n",
      "        10.0517, -2.0818], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.0427,  8.0465,  0.6019, -1.2280, -0.4394, 10.0000, -0.4394, -1.3235,\n",
      "        10.0000, -2.0427], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00180064607411623\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.0216,  5.8695,  8.0254, 10.0516], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.4050,  1.8464,  1.8944, -2.1481,  8.0384,  4.6511,  3.1547, -0.4050,\n",
      "         3.1112, -1.3589], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4406,  1.8170,  1.8393, -2.0153,  8.0464,  4.5975,  3.1660, -0.4406,\n",
      "         3.1660, -1.3208], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.003159563522785902\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.0396, -1.2511, -1.1298, -2.0069], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.0069, -0.4153, -0.3984, -1.1298,  0.6235, -2.0069, -2.0396,  3.1524,\n",
      "         1.7999,  6.2286], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.0169, -0.4388, -0.3872, -1.3246,  0.6590, -2.0169, -2.0169,  3.1713,\n",
      "         1.8372,  6.2372], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.004240061622112989\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1361, -0.4050, -0.3540, -1.2965], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.4050,  0.6290, -1.1570, -1.2491, -1.2965, -2.1361, -0.4050,  3.1186,\n",
      "        -0.3540, -1.2965], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3081,  0.6563, -1.3186, -1.2838, -1.3186, -2.0413, -0.3081,  3.1733,\n",
      "        -0.4339, -1.3186], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.006620677653700113\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4065,  0.6492, -0.4488, -0.4034], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.4488, -0.3439,  3.1286, -1.0897, -0.3567, -1.2460, -1.2460, -0.3567,\n",
      "        -0.3564, -0.4488], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4157, -0.4157,  3.1633, -1.3208, -0.3331, -1.3095, -1.3095, -0.3331,\n",
      "        -0.3331, -0.4157], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.007168313022702932\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4871,  1.8490,  0.6374, -0.3921], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.3915,  4.6200,  6.2399, -1.2153,  0.6374,  3.1222, -1.2536, -0.3921,\n",
      "        10.0498, -2.0945], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3947,  4.6159,  6.2391, -1.2820,  0.6641,  3.1580, -1.3254, -0.3947,\n",
      "        10.0000, -2.1282], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0015253729652613401\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4633, 3.1319, 1.7800, 0.8005], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.0965, -0.4544,  3.1319, 10.0434, -0.4544, -2.0965, -0.4544, -1.1652,\n",
      "         0.7249,  1.8014], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.1313, -0.3768,  3.1529, 10.0000, -0.3768, -2.1313, -0.3768, -1.3239,\n",
      "         0.6212,  1.8087], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.005881762597709894\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7192, 3.1702, 4.6159, 1.8928], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 4.5854,  3.1294, -2.1375,  4.6159, -2.1375, -0.5285, -2.0982, -1.2616,\n",
      "         0.6633,  4.6159], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.6040,  3.1543, -2.1355,  4.6040, -2.1355, -0.4030, -2.1355, -1.3015,\n",
      "         0.6191,  4.6040], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.002193596214056015\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.9351, 4.6250, 4.5867, 6.2200], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([10.0251, -1.1775,  3.1318,  6.2200,  6.2200,  1.8530,  4.6150, 10.0251,\n",
      "         6.2200,  0.7244], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([10.0000, -1.2022,  3.1535,  6.2243,  6.2243,  1.8186,  4.5980, 10.0000,\n",
      "         6.2243,  0.6363], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0011611831141635776\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.9902, 4.6468, 6.2816, 8.0188], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.3008,  0.6196, -2.3158, -0.2975, -1.1742, -1.2850, -0.4043,  1.8344,\n",
      "         6.2816,  4.6468], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3591,  0.6165, -2.1565, -0.4424, -1.1896, -1.2678, -0.3507,  1.8317,\n",
      "         6.2169,  4.5926], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.006030684802681208\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.9559,  5.8608,  7.9757, 10.0056], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 4.6069,  0.6978,  1.8914, -1.1694, -2.0770, -0.3929,  0.5606, -0.2431,\n",
      "        -1.1694,  1.8453], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.5935,  0.6585,  1.8336, -1.1820, -2.1731, -0.3499,  0.6608, -0.3499,\n",
      "        -1.1820,  1.8137], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.003892711829394102\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3373, -1.3123, -1.4324, -2.2591], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.2591,  0.5916, -2.2591, -0.3730,  1.8548,  1.7865,  4.6037, -1.1788,\n",
      "        -0.3730,  6.2143], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.1811,  0.6079, -2.1811, -0.3444,  1.8391,  1.8391,  4.5928, -1.1792,\n",
      "        -0.3444,  6.2059], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0017281284090131521\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.1476, -0.3313, -0.3166, -2.1150], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.3262,  1.8651, -0.3505, -0.3166,  9.9920,  6.2188,  0.7295, -0.3505,\n",
      "         0.6764, -1.1784], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.2849,  1.8430, -0.3435, -0.4804, 10.0000,  6.2028,  0.6396, -0.3435,\n",
      "         0.6786, -1.1831], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0037539154291152954\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3731,  0.5789,  0.5396, -1.4597], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 8.0004,  6.2665,  4.5918,  6.2204,  6.2204,  0.5657,  4.5918, -0.7955,\n",
      "        -0.5369,  0.7309], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 7.9887,  6.2004,  4.5984,  6.2004,  6.2004,  0.6759,  4.5984, -0.3969,\n",
      "        -0.4790,  0.6374], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.018853282555937767\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4670,  1.7684,  0.9453, -0.4573], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.3289,  6.2190,  4.5825, -0.3429, -2.2589,  7.9957,  7.9957,  9.9810,\n",
      "         0.5973,  0.5303], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3595,  6.1961,  4.5971, -0.3595, -2.2202,  7.9829,  7.9829, 10.0000,\n",
      "         0.5916,  0.6291], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0013938758056610823\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7100, 1.8483, 3.1470, 0.5959], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.3383,  4.5793, -0.3277,  7.9425, -1.4570, -2.2584, -2.2378, -0.3383,\n",
      "         4.5787, -0.3383], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3770,  4.5950, -0.3770,  7.9791, -1.2145, -2.2417, -2.2417, -0.3770,\n",
      "         4.5950, -0.3770], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.006787390913814306\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7779, 3.0914, 4.5829, 1.8142], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.3297,  4.5607, -0.4555,  1.8283, -0.3297,  6.2127, -2.2322, -0.3297,\n",
      "        -1.4211, -0.3319], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3963,  4.5914, -0.4674,  1.8386, -0.3963,  6.1889, -2.2511, -0.3963,\n",
      "        -1.2142, -0.3963], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.006237880326807499\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.0397, 4.5532, 4.5782, 6.2023], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.5877, -2.1954,  0.6600,  1.7298,  9.9635,  6.2023, -0.3399,  4.5793,\n",
      "        -0.3399,  1.7756], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.5568, -2.2360,  0.5980,  1.8371, 10.0000,  6.1837, -0.4060,  4.5820,\n",
      "        -0.4060,  1.7610], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0028596720658242702\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0708, 4.5816, 6.2346, 7.9768], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.2916,  1.8262,  1.7417, -0.3678, -1.2446, -1.3524, -0.3678,  0.8001,\n",
      "        -0.3678, -0.4558], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.2654,  1.8198,  1.8198, -0.4104, -1.2654, -1.2654, -0.4104,  0.6732,\n",
      "        -0.4104, -0.5078], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0039107282646000385\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([8.0660, 5.7936, 7.9039, 9.9523], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.6396, -1.2829, -0.4004, -0.3317,  0.6340, -0.3968, -0.4856, -0.4004,\n",
      "        -0.4004, -0.3375], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.6129, -1.2985, -0.4078, -0.5144,  0.6655, -0.4078, -0.5144, -0.4078,\n",
      "        -0.4078, -0.4078], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.004137279465794563\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.9285, -1.2829, -1.3029, -2.1166], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 1.8407, -0.4287, -2.3357,  1.7719,  0.8122,  4.5399, -0.4180,  0.5806,\n",
      "        -0.4287, -0.3618], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.7777, -0.4053, -2.1546,  1.7782,  0.6567,  4.5467, -0.4053,  0.6567,\n",
      "        -0.4053, -0.5009], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0087440749630332\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.0294, -0.4941, -0.5217, -2.3292], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 6.1566,  1.8017,  6.1566, -2.0554, -0.4292,  9.9460, -1.3879,  9.9460,\n",
      "        -2.0957, -2.0957], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 6.1711,  1.7608,  6.1711, -2.1532, -0.4090, 10.0000, -1.3663, 10.0000,\n",
      "        -2.1532, -2.1532], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.002498326124623418\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4805,  0.5858,  0.5189, -1.4798], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 7.9707,  0.5938,  0.6257, -1.4416, -0.4628,  6.1540,  1.8097,  9.9513,\n",
      "         4.5277, -1.2721], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 7.9561,  0.6288,  0.6288, -1.3984, -0.4065,  6.1736,  1.7863, 10.0000,\n",
      "         4.5386, -1.3984], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0025864250492304564\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.8134,  0.6532,  1.8328, -0.3974], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 7.9750,  4.5053,  3.1023, -1.4782, -2.0769,  9.9597, -1.4782, -0.4439,\n",
      "         1.8166, -0.4775], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 7.9637,  4.5393,  3.0709, -1.4189, -2.1297, 10.0000, -1.4189, -0.4026,\n",
      "         1.7936, -0.4026], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0021573451813310385\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7427, 1.7912, 3.0330, 0.4843], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 4.5256, -1.4994,  9.9707, -0.4797, -0.4797,  0.4843,  0.6337,  3.1079,\n",
      "        -2.0718, -1.2550], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.5428, -1.4427, 10.0000, -0.4037, -0.4037,  0.6491,  0.6491,  3.0730,\n",
      "        -2.1295, -1.4304], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.007861947640776634\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8154, 3.0969, 4.5386, 1.7941], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 7.9906, -0.4276,  4.5386, -0.4665,  3.0469, -0.4665, -0.4665, -0.4665,\n",
      "         4.5386,  1.8174], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 7.9872, -0.4144,  4.5507, -0.4144,  3.0847, -0.4144, -0.4144, -0.4144,\n",
      "         4.5507,  1.7422], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0018426710739731789\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.0429, 4.5442, 4.5354, 6.1753], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 3.0429, -1.2987,  4.5609, -0.4091, -1.2555,  0.6568,  0.5650,  1.7740,\n",
      "        -0.4903,  1.7910], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.1048, -1.5165,  4.5577, -0.4311, -1.5165,  0.6119,  0.6866,  1.7712,\n",
      "        -0.4893,  1.7780], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.013682986609637737\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0629, 4.5831, 6.2147, 8.0021], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.4268, -0.4268, -0.5838,  4.5593, -1.0190, -0.4268,  3.0898, -2.1242,\n",
      "         4.5593, -2.1242], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4273, -0.4273, -0.4273,  4.5568, -1.4287, -0.4273,  3.1033, -2.1678,\n",
      "         4.5568, -2.1678], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.019640127196907997\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.0511,  5.8106,  7.9179, 10.0154], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 8.0066, -0.4221,  0.6107,  3.0982, -1.4970, -0.4589, -1.3368, -2.1702,\n",
      "         3.1009,  3.0982], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 8.0139, -0.4222,  0.6675,  3.1049, -1.4722, -0.4222, -1.4001, -2.1924,\n",
      "         3.1049,  3.1049], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.000985021353699267\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.0798, -1.3432, -1.3555, -2.2170], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.3432,  1.8409, -1.3695, -2.2170,  1.8091, -2.2170, -1.4831, -0.4582,\n",
      "        -1.3555,  3.1118], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3715,  1.7979, -1.4442, -2.2089,  1.8006, -2.2089, -1.4442, -0.4195,\n",
      "        -1.4442,  3.1044], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0019348098430782557\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.1631, -0.3812, -0.4931, -2.2553], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 1.8250,  1.8250, -0.4313, -2.2668,  4.5574, -0.4313, 10.0313, 10.0313,\n",
      "        -0.4313, -0.4313], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.8115,  1.8115, -0.4200, -2.2186,  4.5534, -0.4200, 10.0000, 10.0000,\n",
      "        -0.4200, -0.4200], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0005169009673409164\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5238,  0.6935,  0.6281, -1.3725], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 3.1329,  4.5552,  8.0186,  1.8324, -1.3687,  8.0186,  1.8029,  3.1250,\n",
      "         1.8768, -1.4333], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.0997,  4.5531,  8.0316,  1.8196, -1.3217,  8.0316,  1.8125,  3.0997,\n",
      "         1.8125, -1.3813], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0011391262523829937\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.5844,  0.6597,  1.7923, -0.4397], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.4056, -1.4056,  0.7059, -0.4404, -0.4404,  1.8188,  4.5543, -0.3441,\n",
      "         4.5595, -0.4407], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3608, -1.3608,  0.6291, -0.4314, -0.4314,  1.8226,  4.5547, -0.3624,\n",
      "         4.5547, -0.4314], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.001053704647347331\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6147, 1.8874, 3.1442, 0.6033], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.3561, -1.4057, -0.4571, -1.3713,  1.7991, -2.3561, 10.0440,  8.0271,\n",
      "        -2.3561, -0.4413], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2651, -1.3056, -0.4428, -1.3477,  1.8230, -2.2651, 10.0000,  8.0396,\n",
      "        -2.2651, -0.4428], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.003825231920927763\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6746, 3.1218, 4.5717, 1.8443], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.4500,  8.0351,  0.6690,  0.7138, 10.0504,  0.6690, -2.3277,  6.1917,\n",
      "         1.8235, 10.0504], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3682,  8.0453,  0.6412,  0.6026, 10.0000,  0.6412, -2.2872,  6.2315,\n",
      "         1.8176, 10.0000], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0029042765963822603\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.9407, 4.5561, 4.5732, 6.2047], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 3.1643, -0.3753, -0.4360,  0.6780, -0.4358,  0.5741, -1.2784, -1.2784,\n",
      "        -1.2784,  0.6570], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.1195, -0.3742, -0.4833,  0.6319, -0.4833,  0.6319, -1.3544, -1.3544,\n",
      "        -1.3544,  0.6038], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0032096474897116423\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.9758, 4.5740, 6.2345, 8.0342], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.8144, -1.4937,  4.5779,  6.2345, -0.5140,  4.5779,  1.8579,  4.5730,\n",
      "         1.7835,  1.8579], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.6285, -1.3502,  4.5869,  6.2308, -0.3879,  4.5869,  1.8461,  4.5869,\n",
      "         1.8461,  1.8461], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.007561950944364071\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.9661,  5.8006,  7.9457, 10.0348], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([10.0348, -1.5421, -2.2708,  4.5849,  1.8595,  3.1104, -1.2665,  1.7777,\n",
      "         0.6215, -0.4474], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([10.0000, -1.3522, -2.3246,  4.5827,  1.7993,  3.1264, -1.3522,  1.8489,\n",
      "         0.5999, -0.4990], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.005960510112345219\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.4016, -1.4712, -1.4598, -2.2698], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.2698,  1.8227, -2.2698, -2.2698, -1.3998, -2.2698,  6.1902, -0.4309,\n",
      "         3.1566, -2.2698], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.3138,  1.7973, -2.3138, -2.3138, -1.3653, -2.3138,  6.2153, -0.4952,\n",
      "         3.1228, -2.3138], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0017411645967513323\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.3049, -0.4184, -0.4141, -1.3335], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.3335, -0.4211,  0.5933,  8.0033, -0.4211, -0.4211, -0.4211,  3.1065,\n",
      "        -1.3335, -0.4211], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3727, -0.4929,  0.6128,  8.0032, -0.4929, -0.4929, -0.4929,  3.1230,\n",
      "        -1.3727, -0.4929], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0029473924078047276\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.5321,  0.5757, -0.4405, -0.4936], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.4031, -1.3766, -0.4443,  4.5617, -1.4623,  7.9891, -0.4936,  6.1570,\n",
      "         0.6918, -1.3766], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4819, -1.3764, -0.4546,  4.5413, -1.3181,  7.9885, -0.4819,  6.1902,\n",
      "         0.6591, -1.3764], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0029858043417334557\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4499,  1.8549,  0.6763, -0.4276], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.4148, -2.3872,  0.5955,  9.9722, -0.4542,  7.9755, -1.4148,  3.1144,\n",
      "         1.8077, -2.3299], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3699, -2.2914,  0.6694, 10.0000, -0.4640,  7.9750, -1.3699,  3.1040,\n",
      "         1.8030, -2.2914], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0021149602252990007\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4423, 3.1255, 1.7529, 0.6651], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 9.9619,  9.9619,  0.5984,  0.5656,  3.1255,  7.9658, -0.3383, -2.3587,\n",
      "        -0.4130,  7.9658], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([10.0000, 10.0000,  0.6494,  0.6716,  3.0976,  7.9657, -0.3603, -2.2785,\n",
      "        -0.4614,  7.9657], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.002676511649042368\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6476, 3.1158, 4.5546, 1.7534], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 3.0956, -0.4571,  4.5546,  4.5546,  0.7109,  7.9605,  3.0956,  1.8125,\n",
      "         1.8504,  1.8504], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.0991, -0.4416,  4.5215,  4.5215,  0.6312,  7.9621,  3.0991,  1.7860,\n",
      "         1.8099,  1.8099], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0012778362724930048\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.9880, 4.5815, 4.5715, 6.1396], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.4383, -2.3156,  6.1396,  6.1396, -0.4528,  9.9559, -0.4890,  1.8307,\n",
      "         1.8239, -0.4528], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4597, -2.2414,  6.1615,  6.1615, -0.4385, 10.0000, -0.4385,  1.7842,\n",
      "         1.7842, -0.4385], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0015557289589196444\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.9959, 4.5783, 6.2080, 7.9573], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.4856,  1.8222, -0.5176,  1.8164,  9.9590, -2.2974,  7.9573,  6.1532,\n",
      "         1.8164, -1.4206], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4408,  1.7886, -0.4408,  1.7886, 10.0000, -2.2228,  7.9631,  6.1616,\n",
      "         1.7886, -1.3283], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0026455377228558064\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.9948, 5.8058, 7.9179, 9.9671], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 4.5601, -0.5047, -0.4754,  6.2122,  3.0900, -1.3276,  3.1104,  7.9620,\n",
      "         3.0255, -0.5047], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.5558, -0.4474, -0.4564,  6.1658,  3.1041, -1.3476,  3.1041,  7.9704,\n",
      "         3.1041, -0.4474], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0015968149527907372\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1650, -1.4231, -1.3034, -2.2200], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.6340,  6.1985, -0.4820,  9.9768, -1.4583, -0.4334,  7.9690, -1.3034,\n",
      "        -0.4334, -1.4583], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.6140,  6.1721, -0.4532, 10.0000, -1.4295, -0.4532,  7.9791, -1.4295,\n",
      "        -0.4532, -1.4295], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0020913551561534405\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2496, -0.4999, -0.4719, -1.4340], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.4816,  0.6077, -0.4588,  1.7842, -2.1944, -1.4180, -0.4588, -2.1944,\n",
      "        -1.3017, -1.4340], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4574,  0.6636, -0.4574,  1.8043, -2.1715, -1.4247, -0.4574, -2.1715,\n",
      "        -1.4247, -1.4247], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0020438837818801403\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4049,  0.5989, -0.4395, -0.4374], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.1784,  9.9984,  3.0966, -1.4000,  0.6148, -0.4641, -1.4000, -0.4395,\n",
      "         9.9984,  3.0833], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.1881, 10.0000,  3.0873, -1.4177,  0.5955, -0.4610, -1.4177, -0.4610,\n",
      "        10.0000,  3.0873], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0001671701029408723\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4086,  1.7663,  0.6041, -0.4632], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 1.8311, -2.1669, -2.2006, -0.4465,  3.1059,  0.5586, -1.3727,  4.5329,\n",
      "        10.0078, -0.4465], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.7953, -2.2028, -2.2028, -0.4634,  3.0796,  0.5909, -1.4110,  4.6356,\n",
      "        10.0000, -0.4634], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.001696653664112091\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5176, 3.0787, 1.7361, 0.6867], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 7.9972, -0.4505, -2.1651, -0.4505, -0.4505,  0.5592, -1.3475, -1.4378,\n",
      "        -0.4505, -0.4615], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 8.0108, -0.4647, -2.2127, -0.4647, -0.4647,  0.5847, -1.4012, -1.4012,\n",
      "        -0.4647, -0.4228], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.000963590748142451\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7094, 3.0602, 4.5367, 1.7884], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.4242, -1.3396, -1.4748,  1.8156,  1.8156,  0.5671, -1.4748, -0.4605,\n",
      "         6.2697, -0.4605], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3956, -1.3956, -1.4170,  1.7946,  1.7946,  0.5805, -1.4170, -0.4631,\n",
      "         6.1998, -0.4631], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0016586066922172904\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1347, 4.5612, 4.5900, 6.2665], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 4.5616, -0.4691, -1.3789, -2.1741, -1.3315,  3.0799, -1.3770, -0.4691,\n",
      "        -0.4352, -1.3315], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.6399, -0.4601, -1.3917, -2.2410, -1.3917,  3.0836, -1.3917, -0.4601,\n",
      "        -0.4601, -1.3917], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0019024250796064734\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0532, 4.5687, 6.1886, 8.0039], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.5761,  0.6040,  3.1349,  4.5870, 10.0164,  1.7504, -0.4238,  1.7221,\n",
      "        -0.4696, -1.3392], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.5835,  0.5835,  3.0871,  4.6262, 10.0000,  1.8004, -0.4564,  1.7729,\n",
      "        -0.4564, -1.3814], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0012664112728089094\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.0555,  5.8062,  7.8911, 10.0125], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 6.2320,  1.7982, -0.4119,  1.7525, -1.3533, -0.4660,  6.2320, -1.3533,\n",
      "         4.5876,  1.7525], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 6.2021,  1.8057, -0.4523,  1.8057, -1.3718, -0.4523,  6.2021, -1.3718,\n",
      "         4.6088,  1.8057], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0010442828061059117\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1127, -1.4566, -1.3831, -2.2344], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 1.7660, -2.1127, -1.4566,  1.7660,  6.2063, -0.4650,  3.1178, -1.4566,\n",
      "        -0.4650,  1.7963], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.8025, -2.2448, -1.4268,  1.8025,  6.2006, -0.4456,  3.0993, -1.4268,\n",
      "        -0.4456,  1.8025], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0023029600270092487\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1551, -0.4420, -0.3905, -1.3887], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.3714, -1.4415, -2.1303, -1.3714,  1.8008, -0.4321,  0.5555,  3.0885,\n",
      "        -0.4729, -1.4415], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3515, -1.4256, -2.2342, -1.3515,  1.7994, -0.4374,  0.6055,  3.1059,\n",
      "        -0.4397, -1.4256], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0016030616825446486\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3964,  0.6337, -0.4466, -0.4381], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 4.5709, -1.4219, -0.4466,  4.5709, -2.2490, -0.4260,  6.1640, -0.4466,\n",
      "         3.1145, -1.4044], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.5476, -1.4233, -0.4297,  4.5476, -2.2123, -0.3908,  6.2027, -0.4297,\n",
      "         3.1139, -1.3378], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0010183874983340502\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4354,  1.7848,  0.5970, -0.5055], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 3.1018,  0.6273, -0.4375,  0.5970,  8.0063,  6.1570,  6.1570,  4.6069,\n",
      "        -1.4100, -1.4100], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.1154,  0.6253, -0.4233,  0.6063,  8.0040,  6.2057,  6.2057,  4.5413,\n",
      "        -1.3305, -1.3305], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.002215995453298092\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4878, 3.1120, 1.7519, 0.6548], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 1.8100,  0.6341, 10.0094, -1.3906, -1.3906,  1.8087, -0.3653, -1.4127,\n",
      "        -1.3219, -0.3653], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.7972,  0.6290, 10.0000, -1.4222, -1.4222,  1.7972, -0.4175, -1.4222,\n",
      "        -1.3287, -0.4175], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0008008527802303433\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6760, 3.1125, 4.5664, 1.7822], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 4.5894,  8.0124, -0.4387, 10.0128, -2.2114,  1.7972, 10.0128, -2.3388,\n",
      "        -0.4387, -1.3273], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.5607,  8.0115, -0.4064, 10.0000, -2.1946,  1.8062, 10.0000, -2.1946,\n",
      "        -0.4064, -1.3422], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.002461842494085431\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.0340, 4.5976, 4.5823, 6.1912], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.5895,  0.6381, -1.4220,  0.6826, -1.3867,  3.1219, -2.2071,  0.5895,\n",
      "        -0.3938, -0.4416], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.6215,  0.6071, -1.3598,  0.6485, -1.4077,  3.1029, -2.2183,  0.6215,\n",
      "        -0.3980, -0.3980], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0010886866366490722\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0131, 4.6059, 6.1963, 8.0188], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.4000, -0.4180, -1.3745, -0.4342, -2.2790, -0.3715, -1.3897,  0.6677,\n",
      "        -0.4342, 10.0201], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4035, -0.3927, -1.3762, -0.3927, -2.2370, -0.3927, -1.3762,  0.6207,\n",
      "        -0.3927, 10.0000], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0009097728761844337\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.9997,  5.8318,  7.8825, 10.0222], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.4175,  6.2138, 10.0222, -2.3073,  8.0209,  8.0209,  8.0209, -0.4261,\n",
      "         3.0745, -0.4261], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3994,  6.2188, 10.0000, -2.2578,  8.0200,  8.0200,  8.0200, -0.3908,\n",
      "         3.0991, -0.3908], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0006410839268937707\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3199, -1.4333, -1.4155, -2.2087], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.3806, -0.4431, -0.4141,  3.0721, -1.4155, -0.4141,  1.7989, -2.2087,\n",
      "        10.0224, -2.2087], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3988, -0.3905, -0.3905,  3.0976, -1.3988, -0.3905,  1.8018, -2.2740,\n",
      "        10.0000, -2.2740], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0014162559527903795\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2714, -0.4545, -0.4354, -1.3801], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.3925,  1.8300,  1.7723,  1.8300,  6.2280,  0.6550,  1.7506,  0.6713,\n",
      "        -1.4160,  8.0195], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3959,  1.7726,  1.7726,  1.7726,  6.2175,  0.6470,  1.7955,  0.6148,\n",
      "        -1.3919,  8.0177], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0012573805870488286\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3758,  0.6571, -0.3707, -0.3141], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.4735,  3.0944,  0.6129,  4.5669, -0.4719,  4.5669,  3.1011,  4.5669,\n",
      "         0.5862, -0.4065], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4065,  3.1102,  0.6328,  4.6088, -0.3968,  4.6088,  3.1102,  4.6088,\n",
      "         0.6071, -0.4086], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.001656585605815053\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-1.3785, -0.3088, -0.3506, -2.2358,  4.5832, -0.4096, -2.2358,  7.9071,\n",
      "        -0.3506,  4.5832], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3686, -0.4149, -0.4149, -2.2568,  4.6027, -0.4149, -2.2568,  8.0117,\n",
      "        -0.4149,  4.6027], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.003223004285246134\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4738, 3.0878, 1.7959, 0.7096], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.6491, -0.3425,  4.6017,  1.7883, -0.3425,  0.6258,  4.6017, -0.4700,\n",
      "        -0.3425, 10.0023], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.6095, -0.4158,  4.5881,  1.7790, -0.4158,  0.6204,  4.5881, -0.3904,\n",
      "        -0.4158, 10.0000], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.002453734166920185\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6684, 3.1016, 4.6063, 1.8317], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.6283,  4.6063,  1.8030, -0.3397, -0.3397,  3.1489, -1.4655,  4.5629,\n",
      "         4.5629, -0.3608], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.6252,  4.5753,  1.8340, -0.4110, -0.4110,  3.1456, -1.3948,  4.5753,\n",
      "         4.5753, -0.4110], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.001992822391912341\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.0054, 4.5743, 4.6154, 6.1794], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 9.9821,  3.1469, -2.3557,  9.9821,  1.8165,  4.6154, -1.4127, -0.3878,\n",
      "         9.9821,  9.9821], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([10.0000,  3.1446, -2.2714, 10.0000,  1.8322,  4.5614, -1.3702, -0.4020,\n",
      "        10.0000, 10.0000], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0013554907636716962\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.9953, 4.5888, 6.2308, 7.9852], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.2867, -2.2867, -0.3974,  6.1714,  3.1164, -0.4308,  7.9852, -2.2867,\n",
      "         7.9852,  6.1714], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2715, -2.2715, -0.3921,  6.1867,  3.1404, -0.4182,  7.9809, -2.2715,\n",
      "         7.9809,  6.1867], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.000195942324353382\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.9906, 5.8233, 7.9353, 9.9763], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.4439, -1.4061,  3.1364, -1.4061,  1.8480, -2.2851,  9.9763, -0.4178,\n",
      "         1.8394,  0.6488], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3843, -1.3442,  3.1354, -1.3442,  1.8145, -2.2511, 10.0000, -0.3843,\n",
      "         1.8191,  0.6632], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0015786954900249839\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3211, -1.3819, -1.4325, -2.2700], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 7.9820,  6.1728, -0.4579,  0.6836,  0.6836, -0.4579,  6.1728, -0.4579,\n",
      "        -0.4579,  0.6428], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 7.9806,  6.1838, -0.3847,  0.6675,  0.6675, -0.3847,  6.1838, -0.3847,\n",
      "        -0.3847,  0.5981], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0024143331684172153\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4106, -0.4100, -0.5270, -2.1692], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.4223, -0.4446, -1.4124, -0.5270, -0.4446, -1.3755, -0.4223,  3.1289,\n",
      "        -0.4446, -1.4124], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3957, -0.3957, -1.3445, -0.4305, -0.3957, -1.3445, -0.3957,  3.1457,\n",
      "        -0.3957, -1.3445], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0028369787614792585\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.6228,  0.6030,  0.6135, -1.3844], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.2155, -2.2155, -1.3593, -1.4451,  0.5894, -0.4843, -1.4012, -1.3593,\n",
      "        -0.4045, -0.4045], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2187, -2.2187, -1.3551, -1.3551,  0.6517, -0.4604, -1.3867, -1.3551,\n",
      "        -0.4168, -0.4168], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0013129811268299818\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3836,  1.7938,  1.0562, -0.3729], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 7.9892,  3.0387,  6.2072,  1.8555, -1.3739, -0.3693, -0.3910, -0.3693,\n",
      "         3.0387, -1.3464], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 7.9910,  3.1835,  6.1902,  1.8829, -1.3353, -0.4363, -0.4363, -0.4363,\n",
      "         3.1835, -1.3353], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.005563431419432163\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6442, 1.7982, 3.2080, 0.6895], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 6.2292,  7.9904, -1.4188, -0.3644, -0.3810, -0.3554,  4.6376,  0.5765,\n",
      "        -1.3471, -1.2683], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 6.1914,  7.9929, -1.3797, -0.4323, -0.4486, -0.4486,  4.6063,  0.6817,\n",
      "        -1.3348, -1.3348], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0037463121116161346\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7269, 3.0739, 4.6180, 1.8404], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 7.9907,  0.5668, -1.3369, -0.4176,  4.6180,  9.9923,  6.2436, -1.3369,\n",
      "        -2.1232,  4.6180], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 7.9930,  0.5977, -1.3412, -0.4284,  4.6192, 10.0000,  6.1916, -1.3412,\n",
      "        -2.1306,  4.6192], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00039325625402852893\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1854, 4.5654, 4.6840, 6.2520], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.4085, -2.1081,  9.9925,  6.2520, -2.1081, -2.1081, -0.3849,  9.9925,\n",
      "         4.6009,  9.9925], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3729, -2.1205, 10.0000,  6.1928, -2.1205, -2.1205, -0.4550, 10.0000,\n",
      "         4.6268, 10.0000], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0011003179242834449\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1034, 4.5696, 6.2672, 7.9942], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 3.0982,  3.0982,  4.5850, -0.4000,  3.1907,  9.9931,  0.6089, -0.3870,\n",
      "        -1.2373, -1.3444], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.1265,  3.1265,  4.6266, -0.4519,  3.1265, 10.0000,  0.6255, -0.4519,\n",
      "        -1.3600, -1.3600], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.002998472424224019\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([8.1335, 5.8006, 7.9747, 9.9916], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 3.1758, -1.2509,  4.6763, -0.3683,  7.9941, -0.3930, -0.3930,  6.2450,\n",
      "         0.5637,  3.1758], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.1128, -1.3738,  4.6205, -0.4431,  7.9924, -0.4431, -0.4431,  6.1947,\n",
      "         0.6643,  3.1128], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.004942550323903561\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.0842, -1.3896, -1.2941, -2.1358], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.3896, -2.1358, -0.4822, -0.4121, -1.2941, -2.1358,  3.1409,  9.9895,\n",
      "         1.7658, -1.3843], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4011, -2.1647, -0.4455, -0.4302, -1.3924, -2.1647,  3.0959, 10.0000,\n",
      "         1.8268, -1.3924], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.001905743032693863\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.0975, -0.4580, -0.4473, -1.3605], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.3605,  3.0998,  0.5855, -0.4328, -0.4580, -0.4346,  1.7800, -2.1200,\n",
      "        -0.4217,  9.9870], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4026,  3.0799,  0.5998, -0.4575, -0.4682, -0.4162,  1.7898, -2.2130,\n",
      "        -0.4162, 10.0000], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0012378310784697533\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2804,  0.6615, -0.4392, -0.4571], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.4298,  3.0898, -2.0827,  0.6223,  0.6094,  7.9916, -0.4571,  0.5763,\n",
      "         0.5964, -0.4392], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4399,  3.0697, -2.2414,  0.5805,  0.5805,  7.9859, -0.4047,  0.6564,\n",
      "         0.6079, -0.4047], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.003878008108586073\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.2605,  1.8389,  0.6671, -0.4081], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.3811, -1.3802,  9.9835, -0.4702, -1.3802, -1.3802, -2.2374,  7.9918,\n",
      "        -2.2374,  0.6671], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3905, -1.3909, 10.0000, -0.4002, -1.3909, -1.3909, -2.2357,  7.9852,\n",
      "        -2.2357,  0.6550], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0005803421372547746\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6690, 3.1538, 1.8152, 0.7168], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.2517,  1.7563, -1.3926, -2.2511,  6.1871,  7.9931,  1.8371, -0.4204,\n",
      "        -0.2634, -0.4316], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3937,  1.7329, -1.3888, -2.2312,  6.1937,  7.9861,  1.8385, -0.3959,\n",
      "        -0.4495, -0.3959], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0057739270851016045\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7676, 3.1046, 4.5231, 1.7656], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.4293, -0.5268,  7.9968, -0.4293, -0.4162, -0.4819,  6.1817,  9.9882,\n",
      "         7.9968, -1.3596], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3839, -0.4236,  7.9894, -0.3839, -0.4236, -0.3839,  6.1971, 10.0000,\n",
      "         7.9894, -1.3841], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00255317660048604\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2809, 4.6204, 4.6260, 6.1824], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.6360,  6.1824, -2.1493, -1.3503, -0.4248,  3.2809,  4.6260,  0.6624,\n",
      "         3.1068,  0.6892], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.6456,  6.2015, -2.2152, -1.3720, -0.3797,  3.0770,  4.5641,  0.5843,\n",
      "         3.0770,  0.6456], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.006161120254546404\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1480, 4.6221, 6.2503, 8.0080], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.6832,  4.5566,  1.7902, -1.3225, -0.4553, -1.3225,  0.6587, -0.3883,\n",
      "         1.7878, 10.0018], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.6364,  4.5607,  1.7334, -1.3616, -0.3851, -1.3616,  0.6111, -0.3851,\n",
      "         1.7334, 10.0000], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0018634104635566473\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.1330,  5.8411,  7.9636, 10.0095], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 1.8079, -0.3739,  0.6712,  8.0143,  4.5831,  1.7788, 10.0095,  3.0557,\n",
      "        -0.3739,  0.6468], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.7502, -0.4010,  0.6289,  8.0086,  4.5588,  1.7502, 10.0000,  3.1248,\n",
      "        -0.4010,  0.6271], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0013275004457682371\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2783, -1.3790, -1.2953, -2.2681], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 6.1758, -0.3660, -0.3660, -0.3660,  0.6141,  8.0196, -2.2681,  1.8047,\n",
      "        -2.2783, -0.3660], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 6.2176, -0.4142, -0.4142, -0.4142,  0.6369,  8.0147, -2.1658,  1.8441,\n",
      "        -2.1658, -0.4142], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.003622803371399641\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2545, -0.4212, -0.3573, -1.3921], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 3.0852,  8.0246, -0.3840,  3.1424,  8.0246, -0.3840, -0.3840,  0.5133,\n",
      "         3.1424, -1.3790], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.1461,  8.0216, -0.4204,  3.1461,  8.0216, -0.4204, -0.4204,  0.6287,\n",
      "         3.1461, -1.3859], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0021112922113388777\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4339,  0.6411, -0.4123, -0.4879], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 1.8044,  6.1864, -2.2576,  0.6664, -0.4123, 10.0300, -0.4123, -1.3312,\n",
      "        -1.3762, -1.4339], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.8460,  6.2250, -2.1981,  0.5870, -0.4230, 10.0000, -0.4230, -1.3328,\n",
      "        -1.3805, -1.3328], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0024429496843367815\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4727,  1.8085,  0.6383, -0.4210], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 3.1195,  0.5989,  4.5999,  0.6433, -0.4161, -0.3988, -1.3830, -0.4396,\n",
      "         1.7674,  8.0284], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.1343,  0.5734,  4.5744,  0.6276, -0.3840, -0.4210, -1.3589, -0.4210,\n",
      "         1.7826,  8.0301], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0004458727198652923\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5206, 3.1652, 1.8290, 0.7683], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 1.8141, -0.4127,  3.0870, -0.4667, -0.4127,  6.2020, -0.4667, -1.3709,\n",
      "         4.6093, -2.2390], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.8487, -0.4325,  3.1241, -0.4177, -0.4325,  6.2269, -0.4177, -1.3714,\n",
      "         4.5818, -2.2338], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0009566272492520511\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6758, 3.1243, 4.5743, 1.8667], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 3.1634, -0.4812, -2.2271,  1.8181,  1.6758, -0.4812, -1.4701,  0.6649,\n",
      "        -0.4698, -0.4812], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.1169, -0.4174, -2.2347,  1.8471,  1.7812, -0.4174, -1.3726,  0.5983,\n",
      "        -0.4174, -0.4174], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.004307697061449289\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1176, 4.5821, 4.5633, 6.2290], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.4766, -0.4766, -2.2129,  0.6681, -0.4304, -0.4512,  8.0324, -0.4766,\n",
      "         6.2290, -0.4766], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4275, -0.4275, -2.2506,  0.5937, -0.4447, -0.4275,  8.0412, -0.4275,\n",
      "         6.2291, -0.4275], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0017420481890439987\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0716, 4.5857, 6.2133, 8.0323], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.4101,  3.0995, -0.4306,  4.5890, -0.4527, -0.4527, -2.2051, -0.4546,\n",
      "         8.0323,  8.0323], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4046,  3.1301, -0.4393,  4.6158, -0.4267, -0.4267, -2.2690, -0.4393,\n",
      "         8.0429,  8.0429], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0007656860398128629\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.1019,  5.8184,  7.9333, 10.0476], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.4376, -0.4164,  0.5976, -0.3605,  3.1246,  8.0315, -0.4376,  0.6257,\n",
      "        -2.2392, 10.0476], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4464, -0.4464,  0.5030, -0.4464,  3.1383,  8.0428, -0.4464,  0.5030,\n",
      "        -2.2793, 10.0000], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0036622225306928158\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2650, -1.4253, -1.4169, -2.2092], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 3.1394,  0.6334, -0.4155,  3.1117,  4.6091,  8.0279, 10.0430, -1.3528,\n",
      "         6.2441,  4.5773], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.1482,  0.6149, -0.4478,  3.1482,  4.6197,  8.0387, 10.0000, -1.3954,\n",
      "         6.2251,  4.6197], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0008844932308420539\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2419, -0.4666, -0.4306, -1.3615], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 1.7134,  4.5839,  8.0215, -0.3959, -0.3959, -1.3512,  1.7134,  3.1193,\n",
      "        -0.3959, -0.3959], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.8073,  4.6124,  8.0303, -0.4483, -0.4483, -1.3876,  1.8073,  3.1565,\n",
      "        -0.4483, -0.4483], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0032248657662421465\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3531,  0.6173, -0.3937, -0.4067], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 1.8951,  3.1087, -1.4049,  3.1131, -0.3937,  3.1087,  1.7902, -0.4251,\n",
      "        -0.3937, -2.2234], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.7978,  3.1591, -1.3826,  3.1591, -0.4444,  3.1591,  1.7978, -0.4444,\n",
      "        -0.4444, -2.2644], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.002445096615701914\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4477,  1.8009,  0.6284, -0.4026], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.4012, -0.5018,  8.0065, -1.3818,  0.6235,  3.1242, -0.4313, -1.3818,\n",
      "         3.1022, -0.4159], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4325, -0.4389,  8.0112, -1.3882,  0.6208,  3.1447, -0.4325, -1.3882,\n",
      "         3.1447, -0.4325], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0007557991775684059\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5431, 3.1409, 1.8254, 0.7489], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.3155,  0.6204,  7.9983,  3.0937,  0.6449, -1.4055,  6.1996, -1.3966,\n",
      "         1.8127, -2.2452], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3990,  0.6314,  8.0010,  3.1280,  0.6314, -1.3990,  6.1985, -1.3881,\n",
      "         1.8268, -2.2447], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0008778917836025357\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7124, 3.1096, 4.5684, 1.8174], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 7.9912,  0.5860, -1.3620, -1.4096, -0.4191,  0.6593,  3.1096,  7.9912,\n",
      "         0.6147,  3.0887], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 7.9921,  0.6472, -1.3782, -1.3739, -0.4066,  0.6439,  3.1115,  7.9921,\n",
      "         0.6439,  3.1115], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0007056742906570435\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1007, 4.5930, 4.5584, 6.1796], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.4263, -1.4147, -1.4147, -0.4420,  6.1796, -0.4241, -0.4241, -0.4058,\n",
      "        -1.4147,  0.5726], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3626, -1.3652, -1.3652, -0.3992,  6.1880, -0.3992, -0.3992, -0.4601,\n",
      "        -1.3652,  0.6440], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0022594849579036236\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0593, 4.5911, 6.2060, 7.9857], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 6.1814, -2.1666, -0.4227,  7.9857, -2.2493, -0.4227,  0.6166,  0.6658,\n",
      "        -2.2493,  4.5408], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 6.1871, -2.2185, -0.4008,  7.9832, -2.2185, -0.4008,  0.6552,  0.6538,\n",
      "        -2.2185,  4.5633], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0007728738710284233\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([8.0733, 5.8241, 7.9160, 9.9783], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.3536,  3.1097, -1.3875,  9.9783,  0.6202,  1.8487,  4.5322, -0.4375,\n",
      "        -0.4185,  0.5596], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3657,  3.0789, -1.3767, 10.0000,  0.6638,  1.7987,  4.5638, -0.4027,\n",
      "        -0.4027,  0.6557], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0017784582450985909\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2811, -1.3565, -1.3789, -2.2153], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 4.5329, -2.2153, -0.4087, -1.3694,  1.8233, -1.3694,  0.6280, -0.4087,\n",
      "        -0.4210, -0.4087], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.5637, -2.2208, -0.4078, -1.3789,  1.8132, -1.3789,  0.6518, -0.4078,\n",
      "        -0.4078, -0.4078], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00020024020341224968\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3887, -0.4144, -0.4457, -2.1497], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.4281,  7.9826, -0.4079, -2.2736, -2.2021,  6.1787,  0.6384, -1.3601,\n",
      "        -0.3965,  1.8373], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4722,  7.9770, -0.4141, -2.2241, -2.2241,  6.1843,  0.6473, -1.3605,\n",
      "        -0.4141,  1.8301], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0005411996389739215\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5954,  0.6304,  0.6515, -1.3403], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 7.9807,  1.8242, -1.3760,  3.1777,  4.5450,  0.5727,  0.6427,  4.5722,\n",
      "        -0.4100, -1.3438], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 7.9748,  1.8599, -1.3457,  3.0905,  4.5584,  0.6500,  0.6417,  4.5584,\n",
      "        -0.4216, -1.3457], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0016322468873113394\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3806,  1.8232,  1.0378, -0.4038], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 9.9700,  7.9790,  0.6565,  0.5765, -0.3713, -2.1841, -2.1425, -0.3690,\n",
      "        -0.4418, -0.4480], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([10.0000,  7.9730,  0.6990,  0.6990, -0.4273, -2.1941, -2.1941, -0.4273,\n",
      "        -0.4356, -0.4356], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.002727093640714884\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6754, 1.8290, 3.1911, 0.6715], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.6638,  0.6391, -0.3702, -2.1755, -1.3699, -0.3666, -2.1755,  6.1689,\n",
      "        -1.3699, -0.3666], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.6351,  0.6351, -0.4248, -2.1864, -1.3756, -0.4248, -2.1864,  6.1789,\n",
      "        -1.3756, -0.4248], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0011003019753843546\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7404, 3.0727, 4.5612, 1.7771], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.3450, -1.3450,  4.5739, -1.3450,  4.5612,  6.2211, -1.3624, -0.3736,\n",
      "        -1.3450,  6.1638], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3481, -1.3481,  4.5474, -1.3481,  4.5474,  6.1759, -1.3822, -0.4160,\n",
      "        -1.3481,  6.1759], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0005310704582370818\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1421, 4.5624, 4.5661, 6.1642], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.6580,  0.6475,  1.8245,  0.6616,  4.5553,  0.7676, -0.3848,  0.6532,\n",
      "        -1.3521, -0.4356], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.6421,  0.6421,  1.8475,  0.6760,  4.5478,  0.6421, -0.4078,  0.6760,\n",
      "        -1.3646, -0.4325], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0018055280670523643\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0986, 4.5771, 6.1998, 7.9670], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.3447,  9.9631,  3.1624, -0.4160,  0.6707,  3.1812,  9.9631, -1.3254,\n",
      "         4.5464,  3.1812], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3680, 10.0000,  3.0918, -0.3963,  0.6562,  3.0918, 10.0000, -1.3832,\n",
      "         4.5437,  3.0918], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0028175266925245523\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([8.1264, 5.8137, 7.9057, 9.9658], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.3699, -1.3699,  1.8301, -0.4019, -0.4543,  1.8527,  9.9658, -0.4019,\n",
      "         1.8301, -1.2799], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3941, -1.3941,  1.8497, -0.3936, -0.4442,  1.8400, 10.0000, -0.3936,\n",
      "         1.8497, -1.3624], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0010305828182026744\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1747, -1.3309, -1.3596, -2.1914], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 7.9025, -0.4324,  6.1576, -1.3409, -1.3782,  3.1524,  3.1522, -1.3782,\n",
      "         4.5601, -1.3596], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 7.9732, -0.3886,  6.1710, -1.4016, -1.4016,  3.1041,  3.1041, -1.4016,\n",
      "         4.5419, -1.4016], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.001862037694081664\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3031, -0.3793, -0.4555, -2.1947], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.4369, -0.4255,  6.1568,  1.8262,  4.5778, -0.4399, -0.4399, -2.2055,\n",
      "        -2.2055,  1.8734], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3900, -0.3900,  6.1699,  1.8309,  4.5411, -0.3900, -0.3900, -2.1964,\n",
      "        -2.1964,  1.8295], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0012074902188032866\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4823,  0.6995,  0.6826, -1.3091], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.3650,  3.1440, -1.3535, -1.3358,  3.1336, -1.3772,  3.1440, -0.4367,\n",
      "         3.1576, -1.3650], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3742,  3.1358, -1.3742, -1.3406,  3.1358, -1.3742,  3.1358, -0.3987,\n",
      "         3.1358, -1.3742], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00026919643278233707\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6492,  0.6810,  1.8464, -0.4017], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.3756, -0.4401,  1.8715,  6.1680, -1.3694, -0.4094, -1.3016, -0.4401,\n",
      "         1.8323, -1.3462], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3520, -0.4084,  1.8113,  6.1711, -1.3520, -0.4084, -1.3411, -0.4084,\n",
      "         1.8297, -1.3520], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0008099839324131608\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6772, 1.8215, 3.1429, 0.5992], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.3801,  7.9697, -1.3550, -0.4249,  7.9697, -2.2237,  1.8179,  3.1306,\n",
      "        -0.4249, -0.4386], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3621,  7.9840, -1.3421, -0.4167,  7.9840, -2.2082,  1.8286,  3.1687,\n",
      "        -0.4167, -0.4167], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0003321460972074419\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8237, 3.1359, 4.6447, 1.8198], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.3495,  4.6447,  1.8133,  0.6975, -2.2253, -0.3559, -0.4344, -0.3559,\n",
      "        -0.4190, -2.2253], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3203,  4.5631,  1.8253,  0.6738, -2.2071, -0.4226, -0.4226, -0.4226,\n",
      "        -0.4226, -2.2071], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0017911121249198914\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1572, 4.5755, 4.6051, 6.1956], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 6.1956,  3.1235,  6.1956, -0.4456,  1.8077,  0.6752,  3.1572,  4.6419,\n",
      "        -0.4197, -0.3895], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 6.1794,  3.1777,  6.1794, -0.4436,  1.8112,  0.6771,  3.1777,  4.5761,\n",
      "        -0.4188, -0.4436], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0011156199034303427\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0893, 4.5716, 6.2260, 7.9840], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.4196,  4.6056,  4.6056,  4.6306, -1.3675, -0.4196, -0.4196, -0.3828,\n",
      "         0.6502,  6.2260], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4148,  4.5919,  4.5919,  4.5919, -1.3344, -0.4148, -0.4148, -0.3719,\n",
      "         0.6750,  6.1856], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0005411012098193169\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.1230,  5.8174,  7.9470, 10.0088], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 4.6145, -0.3976,  7.9932,  0.6829, -0.3879,  3.0972, -0.4202, -1.3377,\n",
      "         7.9932, -1.3769], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.6118, -0.4096,  8.0079,  0.6532, -0.4096,  3.1531, -0.4096, -1.3519,\n",
      "         8.0079, -1.3519], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0005997637053951621\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2445, -1.3636, -1.3790, -2.1939], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.6944, -1.3359,  6.2548,  0.6132,  1.8123, -2.1627, -0.3734, -0.4520,\n",
      "        -2.2547, -0.4520], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.6498, -1.3599,  6.2016,  0.6311,  1.7815, -2.2273, -0.4043, -0.4481,\n",
      "        -2.2273, -0.4481], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0012578013120219111\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3476, -0.3710, -0.4472, -2.1714], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.6893, 10.0223,  1.8376,  0.6045,  4.5887, 10.0223,  8.0090,  1.8077,\n",
      "         8.0090,  1.8051], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.6538, 10.0000,  1.7805,  0.6648,  4.6384, 10.0000,  8.0201,  1.7805,\n",
      "         8.0201,  1.7805], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0013221987755969167\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5333,  0.6766,  0.6721, -1.2882], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 6.2683,  4.5886, -2.1844, -2.1844, -0.4014,  3.1015, 10.0227, -0.4014,\n",
      "        -0.4014,  4.5886], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 6.2116,  4.6414, -2.2233, -2.2233, -0.4017,  3.1298, 10.0000, -0.4017,\n",
      "        -0.4017,  4.6414], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0013113918248564005\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6345,  0.6856,  1.8591, -0.3565], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.1961,  3.1140, -0.3896, -0.3654, -1.3514, 10.0173, -1.3597,  4.5963,\n",
      "        -1.3558, -0.3896], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2195,  3.1366, -0.4004, -0.4004, -1.3489, 10.0000, -1.3427,  4.6274,\n",
      "        -1.3427, -0.4004], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.000425536185503006\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6388, 1.8146, 3.1331, 0.6048], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.3830,  3.0783,  0.6756,  1.8703, -1.3440, -0.3810, -2.2077, 10.0095,\n",
      "         3.1142,  6.2334], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3919,  3.1440,  0.6057,  1.8198, -1.3422, -0.3981, -2.2096, 10.0000,\n",
      "         3.1440,  6.2091], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0013698162510991096\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7732, 3.1180, 4.6013, 1.7928], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 4.6013, -2.2387, -2.2204, -0.3815, -1.3859,  1.7888,  0.6530,  6.2126,\n",
      "         0.6330,  8.0072], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.5913, -2.2031, -2.2031, -0.3921, -1.3444,  1.8272,  0.6422,  6.2065,\n",
      "         0.6099,  8.0012], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0005693325074389577\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1375, 4.5767, 4.5867, 6.1946], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.3284, -0.4504,  0.6829, -0.3833, -0.3833,  1.7966,  0.6829,  0.6205,\n",
      "        -0.3833, -1.3902], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3426, -0.4148,  0.6428, -0.3854, -0.3854,  1.8277,  0.6428,  0.6428,\n",
      "        -0.3854, -1.3537], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0007490588468499482\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0984, 4.5872, 6.2219, 8.0026], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 1.8427, -0.3933,  6.1789, -0.4015,  3.0923,  0.6134,  6.1789,  0.6409,\n",
      "        -2.2377, -0.3788], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.8233, -0.3826,  6.2023, -0.4114,  3.1267,  0.6479,  6.2023,  0.6479,\n",
      "        -2.1888, -0.4114], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0007546967244707048\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([8.1172, 5.8131, 7.9209, 9.9830], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.3177,  1.8151, -2.2369,  1.8151,  1.8151, -2.2369, -1.3177, -1.3553,\n",
      "         9.9830, -0.3983], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3446,  1.8215, -2.1859,  1.8215,  1.8215, -2.1859, -1.3446, -1.3709,\n",
      "        10.0000, -0.3822], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0007564331172034144\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2050, -1.3260, -1.3581, -2.2268], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.3965, -1.3581,  0.6453,  1.8449, -0.4027, -0.4498, -0.4027, -0.4027,\n",
      "         4.5707, -0.4498], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3758, -1.3758,  0.6476,  1.8189, -0.3853, -0.4225, -0.3853, -0.3853,\n",
      "         4.5516, -0.4225], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0004177825467195362\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3698, -0.3980, -0.4920, -2.2868], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 1.8229, -1.3746, -2.2172, -1.3601, -0.4920,  0.6783,  8.0032,  0.6783,\n",
      "         8.0032,  0.6783], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.8182, -1.3741, -2.2029, -1.3741, -0.4299,  0.6490,  7.9832,  0.6490,\n",
      "         7.9832,  0.6490], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0007652381318621337\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5746,  0.6260,  0.5928, -1.3937], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.3730, -1.3748, -1.3638,  6.1029,  6.1706,  3.1186, -1.3524,  9.9818,\n",
      "        -2.2081,  3.1007], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3694, -1.3694, -1.3665,  6.2035,  6.2035,  3.1059, -1.3665, 10.0000,\n",
      "        -2.2172,  3.1059], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0012057546991854906\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6563,  0.6662,  1.8222, -0.4015], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.4040, -0.4040, -0.4057, -0.4040, -0.4734,  6.1723,  3.0998,  0.6050,\n",
      "        -2.2031,  8.0009], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4121, -0.4121, -0.4121, -0.4121, -0.4406,  6.2008,  3.1039,  0.6386,\n",
      "        -2.2237,  7.9817], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00040695894858799875\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6985, 1.8219, 3.1360, 0.6137], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.3925, -0.4068,  1.8307, -1.3547, -0.4068,  6.1763,  4.5564, -0.4798,\n",
      "        -1.3619, -0.3925], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4200, -0.4200,  1.7902, -1.3635, -0.4200,  6.1975,  4.5586, -0.4453,\n",
      "        -1.3635, -0.4200], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0005216283025220037\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7894, 3.1079, 4.5525, 1.7596], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 9.9761, -1.3876,  3.1353,  3.1353, -0.4136, -1.3953, -1.3501, -1.3654,\n",
      "        -1.3654,  0.5862], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([10.0000, -1.3766,  3.0973,  3.0973, -0.4261, -1.3766, -1.3634, -1.3634,\n",
      "        -1.3634,  0.6419], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0007372489781118929\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1772, 4.5599, 4.5542, 6.1838], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 3.1091,  1.8309,  3.1294, -0.4198, -1.3462,  6.1838,  4.5524,  4.5542,\n",
      "         1.8238,  4.5524], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.0972,  1.7961,  3.0972, -0.4584, -1.3641,  6.1904,  4.5654,  4.5654,\n",
      "         1.8164,  4.5654], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00047596293734386563\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1427, 4.5763, 6.2023, 7.9853], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 3.1108,  4.5577, -0.4262,  0.6283,  6.1846, -2.1969,  6.1846,  4.5763,\n",
      "        -0.4262, -1.4015], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.1019,  4.5662, -0.4376,  0.6453,  6.1868, -2.2326,  6.1868,  4.5662,\n",
      "        -0.4376, -1.3830], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00024336199567187577\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([8.1821, 5.8067, 7.9104, 9.9729], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 1.8254, -0.4300,  4.5644,  1.8122,  3.1120, -2.1995, -0.4300, -0.4208,\n",
      "        -0.4300,  1.8193], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.8009, -0.4435,  4.5665,  1.8099,  3.1080, -2.2307, -0.4435, -0.4435,\n",
      "        -0.4435,  1.8099], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0002748487167991698\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1579, -1.4117, -1.3666, -2.2070], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 3.1157,  7.9762, -0.4366,  4.5704, -1.3491,  7.9762,  7.9762, -1.3491,\n",
      "        -0.4332, -0.4366], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.1134,  7.9734, -0.4484,  4.5649, -1.3579,  7.9734,  7.9734, -1.3579,\n",
      "        -0.4484, -0.4484], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.175296923378482e-05\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1931, -0.4761, -0.3965, -1.3546], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.3546, -0.4448,  6.2030, -0.4448,  6.1812,  3.1156, -1.3127, -1.4609,\n",
      "         1.8121, -0.4448], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3569, -0.4521,  6.1741, -0.4521,  6.1741,  3.1175, -1.3945, -1.3569,\n",
      "         1.8056, -0.4521], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0018624558579176664\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3913,  0.6090, -0.4602, -0.4572], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 7.9697,  0.6057,  0.6226,  0.5735, -0.4602,  7.9697, -1.3242, -1.3643,\n",
      "         4.5740,  3.1206], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 7.9718,  0.6236,  0.6401,  0.6374, -0.4519,  7.9718, -1.3919, -1.3710,\n",
      "         4.5636,  3.1166], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0009531178511679173\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4259,  1.8258,  0.6167, -0.4687], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 4.5723,  0.6436,  0.5788,  6.1828, -2.2207, -1.3739,  0.6123, -2.2207,\n",
      "        -0.4724,  3.1033], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.5645,  0.6156,  0.6384,  6.1722, -2.2445, -1.3834,  0.6432, -2.2445,\n",
      "        -0.4711,  3.1151], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0006820957059971988\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5440, 3.1370, 1.7641, 0.6258], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 1.7919, -2.2251,  7.9694, -0.4161,  6.1843,  1.8151, -1.3829, -1.4022,\n",
      "        -0.4370,  3.1054], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.7949, -2.2461,  7.9744, -0.4416,  6.1725,  1.7949, -1.3904, -1.3745,\n",
      "        -0.4650,  3.1153], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0003376103122718632\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8021, 3.1306, 4.5736, 1.7765], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.3819, -0.4840, -1.3674, -0.4117,  0.6278, -1.3969, -0.4779, -1.3924,\n",
      "        -0.4382,  6.1878], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3944, -0.4441, -1.3705, -0.4350,  0.6124, -1.3705, -0.4441, -1.3944,\n",
      "        -0.4441,  6.1731], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0004636997473426163\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1791, 4.5733, 4.5566, 6.1852], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 3.1411, -0.4825, -1.3986,  0.6088, -0.4825, -1.3940, -2.2640, -0.4787,\n",
      "         7.9717,  6.1852], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.1185, -0.4434, -1.3950,  0.6374, -0.4434, -1.3722, -2.2387, -0.4434,\n",
      "         7.9781,  6.1745], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0006927464855834842\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1424, 4.5696, 6.1910, 7.9744], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 9.9786,  0.6320, -1.3993,  0.6153,  0.6130,  6.1874,  9.9786, -1.3664,\n",
      "        -1.3664,  3.1370], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([10.0000,  0.6227, -1.3887,  0.6305,  0.6119,  6.1770, 10.0000, -1.3887,\n",
      "        -1.3887,  3.1251], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00025848476798273623\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([8.1889, 5.8074, 7.9142, 9.9846], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 1.8039,  7.9795, -1.3949,  0.6305, -0.4606, -0.4606,  9.9846,  1.7907,\n",
      "        -1.3949, -0.4663], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.8191,  7.9862, -1.3795,  0.6304, -0.4482, -0.4482, 10.0000,  1.8191,\n",
      "        -1.3795, -0.4482], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00024278470664285123\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2408, -1.4085, -1.3577, -2.2303], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 6.1968, -2.2855, -0.4550,  3.1252,  3.1218,  0.6140,  1.7700, -0.4483,\n",
      "         7.9864, -0.4382], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 6.1877, -2.2219, -0.4508,  3.1412,  3.1412,  0.6380,  1.8346, -0.4508,\n",
      "         7.9934, -0.4351], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0009579531033523381\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2642, -0.4733, -0.4110, -1.3755], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.2324, -0.4110,  3.1480, -0.4110, -0.4439, -0.4110, -1.3755,  4.5718,\n",
      "        -2.2719,  0.6244], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2311, -0.4499,  3.1424, -0.4499, -0.4499, -0.4499, -1.3699,  4.5825,\n",
      "        -2.2311,  0.6136], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0006547770462930202\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4180,  0.6220, -0.4444, -0.4246], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.4907, 10.0070, -0.4444, -2.2381, -1.3765, -0.4444, -0.4320, -2.2381,\n",
      "         4.5589,  4.5981], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4326, 10.0000, -0.4402, -2.2543, -1.3898, -0.4402, -0.4310, -2.2543,\n",
      "         4.5868,  4.5868], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0005068351747468114\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4877,  1.7964,  0.6100, -0.4502], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.4235, -0.4262, -0.4262,  0.6317,  0.6317, -0.4448, -2.2482, -0.4767,\n",
      "        -0.4108, -1.4235], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3835, -0.4278, -0.4278,  0.6168,  0.6168, -0.4315, -2.2756, -0.4381,\n",
      "        -0.4315, -1.3835], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.000648770947009325\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5167, 3.1276, 1.7734, 0.6679], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 1.8117, -0.4431,  3.1664,  8.0069,  0.6679, -2.2535, -0.4431, -0.4241,\n",
      "         0.6237, -1.4221], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.8103, -0.4259,  3.1304,  8.0131,  0.6201, -2.2799, -0.4259, -0.4253,\n",
      "         0.5954, -1.3816], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0007347959908656776\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7947, 3.1378, 4.5902, 1.8437], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 1.7911,  0.6428, -0.4230, -1.4162, -1.4341, -2.2597, -0.4383, -1.4341,\n",
      "        -1.4341,  0.6450], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.8077,  0.6120, -0.4214, -1.3807, -1.4295, -2.2746, -0.4195, -1.4295,\n",
      "        -1.4295,  0.6260], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00034882972249761224\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1557, 4.5661, 4.5460, 6.2035], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.6431, -2.2621,  3.1258, -0.3921, 10.0183, -0.4792,  4.5928,  8.0110,\n",
      "         4.5928,  8.0110], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.6171, -2.2673,  3.1335, -0.4156, 10.0000, -0.4410,  4.5831,  8.0165,\n",
      "         4.5831,  8.0165], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0003360006376169622\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1460, 4.5744, 6.1917, 8.0122], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.3908,  0.6176,  4.5931, -2.2657, 10.0185, -2.2657, 10.0185,  3.1162,\n",
      "         8.0122,  1.8040], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4122,  0.6020,  4.5810, -2.2601, 10.0000, -2.2601, 10.0000,  3.1338,\n",
      "         8.0167,  1.8046], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00019233726197853684\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.1920,  5.8135,  7.9058, 10.0160], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.4242, -2.2328, -1.4307, -2.2328,  1.7619, -1.3935, -0.4935,  4.5755,\n",
      "        -0.3919,  3.1144], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4097, -2.2541, -1.4283, -2.2541,  1.8110, -1.3862, -0.4097,  4.5784,\n",
      "        -0.4097,  3.1319], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0011246525682508945\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2728, -1.3843, -1.4164, -2.2690], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.3888, -1.4140, -0.4803, -1.3843, -0.3990, -2.2690,  0.6466, 10.0133,\n",
      "        -0.4174, -0.4174], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4234, -1.4234, -0.4129, -1.3918, -0.4129, -2.2459,  0.6409, 10.0000,\n",
      "        -0.4129, -0.4129], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0006859611603431404\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4344, -0.4420, -0.4674, -2.2495], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.2672,  0.6025,  3.1338, -0.4607,  8.0098, -0.4607,  8.0098, -1.3974,\n",
      "         6.1921,  0.6025], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2395,  0.6034,  3.1399, -0.4577,  8.0091, -0.4577,  8.0091, -1.4099,\n",
      "         6.2088,  0.6034], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0001257938565686345\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.6085,  0.6298,  0.6161, -1.3449], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.4087,  6.1918, -1.3942, -1.3803, -0.4087, -1.3803,  4.5666, 10.0076,\n",
      "        -0.4228, 10.0076], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4303,  6.2079, -1.3885, -1.3885, -0.4303, -1.3885,  4.5726, 10.0000,\n",
      "        -0.4303, 10.0000], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00015716080088168383\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6370,  0.6667,  1.8673, -0.3631], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 6.1915,  8.0064,  0.6424, -1.4105, -0.4094, -1.3677,  4.6082, -0.4344,\n",
      "        -0.4344,  1.8276], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 6.2058,  8.0035,  0.6805, -1.3707, -0.4370, -1.4086,  4.5723, -0.4370,\n",
      "        -0.4370,  1.7999], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.000775660271756351\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6484, 1.8071, 3.1449, 0.6487], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.2634,  1.8672,  6.1951, -1.3771, -1.3705, -0.4143,  1.8672,  0.6662,\n",
      "         6.1951, -0.4143], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2308,  1.8304,  6.2035, -1.3640, -1.3640, -0.4415,  1.8304,  0.6804,\n",
      "         6.2035, -0.4415], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0005796338664367795\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7849, 3.1254, 4.5983, 1.8293], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 3.1466, -1.3903, -0.4250, -0.4250, -2.2605, -0.4250, -0.4250, -1.4301,\n",
      "        -2.2605,  9.9984], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.1385, -1.4118, -0.4446, -0.4446, -2.2293, -0.4446, -0.4446, -1.3632,\n",
      "        -2.2293, 10.0000], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0008504491997882724\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1587, 4.5755, 4.5688, 6.2086], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.4537, -0.4278,  8.0009, -2.2478, -0.4278,  4.5893, -1.3625,  6.2086,\n",
      "        -0.4160,  0.6705], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4465, -0.4706,  7.9980, -2.2266, -0.4706,  4.5878, -1.3744,  6.2009,\n",
      "        -0.4465,  0.6478], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0005828713183291256\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1445, 4.5727, 6.1878, 8.0003], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.3956,  1.7865,  0.5936, -0.4625, -0.4625,  6.2151,  9.9973, -1.3956,\n",
      "         0.5703, -0.4550], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3913,  1.8227,  0.6079, -0.4472, -0.4472,  6.2003, 10.0000, -1.3913,\n",
      "         0.6496, -0.4472], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0008580888388678432\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([8.1889, 5.8066, 7.8914, 9.9974], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.3624, -0.4718, -0.4428,  4.5773,  3.1335, -0.4718, -1.3659,  1.7886,\n",
      "        -0.4718,  0.6119], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4112, -0.4493, -0.4578,  4.5988,  3.1159, -0.4493, -1.3985,  1.8202,\n",
      "        -0.4493,  0.6466], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0008179727010428905\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2421, -1.3662, -1.4014, -2.2147], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 8.0001, -0.4743, -2.2421,  1.7981, -0.4514,  0.6450,  6.2235, -0.4743,\n",
      "         9.9965,  1.7927], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 7.9968, -0.4491, -2.2296,  1.8160, -0.4496,  0.6183,  6.2001, -0.4491,\n",
      "        10.0000,  1.8160], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0003572943969629705\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4250, -0.4552, -0.4864, -2.2271], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 9.9960,  8.0003, -1.3764, -0.4714,  1.8197, -2.2716,  4.5669, -2.2083,\n",
      "        -0.4720, -0.4714], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([10.0000,  7.9964, -1.4157, -0.4508,  1.8032, -2.2357,  4.6020, -2.2357,\n",
      "        -0.4508, -0.4508], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0006411639042198658\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5997,  0.6101,  0.5787, -1.3370], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 1.7936,  6.1793, -2.2109,  6.2208,  9.9943, -0.4792, -0.4641, -0.4641,\n",
      "         4.5705, -2.2109], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.8124,  6.1991, -2.2447,  6.1991, 10.0000, -0.4407, -0.4545, -0.4545,\n",
      "         4.5988, -2.2447], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0005994464154355228\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6393,  0.6293,  1.8023, -0.3818], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 3.1273, -0.4470,  9.9908,  0.6393, -0.4539, -0.4539, -1.3905, -2.2636,\n",
      "        -0.4661, -0.4539], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.1210, -0.4573, 10.0000,  0.6221, -0.4573, -0.4573, -1.4195, -2.2522,\n",
      "        -0.4573, -0.4573], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0001614990906091407\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6872, 1.8035, 3.1307, 0.6683], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.2360,  3.1277, -0.4442, -2.2230,  0.5993,  4.5880, -0.4518,  1.8016,\n",
      "         6.2016,  4.5880], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2522,  3.1292, -0.4606, -2.2522,  0.6209,  4.5814, -0.4606,  1.8176,\n",
      "         6.1946,  4.5814], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0002327590191271156\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8169, 3.1288, 4.5946, 1.8363], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.4351,  6.1955,  6.1923, -0.4576, -0.4351,  1.7962, -2.2485,  3.1746,\n",
      "        -0.4351,  0.6504], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4613,  6.1926,  6.1926, -0.4613, -0.4613,  1.7992, -2.2461,  3.1351,\n",
      "        -0.4613,  0.6166], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0004798651207238436\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1674, 4.5696, 4.5643, 6.1812], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.6463, -0.4353, -2.2609, -2.2609,  6.1812, -2.2609,  3.1298,  0.6256,\n",
      "         9.9825, -2.2261], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.6154, -0.4588, -2.2462, -2.2462,  6.1906, -2.2462,  3.1397,  0.6343,\n",
      "        10.0000, -2.2462], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0003126097726635635\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1597, 4.5757, 6.1964, 7.9887], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 9.9819,  4.6014,  0.6062, -1.4273, -0.4400, -0.4385,  1.8200, -0.4400,\n",
      "        -0.4717,  7.9887], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([10.0000,  4.5562,  0.6164, -1.4081, -0.4544, -0.4024,  1.8129, -0.4544,\n",
      "        -0.4544,  7.9837], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0004931435687467456\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([8.1986, 5.8120, 7.9004, 9.9845], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 7.9903, -0.4326,  0.6171, -0.4469, -0.4734, -0.4469, -0.4469, -0.4469,\n",
      "         4.5979, -2.2366], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 7.9860, -0.4446,  0.6264, -0.4492, -0.4492, -0.4492, -0.4492, -0.4492,\n",
      "         4.5554, -2.2548], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0002998892159666866\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2338, -1.3896, -1.3989, -2.2695], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.4663, -2.2695,  1.7993,  4.5905, -0.4537, -0.4537,  3.1256,  3.1110,\n",
      "        -1.4320, -0.4729], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4445, -2.2506,  1.7999,  4.5584, -0.4445, -0.4445,  3.1315,  3.1315,\n",
      "        -1.4191, -0.4445], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00034670112654566765\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3802, -0.4257, -0.4417, -2.2324], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 9.9931,  4.5816,  1.7952,  7.9965,  0.6367,  1.7940, -0.4568,  6.1842,\n",
      "        -1.3885,  0.5878], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([10.0000,  4.5658,  1.8146,  7.9938,  0.6146,  1.7956, -0.4434,  6.1969,\n",
      "        -1.3832,  0.6533], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0005832564202137291\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5766,  0.6290,  0.5992, -1.3474], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.4197,  9.9990, -0.4612,  4.5762,  4.5762,  3.1253, -2.2281,  1.7785,\n",
      "         4.5488, -0.4622], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4159, 10.0000, -0.4444,  4.5759,  4.5759,  3.1185, -2.2509,  1.8128,\n",
      "         4.5759, -0.4444], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0003086962387897074\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6341,  0.6294,  1.8004, -0.4029], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 6.2051,  3.1359, -1.3914,  1.8293, 10.0042, -0.4547,  0.6319,  1.8368,\n",
      "         0.6145,  3.1096], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 6.2042,  3.1175, -1.3862,  1.7987, 10.0000, -0.4470,  0.6204,  1.8098,\n",
      "         0.6088,  3.1175], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0002344173553865403\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6735, 1.8026, 3.1173, 0.6565], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.6318, -0.4413, -2.2250,  8.0074,  3.1173,  3.1173,  4.5838, -1.3942,\n",
      "         1.8084, -0.4413], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.6275, -0.4510, -2.2357,  8.0072,  3.1172,  3.1172,  4.5916, -1.3785,\n",
      "         1.8056, -0.4510], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.357883103191853e-05\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7971, 3.1211, 4.5735, 1.8305], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.3636, -1.3636,  8.0097, -0.4452,  0.6160,  0.6332,  6.2192,  3.1211,\n",
      "        -1.3934,  0.6612], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3887, -1.3887,  8.0101, -0.4532,  0.6013,  0.6332,  6.2088,  3.1162,\n",
      "        -1.3738,  0.6349], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0002753746521193534\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1593, 4.5855, 4.5749, 6.2224], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.3621, -0.4357, -1.3621, -1.3909, -1.3909, -2.2336, -2.2285, -1.3909,\n",
      "         1.8150, -0.4357], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3801, -0.4537, -1.3801, -1.3801, -1.3801, -2.2258, -2.2258, -1.3801,\n",
      "         1.8128, -0.4537], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00017253241094294935\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1503, 4.5785, 6.1950, 8.0131], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 3.1168, -2.2354, -0.4629, 10.0155, -0.4153, -2.2383,  6.2260,  0.6788,\n",
      "        -1.3696, -1.3928], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.1101, -2.2326, -0.4532, 10.0000, -0.4532, -2.2326,  6.2118,  0.5999,\n",
      "        -1.3738, -1.3780], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0008511842461302876\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.1899,  5.8161,  7.9005, 10.0162], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 4.5577, 10.0162, 10.0162, -1.3863, -0.4504, 10.0162, -0.4504, -1.3863,\n",
      "         3.1198,  8.0139], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.6038, 10.0000, 10.0000, -1.3792, -0.4488, 10.0000, -0.4488, -1.3792,\n",
      "         3.1019,  8.0146], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0003338359820190817\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2521, -1.3872, -1.3982, -2.2422], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.3705,  1.7904,  6.2202,  3.1103,  1.7891, -2.2422,  6.2202,  1.8062,\n",
      "         8.0111,  3.1103], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3834,  1.8087,  6.2100,  3.0993,  1.8087, -2.2484,  6.2100,  1.7992,\n",
      "         8.0106,  3.0993], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00014173131785355508\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4070, -0.4367, -0.4536, -2.2374], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.6530,  4.5542, -2.2474,  1.8174, -0.4643, -0.4643, -0.4437, -1.4199,\n",
      "         3.1149, -0.4437], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.6179,  4.5911, -2.2469,  1.7947, -0.4439, -0.4439, -0.4439, -1.3862,\n",
      "         3.0988, -0.4439], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.000534378457814455\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5570,  0.6485,  0.6386, -1.3066], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.4564,  6.1939, -2.2397, -2.2524, -2.2397, -2.2524, -0.4614, -2.2614,\n",
      "         3.1156, -0.4412], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4193,  6.2053, -2.2499, -2.2499, -2.2499, -2.2499, -0.4441, -2.2499,\n",
      "         3.1047, -0.4163], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00028914655558764935\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6265,  0.6263,  1.8113, -0.3984], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.3765,  8.0031,  4.5699,  6.1479, -0.4285, -1.4097, -0.4472,  3.1128,\n",
      "         3.1128, -1.3324], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3856,  7.9988,  4.5742,  6.2028, -0.4457, -1.3856, -0.4278,  3.1129,\n",
      "         3.1129, -1.3856], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0007228017202578485\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6656, 1.7929, 3.1192, 0.6351], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 6.1821, -0.4453, -0.4453, -0.4436,  0.6232,  9.9918,  1.7948,  0.6163,\n",
      "         4.5795, -2.2382], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 6.1978, -0.4459, -0.4459, -0.4193,  0.6185, 10.0000,  1.8073,  0.6127,\n",
      "         4.5639, -2.2529], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00015554198762401938\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8011, 3.1045, 4.5858, 1.8025], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.6294,  1.7905,  0.6285,  1.8276, -0.4392, -0.4392,  1.7924,  6.1747,\n",
      "         1.8032, -0.4403], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.6448,  1.8099,  0.6150,  1.8099, -0.4445, -0.4445,  1.7900,  6.1935,\n",
      "         1.7900, -0.4203], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00020985717128496617\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1434, 4.5749, 4.5692, 6.1686], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 6.1686,  0.6282,  3.0997, -0.4375,  1.8260, -0.4320, -2.2562,  6.1686,\n",
      "        -0.4375,  0.6269], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 6.1888,  0.6434,  3.1295, -0.4403,  1.8093, -0.4255, -2.2432,  6.1888,\n",
      "        -0.4403,  0.6434], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0002719419135246426\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1704, 4.5726, 6.2116, 7.9821], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.2540, -1.3719,  7.9821,  0.6306,  0.6185,  0.6306,  5.8010,  6.1669,\n",
      "         4.5689,  4.5726], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2347, -1.3792,  7.9788,  0.6362,  0.6212,  0.6362,  6.1839,  6.1839,\n",
      "         4.5502,  4.5502], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.014828605577349663\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([8.2186, 5.8366, 7.9237, 9.9650], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.4542, -1.3596,  7.9585, -0.4353, -0.4418,  1.8128, -1.4011,  1.8094,\n",
      "        -0.4418,  0.6142], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4510, -1.3654,  7.9685, -0.4232, -0.4232,  1.8034, -1.3908,  1.8034,\n",
      "        -0.4232,  0.6324], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0001546490821056068\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2420, -1.3519, -1.3981, -2.2538], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 1.8018, -1.3886, -2.2538, -1.3981,  1.8018,  3.1271, -0.4354, -0.4391,\n",
      "         4.5835, -0.4396], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.8038, -1.3856, -2.2167, -1.3856,  1.8038,  3.1251, -0.4164, -0.4164,\n",
      "         4.5418, -0.4164], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0004710252396762371\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3595, -0.3903, -0.4213, -2.2409], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 6.1606, -0.4423,  4.5822, -2.2447,  7.9214,  6.1606, -0.4344, -1.3995,\n",
      "        -0.4315,  1.8300], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 6.1293, -0.4554,  4.5446, -2.2143,  7.9532,  6.1293, -0.4143, -1.3853,\n",
      "        -0.4143,  1.8050], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0007009421242401004\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5538,  0.6470,  0.6202, -1.3625], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.3809,  7.9109, -0.4280, -0.3903,  0.6910,  3.1183, -1.3513, -0.4280,\n",
      "        -1.3809,  4.5790], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3824,  7.9490, -0.4159, -0.4177,  0.6178,  3.1211, -1.3513, -0.4159,\n",
      "        -1.3824,  4.5475], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0008859740337356925\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6407,  0.6513,  1.8061, -0.4401], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 3.1181,  4.5759,  9.9401, -2.2161,  3.1449,  9.9401,  4.6508,  3.1181,\n",
      "         1.8061,  0.6407], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.1183,  4.5532, 10.0000, -2.2190,  3.1183, 10.0000,  4.5532,  3.1183,\n",
      "         1.8063,  0.6255], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0018157720332965255\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6724, 1.8217, 3.1205, 0.6061], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.6422, -1.3687,  7.9069,  6.1914, -2.1961,  7.9069, -0.4081, -1.3687,\n",
      "        -0.3994, -1.3771], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.6418, -1.3634,  7.9549,  6.1162, -2.2195,  7.9549, -0.4221, -1.3634,\n",
      "        -0.4222, -1.3634], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0011768089607357979\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7998, 3.1352, 4.5730, 1.7710], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.3991,  3.1214, -2.1824,  1.8246, -0.3979,  7.9133,  1.8246, -2.2067,\n",
      "        -0.3991,  3.1850], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4248,  3.1157, -2.2100,  1.8093, -0.4248,  7.9611,  1.8093, -2.2100,\n",
      "        -0.4248,  3.1157], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0010415177093818784\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1726, 4.6548, 4.6003, 6.2084], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.6431,  1.8216,  4.5760,  6.2084,  4.5760,  4.6003, -0.4098,  1.8292,\n",
      "        -0.3947, -2.1762], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.6395,  1.8234,  4.5876,  6.1277,  4.5876,  4.5876, -0.4180,  1.8121,\n",
      "        -0.4257, -2.2028], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0008976827375590801\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1472, 4.6670, 6.1951, 7.9265], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 7.9265,  9.9654,  9.9654,  1.8216,  1.8310,  6.2033, -0.4134, -0.3847,\n",
      "         6.2034,  1.8216], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 7.9688, 10.0000, 10.0000,  1.8241,  1.8152,  6.1338, -0.4133, -0.4263,\n",
      "         6.1338,  1.8241], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0015854971716180444\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([8.2186, 5.9885, 7.9534, 9.9732], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 1.8058, -0.3879,  4.5811, -0.4149,  4.5857,  3.1380,  0.5640, -0.3944,\n",
      "        -0.3944, -1.3707], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.8169, -0.4232,  4.5703, -0.4120,  4.5703,  3.1230,  0.6253, -0.4232,\n",
      "        -0.4232, -1.3503], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0007783501059748232\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2516, -1.3632, -1.3325, -2.1817], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.5594,  1.8069, -1.3632, -1.3632, -0.3985, -0.3966, -0.4203, -1.3668,\n",
      "         4.6916, -2.1817], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.6456,  1.8144, -1.3563, -1.3563, -0.4150, -0.4150, -0.4134, -1.3587,\n",
      "         4.5588, -2.1993], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.002627383451908827\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2812, -0.4465, -0.4042, -1.3722], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 3.1285, -0.3938, -0.4505,  1.8257,  4.5800,  1.8049, -2.1798,  6.1788,\n",
      "        -1.3722, -1.3287], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.1220, -0.4096, -0.4776,  1.8285,  4.5609,  1.8157, -2.1958,  6.1682,\n",
      "        -1.3638, -1.3638], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0003188184346072376\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4044,  0.6629, -0.3953, -0.3961], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.3318, -1.3759,  6.1802, -2.1847, -1.3318,  0.6582,  1.8032,  0.6330,\n",
      "        -0.4025, -1.3214], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3681, -1.3681,  6.1782, -2.1986, -1.3681,  0.6348,  1.8136,  0.6422,\n",
      "        -0.4673, -1.3623], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0009494048426859081\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.5129,  1.8246,  0.6275, -0.4510], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.2011, 10.0061, -1.3762, -0.3984,  1.8133,  0.5545, -0.3984, -0.4089,\n",
      "        -0.4093,  0.6275], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2104, 10.0000, -1.3680, -0.3973,  1.8101,  0.6320, -0.3973, -0.3973,\n",
      "        -0.4084,  0.6421], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0006549282697960734\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4802, 3.1442, 1.8027, 0.6306], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.2839,  0.5560,  1.8025,  3.1409, -2.2160, -2.2160, -1.3564,  7.9922,\n",
      "        -1.3564,  4.5726], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2208,  0.6267,  1.8058,  3.1153, -2.2208, -2.2208, -1.3688,  8.0102,\n",
      "        -1.3688,  4.5603], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0010482908692210913\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7391, 3.1374, 4.5677, 1.7734], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.4074, -1.3731, -0.4074, -0.4074, -0.4109, -1.3731, -0.4109,  1.8205,\n",
      "        -1.4052,  0.5958], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3923, -1.3698, -0.3923, -0.3923, -0.3923, -1.3698, -0.3923,  1.8273,\n",
      "        -1.3698,  0.6385], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0004509389982558787\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.0712, 4.6311, 4.5452, 6.1802], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.2399,  4.5673,  4.6311,  4.5673, -0.4060, -0.4060, 10.0193, -1.3796,\n",
      "         6.1802, -1.3786], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2408,  4.5622,  4.5622,  4.5622, -0.3963, -0.3963, 10.0000, -1.3778,\n",
      "         6.2046, -1.3651], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0006153842550702393\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1578, 4.6937, 6.2191, 8.0115], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 6.1898,  1.7969, 10.0238, 10.0238, -1.3931, -0.4067, -0.4028,  4.5666,\n",
      "        -1.3634,  3.1098], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 6.2103,  1.8151, 10.0000, 10.0000, -1.3859, -0.4023, -0.4023,  4.5708,\n",
      "        -1.3609,  3.1100], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00019858486484736204\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.2215,  5.9869,  7.9479, 10.0243], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 3.1139, -0.3962,  4.5666,  3.1139,  4.5666, -0.4381, -0.3994, -1.3815,\n",
      "        -0.3994, -0.4379], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.1099, -0.4086,  4.5782,  3.1099,  4.5782, -0.4417, -0.4086, -1.3566,\n",
      "        -0.4086, -0.4417], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00012734293704852462\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2286, -1.4138, -1.3803, -2.2485], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.3969, -2.2485,  1.7710, -2.2485, -0.4054,  6.2031,  6.2031,  4.5518,\n",
      "        -0.4350, -1.4138], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4127, -2.2423,  1.7994, -2.2423, -0.4127,  6.2147,  6.2147,  4.5828,\n",
      "        -0.4059, -1.3915], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00037590780993923545\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2073, -0.4531, -0.3973, -1.3575], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.6515,  4.5685, -0.4056, 10.0230, -1.4148,  6.2081, -2.2464, -0.4478,\n",
      "        -0.4368,  7.9474], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.6037,  4.5873, -0.4136, 10.0000, -1.3931,  6.2153, -2.2414, -0.4502,\n",
      "        -0.3987,  8.0207], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0010609016753733158\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3507,  0.6480, -0.3988, -0.4113], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.2027, -1.3507, -2.2439, -1.3587, 10.0166, -1.3587, -0.4113,  6.2082,\n",
      "        -0.3988,  8.0134], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2382, -1.3578, -2.2382, -1.3578, 10.0000, -1.3578, -0.4168,  6.2121,\n",
      "        -0.4168,  8.0150], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0001990257587749511\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4694,  1.7808,  0.6184, -0.4722], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 3.1080, -0.3968,  0.6450, 10.0097, 10.0097, -0.4016, -0.3968, -0.4016,\n",
      "        -0.4016, -0.4475], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.1199, -0.4195,  0.6027, 10.0000, 10.0000, -0.4195, -0.4195, -0.4195,\n",
      "        -0.4195, -0.4505], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0004122955142520368\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5339, 3.1066, 1.8052, 0.6265], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 8.0042, -0.4122,  0.6580, -1.3753, -0.4387, -0.4122,  1.7795, -0.4387,\n",
      "         3.1040, -0.4274], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 8.0013, -0.4203,  0.5947, -1.3667, -0.3911, -0.4203,  1.7936, -0.3911,\n",
      "         3.1193, -0.4203], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0009221285581588745\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7908, 3.1012, 4.5745, 1.7658], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.4229,  0.6149, -0.4229, -0.4355,  6.2033,  9.9941, -0.4367,  4.5666,\n",
      "        -0.4229,  1.7882], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4208,  0.6094, -0.4208, -0.4208,  6.1994, 10.0000, -0.4542,  4.5829,\n",
      "        -0.4208,  1.7973], grad_fn=<AddBackward0>)\n",
      "LOSS: 9.659887291491032e-05\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1138, 4.5858, 4.5672, 6.2009], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 7.9952,  1.7911,  0.6429, -1.3975,  6.2009,  9.9877, -0.4309, -2.2391,\n",
      "        -1.3798,  7.9952], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 7.9890,  1.7879,  0.6125, -1.3870,  6.1957, 10.0000, -0.4214, -2.2408,\n",
      "        -1.3861,  7.9890], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00014325903612188995\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1787, 4.6676, 6.2446, 7.9918], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.4488,  4.5723, -0.4488,  4.5723,  1.7876,  9.9831,  0.5758,  6.1985,\n",
      "        -1.3858,  3.0959], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4238,  4.5787, -0.4238,  4.5787,  1.7863, 10.0000,  0.6088,  6.1926,\n",
      "        -1.3936,  3.1151], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0003174295707140118\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([8.2399, 5.9574, 7.9801, 9.9814], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.3448, -0.4490, -0.4423,  9.9814,  3.1003,  1.7837,  4.5732,  6.1980,\n",
      "        -0.4426, -1.3507], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3799, -0.4270, -0.4270, 10.0000,  3.1159,  1.7903,  4.5782,  6.1918,\n",
      "        -0.4270, -1.3981], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0005139852873980999\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1750, -1.3917, -1.3663, -2.2272], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.2272, -0.4244, -1.3917,  3.1102, -0.4394, -0.4394,  3.1102, -1.3610,\n",
      "         4.5773, -1.3917], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2297, -0.4528, -1.3702,  3.1196, -0.4314, -0.4314,  3.1196, -1.3912,\n",
      "         4.5776, -1.3702], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00029651803197339177\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2072, -0.4646, -0.4237, -1.3804], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.3804,  7.9915,  0.6327,  7.9915,  1.7873,  6.1960, -0.4350, -1.3534,\n",
      "        -0.4350, -0.4605], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3813,  7.9829,  0.6210,  7.9829,  1.8087,  6.1923, -0.4339, -1.3813,\n",
      "        -0.4339, -0.4339], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00022482406347990036\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3766,  0.6249, -0.4338, -0.4499], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 6.1950, -0.4499, -0.4338, -0.4131,  1.8054,  4.5820, -2.2169, -0.4338,\n",
      "        -0.4603, -0.4131], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 6.1924, -0.4376, -0.4376, -0.4376,  1.8148,  4.5755, -2.2120, -0.4376,\n",
      "        -0.4466, -0.4376], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00017206041957251728\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4409,  1.8090,  0.6428, -0.4479], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.4337,  4.5804,  0.6037,  7.9920, -0.4337,  0.6256, -1.3450, -2.2133,\n",
      "        -2.2133,  1.7923], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4369,  4.5751,  0.6131,  7.9838, -0.4369,  0.6281, -1.3707, -2.2105,\n",
      "        -2.2105,  1.8168], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0001485069515183568\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5380, 3.1077, 1.8118, 0.6222], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 9.9830, -0.4356, -1.3741, -2.2119, -0.4441,  4.5771, -0.4356,  7.9925,\n",
      "         0.6272, -2.2119], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([10.0000, -0.4356, -1.3685, -2.2138, -0.4356,  4.5755, -0.4356,  7.9847,\n",
      "         0.6321, -2.2138], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.885722228209488e-05\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7848, 3.0910, 4.5741, 1.7549], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.4365,  6.1965, -0.4399, -1.3703, -1.3513, -0.4365, -0.4399,  6.1965,\n",
      "        -1.4181, -2.2097], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4337,  6.1945, -0.4337, -1.3718, -1.3684, -0.4337, -0.4337,  6.1945,\n",
      "        -1.3684, -2.2162], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00029111295589245856\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1050, 4.5750, 4.5814, 6.1988], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 1.8048,  6.1988, -2.2080,  1.8048,  7.9959,  9.9887,  4.5706, -1.3770,\n",
      "         3.1270,  3.1103], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.8143,  6.1963, -2.2222,  1.8143,  7.9898, 10.0000,  4.5789, -1.3716,\n",
      "         3.1135,  3.1135], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.44271999085322e-05\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-1.4163, -2.2081,  9.9920, -1.3541,  9.9920,  6.1999,  1.7981,  0.6332,\n",
      "         7.9977,  1.8248], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3752, -2.2280, 10.0000, -1.3752, 10.0000,  6.1980,  1.8109,  0.6423,\n",
      "         7.9928,  1.8012], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0003496393619570881\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1020, 4.5757, 4.5770, 6.2014], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.4401,  4.5757, -0.4215,  9.9963,  9.9963,  4.5770,  9.9963, -1.3700,\n",
      "         4.5660, -0.4401], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4277,  4.5813, -0.4277, 10.0000, 10.0000,  4.5813, 10.0000, -1.3812,\n",
      "         4.5813, -0.4277], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.943437958601862e-05\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1776, 4.6632, 6.2678, 8.0024], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 3.1181, -0.4378,  1.7988,  4.5669,  1.8285,  4.5669,  4.5760, -1.3717,\n",
      "         0.6590, -1.3566], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.1102, -0.4258,  1.8063,  4.5810,  1.8063,  4.5810,  4.5810, -1.3838,\n",
      "         0.6189, -1.3838], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0003668226709123701\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.2431,  5.9619,  8.0193, 10.0031], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.4138,  0.6412,  6.1972,  8.0036, -2.2282, 10.0031, -2.2107,  0.5738,\n",
      "        10.0031,  0.5738], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4253,  0.6292,  6.2032,  8.0028, -2.2317, 10.0000, -2.2317,  0.6454,\n",
      "        10.0000,  0.6454], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0011050977045670152\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1929, -1.3689, -1.3575, -2.2170], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 4.5866,  6.1949, -0.4111,  4.5866,  0.6515,  1.8302,  3.1195, -2.2170,\n",
      "         4.5866,  8.0047], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.5754,  6.2042, -0.4272,  4.5754,  0.6366,  1.8161,  3.1280, -2.2218,\n",
      "         4.5754,  8.0049], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00012445621541701257\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2191, -0.4463, -0.4007, -1.3648], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.3532, -0.4099, -0.4306,  0.6234,  8.0059, -1.3682,  8.0059,  3.1357,\n",
      "         4.5932, -0.4076], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3602, -0.4282, -0.4235,  0.6419,  8.0074, -1.3602,  8.0074,  3.1339,\n",
      "         4.5766, -0.4282], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00015466049080714583\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3764,  0.6361, -0.4023, -0.4113], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.3392,  4.5964,  1.8257,  4.5747, -2.2194,  1.8047, -0.4023,  6.1981,\n",
      "         4.5964, -0.3951], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3556,  4.5783,  1.8255,  4.5783, -2.2053,  1.8089, -0.4275,  6.2063,\n",
      "         4.5783, -0.4275], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0002907175512518734\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4726,  1.8012,  0.6463, -0.4391], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 8.0086,  4.5785,  8.0086, -1.3725,  6.2037,  8.0086,  0.6447, 10.0137,\n",
      "        -2.2209, -2.2209], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 8.0123,  4.5833,  8.0123, -1.3583,  6.2078,  8.0123,  0.6456, 10.0000,\n",
      "        -2.2064, -2.2064], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.868487202562392e-05\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5290, 3.1255, 1.8443, 0.6573], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.6459,  1.7983, -1.3739, -0.4031, -0.4423,  8.0100, -0.4040,  0.6670,\n",
      "         0.6459, -0.4809], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.6723,  1.8130, -1.3628, -0.4201, -0.4201,  8.0140, -0.4201,  0.6185,\n",
      "         0.6723, -0.3997], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0011738569010049105\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7571, 3.0978, 4.5765, 1.7721], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 8.0107, -1.3595,  4.5986, 10.0168, -1.3462, -0.4069,  6.2138, -2.2169,\n",
      "         4.5765,  1.8376], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 8.0151, -1.3793,  4.5924, 10.0000, -1.3669, -0.4138,  6.2096, -2.2116,\n",
      "         4.5924,  1.8091], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00023169603082351387\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.0871, 4.5911, 4.5925, 6.2154], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 3.0871,  4.5687,  3.1335, -2.2200, -1.3785, -1.3785,  4.5687, -2.2200,\n",
      "         1.8142, -0.4146], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.1118,  4.5939,  3.1118, -2.2107, -1.3910, -1.3910,  4.5939, -2.2107,\n",
      "         1.8202, -0.4027], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0003012560191564262\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1676, 4.6697, 6.2747, 8.0087], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 8.0087,  0.6675,  8.0087, 10.0142,  1.8252,  4.5667,  1.8141,  3.1087,\n",
      "         3.1341,  6.2134], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 8.0127,  0.6427,  8.0127, 10.0000,  1.7978,  4.5921,  1.7978,  3.1100,\n",
      "         3.1100,  6.2078], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0003116312436759472\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.2400,  5.9702,  8.0269, 10.0105], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([10.0105,  4.5709, -0.4359,  8.0062,  4.5709, -1.3865, -0.4381,  4.5709,\n",
      "         1.8212,  0.6670], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([10.0000,  4.5873, -0.4038,  8.0094,  4.5873, -1.3923, -0.4083,  4.5873,\n",
      "         1.7991,  0.6391], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00041580581455491483\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2184, -1.3517, -1.3981, -2.2292], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 4.5817, -1.3897,  4.5817,  0.6286,  4.5865,  6.1981,  0.6201, -2.2292,\n",
      "         1.8224,  0.6629], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.5783, -1.3937,  4.5783,  0.6404,  4.5783,  6.2021,  0.6527, -2.2165,\n",
      "         1.8138,  0.6404], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00020659886649809778\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3807, -0.4131, -0.4165, -2.2210], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.3582, -0.4496, -1.3976,  4.5898, -1.3582,  0.6260,  0.6690,  6.1915,\n",
      "         0.6930, -0.4496], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3718, -0.4135, -1.3977,  4.5723, -1.3718,  0.6381,  0.6447,  6.1994,\n",
      "         0.6447, -0.4135], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0006421507569029927\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5618,  0.6482,  0.6364, -1.3515], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.3706, -1.3898,  0.6482, -0.3862,  3.1416,  0.6602,  9.9976,  0.6608,\n",
      "        -0.4455,  3.0975], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3630, -1.3888,  0.6562, -0.4166,  3.1472,  0.6562, 10.0000,  0.6326,\n",
      "        -0.4233,  3.1472], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0004857047460973263\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6926,  0.6523,  1.8445, -0.3927], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.4238, -2.2346, -0.4238,  7.9949, -0.3953,  9.9941,  9.9941,  3.1071,\n",
      "         9.9941, -0.4238], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4307, -2.2400, -0.4307,  7.9947, -0.4241, 10.0000, 10.0000,  3.1540,\n",
      "        10.0000, -0.4307], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00033074963721446693\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6575, 1.8046, 3.1580, 0.6327], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.4239, -0.3934, -1.3732,  1.8046,  7.9932,  4.5685, -1.3732,  6.1877,\n",
      "        -0.4062,  7.9932], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4361, -0.4204, -1.3664,  1.8422,  7.9928,  4.5690, -1.3664,  6.1939,\n",
      "        -0.4361,  7.9928], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0003318284871056676\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8101, 3.0935, 4.6174, 1.7797], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.2251,  7.9911, -0.4102, -0.4259,  4.6174,  0.6307, -0.4259, -0.4054,\n",
      "        -0.4259,  0.6474], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2463,  7.9907, -0.4370, -0.4370,  4.5696,  0.6260, -0.4370, -0.4084,\n",
      "        -0.4370,  0.6589], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00039727980038151145\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1536, 4.5751, 4.6136, 6.1919], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.4318,  1.8070, -0.4318,  7.9893, -1.3907, -1.3806,  4.6101, -0.4318,\n",
      "        -0.4279,  0.6622], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4337,  1.7917, -0.4337,  7.9892, -1.3594, -1.3667,  4.5727, -0.4337,\n",
      "        -0.4085,  0.6263], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00044781799078918993\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2124, 4.6577, 6.2910, 7.9885], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.4332, -2.2399,  4.5969,  1.8078, -0.4419,  3.1510,  3.1052, -0.4332,\n",
      "        -1.3700, -1.3953], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4285, -2.2330,  4.5784,  1.7946, -0.4285,  3.1372,  3.1372, -0.4285,\n",
      "        -1.3787, -1.3652], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00029860023641958833\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([8.2892, 5.9565, 8.0351, 9.9874], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.6657, -2.2328, -0.4319,  0.6442,  1.8109,  1.8149, -0.4144,  6.2064,\n",
      "         1.8109,  4.5895], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.6298, -2.2238, -0.4202,  0.6334,  1.8281,  1.8281, -0.4242,  6.1896,\n",
      "         1.8281,  4.5858], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0002784676034934819\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1931, -1.3529, -1.3946, -2.2251], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.4559,  1.8176, -0.4219,  0.6273,  9.9877, -0.4559,  0.6429,  9.9877,\n",
      "         9.9877,  6.2130], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4214,  1.8193, -0.4214,  0.6358, 10.0000, -0.4214,  0.6259, 10.0000,\n",
      "        10.0000,  6.1901], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00037233243347145617\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3722, -0.4202, -0.4418, -2.2530], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 4.5599,  4.6075, -1.3496, -0.4458,  1.8043, -0.4551,  6.2193, -1.3669,\n",
      "         1.8043, -1.3664], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.5974,  4.5974, -1.3782, -0.4218,  1.8038, -0.4218,  6.1931, -1.3716,\n",
      "         1.8038, -1.3716], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0004756219277624041\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5256,  0.6597,  0.6320, -1.3567], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.4524,  3.1264,  9.9940,  7.9956, -2.1995, -1.3666,  9.9940, -0.4399,\n",
      "         1.8132, -1.3514], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4252,  3.1009, 10.0000,  7.9946, -2.2162, -1.3744, 10.0000, -0.4248,\n",
      "         1.8041, -1.3793], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0002897651866078377\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.7059,  0.6517,  1.8203, -0.4182], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.4477, -0.4472,  1.8000,  1.8203,  9.9973,  0.6198, -0.4292,  0.6237,\n",
      "         4.5554, -1.3770], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4288, -0.4288,  1.8042,  1.8120, 10.0000,  0.6200, -0.4288,  0.6331,\n",
      "         4.5988, -1.3786], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0002758178161457181\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6785, 1.8116, 3.1271, 0.6237], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.3710,  0.6516, -1.3621,  0.7088, -0.4412,  0.6306,  1.8115,  9.9999,\n",
      "         3.1271,  0.6249], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3781,  0.6445, -1.3786,  0.6445, -0.4340,  0.6186,  1.8144, 10.0000,\n",
      "         3.1040,  0.6186], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0005281417979858816\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8014, 3.0785, 4.5665, 1.7505], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 1.8365,  1.8365,  1.8077,  6.2163,  4.5886, -2.2075,  3.0785,  6.2163,\n",
      "         6.2163, -0.4340], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.8165,  1.8165,  1.8165,  6.2042,  4.5947, -2.2249,  3.1099,  6.2042,\n",
      "         6.2042, -0.4384], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00026587731554172933\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2109, 4.5877, 4.6138, 6.2097], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.6457, 10.0048, 10.0048, -0.4292,  8.0076,  3.1158,  1.8081, -0.4292,\n",
      "        -0.4292, 10.0048], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.6549, 10.0000, 10.0000, -0.4412,  8.0043,  3.1134,  1.8190, -0.4412,\n",
      "        -0.4412, 10.0000], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.214410288725048e-05\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2392, 4.6676, 6.2933, 8.0093], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.3638,  1.8370,  0.6197, -0.4292, -0.4292, -0.4292, -2.2300,  3.1313,\n",
      "         1.8098,  0.6465], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3688,  1.8182,  0.6206, -0.4422, -0.4422, -0.4422, -2.2187,  3.1147,\n",
      "         1.8182,  0.6533], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00014126519090496004\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.3053,  5.9638,  8.0373, 10.0058], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.6326, 10.0058,  3.1269,  8.0106, 10.0058,  0.6491, -2.2188, -0.4344,\n",
      "        -1.3631, -1.3631], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.6234, 10.0000,  3.1145,  8.0053, 10.0000,  0.6463, -2.2230, -0.4420,\n",
      "        -1.3682, -1.3682], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.6895591367501765e-05\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2161, -1.3636, -1.3652, -2.2258], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 6.1890,  0.6459, -0.4406,  1.8214,  8.0108, 10.0052, -2.2258, 10.0052,\n",
      "        -0.4251,  3.1211], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 6.2097,  0.6393, -0.4418,  1.8090,  8.0047, 10.0000, -2.2273, 10.0000,\n",
      "        -0.4418,  3.1142], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00010526364349061623\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3943, -0.3993, -0.4164, -2.2494], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.3726,  8.0319, -0.4465,  3.1152,  1.8121, -0.4465, -1.3996, -2.2325,\n",
      "         4.5707,  6.1839], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3848,  8.0034, -0.4407,  3.1136,  1.8037, -0.4407, -1.3848, -2.2277,\n",
      "         4.5655,  6.2087], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00019885078654624522\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5753,  0.6537,  0.6232, -1.3802], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 4.5937, -0.4200,  4.5683,  6.1834,  8.0093, -1.3720,  1.8112,  1.8112,\n",
      "        -0.4524,  6.1834], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.5651, -0.4365,  4.5651,  6.2084,  8.0037, -1.3843,  1.8171,  1.8171,\n",
      "        -0.4402,  6.2084], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.000274701596936211\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6223,  0.6392,  1.7913, -0.4604], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.4256,  4.5660,  1.7913, -0.4251, -1.4012, -1.3679, -0.4572, -1.3735,\n",
      "         6.1899,  0.6544], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4363,  4.5709,  1.7929, -0.4363, -1.3830, -1.3546, -0.4397, -1.3830,\n",
      "         6.2076,  0.6122], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0003270651795901358\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6238, 1.8150, 3.1006, 0.6048], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.3712, -0.4468,  1.8136, 10.0064, -1.3989, -2.2485,  0.5600, -0.4584,\n",
      "        -0.4468, -1.4081], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3567, -0.4411,  1.8175, 10.0000, -1.3842, -2.2340,  0.6400, -0.4411,\n",
      "        -0.4411, -1.3567], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0010093625169247389\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7953, 3.1037, 4.5693, 1.7669], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 4.5921, -0.4559, 10.0066,  8.0059, -1.3830,  6.2057, -0.4559,  4.5693,\n",
      "        -2.2453,  0.6122], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.5851, -0.4490, 10.0000,  8.0060, -1.3658,  6.2053, -0.4490,  4.5851,\n",
      "        -2.2447,  0.6274], grad_fn=<AddBackward0>)\n",
      "LOSS: 9.662377124186605e-05\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2209, 4.5855, 4.5957, 6.2117], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 1.7985,  1.8007, -0.4510, -1.3861, -0.4510, -0.4510, -0.4380, -1.3920,\n",
      "         0.6193, -0.4510], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.7914,  1.7914, -0.4552, -1.3942, -0.4552, -0.4552, -0.4552, -1.3748,\n",
      "         0.6139, -0.4552], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.918259118217975e-05\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2395, 4.6577, 6.2681, 8.0023], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 6.2170, -1.3791,  0.6103, -0.4361,  6.2170, -0.4472,  0.6159,  0.6226,\n",
      "         6.2170,  4.5775], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 6.2021, -1.3925,  0.6119, -0.4595,  6.2021, -0.4595,  0.6156,  0.6181,\n",
      "         6.2021,  4.5953], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0001878788461908698\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.3170,  5.9519,  8.0138, 10.0047], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.4259,  6.2154, -2.2275, -0.4457, -0.4457, -0.4457,  4.5812,  1.7833,\n",
      "         8.0012, -2.2275], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4537,  6.2011, -2.2445, -0.4618, -0.4618, -0.4618,  4.5938,  1.7928,\n",
      "         8.0042, -2.2445], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00025986426044255495\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2088, -1.4051, -1.3840, -2.2308], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.4477, -0.4400,  6.2100, 10.0028, 10.0028,  0.6040,  6.2100,  7.9998,\n",
      "         3.1017, -0.4477], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4626, -0.4564,  6.1998, 10.0000, 10.0000,  0.6007,  6.1998,  8.0025,\n",
      "         3.1261, -0.4626], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00015478952263947576\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2282, -0.4607, -0.4430, -1.3890], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.3890,  4.5858,  4.5702,  0.6197, -0.4517, -1.3717,  7.9987,  0.6014,\n",
      "        -0.4404, -1.3861], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3987,  4.5827,  4.5827,  0.6130, -0.4630, -1.3987,  8.0006,  0.5973,\n",
      "        -0.4630, -1.3987], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00018438136612530798\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3770,  0.5961, -0.4588, -0.4489], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.2413, -0.4588,  9.9979, -1.3910,  0.5982, -0.4511,  1.7919,  3.1201,\n",
      "         1.7733,  1.7733], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2505, -0.4635, 10.0000, -1.3982,  0.6127, -0.4614,  1.7907,  3.1279,\n",
      "         1.7907,  1.7907], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00011464237468317151\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3943,  1.7953,  0.6137, -0.4477], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 9.9953,  6.1869,  4.5851,  4.5788,  0.6011, -0.4573,  0.6137,  0.5954,\n",
      "        -0.4664, -0.4664], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([10.0000,  6.1958,  4.5682,  4.5682,  0.6086, -0.4623,  0.6158,  0.5987,\n",
      "        -0.4623, -0.4623], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.303167174337432e-05\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6377, 3.1276, 1.8342, 0.6536], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.4738,  4.5817, -0.4738, -1.3866, -2.2551, -1.3866,  9.9941, -2.2551,\n",
      "         0.5988, -0.4643], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4611,  4.5647, -0.4611, -1.3999, -2.2586, -1.3999, 10.0000, -2.2586,\n",
      "         0.6179, -0.4611], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00014000287046656013\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8378, 3.1018, 4.5788, 1.7683], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.4776, -1.3944,  7.9947, -0.4650, -0.4776, -2.2241, -0.4776,  6.2552,\n",
      "         5.9535, -2.2602], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4607, -1.3961,  7.9947, -0.4575, -0.4607, -2.2593, -0.4607,  6.1952,\n",
      "         6.1952, -2.2593], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.006419194396585226\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2309, 4.5864, 4.5676, 6.1759], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.4003, -0.4776,  0.6066, -0.4658, -2.2631,  4.5754,  1.7871, -0.4776,\n",
      "        -0.4369, -0.4761], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4124, -0.4571,  0.6225, -0.4555, -2.2542,  4.5583,  1.7780, -0.4571,\n",
      "        -0.4571, -0.4571], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00025666601140983403\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2431, 4.6843, 6.2366, 7.9754], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.4764,  9.9878,  6.1735, -0.4722,  4.5722, -0.4630,  9.9878,  3.0868,\n",
      "         0.6518, -0.4722], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4549, 10.0000,  6.1779, -0.4549,  4.5561, -0.4549, 10.0000,  3.1150,\n",
      "         0.5995, -0.4549], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00052267947467044\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([8.3265, 6.0171, 7.9834, 9.9892], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 7.9704, -0.4553, -2.2438,  0.6185, -0.4612,  3.0898, -0.4612,  4.5698,\n",
      "        -2.2438,  7.9704], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 7.9903, -0.4472, -2.2379,  0.6082, -0.4545,  3.1128, -0.4545,  4.5583,\n",
      "        -2.2379,  7.9903], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00017791618301998824\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2336, -1.4039, -1.3649, -2.2604], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 4.5662,  6.1796, -0.4463,  4.5662,  1.7972,  1.7972,  1.7972, -0.4497,\n",
      "         6.1796,  6.1796], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.5616,  6.1705, -0.4412,  4.5616,  1.7851,  1.7851,  1.7851, -0.4545,\n",
      "         6.1705,  6.1705], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.770351658109576e-05\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2527, -0.4475, -0.4119, -1.4012], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.4475,  7.9653, -0.4637,  1.8053, -0.4410,  6.1812, -1.3575,  6.1812,\n",
      "        -0.4577, -0.4410], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4375,  7.9920, -0.4545,  1.8375, -0.4545,  6.1687, -1.3707,  6.1687,\n",
      "        -0.4545, -0.4545], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0002793593448586762\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4084,  0.6075, -0.4360, -0.4606], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.3547, -0.4378,  3.1542, -0.4606, -0.4606,  3.1027,  7.9649,  0.6196,\n",
      "        -2.2608,  9.9918], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3660, -0.4369,  3.1001, -0.4533, -0.4533,  3.1001,  7.9927,  0.6223,\n",
      "        -2.2193, 10.0000], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0005743991350755095\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4105,  1.8087,  0.6327, -0.4487], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.4314,  1.8040, -0.4016,  4.5546,  0.6262, -1.3541,  0.5970, -2.2487,\n",
      "         9.9935,  7.9665], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4536,  1.7956, -0.4536,  4.5635,  0.6278, -1.3615,  0.6102, -2.2187,\n",
      "        10.0000,  7.9941], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0005285179940983653\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6270, 3.1492, 1.8456, 0.6551], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.5804,  6.1819,  7.9686, -0.4454, -0.4306,  3.1057,  9.9945,  1.8111,\n",
      "        -1.4114, -0.4306], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4377,  6.1717,  7.9951, -0.4402, -0.4489,  3.0960, 10.0000,  1.8343,\n",
      "        -1.3952, -0.4489], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.002279021078720689\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8003, 3.1200, 4.5428, 1.7403], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 7.9704,  4.6366,  7.9704, -0.4388, -0.4522,  0.5951,  1.7923,  4.5428,\n",
      "        -1.3766, -0.4388], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 7.9948,  4.5625,  7.9948, -0.4419, -0.4511,  0.6131,  1.7874,  4.5625,\n",
      "        -1.3797, -0.4419], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0007453910657204688\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2294, 4.6349, 4.5485, 6.1840], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 1.7764, -1.3893,  4.5378,  0.6193,  6.1840, -1.3867, -0.4332, -0.4453,\n",
      "        -0.4453,  3.0922], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.7830, -1.3899,  4.5656,  0.6384,  6.1777, -1.3899, -0.4379, -0.4379,\n",
      "        -0.4379,  3.0840], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0001427982497261837\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2429, 4.7514, 6.2081, 7.9790], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.2464,  6.1838, -1.3794, -1.3765,  6.1838,  4.5378,  0.5989, -0.5385,\n",
      "        -0.4282,  4.5378], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2585,  6.1811, -1.3969, -1.3969,  6.1811,  4.5654,  0.6100, -0.4402,\n",
      "        -0.4347,  4.5654], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0012236757902428508\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([8.3463, 6.0964, 7.9543, 9.9924], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.4115, -1.3656,  0.6186,  6.1768,  6.1768, -1.3799,  0.6451, -1.3799,\n",
      "         3.1364, -2.2476], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4236, -1.3944,  0.5948,  6.1820,  6.1820, -1.3944,  0.6396, -1.3944,\n",
      "         3.0914, -2.2602], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00042271491838619113\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1809, -1.4202, -1.3960, -2.2528], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 1.8173, -0.4753,  6.1703, -1.4202,  3.1275,  0.6158, -0.4404,  9.9893,\n",
      "        -1.3773, -0.4228], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.7912, -0.4512,  6.1821, -1.4177,  3.1047,  0.6034, -0.4395, 10.0000,\n",
      "        -1.3840, -0.4395], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00025229237508028746\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1855, -0.4580, -0.4136, -1.3747], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 3.1244,  4.5484,  9.9877,  1.8118,  4.6080, -0.4580, -2.2560,  0.5966,\n",
      "         6.1665,  1.7652], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.1195,  4.5499, 10.0000,  1.8016,  4.5499, -0.4556, -2.2502,  0.6164,\n",
      "         6.1827,  1.8011], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0005638602888211608\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3417,  0.6097, -0.4293, -0.4242], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.4293,  3.1178,  1.7592, -1.3852,  0.6097,  3.1178,  7.9758, -1.3696,\n",
      "        -2.2914, -0.4982], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4513,  3.1311,  1.8060, -1.3634,  0.6257,  3.1311,  7.9901, -1.3634,\n",
      "        -2.2467, -0.4398], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0009409422054886818\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3164,  1.8058,  0.6457, -0.4210], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 3.1158, -1.3650,  9.9900, -0.4911,  4.5953, -0.4911, -0.4318,  7.9856,\n",
      "         7.9856, -0.4645], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.1358, -1.3646, 10.0000, -0.4468,  4.5560, -0.4468, -0.4497,  7.9910,\n",
      "         7.9910, -0.4545], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0006459325086325407\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6923, 3.0974, 1.8110, 0.6283], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 6.1802,  1.7686,  4.5909, -0.4174,  3.1256,  1.8110, -1.3315,  3.2777,\n",
      "        -0.4174,  0.6078], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 6.1905,  1.7983,  4.5622, -0.4436,  3.1318,  1.7877, -1.3717,  3.1318,\n",
      "        -0.4436,  0.6188], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0026782520581036806\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.9103, 3.1365, 4.5948, 1.7921], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.4229,  1.7811, -0.4534, -0.4229,  9.9956, -1.4264, -0.4534,  0.6330,\n",
      "        -2.2500,  0.5773], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4369,  1.7936, -0.4369, -0.4369, 10.0000, -1.3924, -0.4369,  0.6352,\n",
      "        -2.2725,  0.6030], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0003436081751715392\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2350, 4.5935, 4.5321, 6.1802], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 9.9981, -0.4634,  6.1802, -0.4634, -2.2494, -0.4634,  6.1802, -0.4634,\n",
      "        -0.4634,  1.8145], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([10.0000, -0.4290,  6.1968, -0.4290, -2.2698, -0.4290,  6.1968, -0.4290,\n",
      "        -0.4290,  1.7892], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0007520558428950608\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2565, 4.7570, 6.2360, 7.9988], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.7178, -0.4546,  6.1834,  0.6345,  3.1062, -0.4316,  3.1042, -1.3456,\n",
      "        -2.2493,  6.1834], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.6180, -0.4268,  6.1989,  0.6451,  3.1484, -0.4268,  3.1484, -1.3640,\n",
      "        -2.2634,  6.1989], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.001562335412018001\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.3320,  6.0936,  7.9852, 10.0033], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.4389,  1.8314, -0.3329, -0.4316, -1.3623,  1.8205,  1.8098,  0.6403,\n",
      "        -0.4389,  4.6182], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4237,  1.7984, -0.4574, -0.4237, -1.3476,  1.8003,  1.7984,  0.6482,\n",
      "        -0.4237,  4.5700], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.002023396547883749\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1742, -1.3899, -1.3799, -2.2450], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 4.5933, -2.2450,  0.6301,  4.6247, -2.2450,  8.0049,  3.1225, -0.4062,\n",
      "        -0.4176,  1.8116], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.5797, -2.2419,  0.6419,  4.5797, -2.2419,  8.0072,  3.1622, -0.4329,\n",
      "        -0.4206,  1.8102], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0004685844178311527\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1758, -0.4046, -0.3491, -1.3357], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.2384, -0.3976,  1.8156,  1.8136,  6.2156,  8.0103, -2.2384, 10.0141,\n",
      "         0.6507,  4.6252], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2260, -0.4194,  1.8024,  1.8216,  6.2093,  8.0127, -2.2260, 10.0000,\n",
      "         0.6340,  4.5940], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00025113538140431046\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3845,  0.6473, -0.3855, -0.4164], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 6.2589, -0.4164,  4.5941, -2.2317,  4.6195,  0.6473, -0.3855, -1.3319,\n",
      "        -2.1861,  4.6195], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 6.2141, -0.4174,  4.6091, -2.2166,  4.6091,  0.6270, -0.4174, -1.3054,\n",
      "        -2.2166,  4.6091], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0005745243979617953\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4395,  1.8031,  0.6392, -0.4276], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.3817,  1.8405, -1.3281,  0.6392,  3.1413,  0.6454,  0.6152, -1.3281,\n",
      "         0.6493,  6.2543], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4156,  1.8080, -1.3040,  0.6228,  3.1477,  0.6394,  0.6907, -1.3040,\n",
      "         0.6228,  6.2202], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.001126109971664846\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5879, 3.1260, 1.8299, 0.6738], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 3.1260, -0.3959, -0.3916, -1.2944,  3.1367,  0.6564, -1.3206,  6.2671,\n",
      "         1.8654, 10.0337], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.1309, -0.3746, -0.4136, -1.3133,  3.1309,  0.6461, -1.3133,  6.2262,\n",
      "         1.8230, 10.0000], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.000612608331721276\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7701, 3.1320, 4.5708, 1.8027], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 1.8456,  6.2761,  1.8347, -1.3551, -0.4048,  1.8032,  3.1323, -1.3580,\n",
      "         6.2761, -0.4048], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.8184,  6.2304,  1.8184, -1.3125, -0.4110,  1.8191,  3.1137, -1.3224,\n",
      "         6.2304, -0.4110], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0008937984821386635\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1005, 4.6053, 4.5452, 6.2759], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 4.5452,  6.2759,  4.5550,  1.8256, -0.3414,  0.6769,  4.5550, -1.3118,\n",
      "         8.0404,  0.6769], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.6483,  6.2364,  4.6483,  1.8164, -0.5013,  0.6431,  4.6483, -1.3332,\n",
      "         8.0362,  0.6431], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.005802522413432598\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1485, 4.7653, 6.2266, 8.0399], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.6550,  0.6741, -1.3182,  0.5645,  1.8256, -2.1859, -0.4146, -0.4146,\n",
      "         0.5645,  0.6550], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.6218,  0.6619, -1.3284,  0.6430,  1.8307, -2.1996, -0.4105, -0.4105,\n",
      "         0.6430,  0.6218], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.001503931824117899\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.2125,  6.1055,  7.9542, 10.0296], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.4215, -1.3265,  4.5744, -0.4215, -1.3260,  1.8580, -0.4362,  8.0386,\n",
      "         1.8099, -0.3663], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4102, -1.3181,  4.5965, -0.4102, -1.3297,  1.8374, -0.4102,  8.0266,\n",
      "         1.8294, -0.4102], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00043781986460089684\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2358, -1.3078, -1.3396, -2.1887], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 1.8657,  6.1076, -0.3716,  0.6912, -1.3815, -0.3566,  3.1620, -0.4231,\n",
      "        -0.3716, -1.3396], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.8458,  6.2331, -0.4082,  0.6791, -1.3209, -0.4402,  3.1243, -0.4082,\n",
      "        -0.4082, -1.3307], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.003135666949674487\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3876, -0.3681, -0.3987, -2.1940], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 1.7910,  8.0277, -0.4481, 10.0166,  1.8246,  8.0277,  4.5896, -1.3564,\n",
      "        -0.4235,  0.6952], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.8398,  8.0150, -0.4030, 10.0000,  1.8398,  8.0150,  4.5469, -1.3334,\n",
      "        -0.4030,  0.6421], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0010843424824997783\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5474,  0.6749,  0.5842, -1.3692], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 4.5925, -1.3679,  0.6749,  6.2302,  1.7670, -0.4244,  8.0211,  4.5925,\n",
      "         8.0211,  3.1557], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.5304, -1.3394,  0.5706,  6.2190,  1.8489, -0.4005,  8.0109,  4.5304,\n",
      "         8.0109,  3.1332], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0027485303580760956\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.5788,  0.6348,  1.7329, -0.4867], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.1831, -1.3500,  6.1427,  3.1472,  0.6348,  6.1427,  0.6763,  4.5891,\n",
      "        -2.1831,  0.6763], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.1776, -1.3546,  6.2161,  3.1302,  0.5596,  6.2161,  0.5596,  4.5284,\n",
      "        -2.1776,  0.5596], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.004769522696733475\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6280, 1.8499, 3.1590, 0.6740], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 4.5937,  8.0136, -2.1704,  3.1560,  0.6738,  0.6738, -0.4277,  4.5937,\n",
      "        -2.1704, -0.4365], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.5467,  8.0126, -2.2021,  3.1343,  0.6338,  0.6338, -0.3936,  4.5467,\n",
      "        -2.2021, -0.3936], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0013088510604575276\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7898, 3.1393, 4.5836, 1.8088], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 4.5836,  8.0108, -1.3733,  0.5919,  4.5836, -2.1674,  1.8088, -0.4167,\n",
      "         1.7721, -1.3542], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.5689,  8.0157, -1.3937,  0.6286,  4.5689, -2.2071,  1.7978, -0.4361,\n",
      "         1.8357, -1.4004], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0010457776952534914\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.0611, 4.5804, 4.5352, 6.2111], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([10.0200,  0.6115, -1.3783,  6.2111, -2.1739,  4.5745, -0.5007, -1.3783,\n",
      "         1.8066, -1.3795], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([10.0000,  0.6217, -1.3980,  6.2073, -2.2063,  4.5900, -0.4781, -1.3980,\n",
      "         1.8284, -1.3981], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0003911650856025517\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1335, 4.7611, 6.2091, 8.0031], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.4058, -0.4237, -0.4425, -1.3367, -2.1882,  3.1381,  6.2091,  0.6096,\n",
      "         1.8198, -0.4058], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4616, -0.4616, -0.4616, -1.3983, -2.2030,  3.1123,  6.2028,  0.6147,\n",
      "         1.8243, -0.4616], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0012792185880243778\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.2276,  6.1271,  7.9532, 10.0173], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.3800, -0.4093, 10.0173, -0.4093, -0.4253,  0.5305, -2.2349, -0.4093,\n",
      "         0.5910, -2.2135], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3903, -0.4652, 10.0000, -0.4652, -0.4652,  0.6505, -2.2159, -0.4652,\n",
      "         0.6505, -2.2159], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.002968519926071167\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2281, -1.4294, -1.3877, -2.2422], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([10.0132,  4.5391,  0.5384,  7.9904,  0.5992,  4.5391,  3.0962,  6.2453,\n",
      "        -0.4384,  1.7703], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([10.0000,  4.6208,  0.6335,  8.0119,  0.6335,  4.6208,  3.0852,  6.1913,\n",
      "        -0.4560,  1.7866], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0027780348900705576\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.3010, -0.4888, -0.4998, -1.4296], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.4219, -0.4538,  7.9807, -0.4888,  0.5639,  3.0706, -1.4245,  0.6065,\n",
      "         7.9807,  0.6213], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4229, -0.4408,  8.0027, -0.5117,  0.6090,  3.0781, -1.4399,  0.6090,\n",
      "         8.0027,  0.6302], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00040856655687093735\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4130,  0.5533,  0.4975, -1.4527], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 9.9932, -1.4443,  1.7559, -0.4942, -0.5257,  4.5215, -0.4942,  6.2118,\n",
      "        -0.5257,  7.9732], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([10.0000, -1.4252,  1.7413, -0.4244, -0.4244,  4.5906, -0.4244,  6.1759,\n",
      "        -0.4244,  7.9939], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.003736448008567095\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3805,  1.8276,  0.9531, -0.4598], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.5001,  4.5345,  0.5529,  0.5529,  3.0920,  0.5527, -1.4196, -0.5001,\n",
      "        -2.3264,  6.1908], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4305,  4.5718,  0.6468,  0.6468,  3.0810,  0.6449, -1.4341, -0.4305,\n",
      "        -2.2776,  6.1711], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.004029200877994299\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6087, 1.7860, 3.0653, 0.5795], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.5021,  0.5463, -0.5021, -1.4393, -0.5021,  0.5463, -0.4783, -2.3390,\n",
      "         3.0818,  6.1333], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4475,  0.6321, -0.4475, -1.4082, -0.4475,  0.6321, -0.4462, -2.2953,\n",
      "         3.1081,  6.1683], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.002949207555502653\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8466, 3.1499, 4.5911, 1.7907], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.3995, -1.4236, -0.4487,  9.9685, -1.4236, -0.4550,  1.8191, -0.4550,\n",
      "         0.6068, -0.4931], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3686, -1.3686, -0.4991, 10.0000, -1.3686, -0.4560,  1.7671, -0.4560,\n",
      "         0.6023, -0.4539], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0014821195509284735\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.0704, 4.5820, 4.5362, 6.1435], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 1.7784, -0.4929,  0.5584, -0.4929,  3.0691,  3.1522, -1.3914,  3.1042,\n",
      "         0.5923, -0.3799], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.7938, -0.4665,  0.6305, -0.4665,  3.1550,  3.1550, -1.3419,  3.1550,\n",
      "         0.6179, -0.4665], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0027387342415750027\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1602, 4.7815, 6.2069, 7.9638], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.4802, -1.4413,  4.5768, -0.4759,  0.5791,  1.8079,  1.8064,  0.5791,\n",
      "        -2.2037,  4.6235], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4695, -1.3154,  4.5244, -0.4664,  0.5791,  1.7693,  1.7693,  0.5791,\n",
      "        -2.2247,  4.5244], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.003192154224961996\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([8.2588, 6.1478, 7.9405, 9.9672], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.4162,  1.7461,  1.7461, -1.3725,  3.0904,  3.0904,  6.1461, -0.3758,\n",
      "         9.9672, -2.2245], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3144,  1.8038,  1.8038, -1.3382,  3.1518,  3.1518,  6.1721, -0.4615,\n",
      "        10.0000, -2.2030], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0035259989090263844\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1675, -1.3635, -1.3316, -2.2224], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.4403,  0.6812,  1.7671,  4.5799,  9.9723, -2.2224, -0.4340, -1.4109,\n",
      "        -1.3647,  3.0826], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4333,  0.5940,  1.7791,  4.5407, 10.0000, -2.1985, -0.4531, -1.3652,\n",
      "        -1.3652,  3.1219], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0014675249112769961\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2346, -0.4246, -0.4394, -1.3586], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.4039,  7.9837,  9.9813, -0.4150, -0.4246, -0.4246,  1.8229,  0.6270,\n",
      "        -1.2939, -1.3157], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4102,  7.9831, 10.0000, -0.4102, -0.4060, -0.4060,  1.7535,  0.5713,\n",
      "        -1.3667, -1.3667], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0016933506121858954\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3100,  0.6768,  0.6160, -1.3045], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 3.0448,  1.7396,  4.5186, -1.2874, -0.4092,  6.1925, -0.3782,  1.7994,\n",
      "         1.8270,  1.8392], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.0668,  1.7403,  4.5733, -1.3626, -0.3940,  6.1942, -0.3940,  1.8621,\n",
      "         1.7403,  1.8621], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.002157387789338827\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3932,  1.7986,  0.9347, -0.4539], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 4.5035, -0.3979, -0.4656, -0.3979, -0.3558,  4.5035,  4.5091, -2.2499,\n",
      "        -0.3979, 10.0009], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.5852, -0.3847, -0.3847, -0.3847, -0.3847,  4.5852,  4.5852, -2.1485,\n",
      "        -0.3847, 10.0000], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.003733086632564664\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6299, 1.8161, 3.0567, 0.6344], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.3594,  0.6344,  1.7614, -2.1061,  0.6810, -1.3237, -0.3734, -0.3734,\n",
      "         8.0043,  1.7614], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3742,  0.6156,  1.7510, -2.1542,  0.6513, -1.3742, -0.3871, -0.3871,\n",
      "         8.0022,  1.7510], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0006912917015142739\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8383, 3.1572, 4.5314, 1.8121], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.1097, -1.3508, -1.2733, -1.3158, -0.3843,  0.6808,  4.5314,  1.8143,\n",
      "         0.6808, -0.3898], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.1459, -1.3653, -1.3653, -1.3653, -0.3738,  0.6529,  4.5741,  1.7607,\n",
      "         0.6529, -0.4457], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.002195477718487382\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1192, 4.6117, 4.5556, 6.1776], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 9.9980,  1.8424, -1.3508, -0.3950, -1.2751, -1.3121,  4.8114,  1.8052,\n",
      "         1.7848, -0.3524], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([10.0000,  1.8931, -1.3487, -0.3900, -1.3487, -1.3487,  4.5598,  1.7703,\n",
      "         1.7703, -0.3900], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0075506470166146755\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1887, 4.7946, 6.2153, 7.9994], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.3774,  0.6248,  0.6736,  3.0887, -1.3015,  0.6736,  0.6611,  7.9994,\n",
      "        -1.2906,  6.1889], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3937,  0.6100,  0.6629,  3.0990, -1.3295,  0.6629,  0.6100,  8.0016,\n",
      "        -1.3295,  6.1994], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.000583446235395968\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.2859,  6.1418,  7.9624, 10.0047], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.3836, 10.0047, -0.3799, -1.2949, -0.3799,  6.1989, -0.3766,  0.6619,\n",
      "         1.7946, -0.3539], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4368, 10.0000, -0.3976, -1.3114, -0.3976,  6.1979, -0.3527,  0.6151,\n",
      "         1.7888, -0.3976], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0008441481622867286\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1710, -1.3060, -1.3290, -2.2219], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.7257,  1.8047, -0.3836,  1.7997, -1.2933, -0.3836, -1.3060,  6.2060,\n",
      "         1.7997,  1.8592], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.6243,  1.7973, -0.4309,  1.7973, -1.2930, -0.4309, -1.3400,  6.1959,\n",
      "         1.7973,  1.8991], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.001766417408362031\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3414, -0.3947, -0.3815, -2.1865], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.4112,  3.2203,  0.6594, -2.2548,  1.8640,  7.9932, -1.3690,  6.2111,\n",
      "         7.9932,  6.1160], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4066,  3.1221,  0.6776, -2.1892,  1.8983,  8.0072, -1.2816,  6.1939,\n",
      "         8.0072,  6.1939], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0029862287919968367\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2630,  0.7067,  0.7053, -1.2925], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.3307,  4.5943, -2.2756,  1.9391, -1.2949, -0.3539,  3.1314,  6.1062,\n",
      "         3.1568, -0.4133], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4134,  4.5937, -2.2107,  1.8860, -1.2853, -0.4134,  3.1372,  6.1903,\n",
      "         3.1372, -0.4134], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.002499990165233612\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3725,  1.8044,  1.0208, -0.4468], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.4106,  1.8129, -1.3027,  0.7469,  4.6053,  0.6013,  0.7469, -1.3764,\n",
      "         4.6053, -0.3541], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4108,  1.8219, -1.2900,  0.6796,  4.5950,  0.6240,  0.6796, -1.2900,\n",
      "         4.5950, -0.4108], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0020706229843199253\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6340, 1.7889, 3.1345, 0.6253], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 3.1968,  4.6075, 10.0057,  6.2199,  3.1345,  6.2199, -0.3603, -0.3603,\n",
      "         4.6075,  4.6075], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.1467,  4.5979, 10.0000,  6.1773,  3.1467,  6.1773, -0.4072, -0.4072,\n",
      "         4.5979,  4.5979], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0010987472487613559\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8506, 3.1464, 4.6063, 1.8341], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.2905,  6.2167, 10.0052,  6.2167, -1.3262,  4.6063,  1.8751, -1.3262,\n",
      "        -1.4006, -2.2905], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2386,  6.1738, 10.0000,  6.1738, -1.3048,  4.5950,  1.8713, -1.3048,\n",
      "        -1.3048, -2.2386], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0019309690687805414\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1597, 4.5859, 4.6387, 6.2127], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 7.9718, -0.4630, -2.2646,  3.1781, -1.3868,  3.1781, -1.3326,  6.2127,\n",
      "        -0.4082,  3.1309], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 8.0063, -0.4121, -2.2481,  3.1469, -1.3799,  3.1469, -1.3276,  6.1747,\n",
      "        -0.4086,  3.1469], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0007770644733682275\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1655, 4.7311, 6.2424, 7.9750], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([10.0084, -0.3857,  6.2064, -0.4038, -2.2386,  0.7267,  0.6419, 10.0084,\n",
      "        -1.3458, -0.4123], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([10.0000, -0.4150,  6.1775, -0.4150, -2.2381,  0.6787,  0.6220, 10.0000,\n",
      "        -1.3503, -0.4150], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00046888162614777684\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.2875,  6.1138,  8.0236, 10.0079], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.4285,  0.6479, -0.3955,  7.9770, -0.4756, -0.3955, -2.2178,  6.1961,\n",
      "         4.6165, -0.3955], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4195,  0.6749, -0.4195,  8.0071, -0.4317, -0.4195, -2.2314,  6.1793,\n",
      "         4.5765, -0.4195], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0007448049145750701\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1432, -1.4122, -1.3637, -2.1998], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.6210,  6.2440,  0.5870, -1.3621,  0.7086, 10.0074, -2.1998, -0.4098,\n",
      "        -1.4122,  4.6142], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.6192,  6.1825,  0.6192, -1.3905,  0.6698, 10.0000, -2.2273, -0.4245,\n",
      "        -1.3905,  4.5702], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0010566782439127564\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1802, -0.4414, -0.4652, -1.3553], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.4099,  0.6502,  0.5826, 10.0093, -0.4414,  4.7300, -0.5084, -0.4099,\n",
      "         6.1868,  0.6261], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4259,  0.5912,  0.6097, 10.0000, -0.4360,  4.5682, -0.4463, -0.4259,\n",
      "         6.1874,  0.5912], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.003612323896959424\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2750,  0.6199,  0.6104, -1.3188], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 1.8509, -0.4509, -1.3603, -1.3603, -0.4149, -0.3670, -1.3188, -0.4508,\n",
      "         0.6199, 10.0126], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.8038, -0.4421, -1.4058, -1.4058, -0.4270, -0.4540, -1.4058, -0.4270,\n",
      "         0.5944, 10.0000], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.002308210823684931\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3846,  1.7797,  0.9602, -0.4330], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 4.5773,  1.8527, -1.3761, -1.3761, -0.4673, -0.4713,  3.0966, -0.4713,\n",
      "        -1.3936, -0.4417], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.5813,  1.8041, -1.3975, -1.3975, -0.4454, -0.4207,  3.1196, -0.4207,\n",
      "        -1.4205, -0.4454], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0010159716475754976\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6108, 1.7390, 3.0924, 0.6237], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.4807,  7.9907, -1.3871,  4.5706,  3.0924,  3.0924, -1.3871, -0.4890,\n",
      "         1.7609, -0.5074], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4209,  8.0107, -1.3923,  4.5840,  3.1135,  3.1135, -1.3923, -0.4405,\n",
      "         1.7832, -0.4209], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0015433828812092543\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8119, 3.1014, 4.5705, 1.8311], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 4.5900, -0.4801,  1.7927, -0.4801, -2.1681,  7.9918, -0.4284, -2.2140,\n",
      "         1.7713, -0.4576], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.5862, -0.4283,  1.8010, -0.4283, -2.2110,  8.0101, -0.4622, -2.2110,\n",
      "         1.7899, -0.4283], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0009980601025745273\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1231, 4.5424, 4.6024, 6.2099], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 4.5756, -1.3897, -0.4658,  3.1055,  4.5756, -2.2138, -0.4664, -0.4655,\n",
      "         6.2099, 10.0105], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.5889, -1.3867, -0.4392,  3.1181,  4.5889, -2.1871, -0.4392, -0.4392,\n",
      "         6.1943, 10.0000], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00037279468961060047\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1383, 4.6882, 6.2166, 7.9950], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.6066,  7.9950, -2.1716,  6.2092,  6.2092,  4.6142,  7.9950, 10.0088,\n",
      "         3.1263, -0.3933], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.5953,  8.0079, -2.1652,  6.1955,  6.1955,  4.5882,  8.0079, 10.0000,\n",
      "         3.1252, -0.4786], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0008904401329346001\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.2474,  6.0504,  7.9800, 10.0064], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.3946, -0.4362,  0.5826,  1.8080, -1.4027, 10.0064, 10.0064, -1.3816,\n",
      "        -0.4775, -1.3816], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3437, -0.4637,  0.6444,  1.7832, -1.3437, 10.0000, 10.0000, -1.3712,\n",
      "        -0.4637, -1.3712], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0011740506161004305\n",
      "---\n",
      "end game\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import gym \n",
    "import gym_examples\n",
    "from dqn import Agent\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('gym_examples/GridWorld-v0', size=4)\n",
    "agent = Agent(gamma=0.9, epsilon=0.5, batch_size=10, n_actions=4, eps_end=0.01, input_dims=[16], lr=0.003)\n",
    "scores, eps_hist,avg_scores_dqn = [], [], []\n",
    "n_games = 100\n",
    "\n",
    "for i in range(n_games):\n",
    "    score = 0\n",
    "    done=False\n",
    "    observation = env.reset()[0]['agent']\n",
    "    counter = 0\n",
    "\n",
    "    while not done:\n",
    "        agent_location = env.get_agent_location()\n",
    "        print(f\"Agent's Location: {agent_location}\")\n",
    "\n",
    "        # choose an action based on current state of env\n",
    "        action = agent.choose_action(observation)\n",
    "\n",
    "        observation_, reward, done, truncated, info = env.step(action)\n",
    "        observation_ = observation_['agent'] # since one hot encoded state is nested in dictionary\n",
    "\n",
    "        score += reward\n",
    "\n",
    "        # store transition and update weights\n",
    "        agent.store_transitions(observation, action, reward, observation_, done)\n",
    "        agent.learn()\n",
    "\n",
    "        #update state\n",
    "        observation = observation_\n",
    "        \n",
    "        # end of an episode\n",
    "        counter += 1\n",
    "        print('---')\n",
    "\n",
    "    scores.append(score)\n",
    "    eps_hist.append(agent.epsilon)\n",
    "    print('end game')\n",
    "\n",
    "    avg_score = np.mean(scores)\n",
    "    \n",
    "    avg_scores_dqn.append(avg_score)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Need to make it average over these 500 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the average score per episode\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "episodes = list(range(1, n_games + 1))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(episodes, avg_scores_dqn, marker='o', linestyle='-', color='b', label='Average Score per Episode')\n",
    "plt.title('Scores Over 100 Episodes')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Score')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(avg_scores_dqn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.0393,  0.0546, -0.0600, -0.0626], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.0393,  0.0546, -0.0600, -0.0626], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.0338,  0.0528, -0.1062, -0.0641], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.0024, -0.0279, -0.1279, -0.1193], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.1971, -0.1378, -0.2520, -0.1779], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3314, -0.2929, -0.3899, -0.2766], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.5632, -0.4826, -0.6300, -0.4441], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.8609, -0.7317, -0.8312, -0.7397], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9470, -0.8604, -0.9934, -0.8032], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2828, -1.1569, -1.2236, -1.1755], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4082, -1.2851, -1.3478, -1.3833], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4008, -1.3184, -1.3749, -1.4009], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3489, -1.2543, -1.3221, -1.3469], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2966, -1.1798, -1.2677, -1.2861], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.1786, -1.0388, -1.1610, -1.1522], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.1244, -0.9824, -1.1109, -1.0955], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-0.8281, -0.6980, -0.8875, -0.7966], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-0.8136, -0.7019, -0.8736, -0.7890], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.9175, -0.8905, -1.0201, -0.9120], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.0155, -1.1017, -1.0013, -1.0625], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.8837, -0.9679, -0.9293, -0.9476], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9075, -1.0462, -0.9528, -1.0026], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.0070, -1.1861, -1.0006, -1.1123], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9781, -1.1315, -0.9752, -1.0833], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9106, -1.0050, -0.9424, -1.0011], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9634, -1.0708, -0.9470, -1.0508], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.7203, -0.8449, -0.8033, -0.8084], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2214, -1.2929, -1.1514, -1.2502], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2523, -2.2070, -2.0728, -2.0774], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.4388, -2.3787, -2.2795, -2.2361], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.4946, -2.4222, -2.2914, -2.2232], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.0306, -2.1438, -2.1789, -1.9707], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1112, -2.2446, -2.0733, -2.0767], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8946, -1.8033, -1.8149, -1.6543], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8138, -1.7311, -1.7438, -1.5647], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.7051, -1.7498, -1.7986, -1.5830], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.7211, -1.6257, -1.6327, -1.4591], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.7655, -1.6674, -1.6674, -1.5127], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8324, -1.7254, -1.7171, -1.5984], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9060, -1.7846, -1.7683, -1.6863], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9856, -1.8382, -1.8194, -1.7721], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.0689, -1.8905, -1.8701, -1.8553], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2467, -2.0767, -1.9762, -2.1188], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2484, -2.0447, -1.9560, -2.0914], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2419, -2.0043, -1.9245, -2.0528], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2073, -1.9560, -1.8877, -2.0045], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1737, -1.9126, -1.8529, -1.9567], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9138, -1.6235, -1.6718, -1.6398], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8992, -1.6215, -1.6668, -1.6381], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9386, -1.7313, -1.8114, -1.7627], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8880, -1.6660, -1.7011, -1.6746], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8976, -1.6994, -1.7271, -1.7041], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9043, -1.7305, -1.7502, -1.7332], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9148, -1.7705, -1.7750, -1.7669], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9452, -1.8869, -1.9205, -1.9017], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9222, -1.8521, -1.8205, -1.8448], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.7922, -1.7950, -1.7784, -1.7682], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9975, -2.0935, -2.1150, -2.1126], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2355, -2.4319, -2.5603, -2.4974], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "randomly selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-2.5193, -2.7119, -2.8796, -2.6097], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-2.4482, -2.5839, -2.7243, -2.4662], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-2.3916, -2.4761, -2.5899, -2.3561], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-2.3257, -2.3156, -2.3287, -2.2797], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.4463, -2.1068, -2.3535, -2.1949], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-2.3918, -2.3276, -2.2991, -2.3986], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.7399, -2.5206, -2.4908, -2.4247], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.5916, -2.4595, -2.4556, -2.2759], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9415, -1.8664, -1.9219, -1.4584], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.7416, -2.4747, -2.6669, -1.9116], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-2.8531, -2.9171, -2.8600, -2.1321], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.8060, -2.6439, -2.7964, -1.9203], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.8645, -3.0047, -2.8615, -2.2271], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.7646, -2.9679, -2.7787, -2.2527], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.6414, -2.5405, -2.6645, -2.1352], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.6401, -2.5024, -2.6401, -2.1830], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6265, -2.5923, -2.5959, -2.3840], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6741, -2.6136, -2.5503, -2.4139], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.5825, -2.6900, -2.6884, -2.5406], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6120, -2.7352, -2.6927, -2.5296], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.7999, -2.8477, -2.6937, -2.6703], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.8883, -2.9826, -2.8332, -2.7302], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.0421, -3.1777, -3.0481, -2.8786], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.2071, -3.3946, -3.2780, -3.0404], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.2234, -3.5868, -3.5918, -3.0931], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.6667, -3.7664, -3.8401, -3.2356], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.5724, -3.6534, -3.7305, -3.1902], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.3221, -2.9957, -3.2928, -3.1764], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-2.9520, -2.7894, -3.0140, -2.9656], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-3.3087, -3.1075, -3.3967, -3.1767], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-3.8103, -3.4743, -3.7867, -3.7660], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-3.9209, -3.5998, -3.8913, -3.8562], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.0076, -3.8535, -4.0189, -3.8544], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-3.9347, -3.8461, -3.9818, -3.7671], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-3.6569, -3.7461, -3.7761, -3.3470], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.1283, -3.2254, -3.2250, -3.0051], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.2491, -3.3581, -3.2735, -3.0697], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.1777, -3.2751, -3.1591, -2.9748], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.1478, -3.2465, -3.0730, -2.9336], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.0808, -3.2348, -3.0216, -2.9008], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.3905, -3.4941, -3.1967, -3.1632], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.4984, -3.6066, -3.2903, -3.2901], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.6003, -3.7121, -3.3852, -3.4286], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andyyang/Desktop/DS598 Reinforcement Learning/gym-examples/.env/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.6709, -3.6773, -3.5858, -3.7057], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3929, -3.3479, -3.4132, -3.5471], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3614, -3.3636, -3.3885, -3.4042], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3137, -3.4334, -3.5322, -3.6777], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.3676, -3.3704, -3.4579, -3.4435], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3946, -3.4568, -3.5748, -3.6374], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4377, -3.4805, -3.5853, -3.4977], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.6243, -3.5415, -3.5617, -3.2617], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.7189, -3.5692, -3.4647, -3.1981], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.8855, -3.7141, -3.5461, -3.2825], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.0534, -3.8635, -3.6556, -3.4789], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-4.1188, -4.0578, -3.8964, -3.8165], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.2192, -4.1594, -3.9784, -4.0379], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.4685, -4.3990, -4.3815, -4.6430], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0649, -3.9212, -4.1609, -4.3391], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.8366, -3.7435, -4.0679, -3.6731], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.6634, -3.4824, -3.9451, -3.7168], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.7813, -3.6291, -4.0596, -3.6048], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.6552, -3.3884, -3.9020, -3.5567], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.8766, -3.5357, -3.9927, -3.7261], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1828, -3.8826, -4.2181, -4.1578], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1297, -3.8799, -4.1211, -4.3857], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.6972, -3.5650, -3.6209, -3.5633], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.9214, -4.0238, -3.8792, -3.9421], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.9448, -4.0952, -4.0600, -3.9716], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-3.9870, -4.0918, -4.1743, -4.1663], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.8572, -3.6497, -3.7863, -3.9474], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.9073, -3.6609, -3.7512, -4.0540], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.9792, -3.6910, -3.7255, -4.1582], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3072, -4.0165, -3.9702, -4.2092], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0531, -3.9553, -3.6878, -4.2723], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.7694, -3.8652, -3.5973, -3.5760], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0424, -4.2550, -4.1582, -4.3837], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-4.2024, -4.3425, -4.2190, -4.6285], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-4.1858, -4.2979, -4.2488, -4.6454], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-4.1384, -4.2157, -4.2594, -4.5436], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-4.0911, -4.1239, -4.2050, -4.4340], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-4.1512, -4.2084, -4.1204, -4.5210], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.4558, -4.5833, -4.2728, -4.9483], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5751, -4.7921, -4.4659, -5.0477], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6723, -4.9264, -4.6442, -5.1025], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7203, -5.0126, -4.7939, -5.1002], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8200, -5.0621, -4.9260, -4.9994], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6782, -4.6917, -4.8023, -4.6215], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5353, -4.7370, -4.9053, -4.4288], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5434, -4.5745, -4.7989, -4.2307], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5687, -4.4867, -4.7451, -4.3358], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3671, -4.4773, -4.5654, -4.5632], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-4.6943, -4.7562, -4.4409, -4.9682], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.4884, -4.7102, -4.4115, -4.6486], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7169, -4.8241, -4.4604, -4.9125], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6735, -4.8615, -4.4891, -4.7838], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6341, -4.8559, -4.5604, -4.6633], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6314, -4.8457, -4.6844, -4.6854], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7492, -4.7273, -4.7877, -4.8548], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6069, -4.4271, -4.6506, -4.7611], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6994, -4.3721, -4.5759, -4.9491], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6776, -4.2747, -4.5175, -4.9367], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6033, -4.1709, -4.4022, -4.8961], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5192, -4.1106, -4.2715, -4.8293], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.4306, -4.1032, -4.1745, -4.7349], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3806, -4.1707, -4.1271, -4.6597], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0263, -4.3287, -3.8103, -2.9033], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.0608, -2.4989, -1.6541,  2.1915], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.9548, -2.8595, -0.6825, 10.4742], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.8370, -5.0211, -4.5617, -4.2011], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.0285, -5.1694, -4.7216, -4.4759], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.2993, -5.4625, -5.0322, -5.2400], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.1740, -5.2502, -4.9930, -5.2868], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7648, -4.6654, -4.5345, -4.7621], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0208, -5.1216, -4.9706, -5.2519], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.1911, -5.2987, -5.1261, -5.5168], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.5450, -5.4657, -5.3829, -5.9046], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.8486, -5.5559, -5.6145, -6.1527], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.8733, -5.1335, -5.2567, -5.8590], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.3255, -5.4722, -5.5872, -6.7706], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.0321, -5.1738, -5.3659, -6.4703], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0106, -4.6223, -4.3171, -4.8073], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.1057, -4.1113, -2.9886,  1.1404], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.7458, -2.7321, -0.1766, 11.8796], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.6804, -5.5417, -5.2420, -5.1009], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.8192, -5.7497, -5.4312, -5.2937], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.9520, -5.9455, -5.6509, -5.5587], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.0825, -6.1028, -5.7605, -5.8965], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.3004, -6.2154, -5.9234, -6.3915], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.9429, -5.6899, -5.5056, -5.7642], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.7902, -5.5205, -5.3653, -5.4572], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.6385, -5.3085, -5.2040, -5.1746], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.9361, -5.4219, -5.3953, -5.2111], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.0247, -5.3502, -5.3930, -5.0691], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.2642, -5.4373, -5.5541, -5.1702], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.5272, -5.6089, -5.7993, -5.4058], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.7959, -5.8466, -6.0317, -5.6893], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.8625, -6.0717, -6.2158, -5.9283], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.8020, -6.2071, -6.2797, -6.0385], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.6821, -6.2162, -6.2540, -6.0803], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.4542, -6.1383, -6.1666, -5.9996], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0832, -5.1711, -5.0578, -5.3094], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3056, -5.4569, -5.1046, -5.5863], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.4264, -5.5447, -5.1265, -5.7992], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.8359, -5.8005, -5.3051, -6.1434], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.0228, -5.9262, -5.4514, -6.2157], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.1190, -5.9981, -5.5511, -6.1369], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.1339, -6.0300, -5.6434, -6.0241], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.6756, -5.7962, -5.7079, -5.7010], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.9255, -6.0711, -6.0329, -5.3363], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-5.8281, -5.5019, -6.1163, -6.1068], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-5.7668, -5.6392, -6.0968, -6.3152], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-5.7944, -5.2919, -5.5190, -5.6147], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-5.7252, -5.3370, -5.2685, -5.9521], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.6175, -5.0078, -4.3625, -5.6964], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9979, -1.3296,  0.8494,  6.7054], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9471, -1.3659,  0.8166,  6.3481], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.5791, 0.1429, 2.3312, 8.7524], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 1.8643,  0.2223,  2.6736, 10.0545], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.5052, -6.5472, -5.6681, -6.6456], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.9554, -6.1627, -4.9886, -5.6824], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.8598, -6.1299, -4.9263, -5.4869], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.8364, -6.1711, -5.0446, -5.4913], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.7751, -6.1061, -4.9698, -5.4783], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.0302, -6.2056, -4.9925, -6.1041], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.2300, -6.3192, -5.2284, -6.5038], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.4643, -6.4469, -5.5464, -6.9677], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.9198, -6.5282, -6.5072, -8.0158], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.3982, -6.0583, -6.6922, -7.3193], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.9840, -5.2940, -5.7247, -5.9235], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-5.4735, -5.1173, -5.2547, -5.0290], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-5.9096, -5.8697, -5.4656, -5.7147], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-6.4327, -6.3275, -5.6083, -6.1914], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.9323, -6.3778, -5.9810, -6.2663], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-5.2743, -4.9160, -4.7895, -5.5027], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-5.7187, -4.3776, -5.2039, -5.9479], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-5.8952, -3.8569, -4.9555, -5.9401], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-6.0810, -4.0193, -5.3108, -6.4633], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-6.5918, -4.0041, -5.6927, -6.8034], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.9096, -4.2924, -6.0076, -7.0164], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.9041, -3.4476, -2.1967, -5.2913], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.6956, -3.6538, -1.5258, -4.8509], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6839, -2.6155,  3.0401, -3.8447], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.4567, 1.8853, 3.4494, 7.2665], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.4495, 2.5783, 3.6554, 8.4056], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.0576, 2.9078, 4.3433, 9.9877], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-6.3028, -6.5014, -6.1536, -5.8673], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.9878, -6.4574, -6.0113, -5.7658], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.4472, -6.3513, -5.9267, -5.6873], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.3512, -6.3152, -5.9861, -5.7457], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.4587, -6.2402, -6.1790, -6.0185], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.4043, -6.5638, -6.7779, -6.6562], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-6.2972, -6.5557, -6.8973, -6.8327], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-6.8053, -6.7067, -7.1477, -7.2144], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-6.9502, -6.3990, -7.0348, -7.0693], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-7.5321, -6.6598, -7.4522, -7.5447], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-7.1436, -5.4056, -5.3837, -6.9370], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.9266, -4.2960, -4.6894, -5.0645], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.3336, -3.7079,  0.0196, -6.7904], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7413, -1.0421,  4.3248, -4.6967], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.6168, 3.3366, 4.0258, 8.3099], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.1626, 3.7363, 4.8757, 9.7785], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-6.7673, -5.7590, -6.5901, -6.7621], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-7.3019, -6.1528, -7.1024, -7.2777], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-7.0717, -5.1279, -5.1156, -7.0278], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.9769, -2.2908, -4.3300, -5.2381], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3875, -0.7934,  4.7655, -6.0854], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.0489, 3.0437, 4.4609, 5.9523], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.9004, 3.9724, 5.1912, 8.8681], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 4.6112,  4.4211,  5.8946, 10.4331], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-6.0291, -5.7822, -6.3966, -6.4297], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-5.2031, -4.9376, -5.7616, -5.5062], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-4.7562, -3.6124, -3.8821, -5.1173], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0496, -2.1804,  0.9289, -7.2655], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0195, -1.9189,  0.7251, -7.5507], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.7577,  1.5679,  5.0751, -4.1905], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.9572, 4.8660, 5.5659, 6.6543], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.5276, 4.8063, 5.5591, 8.2897], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.0133, 5.0407, 6.0267, 9.1639], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-6.3970, -5.5126, -6.7773, -8.4211], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-5.9229, -5.0125, -6.5314, -7.4585], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-6.5780, -4.8889, -6.1374, -8.1698], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.7313, -3.7494, -3.7310, -6.1538], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.5610, -3.4030, -3.0716, -5.3771], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.3762, -3.5362, -2.7341, -4.8369], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3766, -0.0638,  2.9372, -5.0580], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.7911,  3.2711,  5.4872, -1.7427], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.0994, 5.3240, 5.6479, 7.8129], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.9227, 5.2934, 5.4257, 7.3360], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.8897, 5.0499, 5.2007, 8.2044], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.8035, 5.2305, 5.8498, 9.9029], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-7.0761, -6.3867, -6.6395, -7.6621], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-6.8619, -6.2107, -6.8722, -7.3012], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-6.8122, -5.8286, -5.7328, -7.7700], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-6.3866, -5.8020, -5.4625, -7.1222], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-5.9208, -5.5609, -5.2237, -6.5510], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0279, -1.7476, -2.4358, -6.1919], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.1382,  0.2016,  3.3056, -5.7047], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9396,  3.3903,  4.4692, -2.6495], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.8050, 5.4105, 4.3830, 6.6268], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.9087, 5.1862, 4.4906, 7.8814], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.9045, 5.2923, 5.2016, 9.7964], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-6.2863, -5.1740, -6.5289, -6.5599], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.6148, -4.8453, -1.2799, -6.3826], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4793, -0.2658,  3.2255, -5.7213], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.6087,  4.0524, -3.1936, -5.9762], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.8809,  3.5149,  4.4979, -2.0828], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.7939, 5.5797, 4.2769, 6.9459], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.0176, 5.3354, 4.5814, 8.3555], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 6.1252,  5.5946,  5.4194, 10.4710], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-6.4388, -4.5896, -6.5161, -6.6855], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-5.5448, -2.4923, -5.5487, -6.0035], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-5.7453, -1.8220, -5.1859, -6.3072], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-2.6931, -1.0737,  1.2419, -4.2039], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8878,  3.3118,  4.9152, -2.9645], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5858, 5.5093, 5.8463, 0.9199], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.5960, 6.8630, 5.3007, 6.6159], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.6404, 6.6621, 5.2627, 6.6444], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.7481, 6.4377, 5.2345, 6.7362], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.2343, 5.6229, 5.2141, 8.4801], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 6.2386,  5.4470,  5.7435, 10.2528], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-7.1773, -5.5111, -7.0487, -7.1045], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-6.4260, -5.3157, -6.3901, -6.8183], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-4.5497, -3.5914, -3.4453, -5.1265], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.2508,  0.9088,  1.1204, -4.2307], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.7501,  3.2524,  0.3108, -2.9919], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.9984, 2.8270, 5.2630, 2.6257], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.9186, 3.8492, 4.6824, 6.2541], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.2601, 3.7374, 4.8946, 7.8802], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0041, 3.6563, 5.3380, 9.2086], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-7.2069, -6.2837, -6.1022, -7.1923], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.9223, -6.7469, -6.8863, -7.8614], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.4966, -4.1239, -3.9514, -7.3380], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.4785, -1.4202, -3.7014, -7.2425], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.0892,  3.0358,  1.5259, -4.0061], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8431,  0.8835,  3.8381, -3.1431], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8692, 3.2243, 5.4594, 2.2018], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.9075, 4.5135, 5.2249, 6.0521], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.5517, 4.4014, 5.2896, 8.2932], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 6.6975,  4.5619,  5.8008, 10.3573], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-7.1700, -4.6210, -6.8482, -6.8552], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-6.5058, -3.9404, -5.9540, -6.9963], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-5.3437, -1.6500, -2.9809, -6.3449], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-2.3508, -1.6216,  1.3361, -4.1927], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.7119,  2.0047,  3.8504, -2.7827], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.9932, 3.7635, 5.6669, 2.6235], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.8760, 4.8431, 5.6169, 6.2994], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.5300, 4.5135, 5.7570, 8.4992], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 6.8380,  4.6281,  6.7488, 10.8336], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.6315, -4.2511, -5.2029, -5.3582], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1278,  2.2051, -1.0890, -4.8270], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.5085,  4.4197,  3.4417, -2.2006], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.3556, 3.9291, 6.1326, 3.3623], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.5340, 5.1025, 6.5838, 6.9670], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.7485, 4.7744, 6.4217, 8.4973], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.6867, 4.7680, 6.3547, 8.1876], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.6952, 4.8406, 7.0421, 9.6818], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-6.3229, -5.0021, -4.1490, -7.3705], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5286, -2.9560, -2.6591, -5.4407], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6078, -2.2253, -3.4443, -4.7841], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4513,  2.7949, -0.2921, -4.7658], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3508,  4.3387,  3.1284, -2.4387], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.1854, 3.8484, 5.3268, 2.5115], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.1244, 4.7912, 5.6575, 5.6863], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.7962, 4.6450, 5.7875, 7.9298], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.7366, 4.8491, 6.4656, 9.4501], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.4419, -3.0973, -3.7292, -5.3445], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1216, -0.0201, -1.4758, -4.7872], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.7886,  3.0830,  2.1422, -2.9187], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.8713, 3.8796, 4.5861, 3.5472], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.8209, 4.9035, 5.1988, 6.6192], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3886, 5.1247, 5.9297, 8.7380], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.1210, 5.3192, 6.5019, 9.9348], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-6.0914, -4.4684, -5.1275, -4.7090], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-5.3918, -2.6120, -3.2000, -5.3988], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-2.9702,  0.6655,  0.6110, -3.9075], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-0.5510,  0.4875,  2.5294, -2.0933], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5001, 2.6576, 3.5600, 0.3306], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.2678, 3.6905, 4.6176, 2.6608], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.1062, 4.7656, 5.3140, 5.7079], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "randomly selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-7.0091, -4.4223, -5.2633, -5.4888], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-5.9946, -2.5956, -2.9289, -6.1838], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-3.4054,  0.5657,  0.8039, -4.4230], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6731,  2.7998,  2.2761, -3.4798], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.2031, 2.8441, 4.1397, 1.4581], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.6171, 3.7513, 4.8752, 3.3396], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.6574, 4.8464, 5.6340, 6.6318], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.4483, 5.1860, 6.5382, 9.0702], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.4740,  5.6362,  7.3993, 10.6629], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.9047, -3.3288, -3.3010, -3.5595], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.5548, -1.1418, -1.4921, -2.9934], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8573,  1.7275,  1.5127, -2.4246], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.0986,  2.9163,  2.7658, -1.7541], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7745, 2.6356, 3.9745, 2.0263], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.8342, 4.3487, 5.3279, 6.4645], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.1585, 5.1085, 6.9061, 9.5048], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.1793, -3.0004, -2.8591, -4.2157], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3133, -0.6057, -0.9829, -3.0459], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.0643,  1.6468,  1.5441, -2.8665], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.0668,  2.7760,  2.6045, -1.9765], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.4200, 2.3764, 3.5098, 1.2899], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.5626, 3.1586, 4.0307, 2.8544], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.7473, 4.1722, 4.9537, 6.2272], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.9038, 4.3586, 5.5689, 7.7169], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.1066, 5.0831, 6.6568, 9.4572], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-6.1044, -3.2478, -3.4195, -5.4966], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.3244, -0.3998, -0.4134, -4.6031], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-1.2967,  1.4669,  1.5347, -2.6595], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.8467,  2.9049,  2.4840, -1.8320], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.5622, 2.4772, 3.4284, 1.4493], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.7721, 3.2382, 4.1003, 3.1817], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.0565, 4.2326, 5.1054, 6.8038], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3702, 4.4951, 5.8919, 8.4937], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.5957,  5.2351,  7.0029, 10.3363], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-6.1930, -3.2964, -3.5428, -5.6488], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.4405, -0.2916, -0.3665, -4.7314], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-1.2236,  1.4965,  1.5784, -2.5783], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4873,  2.9435,  2.6175, -1.4097], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7328, 2.4924, 3.4750, 1.6645], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.9022, 3.2302, 4.2482, 3.3464], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.1831, 4.2104, 5.2115, 6.8862], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.4315, 4.4444, 5.9713, 8.3707], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.5537, 5.1506, 7.0322, 9.9720], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.9136, -2.9088, -3.2508, -4.9481], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-1.1726,  1.5096,  1.5548, -2.6059], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4146,  2.9061,  2.6171, -1.4487], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6138, 2.3843, 3.3980, 1.3755], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.1263, 4.1121, 5.2536, 6.4729], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.4300, 4.3619, 6.1538, 7.9964], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.5658, 5.0372, 7.3123, 9.5718], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.1911, -2.1518, -1.8678, -3.2067], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.3843,  0.4582, -0.8454, -1.8696], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.5624,  1.7498,  1.7279, -2.3439], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7104, 3.0769, 3.1618, 1.7187], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6944, 3.0293, 3.1352, 1.7242], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6769, 3.0100, 3.0960, 1.7318], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6462, 2.9868, 3.0452, 1.7255], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.5914, 2.9530, 2.9879, 1.6786], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.4879, 2.8873, 2.8778, 1.5896], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.3922, 2.6543, 4.0010, 2.7988], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.8422, 3.9093, 5.0187, 6.2881], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0442, 4.1943, 5.8204, 7.7252], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.2667, 5.0196, 7.0212, 9.5675], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.6527,  0.3572,  0.8570, -2.6730], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3221,  1.5400,  1.5954, -1.7574], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.5185,  1.5208,  0.0522,  0.4077], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.1672,  2.3659,  1.5801, -0.5895], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.2096, 2.8565, 2.7088, 1.5362], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.2458, 2.8192, 4.2030, 2.9397], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.7058, 4.3371, 5.3550, 6.5919], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0076, 4.6813, 6.3202, 8.1574], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.5374,  5.8081,  8.0264, 10.4682], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.5970, -1.6184, -0.9412, -3.5745], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6766,  0.4242, -0.5170, -1.6521], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4583,  1.4601,  1.5876, -1.8207], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.0747,  2.2972,  1.6836, -0.4660], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.0975, 2.7912, 2.7158, 1.3260], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.2677, 2.9636, 4.6257, 2.8043], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.4583, 4.3664, 5.2701, 5.9501], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.8259, 4.7823, 6.2688, 7.5310], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "randomly selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.0429, -1.6873, -1.2389, -3.9631], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.8954,  0.3121, -0.5896, -1.8527], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4256,  1.3857,  1.5413, -1.8276], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.0889,  2.2202,  1.6752, -0.3384], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.1421, 2.7147, 2.5849, 1.2913], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4768,  1.1943,  1.2018, -1.2358], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.0345,  2.0153,  2.1109, -0.4730], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.1621, 2.5466, 2.1532, 1.2718], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.2362, 2.7146, 4.2083, 2.6225], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.5773, 4.2724, 4.8967, 6.0838], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.9775, 4.7140, 6.0451, 7.7869], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.4184, 5.7346, 7.5860, 9.8836], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.7952, -1.0343, -1.1368, -2.7036], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.2936,  0.3388,  0.3467, -2.2871], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3843,  1.2403,  1.3408, -0.8725], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.3842, 1.7663, 1.2620, 0.0266], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.1048, 2.3424, 1.9665, 1.1718], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.0283, 2.4654, 4.1834, 2.2960], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.3543, 4.0399, 4.8815, 5.7932], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.9201, 4.6717, 6.1978, 7.7366], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.3815, 5.7729, 7.7921, 9.8970], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1271, -0.8133, -0.7050, -2.1919], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4577,  0.2973, -0.1422, -0.7044], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3143,  1.1591,  1.2991, -0.8177], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.3716, 1.6937, 1.2277, 0.0416], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.1149, 2.4005, 1.9356, 1.2160], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.9508, 2.5017, 4.2425, 2.2637], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.3005, 4.1007, 4.8879, 5.8565], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.8733, 4.7615, 6.2214, 7.8534], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "randomly selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.8770, -0.6361, -0.6182, -2.0151], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3623,  0.3337, -0.0942, -0.6746], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3658,  1.2241,  1.1565, -0.8533], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3895,  1.2459,  1.1326, -0.8722], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.0349,  1.7815,  1.7594, -0.4655], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.2646, 1.8125, 2.8581, 1.3955], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7736, 2.6574, 4.1617, 2.1131], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.1853, 4.3098, 4.7546, 5.8156], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.7311, 4.9232, 6.0999, 7.8050], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.2098,  6.1341,  7.7751, 10.0488], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-0.9837,  0.3166,  0.6357, -2.1136], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.5501,  1.1698,  1.1744, -0.9744], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.1444,  1.6551,  1.0869, -0.1421], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.9974, 2.6898, 1.7402, 1.1089], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7068, 2.6207, 4.3256, 2.0795], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.1797, 4.2730, 4.8968, 5.8495], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.7363, 4.8828, 6.1723, 7.8184], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.2232,  6.0747,  7.8727, 10.0708], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1746, -0.6907, -0.6643, -2.2703], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.6480,  0.2437, -0.0284, -0.8592], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.5832,  1.0813,  1.0359, -0.9631], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.0964,  1.6071,  1.7263, -0.5285], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.9983, 2.6912, 1.5372, 1.0793], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6427, 2.5200, 4.1887, 1.9976], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.1987, 4.2257, 4.7287, 5.8343], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.7769, 4.8587, 6.0228, 7.8288], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.2388,  6.0460,  7.6873, 10.0451], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.7236, -0.5054, -0.7834, -1.9102], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-0.8092,  0.2680,  0.3300, -1.9053], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4843,  1.0440,  0.6822, -0.8761], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4663,  1.0256,  0.6450, -0.8596], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.0137,  1.6304,  1.4372, -0.4440], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.0178, 1.5714, 2.5778, 1.0821], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.0910, 2.7333, 1.2238, 1.1403], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7284, 2.6262, 4.1031, 2.0921], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.4613, 4.3763, 4.6505, 6.0972], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0515, 5.0327, 6.0556, 8.1035], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.5624,  6.2623,  7.8066, 10.3803], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.3855, -0.4746, -0.7341, -1.6153], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-0.6224,  0.1265,  0.3297, -1.7147], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4291,  0.9458,  0.6210, -0.8011], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.9693, 1.5890, 2.7428, 1.0131], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6577, 2.6797, 4.2101, 1.9526], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.1846, 4.2779, 4.5735, 5.6793], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.8813, 5.0425, 6.0905, 7.7739], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.3126, 6.2537, 7.7484, 9.8722], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.4239, -0.4822, -0.7304, -1.6185], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-0.6576,  0.0665,  0.2759, -1.7265], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.5151,  0.8962,  0.6005, -0.8676], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.0810,  1.6733,  1.5474, -0.4674], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.9208, 1.6420, 2.8288, 0.9874], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6057, 2.7501, 4.2783, 1.9220], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.1715, 4.3580, 4.6438, 5.7575], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.9317, 5.1746, 6.2332, 7.9648], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.3155,  6.3620,  7.8796, 10.0390], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.5278, -0.4918, -0.7655, -1.6358], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.5611,  0.8696,  0.5659, -0.8728], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.1346,  1.6883,  1.5477, -0.4801], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.8237, 1.6240, 2.8227, 0.9087], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.5420, 2.7686, 4.2823, 1.9095], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.1540, 4.3755, 4.6586, 5.8398], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.9118, 5.1583, 6.2534, 8.0700], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.2426,  6.2723,  7.8532, 10.1137], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.5836, -0.6977, -0.8436, -1.6344], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-0.7292, -0.1688,  0.0718, -1.6736], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.6305,  0.7303,  0.4624, -0.8868], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.1941,  1.5630,  1.4979, -0.4982], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7307, 1.4745, 2.7795, 0.8345], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.4864, 2.6424, 4.2308, 1.8727], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.3489, 4.3261, 4.7962, 6.1634], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.9324, 4.9643, 6.2234, 8.0927], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.2971,  6.1076,  7.8262, 10.1147], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.5983, -0.8739, -0.9062, -1.6547], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.2284,  0.5374,  0.5313, -0.7486], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.5914,  0.4250,  1.5131, -0.1954], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.5820,  0.4120,  1.5131, -0.2094], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6254, 1.4039, 2.7489, 0.6322], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.4233, 2.6002, 4.1956, 1.6876], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.9408, 4.8376, 6.1398, 7.9393], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.3199, 5.9801, 7.7406, 9.9670], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.5574, -0.9261, -0.9249, -1.6063], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.6101, -0.0585, -0.2546, -0.7686], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.7543,  0.6297,  0.3643, -0.9921], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.2590,  1.5627,  1.5348, -0.6156], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6490, 1.4581, 2.8504, 0.6425], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.4580, 2.6737, 4.2671, 1.6898], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.5877, 4.3727, 4.8796, 6.2851], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.6050, 4.3883, 4.8926, 6.2920], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1187, 4.9138, 6.2388, 8.0762], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.4949,  6.0685,  7.8367, 10.0669], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.5148, -0.8923, -0.9332, -1.5620], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-0.7737, -0.3100, -0.1035, -1.6199], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.7628,  0.6557,  0.3354, -0.9957], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.2636,  1.6166,  1.5432, -0.6313], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.5636,  0.4469,  1.5456, -0.2172], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6423, 1.4830, 2.8397, 0.6095], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.4214, 2.6667, 4.2128, 1.6014], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.5377, 4.2380, 4.6917, 6.1340], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0816, 4.7008, 6.0593, 7.9507], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.4610, 5.8250, 7.6547, 9.9396], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.4666, -0.8564, -0.9563, -1.5056], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-0.7692, -0.2979, -0.1898, -1.5699], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.2940,  1.5937,  1.4617, -0.6513], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6129, 1.4386, 2.7862, 0.5805], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.4141, 2.6342, 4.2407, 1.6320], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.4242, 4.0765, 4.5799, 6.0597], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0353, 4.5760, 6.1145, 8.0201], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.3983,  5.7206,  7.8134, 10.0459], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.6459, -0.9960, -1.0941, -1.6017], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-0.9348, -0.3978, -0.3177, -1.6441], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9035,  0.5900,  0.2364, -1.0452], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4101,  1.5360,  1.4645, -0.7137], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4823, 1.3928, 2.8057, 0.4775], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.3458, 2.6601, 4.4103, 1.6484], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.2140, 3.9787, 4.5949, 5.9476], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.9060, 4.5225, 6.1994, 7.9683], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.2887,  5.7090,  7.9881, 10.0505], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.0122,  0.5700,  0.2245, -1.1292], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4753,  1.5332,  1.4911, -0.7787], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4379, 1.4082, 2.8379, 0.4178], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.3703, 2.7534, 4.5625, 1.6849], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.2632, 4.0682, 4.6941, 6.0457], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.8936, 4.5677, 6.2298, 7.9580], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.3260,  5.7861,  8.0470, 10.0703], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.8986, -1.1878, -1.1884, -1.8174], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.1678, -0.5436, -0.4627, -1.8285], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.0402,  0.5492,  0.2013, -1.1563], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4613,  1.5410,  1.4715, -0.7734], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4486, 1.4071, 2.7874, 0.4002], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.4414, 2.7947, 4.5759, 1.7314], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.4519, 4.1725, 4.7634, 6.1969], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.4770, 4.1784, 4.7598, 6.1768], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0346, 4.6004, 6.1845, 7.9106], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.5109, 5.8322, 8.0051, 9.9990], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.9011, -1.2209, -1.1521, -1.8524], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9310, -0.3031, -0.5477, -1.0710], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.0752,  0.5240,  0.2091, -1.1965], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4512,  1.5314,  1.4558, -0.7932], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4405, 1.3868, 2.7354, 0.3247], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.4482, 2.7727, 4.5028, 1.6257], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.5665, 4.2172, 4.7830, 6.1740], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1160, 4.6281, 6.1843, 7.8977], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.6168,  5.8810,  8.0144, 10.0113], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.8836, -1.2111, -1.0981, -1.8620], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9311, -0.3220, -0.5250, -1.0949], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3881,  1.3940,  0.3207, -0.6396], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4166, 2.7424, 1.4238, 0.4633], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.4618, 2.8287, 4.5280, 1.6110], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.6152, 4.3095, 4.8361, 6.1918], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1791, 4.7298, 6.2455, 7.9321], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.6725,  6.0002,  8.0752, 10.0246], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.9401, -1.2614, -1.1832, -1.9317], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9903, -0.3595, -0.5637, -1.1682], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.1535,  0.5022,  0.2168, -1.2949], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4097, 2.8164, 1.4541, 0.4461], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.4793, 2.9359, 4.6065, 1.6215], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.5994, 4.3802, 4.8264, 6.1566], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2343, 4.8255, 6.2944, 7.9904], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.6824,  6.0614,  8.0946, 10.0115], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.0106, -1.3232, -1.2957, -1.9907], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.0512, -0.4017, -0.6190, -1.2240], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2016,  0.4863,  0.2016, -1.3412], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4704,  1.5880,  1.4917, -0.8407], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5765, 1.6056, 3.0270, 0.4748], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.4801, 2.9593, 4.6589, 1.6214], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.5570, 4.3484, 4.8364, 6.0997], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2274, 4.7989, 6.3268, 7.9856], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.6651,  6.0429,  8.1114, 10.0028], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.0450, -1.3701, -1.3248, -2.0166], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.0847, -0.4447, -0.6294, -1.2533], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2373,  0.4678,  0.2066, -1.3620], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4684,  1.5927,  1.4956, -0.8123], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5631, 1.6079, 3.0503, 0.4756], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.4645, 2.9690, 4.6357, 1.6286], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.5578, 4.3804, 4.8231, 6.1250], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2214, 4.8097, 6.2941, 8.0052], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.6487,  6.0550,  8.0590, 10.0150], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.0192, -1.3763, -1.3224, -2.0094], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.0718, -0.4678, -0.6392, -1.2529], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2316,  0.4536,  0.1903, -1.3553], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4583,  1.5946,  1.4514, -0.7893], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5399, 1.5849, 3.0042, 0.4511], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.4339, 2.9464, 4.5664, 1.6046], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.5435, 4.3750, 4.7725, 6.1323], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1811, 4.7544, 6.2223, 7.9852], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.6066, 6.0028, 7.9982, 9.9997], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.0612, -0.4807, -0.6355, -1.2528], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2434,  0.4445,  0.1924, -1.3597], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4533,  1.6018,  1.4505, -0.7608], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5295, 1.5855, 3.0374, 0.4520], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.4205, 2.9663, 4.5777, 1.6101], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.5489, 4.4107, 4.8031, 6.1654], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1355, 4.7484, 6.2186, 7.9728], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.5289, 6.0268, 7.9944, 9.9882], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.0189, -1.3830, -1.3054, -2.0078], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1011, -0.5065, -0.6140, -1.2976], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3059,  0.4490,  0.2246, -1.4020], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4833,  1.6369,  1.4903, -0.7555], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5010, 1.6397, 3.0868, 0.4577], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.3932, 3.0474, 4.6145, 1.6521], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.3624, 4.4314, 4.7728, 6.0792], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0442, 4.7864, 6.2267, 8.0038], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.4492,  6.1004,  7.9926, 10.0529], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.0959, -1.4448, -1.3362, -2.0666], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1832, -0.5357, -0.6177, -1.3811], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3825,  0.4580,  0.2471, -1.4687], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4862,  1.6983,  1.5135, -0.7443], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5178, 1.7013, 3.0983, 0.4894], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.4066, 3.0893, 4.5653, 1.6892], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.3614, 4.4559, 4.7220, 6.1251], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0219, 4.7439, 6.1384, 8.0093], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "randomly selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1744, -1.4782, -1.3937, -2.1264], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2510, -0.5555, -0.6490, -1.4428], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4366,  0.4613,  0.2477, -1.5146], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4931,  1.7263,  1.5160, -0.7388], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5288, 1.7256, 3.1185, 0.5015], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.4342, 3.1034, 4.5741, 1.7191], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.4084, 4.4586, 4.7590, 6.1762], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0591, 4.7101, 6.1790, 8.0292], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.4673,  6.0224,  7.9456, 10.0714], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2284, -1.5131, -1.4274, -2.1832], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2879, -0.5798, -0.6718, -1.4813], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4516,  0.4546,  0.2683, -1.5381], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4579,  1.7491,  1.5707, -0.7098], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5534, 1.7339, 3.1377, 0.5054], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.4833, 3.1113, 4.5849, 1.7478], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.4340, 4.4409, 4.7632, 6.1563], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0709, 4.6642, 6.1667, 7.9706], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.4694, 5.9582, 7.9001, 9.9777], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2433, -1.5264, -1.4469, -2.2004], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2940, -0.5898, -0.6945, -1.4872], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4380,  0.4532,  0.2577, -1.5280], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4321,  1.7492,  1.5626, -0.6921], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5385, 1.7008, 3.0935, 0.4578], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.4936, 3.0781, 4.5415, 1.7313], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.4618, 4.4104, 4.7363, 6.1667], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1142, 4.6212, 6.1576, 7.9923], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.5571,  5.9377,  7.9248, 10.0427], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2028, -1.5024, -1.4395, -2.1722], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2570, -0.5781, -0.6893, -1.4567], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3939,  0.4755,  0.2662, -1.4934], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3845,  1.7855,  1.6076, -0.6489], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5912, 1.7345, 3.1473, 0.4947], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.5679, 3.1164, 4.6080, 1.7742], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.5956, 4.4615, 4.8368, 6.2518], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2553, 4.6633, 6.2612, 8.0707], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.7252,  5.9980,  8.0481, 10.1291], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1885, -1.4836, -1.4272, -2.1901], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2529, -0.5623, -0.6734, -1.4733], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3841,  0.5271,  0.3242, -1.5132], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3738,  1.8398,  1.6977, -0.6596], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5796, 1.7761, 3.1825, 0.4569], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.5581, 3.1651, 4.6525, 1.7494], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.4208, 4.4277, 4.7912, 6.0489], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1636, 4.6626, 6.2655, 7.9906], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.5583, 5.9713, 8.0090, 9.9868], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2239, -1.4875, -1.4735, -2.2208], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2797, -0.5624, -0.6991, -1.4979], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3876,  0.5597,  0.3281, -1.5195], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3451,  1.8638,  1.7322, -0.6205], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6154, 1.7886, 3.1680, 0.5110], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6479, 3.1988, 4.6728, 1.8844], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.4706, 4.4554, 4.8214, 6.1860], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2120, 4.6836, 6.2901, 8.1250], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.5709,  5.9689,  7.9870, 10.0813], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2657, -1.5342, -1.5280, -2.2554], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.3037, -0.5781, -0.7318, -1.5136], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4017,  0.5621,  0.3123, -1.5290], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3449,  1.8309,  1.7098, -0.6312], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6184, 1.7584, 3.1085, 0.4932], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6754, 3.1713, 4.6172, 1.8760], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.5409, 4.4778, 4.8231, 6.2103], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2430, 4.6519, 6.2238, 8.0388], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.6531, 5.9325, 7.9205, 9.9896], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3005, -1.5828, -1.5428, -2.2981], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.3285, -0.5913, -0.7440, -1.5423], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4341,  0.5527,  0.3183, -1.5666], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6070, 1.7180, 3.0749, 0.4453], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6972, 3.1410, 4.6085, 1.8710], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.4819, 4.4245, 4.8027, 6.1385], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2321, 4.6289, 6.2500, 8.0504], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.6438,  5.9391,  7.9998, 10.0633], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3105, -1.5852, -1.5182, -2.3112], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.3280, -0.5795, -0.7257, -1.5458], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4405,  0.5811,  0.3491, -1.5702], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3462,  1.8149,  1.7632, -0.6276], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6091, 1.7445, 3.1168, 0.4800], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6960, 3.1671, 4.6486, 1.9173], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.5136, 4.5067, 4.8919, 6.2610], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2369, 4.6694, 6.3130, 8.1200], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.6666,  6.0003,  8.0906, 10.1665], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3139, -1.5452, -1.5179, -2.3134], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.3252, -0.5425, -0.7157, -1.5373], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4398,  0.6375,  0.3545, -1.5638], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3577,  1.8486,  1.7750, -0.6364], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6158, 1.7872, 3.1446, 0.4842], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6903, 3.1905, 4.6541, 1.8879], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.5506, 4.5744, 4.9479, 6.2752], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2310, 4.6896, 6.3340, 8.0521], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.6267,  6.0026,  8.0722, 10.0195], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3013, -1.4884, -1.4872, -2.3189], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.3046, -0.4946, -0.6788, -1.5232], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4053,  0.6840,  0.4080, -1.5525], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3466,  1.8606,  1.7869, -0.6513], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6615, 1.8206, 3.1662, 0.5055], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7310, 3.2031, 4.6326, 1.8863], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.5945, 4.6004, 4.8787, 6.3015], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2954, 4.7211, 6.3185, 8.1056], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.6670,  6.0119,  8.0263, 10.0449], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3270, -1.4730, -1.4765, -2.3409], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.5356, -0.4570, -0.4579, -2.2862], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.7542,  0.5505,  0.6620, -1.4283], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3395,  1.8439,  1.7915, -0.6616], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7112, 1.8431, 3.2077, 0.5536], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7591, 3.2003, 4.6511, 1.8972], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.6077, 4.6038, 4.8896, 6.3037], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3261, 4.7355, 6.3683, 8.1240], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.6852,  6.0157,  8.0828, 10.0353], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.4097, -1.4888, -1.5196, -2.3949], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.5752, -0.4526, -0.4720, -2.3150], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.7764,  0.5598,  0.6404, -1.4481], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3544,  1.8190,  1.7553, -0.6801], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7071, 1.8259, 3.1869, 0.5419], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7402, 3.1692, 4.6134, 1.8635], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.6000, 4.5983, 4.8734, 6.2860], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3316, 4.7411, 6.3784, 8.1215], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.6741,  6.0063,  8.0980, 10.0082], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.4454, -1.5055, -1.5425, -2.4126], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.5929, -0.4562, -0.4800, -2.3252], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.7877,  0.5619,  0.6301, -1.4559], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3508,  1.8066,  1.7467, -0.6693], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7328, 1.8246, 3.1965, 0.5756], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7703, 3.1643, 4.6237, 1.8889], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.6645, 4.6025, 4.9059, 6.3323], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3989, 4.7347, 6.4003, 8.1541], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.7718,  6.0028,  8.1253, 10.0515], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.4438, -1.5080, -1.5292, -2.4119], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.5886, -0.4519, -0.4703, -2.3245], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.7803,  0.5724,  0.6319, -1.4499], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3270,  1.8201,  1.7543, -0.6500], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7638, 1.8410, 3.2039, 0.5942], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8104, 3.1957, 4.6406, 1.8982], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.6854, 4.6203, 4.8634, 6.2852], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.4023, 4.7391, 6.3589, 8.0640], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.8126, 6.0695, 8.1272, 9.9913], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3941, -1.4897, -1.4578, -2.3754], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.3545, -0.4464, -0.6535, -1.4891], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3638,  0.6676,  0.5568, -1.5317], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.2990,  1.8616,  1.8234, -0.6231], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7463, 1.8507, 3.1919, 0.5613], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8297, 3.2388, 4.6702, 1.9114], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.7349, 4.6715, 4.8935, 6.3331], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3818, 4.7351, 6.3238, 8.0210], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.8270,  6.1002,  8.1134, 10.0095], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3625, -1.4654, -1.4016, -2.3550], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.3376, -0.4291, -0.6381, -1.4648], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3483,  0.6736,  0.5855, -1.5111], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.2881,  1.8754,  1.8464, -0.5948], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7135, 1.8240, 3.1552, 0.5328], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8246, 3.2234, 4.6642, 1.9246], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.7412, 4.6610, 4.8797, 6.3611], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3597, 4.6991, 6.2830, 8.0081], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.8094,  6.0640,  8.0685, 10.0071], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3510, -1.4372, -1.4012, -2.3430], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.3252, -0.4185, -0.6438, -1.4428], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3426,  0.6670,  0.5789, -1.4937], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.2705,  1.8810,  1.8444, -0.5557], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7116, 1.8147, 3.1369, 0.5501], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8161, 3.2073, 4.6492, 1.9371], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.7136, 4.6376, 4.8383, 6.3458], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3493, 4.6852, 6.2624, 8.0222], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.7823,  6.0393,  8.0433, 10.0149], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3656, -1.4197, -1.4053, -2.3437], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.3249, -0.4130, -0.6385, -1.4313], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3639,  0.6552,  0.5757, -1.4969], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.2834,  1.8765,  1.8489, -0.5478], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7077, 1.8115, 3.1492, 0.5679], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7956, 3.1901, 4.6578, 1.9338], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.6858, 4.6107, 4.8278, 6.3206], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3631, 4.6783, 6.2816, 8.0408], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.7902,  6.0173,  8.0526, 10.0063], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3600, -1.4121, -1.4030, -2.3524], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.3079, -0.4140, -0.6219, -1.4295], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3591,  0.6435,  0.5881, -1.4996], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.2742,  1.8736,  1.8696, -0.5469], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7270, 1.8135, 3.1752, 0.5760], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8147, 3.1822, 4.6853, 1.9275], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.7101, 4.6038, 4.8460, 6.3042], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3968, 4.6775, 6.3061, 8.0312], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.8242, 6.0165, 8.0802, 9.9787], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3650, -1.4002, -1.4135, -2.3622], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4941, -0.4048, -0.4058, -2.2748], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.7645,  0.6223,  0.6394, -1.4127], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.2831,  1.8880,  1.8714, -0.5608], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7287, 1.8355, 3.1936, 0.5709], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7935, 3.1923, 4.6872, 1.8894], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.6684, 4.6143, 4.8104, 6.2303], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3863, 4.6998, 6.2863, 8.0062], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.8380,  6.0609,  8.0884, 10.0019], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3238, -1.3581, -1.4165, -2.3094], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4454, -0.3772, -0.4084, -2.2268], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.7235,  0.6528,  0.6489, -1.3668], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6230,  0.6986,  1.8922, -0.3039], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7864, 1.8782, 3.2419, 0.6715], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8117, 3.2052, 4.6925, 1.9356], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.6792, 4.6223, 4.7794, 6.2503], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.4021, 4.7043, 6.2471, 8.0224], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.8901,  6.0827,  8.0631, 10.0540], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2661, -1.3630, -1.3794, -2.2755], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4054, -0.3991, -0.3933, -2.2074], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2717,  0.6950,  0.6295, -1.4058], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.2688,  1.8831,  1.8646, -0.5254], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7884, 1.8665, 3.2154, 0.6651], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8175, 3.1998, 4.6631, 1.9247], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.6838, 4.6306, 4.7404, 6.2217], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3972, 4.7135, 6.2050, 7.9850], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.8993,  6.1052,  8.0340, 10.0257], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2471, -1.3613, -1.3650, -2.2662], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4031, -0.4032, -0.4019, -2.2084], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2739,  0.6996,  0.6356, -1.4038], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.2850,  1.8792,  1.8509, -0.5380], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7843, 1.8742, 3.2149, 0.6697], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8107, 3.2122, 4.6637, 1.9310], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.6818, 4.6533, 4.7344, 6.2382], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3396, 4.7146, 6.1756, 7.9670], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.7751, 6.0903, 7.9860, 9.9824], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2684, -1.3637, -1.3668, -2.2774], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4412, -0.4145, -0.4061, -2.2411], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3230,  0.6872,  0.6343, -1.4303], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3657,  1.8524,  1.8266, -0.5774], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6931, 1.8547, 3.1991, 0.6392], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6853, 3.1981, 4.6591, 1.9249], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.3798, 4.5937, 4.6452, 6.1388], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1782, 4.7194, 6.1936, 8.0541], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.5867,  6.0944,  8.0127, 10.1066], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2833, -1.3611, -1.3689, -2.2720], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4495, -0.4142, -0.4036, -2.2403], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3321,  0.6949,  0.6423, -1.4165], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3723,  1.8742,  1.8421, -0.5277], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6818, 1.8739, 3.2261, 0.6848], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6502, 3.2096, 4.6675, 1.9546], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.3637, 4.6444, 4.6744, 6.2233], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1007, 4.7252, 6.1704, 8.0201], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.4421, 6.0632, 7.9339, 9.9708], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2649, -1.3501, -1.3647, -2.2516], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4085, -0.4013, -0.3887, -2.2124], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3088,  0.7073,  0.6383, -1.3854], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3661,  1.8940,  1.8248, -0.4972], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6801, 1.8849, 3.1967, 0.7025], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6260, 3.1980, 4.5870, 1.9523], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.4090, 4.6916, 4.6723, 6.3359], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1240, 4.7590, 6.1456, 8.1085], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.4377,  6.0723,  7.8761, 10.0327], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2184, -1.3498, -1.3453, -2.2298], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1803, -0.3843, -0.4960, -1.3422], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2631,  0.7132,  0.6549, -1.3565], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3261,  1.9086,  1.8420, -0.4702], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7295, 1.9004, 3.2160, 0.7289], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6843, 3.2019, 4.5958, 1.9640], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.4134, 4.6538, 4.7018, 6.2414], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1769, 4.7524, 6.2378, 8.0642], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.4871, 6.0566, 7.9670, 9.9422], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2047, -1.3651, -1.3376, -2.2371], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1756, -0.3860, -0.4824, -1.3521], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2596,  0.6907,  0.6740, -1.3726], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3117,  1.8965,  1.8641, -0.4909], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7532, 1.8950, 3.2368, 0.7188], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7243, 3.1948, 4.6193, 1.9643], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.4381, 4.6308, 4.7399, 6.2261], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2326, 4.7576, 6.3017, 8.1082], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.5586,  6.0683,  8.0365, 10.0062], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2240, -1.3659, -1.3689, -2.2491], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3388, -0.4005, -0.3661, -2.2044], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2855,  0.6840,  0.6494, -1.3946], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.2966,  1.9047,  1.8530, -0.4864], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7615, 1.8945, 3.2159, 0.7156], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7298, 3.1925, 4.5908, 1.9674], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.3872, 4.6085, 4.7055, 6.1943], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1764, 4.7400, 6.2801, 8.0893], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.4600, 6.0465, 8.0012, 9.9748], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2586, -1.3593, -1.3821, -2.2550], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3817, -0.3919, -0.3788, -2.2205], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3448,  0.6776,  0.6401, -1.4324], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3503,  1.8987,  1.8472, -0.5163], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6824, 1.8797, 3.1896, 0.6673], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6741, 3.1997, 4.5914, 1.9781], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.2524, 4.5905, 4.6775, 6.1760], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0914, 4.7375, 6.2896, 8.1206], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.3791,  6.0569,  8.0199, 10.0327], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2573, -1.3520, -1.3753, -2.2329], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3784, -0.3828, -0.3727, -2.2033], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3374,  0.6901,  0.6518, -1.4190], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3424,  1.9103,  1.8652, -0.4972], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6886, 1.8849, 3.1806, 0.6720], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7115, 3.2161, 4.5929, 2.0154], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.3404, 4.6313, 4.7254, 6.2844], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1262, 4.7237, 6.2673, 8.1032], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.4522,  6.0506,  7.9915, 10.0364], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2292, -1.3450, -1.3508, -2.2087], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3366, -0.3784, -0.3658, -2.1700], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2929,  0.6929,  0.6579, -1.3890], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.2952,  1.8905,  1.8591, -0.4739], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7320, 1.8618, 3.1375, 0.6691], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7986, 3.2049, 4.5619, 2.0400], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.4632, 4.6302, 4.7419, 6.3300], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1799, 4.6600, 6.2234, 8.0246], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.5251, 5.9843, 7.9409, 9.9548], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1984, -1.3441, -1.3230, -2.2173], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1716, -0.3823, -0.4750, -1.3469], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2667,  0.6885,  0.6734, -1.3909], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.2781,  1.8561,  1.8616, -0.4948], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5557, 3.1666, 1.8939, 0.8094], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8306, 3.1676, 4.5558, 2.0305], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.4898, 4.5549, 4.7490, 6.3160], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2059, 4.5908, 6.2273, 8.0249], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.5871,  5.9378,  7.9832, 10.0160], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2057, -1.3419, -1.3214, -2.2349], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1853, -0.3827, -0.4665, -1.3611], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2830,  0.6919,  0.6765, -1.4035], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.2846,  1.8611,  1.8761, -0.4934], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5115, 3.1458, 1.8902, 0.7743], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7882, 3.1570, 4.5644, 2.0232], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.3158, 4.4841, 4.6881, 6.1905], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0893, 4.5543, 6.2220, 7.9992], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.4212, 5.9006, 7.9792, 9.9901], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2374, -1.3369, -1.3297, -2.2528], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2167, -0.3757, -0.4662, -1.3843], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3279,  0.6898,  0.6708, -1.4294], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3165,  1.8761,  1.8933, -0.4867], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4621, 3.1491, 1.8926, 0.7766], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7493, 3.1616, 4.5775, 2.0336], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.3017, 4.5137, 4.7096, 6.2788], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1263, 4.5865, 6.2806, 8.1150], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.4947,  5.9406,  8.0515, 10.1210], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2222, -1.3337, -1.3422, -2.2341], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3438, -0.3678, -0.4100, -2.2113], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5877,  0.7042,  0.6988, -1.2779], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6652,  0.6940,  1.8905, -0.3404], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7503, 1.8588, 3.1637, 0.6746], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7850, 3.1539, 4.5672, 2.0026], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.3908, 4.5444, 4.7407, 6.2949], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1468, 4.5708, 6.2720, 8.0005], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.4920, 5.9027, 8.0290, 9.9519], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2070, -1.3374, -1.3401, -2.2231], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3256, -0.3685, -0.4027, -2.2039], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5612,  0.7030,  0.7120, -1.2671], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.2752,  1.8770,  1.9251, -0.4676], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4990, 3.1462, 1.9204, 0.7945], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7843, 3.1676, 4.6578, 2.0273], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.2425, 4.4920, 4.7304, 6.1984], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0779, 4.5689, 6.3475, 8.0469], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.3676,  5.8851,  8.1021, 10.0012], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2194, -1.3309, -1.3314, -2.2165], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3433, -0.3629, -0.3877, -2.2079], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5883,  0.7040,  0.7157, -1.2762], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3155,  1.8668,  1.9282, -0.4651], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4509, 3.1458, 1.9203, 0.8138], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7637, 3.1948, 4.7091, 2.0863], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.1318, 4.5162, 4.7056, 6.1940], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0079, 4.6013, 6.3474, 8.0731], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.2921,  5.9265,  8.1072, 10.0160], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2142, -1.3236, -1.3363, -2.2010], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3282, -0.3609, -0.3852, -2.1942], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5685,  0.7053,  0.7080, -1.2618], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.2851,  1.8615,  1.9037, -0.4555], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5301, 3.1529, 1.9195, 0.8591], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8700, 3.2041, 4.6934, 2.1209], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.2913, 4.5442, 4.6989, 6.2386], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2159, 4.6181, 6.3294, 8.0974], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.6078,  5.9624,  8.1170, 10.0819], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1991, -1.3338, -1.3414, -2.1916], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3074, -0.3722, -0.3832, -2.1870], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5480,  0.6940,  0.6954, -1.2577], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.2700,  1.8337,  1.8504, -0.4842], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5677, 3.1282, 1.8823, 0.8333], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8893, 3.1675, 4.6444, 2.0297], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.3861, 4.5581, 4.6841, 6.2045], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2923, 4.6194, 6.2960, 8.0101], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.6968, 5.9717, 8.0996, 9.9930], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1990, -1.3319, -1.3414, -2.1912], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3047, -0.3687, -0.3717, -2.1890], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5400,  0.7043,  0.7079, -1.2545], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.2601,  1.8504,  1.8590, -0.4849], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6012, 3.1506, 1.8967, 0.8580], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.9197, 3.1920, 4.6921, 2.0312], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.4742, 4.6229, 4.7383, 6.2805], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3543, 4.6784, 6.3311, 8.0412], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.7298, 6.0035, 8.1225, 9.9706], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2009, -1.3337, -1.3374, -2.1983], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3064, -0.3690, -0.3579, -2.2019], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2550,  0.7250,  0.7016, -1.3838], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.2734,  1.8454,  1.8586, -0.5277], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5882, 3.1470, 1.8886, 0.8053], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8880, 3.1759, 4.6905, 1.9354], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.4665, 4.6407, 4.7385, 6.2178], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3429, 4.6899, 6.3217, 7.9760], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.7726, 6.0446, 8.1600, 9.9625], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1980, -1.3326, -1.3307, -2.2054], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1743, -0.3645, -0.4206, -1.3508], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2501,  0.7120,  0.7050, -1.3853], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.2827,  1.8389,  1.8477, -0.5451], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5717, 3.1465, 1.8623, 0.7811], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8737, 3.1646, 4.6745, 1.9177], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.4772, 4.6603, 4.7268, 6.2410], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3415, 4.7032, 6.2968, 7.9863], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.8039,  6.0796,  8.1660, 10.0101], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2055, -1.3257, -1.3350, -2.2080], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3148, -0.3575, -0.3539, -2.2148], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2814,  0.6996,  0.6915, -1.4084], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3134,  1.8279,  1.8213, -0.5850], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.8295, 1.8762, 3.2165, 0.6583], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8201, 3.1500, 4.6371, 1.8438], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.4651, 4.6788, 4.7167, 6.2317], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3141, 4.7120, 6.2742, 7.9669], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.7826,  6.0980,  8.1540, 10.0158], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2113, -1.3236, -1.3341, -2.2039], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3231, -0.3541, -0.3557, -2.2078], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5641,  0.7023,  0.6920, -1.2980], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6595,  0.6880,  1.8349, -0.4224], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.8160, 1.8642, 3.2187, 0.6646], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7959, 3.1344, 4.6344, 1.8455], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.4597, 4.6629, 4.7296, 6.2733], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2852, 4.6668, 6.2708, 7.9746], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.7456,  6.0425,  8.1538, 10.0094], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2105, -1.3316, -1.3334, -2.1925], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3264, -0.3616, -0.3620, -2.1950], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5821,  0.6949,  0.6717, -1.2950], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6338,  0.6799,  1.8200, -0.4244], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7572, 1.8441, 3.2027, 0.6369], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6986, 3.1259, 4.6249, 1.8201], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.1314, 4.5733, 4.5855, 6.0356], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0586, 4.6433, 6.2430, 7.9276], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0108, 4.6559, 6.2521, 7.9567], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.3522, 6.0242, 8.1168, 9.9759], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2342, -1.3287, -1.3312, -2.1814], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3701, -0.3580, -0.3676, -2.1898], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.6324,  0.7027,  0.6673, -1.2890], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.5789,  0.6940,  1.8331, -0.3790], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6829, 1.8660, 3.2221, 0.6991], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6170, 3.1558, 4.6147, 1.9285], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.8906, 4.5839, 4.4501, 6.0534], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.9898, 4.7098, 6.1972, 8.1256], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.3582,  6.0747,  8.0089, 10.1415], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2216, -1.3352, -1.3288, -2.1573], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1793, -0.3706, -0.4160, -1.3079], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3312,  0.6576,  0.6564, -1.3832], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3645,  1.8525,  1.7622, -0.4832], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7054, 1.8315, 3.0764, 0.6617], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6022, 3.0540, 4.3494, 1.8007], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.0227, 4.5594, 4.3845, 6.1026], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.9907, 4.5680, 6.0092, 7.9428], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.3885, 5.9278, 7.7842, 9.9920], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2064, -1.3565, -1.3042, -2.1522], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1604, -0.3922, -0.4236, -1.2952], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2876,  0.6520,  0.6805, -1.3485], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3179,  1.8753,  0.7267, -0.2857], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4830, 3.1805, 1.8385, 0.9179], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6334, 3.0347, 4.3511, 1.8550], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.0966, 4.5264, 4.4669, 6.2675], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0237, 4.4961, 6.0648, 8.0045], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.3696, 5.8363, 7.7925, 9.9928], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2128, -1.3511, -1.2725, -2.1707], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1717, -0.3818, -0.3981, -1.3212], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2927,  0.6722,  0.7141, -1.3725], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3322,  1.8765,  0.7636, -0.3231], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4683, 3.1769, 1.8841, 0.8727], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6555, 3.0561, 4.3751, 1.8378], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.0196, 4.4305, 4.4176, 6.1362], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0813, 4.4767, 6.0888, 8.0434], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.4011,  5.7953,  7.7808, 10.0181], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2299, -1.3348, -1.2994, -2.1918], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1823, -0.3749, -0.4086, -1.3345], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2926,  0.6790,  0.7051, -1.3888], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3347,  1.8805,  0.7438, -0.3388], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4533, 3.1745, 1.8513, 0.8464], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7061, 3.0810, 4.3666, 1.8947], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.0875, 4.4380, 4.4205, 6.2276], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1488, 4.4703, 6.0882, 8.0732], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.5201,  5.8022,  7.7952, 10.0456], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2420, -1.3293, -1.3393, -2.1985], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3511, -0.3757, -0.3642, -2.2001], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2891,  0.6728,  0.6796, -1.3922], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3446,  1.8664,  0.7046, -0.3845], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4682, 3.1706, 1.8246, 0.7985], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7651, 3.0913, 4.3524, 1.8637], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.1974, 4.4526, 4.4345, 6.2149], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2426, 4.4760, 6.1061, 8.0141], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.6688,  5.8414,  7.8719, 10.0394], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2311, -1.3323, -1.3411, -2.1949], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3426, -0.3826, -0.3645, -2.1958], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2665,  0.6682,  0.6891, -1.3753], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3260,  1.8575,  0.7168, -0.3812], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5010, 3.1492, 1.8417, 0.7980], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8361, 3.1189, 4.4022, 1.9098], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.2212, 4.4268, 4.4627, 6.1633], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2614, 4.4681, 6.1311, 7.9500], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.6661, 5.7957, 7.8659, 9.9324], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2269, -1.3506, -1.3534, -2.1982], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3464, -0.4060, -0.4093, -2.1945], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5577,  0.6201,  0.6510, -1.2927], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3156,  1.7923,  1.7802, -0.5112], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5982, 1.6593, 2.8756, 0.3686], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8727, 3.1016, 4.4579, 1.9782], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.2558, 4.4117, 4.5637, 6.2478], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3082, 4.4988, 6.2739, 8.0599], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.7541,  5.8290,  8.0856, 10.0820], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2132, -1.3581, -1.3185, -2.1966], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1735, -0.3936, -0.4346, -1.3346], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2531,  0.6690,  0.6862, -1.3544], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3413,  1.7949,  0.7368, -0.4152], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4521, 2.9995, 1.8849, 0.6909], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.9364, 3.1536, 4.6473, 1.9901], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.2812, 4.4643, 4.6987, 6.1443], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3523, 4.6069, 6.4136, 7.9534], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.8349, 6.0039, 8.2798, 9.9876], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2160, -1.3490, -1.3251, -2.2040], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1904, -0.3741, -0.4273, -1.3527], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2469,  0.6910,  0.6989, -1.3549], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3194,  1.8385,  0.7666, -0.4009], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4653, 3.0259, 1.8879, 0.6863], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.9428, 3.2032, 4.6429, 1.9776], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.3155, 4.5676, 4.6910, 6.1736], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3997, 4.7274, 6.4230, 8.0097], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.8628,  6.1246,  8.2490, 10.0239], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2355, -1.3425, -1.3794, -2.2032], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3570, -0.3887, -0.4374, -2.1909], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5505,  0.6329,  0.6627, -1.2894], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3333,  1.8337,  1.8204, -0.5275], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5646, 1.6565, 2.8793, 0.2879], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8995, 3.1633, 4.5685, 1.9209], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.3489, 4.5794, 4.6789, 6.2063], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3990, 4.6971, 6.4040, 7.9767], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.8641, 6.1012, 8.2522, 9.9801], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2243, -1.3451, -1.3586, -2.1975], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3449, -0.3939, -0.4016, -2.1912], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5500,  0.6295,  0.6843, -1.2905], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3338,  1.8401,  1.8543, -0.5120], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4247, 2.9815, 1.8445, 0.6573], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8670, 3.1508, 4.6193, 1.9000], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.3234, 4.5728, 4.7360, 6.2099], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3526, 4.6802, 6.4485, 7.9679], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.8044, 6.1116, 8.3219, 9.9906], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2080, -1.3469, -1.3316, -2.1977], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1980, -0.3979, -0.4068, -1.3649], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2768,  0.6609,  0.7026, -1.3757], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3537,  1.8125,  0.7224, -0.4167], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4176, 2.9667, 1.8393, 0.6468], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7881, 3.0742, 4.5889, 1.7960], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.2440, 4.5263, 4.6753, 6.1504], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2688, 4.6420, 6.4075, 7.9377], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.7134,  6.1176,  8.2871, 10.0086], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2098, -1.3459, -1.3763, -2.1994], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3407, -0.3853, -0.4263, -2.2036], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5432,  0.6389,  0.6364, -1.2912], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6383,  0.6391,  1.7624, -0.4505], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6447, 1.7314, 3.0970, 0.4220], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7135, 3.0514, 4.5208, 1.7381], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.1783, 4.5464, 4.5732, 6.1590], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1821, 4.6620, 6.2985, 7.9475], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.6091,  6.1359,  8.1747, 10.0146], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2084, -1.3439, -1.3804, -2.2030], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3346, -0.3821, -0.4155, -2.2109], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5300,  0.6479,  0.6467, -1.2869], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6375,  0.6538,  1.7810, -0.4472], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6638, 1.7573, 3.1678, 0.4556], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7141, 3.0846, 4.5835, 1.7529], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.1594, 4.5739, 4.5769, 6.1721], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1673, 4.6912, 6.3047, 7.9732], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.5610, 6.1048, 8.1560, 9.9710], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2086, -1.3568, -1.3673, -2.2086], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3371, -0.3973, -0.4030, -2.2220], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5322,  0.6406,  0.6437, -1.2926], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4392,  1.6963,  1.6892, -0.6497], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6451, 1.7301, 3.1531, 0.4376], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6821, 3.0516, 4.5600, 1.7250], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.1372, 4.5426, 4.5295, 6.1768], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1446, 4.6598, 6.2606, 7.9981], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.5297,  6.0768,  8.1030, 10.0060], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2237, -1.3608, -1.4018, -2.2139], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3461, -0.4000, -0.4172, -2.2296], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5439,  0.6354,  0.6362, -1.3091], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4517,  1.6820,  1.6744, -0.6590], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6272, 1.7209, 3.1401, 0.4291], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6745, 3.0647, 4.5652, 1.7336], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.1314, 4.5551, 4.5057, 6.1828], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1317, 4.6670, 6.2165, 7.9904], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1303, 4.6731, 6.1929, 7.9858], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.5037, 6.1168, 7.9893, 9.9951], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2164, -1.3554, -1.4009, -2.2041], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3346, -0.4035, -0.4090, -2.2158], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5400,  0.6235,  0.6351, -1.3102], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4250,  1.7006,  1.6784, -0.6332], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6412, 1.7127, 3.1173, 0.4399], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7337, 3.0964, 4.5726, 1.7977], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.1730, 4.5710, 4.4843, 6.2139], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1524, 4.6768, 6.1716, 8.0043], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.5186,  6.1243,  7.9527, 10.0001], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2156, -1.3857, -1.3837, -2.2095], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2063, -0.4456, -0.4084, -1.3667], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3394,  0.6083, -0.3976, -0.4113], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3626,  1.7629,  0.6098, -0.4008], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4930, 3.1149, 1.7886, 0.7724], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7579, 3.1284, 4.5803, 1.8131], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.1734, 4.5902, 4.4963, 6.1862], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1639, 4.7131, 6.1955, 8.0033], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.5348,  6.1769,  7.9894, 10.0119], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2332, -1.3960, -1.3622, -2.2378], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2227, -0.4419, -0.4107, -1.3792], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3518,  0.6131, -0.3884, -0.4196], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3503,  1.8035,  0.6422, -0.3846], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4835, 3.1442, 1.7883, 0.7541], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7596, 3.1717, 4.5635, 1.8195], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.1781, 4.6326, 4.4926, 6.1863], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1664, 4.7496, 6.1804, 7.9990], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.5352, 6.2106, 7.9625, 9.9916], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2395, -1.3755, -1.3570, -2.2477], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2249, -0.4262, -0.4216, -1.3758], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3575,  0.6211, -0.4030, -0.4216], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3473,  1.8125,  0.6351, -0.3869], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4774, 3.1414, 1.7747, 0.7353], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7639, 3.1755, 4.5492, 1.8184], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.1876, 4.6300, 4.5008, 6.1936], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1707, 4.7392, 6.1880, 8.0037], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.5439,  6.1923,  7.9737, 10.0015], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2587, -1.3766, -1.3625, -2.2605], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2297, -0.4302, -0.4339, -1.3735], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3845,  0.4886,  0.5796, -1.4721], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3716,  1.7722,  0.5905, -0.4165], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4572, 3.1025, 1.7446, 0.7052], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7655, 3.1465, 4.5442, 1.8182], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.1858, 4.6045, 4.4968, 6.1825], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1722, 4.7115, 6.1970, 7.9933], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.5503, 6.1585, 7.9962, 9.9928], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2711, -1.3691, -1.3763, -2.2636], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4147, -0.4378, -0.4534, -2.2587], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5658,  0.5372,  0.5723, -1.3452], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3651,  1.8355,  1.7560, -0.5616], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7093, 1.7942, 3.1451, 0.5337], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7962, 3.1579, 4.5857, 1.8541], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.1973, 4.5891, 4.5225, 6.1858], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1893, 4.6998, 6.2273, 8.0009], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.5647, 6.1399, 8.0267, 9.9900], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2611, -1.3600, -1.3866, -2.2548], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4085, -0.4307, -0.4685, -2.2533], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5628,  0.5451,  0.5605, -1.3391], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3892,  1.8173,  1.7383, -0.5879], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7135, 1.7997, 3.1410, 0.5492], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7860, 3.1491, 4.5674, 1.8471], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.2184, 4.5996, 4.5310, 6.2213], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1915, 4.7127, 6.2167, 8.0135], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.5731,  6.1864,  8.0133, 10.0278], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2446, -1.3745, -1.3936, -2.2386], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4041, -0.4432, -0.4923, -2.2395], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5525,  0.5475,  0.5817, -1.3349], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3964,  1.8206,  1.7346, -0.5931], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7132, 1.8137, 3.1260, 0.5571], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7803, 3.1649, 4.5588, 1.8400], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.2087, 4.6063, 4.5317, 6.1889], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1819, 4.7077, 6.2158, 7.9732], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.5639, 6.1907, 8.0238, 9.9885], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2378, -1.4204, -1.3845, -2.2480], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2360, -0.4933, -0.4553, -1.4013], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3752,  0.6020, -0.4134, -0.4375], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3918,  1.7650,  0.6014, -0.4583], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4182, 3.0913, 1.7849, 0.6221], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8142, 3.1498, 4.5802, 1.8526], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.2835, 4.5671, 4.5816, 6.2238], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2744, 4.6447, 6.2477, 8.0019], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.7246,  6.1457,  8.0798, 10.0610], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2410, -1.4264, -1.3904, -2.2500], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2515, -0.5267, -0.4713, -1.4231], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3859,  0.5992, -0.4280, -0.4486], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3965,  1.7686,  0.5830, -0.4778], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4164, 3.0998, 1.7688, 0.5906], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8153, 3.1446, 4.5554, 1.8039], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.3010, 4.5673, 4.5559, 6.1717], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3090, 4.6483, 6.2295, 7.9694], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.7593,  6.1474,  8.0671, 10.0159], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2414, -1.3785, -1.4067, -2.2337], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4433, -0.4609, -0.5745, -2.2477], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5526,  0.5507,  0.6361, -1.3650], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3991,  1.8272,  1.7501, -0.6076], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7363, 1.8085, 3.1140, 0.5553], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8273, 3.1666, 4.5744, 1.8299], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.3386, 4.6061, 4.5907, 6.2237], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3400, 4.6730, 6.2563, 7.9871], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.7925,  6.1554,  8.0946, 10.0010], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2319, -1.3836, -1.3859, -2.2245], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4256, -0.4713, -0.5386, -2.2379], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5523,  0.5395,  0.6646, -1.3650], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3905,  1.8150,  1.7909, -0.5948], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7522, 1.8081, 3.1420, 0.5734], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8474, 3.1623, 4.6112, 1.8375], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.3685, 4.5874, 4.6281, 6.1963], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.4050, 4.6787, 6.3013, 8.0027], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.8515, 6.1489, 8.1149, 9.9983], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2354, -1.3668, -1.3780, -2.2237], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4031, -0.4509, -0.4682, -2.2338], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5599,  0.5457,  0.6792, -1.3627], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3988,  1.8081,  1.7968, -0.5895], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7374, 1.8069, 3.1205, 0.5789], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8232, 3.1594, 4.5872, 1.8351], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.3417, 4.5898, 4.5824, 6.1849], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3783, 4.6876, 6.2449, 8.0059], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.8364,  6.1663,  8.0782, 10.0151], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2568, -1.3711, -1.4016, -2.2376], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4082, -0.4308, -0.4373, -2.2432], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5788,  0.5525,  0.6825, -1.3762], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4127,  1.8086,  1.8080, -0.5997], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7234, 1.8070, 3.1224, 0.5679], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8207, 3.1629, 4.6107, 1.8286], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.3562, 4.5967, 4.6073, 6.1824], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3828, 4.6893, 6.2523, 7.9868], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.8498,  6.1673,  8.1057, 10.0045], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2834, -1.4028, -1.4478, -2.2483], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4129, -0.4297, -0.4158, -2.2482], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4275,  0.6441,  0.6026, -1.4440], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4245,  1.8053,  1.8002, -0.5979], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6989, 1.7951, 3.1034, 0.5644], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7839, 3.1525, 4.5918, 1.8294], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.3041, 4.5993, 4.5633, 6.2044], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2887, 4.6824, 6.1791, 7.9888], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.7322, 6.1436, 8.0317, 9.9945], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3196, -1.4393, -1.5214, -2.2568], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4483, -0.4218, -0.4514, -2.2552], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.6266,  0.5772,  0.6206, -1.3739], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4521,  1.8138,  1.7763, -0.6022], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6495, 1.7927, 3.0901, 0.5405], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7446, 3.1642, 4.6070, 1.8345], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.2534, 4.6129, 4.5639, 6.2203], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2497, 4.7054, 6.2044, 8.0361], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.6756,  6.1508,  8.0659, 10.0233], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3157, -1.4432, -1.4860, -2.2606], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4396, -0.4230, -0.3724, -2.2696], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4434,  0.6385,  0.6289, -1.4634], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4517,  1.8114,  1.8414, -0.6148], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.3590, 3.1251, 1.8388, 0.6034], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7311, 3.1719, 4.6446, 1.8212], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.2311, 4.6168, 4.6026, 6.2034], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2088, 4.7117, 6.2070, 8.0084], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.6389,  6.1805,  8.0520, 10.0101], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2650, -1.4333, -1.3761, -2.2554], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2494, -0.4046, -0.3985, -1.4078], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4299,  0.6240, -0.4130, -0.4692], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3939,  1.8330,  0.6676, -0.4184], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.3874, 3.1584, 1.8344, 0.6351], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7267, 3.1784, 4.5734, 1.8109], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.2415, 4.6656, 4.5637, 6.2134], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1951, 4.7451, 6.1432, 7.9810], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.6335, 6.2226, 7.9915, 9.9832], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2357, -1.3526, -1.3448, -2.2308], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2347, -0.3930, -0.4133, -1.3892], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3512,  0.6712,  0.6313, -1.3946], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4079,  1.8501,  1.8107, -0.5713], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7307, 1.8828, 3.1726, 0.6296], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7648, 3.1928, 4.5689, 1.8405], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.2662, 4.6820, 4.5598, 6.2313], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2247, 4.7579, 6.1558, 8.0067], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.6691,  6.2264,  8.0165, 10.0029], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2536, -1.3390, -1.3477, -2.2299], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3734, -0.4046, -0.3850, -2.2219], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3423,  0.6354,  0.6340, -1.3945], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3848,  1.8172,  1.8226, -0.5718], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4943, 3.1850, 1.8743, 0.7199], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7995, 3.1737, 4.5847, 1.8554], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.2820, 4.6486, 4.5729, 6.2084], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2477, 4.7398, 6.1754, 8.0034], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.7010,  6.2115,  8.0313, 10.0066], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2617, -1.3443, -1.3571, -2.2256], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3687, -0.4316, -0.4087, -2.2158], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3238,  0.6248,  0.6271, -1.3794], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3379,  1.8056,  0.6284, -0.3969], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5374, 3.1878, 1.8789, 0.7509], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7979, 3.1634, 4.5649, 1.8505], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.1825, 4.6143, 4.5056, 6.1193], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1590, 4.7239, 6.1394, 7.9572], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.5567, 6.1908, 7.9888, 9.9421], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2897, -1.3263, -1.3661, -2.2373], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4049, -0.4290, -0.4292, -2.2470], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5353,  0.6358,  0.6346, -1.3027], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6193,  0.6626,  1.8171, -0.4020], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6994, 1.8555, 3.1843, 0.6366], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7349, 3.1789, 4.6003, 1.9001], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.0816, 4.6525, 4.5116, 6.2743], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0865, 4.7521, 6.1673, 8.1508], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.4974,  6.2270,  8.0398, 10.2252], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3007, -1.3437, -1.3464, -2.2603], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4144, -0.4487, -0.4140, -2.2904], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3452,  0.6325,  0.6359, -1.3965], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3997,  1.8037,  0.6211, -0.3997], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4164, 3.1492, 1.8557, 0.7426], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7294, 3.1649, 4.6207, 1.8863], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.0498, 4.6305, 4.5405, 6.2377], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0132, 4.7026, 6.1901, 7.9922], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.3207, 6.0951, 7.9795, 9.8664], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3026, -1.3783, -1.3800, -2.2736], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4149, -0.4465, -0.4252, -2.3060], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3410,  0.6422,  0.6385, -1.3961], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3727,  1.8543,  1.8745, -0.4808], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4192, 3.1510, 1.8776, 0.7677], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7697, 3.2083, 4.6926, 1.9795], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.1516, 4.7281, 4.6666, 6.4754], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1012, 4.8020, 6.3232, 8.2227], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.3263, 6.1437, 8.0271, 9.9960], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2801, -1.3769, -1.4003, -2.2287], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3961, -0.4191, -0.4451, -2.2554], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5422,  0.6428,  0.6393, -1.3230], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6236,  0.6655,  1.8415, -0.4015], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6619, 1.8418, 3.1313, 0.6016], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7587, 3.2206, 4.6413, 1.9421], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.0749, 4.6786, 4.5973, 6.2937], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1383, 4.8286, 6.3385, 8.1924], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.3906,  6.2103,  8.0300, 10.0072], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2491, -1.3886, -1.3760, -2.2196], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2009, -0.4212, -0.4369, -1.3542], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3086,  0.6514,  0.6445, -1.3664], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3225,  1.8851,  1.8961, -0.4288], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4833, 3.1669, 1.8958, 0.7962], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8163, 3.2468, 4.6451, 1.9805], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.1132, 4.6806, 4.6009, 6.2712], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1684, 4.8270, 6.3261, 8.1435], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.4691,  6.2513,  8.0446, 10.0062], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2281, -1.3823, -1.3698, -2.2090], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1961, -0.4197, -0.4375, -1.3530], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3178,  0.6494,  0.6323, -1.3747], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3207,  1.8788,  1.8791, -0.4380], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5106, 3.1733, 1.8981, 0.8085], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8648, 3.2685, 4.6648, 2.0008], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.2025, 4.7189, 4.6550, 6.3148], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2599, 4.8579, 6.3837, 8.1578], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.5687,  6.2869,  8.1162, 10.0185], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2351, -1.3889, -1.3801, -2.2154], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2128, -0.4282, -0.4401, -1.3739], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3559,  0.6378,  0.6169, -1.4026], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3442,  1.8546,  1.8567, -0.4729], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5000, 3.1493, 1.8807, 0.7792], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8584, 3.2302, 4.6541, 1.9526], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.2321, 4.6818, 4.6740, 6.2783], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2606, 4.7862, 6.3786, 8.0516], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.5890, 6.2185, 8.1414, 9.9242], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2399, -1.3890, -1.3834, -2.2274], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2224, -0.4321, -0.4295, -1.3971], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3875,  0.6334, -0.4273, -0.4393], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3636,  1.8423,  0.6548, -0.4016], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4958, 3.1413, 1.8818, 0.7629], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8549, 3.2003, 4.6754, 1.9130], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.3071, 4.6873, 4.7002, 6.3624], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2455, 4.7115, 6.3157, 7.9997], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.6441, 6.2014, 8.1260, 9.9720], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2426, -1.3745, -1.4034, -2.2284], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3665, -0.4090, -0.4303, -2.2600], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5091,  0.6491,  0.6618, -1.3445], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3701,  1.8387,  1.8398, -0.5231], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5004, 3.1679, 1.8659, 0.7698], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8545, 3.2024, 4.6700, 1.9025], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.3090, 4.6832, 4.6647, 6.3513], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2376, 4.6885, 6.2656, 7.9730], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.6811,  6.2157,  8.1230, 10.0124], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2419, -1.3683, -1.3913, -2.2339], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3678, -0.4046, -0.4250, -2.2632], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5071,  0.6574,  0.6692, -1.3454], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3994,  1.8244,  1.8241, -0.5561], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7143, 1.8603, 3.1736, 0.6017], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7883, 3.1862, 4.6449, 1.8742], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.0700, 4.6025, 4.5076, 6.1837], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1110, 4.6778, 6.1939, 8.0054], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.5219,  6.1974,  8.0355, 10.0886], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2683, -1.3835, -1.3893, -2.2369], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3966, -0.4128, -0.4248, -2.2592], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5441,  0.6582,  0.6709, -1.3421], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4453,  1.8311,  1.8192, -0.5361], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6836, 1.8680, 3.1881, 0.6414], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7510, 3.2008, 4.6655, 1.9196], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.9105, 4.6056, 4.4120, 6.1522], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0342, 4.6756, 6.1428, 7.9580], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.3923, 6.1699, 7.9661, 9.9347], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2946, -1.3889, -1.3891, -2.2412], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4203, -0.4143, -0.4204, -2.2628], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5650,  0.6655,  0.6747, -1.3418], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4640,  1.8427,  1.8256, -0.5245], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6893, 1.8846, 3.2126, 0.6721], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7851, 3.2358, 4.7105, 2.0087], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.0188, 4.7422, 4.4919, 6.4795], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0481, 4.7115, 6.1409, 8.0812], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.4598,  6.1934,  8.0011, 10.0831], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2777, -1.3883, -1.3906, -2.2084], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3877, -0.4189, -0.4139, -2.2306], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3729,  0.6789,  0.6663, -1.4074], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4088,  1.8552,  1.8816, -0.5215], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4923, 3.2223, 1.9427, 0.8402], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.9149, 3.2875, 4.8213, 1.9860], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.0424, 4.6873, 4.5315, 6.2084], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3203, 4.7994, 6.3627, 8.1205], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.7549,  6.2835,  8.2104, 10.1172], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2554, -1.3705, -1.3659, -2.2171], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2155, -0.4068, -0.3941, -1.3804], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3751,  0.6673, -0.3935, -0.4056], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3671,  1.8931,  0.7065, -0.3760], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5645, 3.2645, 1.9828, 0.8824], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.9604, 3.3076, 4.8335, 1.9642], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.0957, 4.7121, 4.5261, 6.1721], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3784, 4.8285, 6.3575, 8.0583], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.7414, 6.2408, 8.1278, 9.9087], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2683, -1.3804, -1.3869, -2.2243], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3710, -0.4140, -0.4073, -2.2331], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3749,  0.6447,  0.6594, -1.4272], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3763,  1.8672,  0.6619, -0.3855], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5811, 3.2477, 1.9611, 0.9017], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.9357, 3.2697, 4.7982, 1.9261], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.1696, 4.7374, 4.5534, 6.2840], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3683, 4.8047, 6.3245, 8.0422], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.7656, 6.2485, 8.1327, 9.9231], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2824, -1.3844, -1.3907, -2.2454], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3903, -0.4100, -0.4201, -2.2508], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5320,  0.6615,  0.6719, -1.3472], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3680,  1.8936,  1.8897, -0.5026], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7592, 1.8819, 3.2302, 0.5958], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.9078, 3.2751, 4.8131, 1.9007], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.1695, 4.7743, 4.5532, 6.3260], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3831, 4.8385, 6.3146, 8.1306], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.7985,  6.2910,  8.1353, 10.0437], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2841, -1.3630, -1.3784, -2.2475], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3976, -0.3929, -0.4188, -2.2514], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5378,  0.6680,  0.6819, -1.3549], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3743,  1.8969,  1.9017, -0.5130], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5528, 3.2475, 1.9585, 0.8530], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8922, 3.2735, 4.8164, 1.8633], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.1408, 4.7641, 4.5098, 6.2571], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3614, 4.8273, 6.2703, 8.0772], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.7720,  6.2711,  8.0867, 10.0076], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2741, -1.3698, -1.3808, -2.2420], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3824, -0.3976, -0.4230, -2.2397], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5131,  0.6593,  0.6829, -1.3354], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3662,  1.8926,  1.9086, -0.4936], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5635, 3.2516, 1.9636, 0.8834], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8912, 3.2762, 4.8222, 1.8827], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.1240, 4.7739, 4.4810, 6.2583], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3080, 4.8253, 6.2071, 8.0194], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.7221, 6.2934, 8.0163, 9.9358], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2573, -1.3573, -1.3803, -2.2466], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3671, -0.3803, -0.4208, -2.2438], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4981,  0.6875,  0.6772, -1.3334], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6812,  0.7159,  1.8965, -0.4211], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7704, 1.9508, 3.2563, 0.6247], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8692, 3.3136, 4.7490, 1.8756], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.1571, 4.8687, 4.4715, 6.3443], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2946, 4.8801, 6.1485, 8.0588], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.7351,  6.3780,  7.9680, 10.0193], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2441, -1.3549, -1.3650, -2.2413], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3539, -0.3859, -0.3959, -2.2445], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4934,  0.6947,  0.6830, -1.3310], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6821,  0.7134,  1.8879, -0.4180], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7913, 1.9779, 3.2614, 0.6666], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8544, 3.2986, 4.6807, 1.8749], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.1528, 4.8563, 4.4503, 6.3314], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2888, 4.8579, 6.1251, 8.0513], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.7501,  6.3610,  7.9565, 10.0625], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2406, -1.3802, -1.3652, -2.2384], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2085, -0.3996, -0.3911, -1.3653], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3500,  0.6859, -0.3621, -0.3883], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3621,  1.9273,  0.7103, -0.3647], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5734, 3.3518, 1.9629, 0.9052], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8749, 3.2855, 4.7020, 1.9033], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.1366, 4.8126, 4.4517, 6.2762], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2693, 4.8043, 6.1181, 7.9752], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.7241, 6.2932, 7.9297, 9.9569], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2431, -1.3852, -1.3828, -2.2422], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2094, -0.3998, -0.3938, -1.3678], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3589,  0.6780, -0.3786, -0.3965], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3875,  1.8841,  0.6692, -0.4043], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5598, 3.3097, 1.9232, 0.8890], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8739, 3.2340, 4.6656, 1.9226], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.1364, 4.7240, 4.4185, 6.3175], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2926, 4.7284, 6.1083, 8.0721], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.7470,  6.1947,  7.9186, 10.0792], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2499, -1.3687, -1.3947, -2.2317], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3577, -0.3817, -0.4020, -2.2283], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4632,  0.6875,  0.7090, -1.2963], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3416,  1.8995,  1.9021, -0.4549], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5901, 3.3276, 1.9377, 0.9196], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8898, 3.2450, 4.6734, 1.9244], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.1339, 4.7145, 4.4100, 6.2636], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2869, 4.7316, 6.1014, 8.0017], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.7221, 6.2101, 7.9009, 9.9689], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2509, -1.3483, -1.3738, -2.2371], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3720, -0.3706, -0.3966, -2.2402], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4708,  0.7008,  0.7189, -1.3119], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3382,  1.9445,  1.9199, -0.4454], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.8408, 2.0037, 3.3253, 0.7502], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.9042, 3.2969, 4.6762, 1.9527], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.1107, 4.7442, 4.3942, 6.2326], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2570, 4.7509, 6.0622, 7.9672], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.7154, 6.2624, 7.8705, 9.9649], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2359, -1.3344, -1.3274, -2.2386], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2211, -0.3562, -0.3829, -1.3666], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3422,  0.7441,  0.7205, -1.3969], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3208,  1.9802,  1.9705, -0.4376], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7783, 1.9596, 3.2224, 0.6602], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.9272, 3.3098, 4.6588, 1.9725], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.1486, 4.7340, 4.5380, 6.2437], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2535, 4.7347, 6.1102, 7.9663], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.7534,  6.2726,  7.9614, 10.0180], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2246, -1.3584, -1.2985, -2.2389], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2124, -0.3458, -0.3770, -1.3547], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3286,  0.7494,  0.7197, -1.3882], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.2945,  2.0096,  2.0192, -0.4194], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5509, 3.2678, 1.9613, 0.7959], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.9416, 3.3026, 4.6564, 1.9807], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.1666, 4.7119, 4.6477, 6.2166], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2908, 4.7275, 6.1772, 7.9930], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.8124,  6.2512,  8.0425, 10.0522], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2320, -1.3663, -1.3262, -2.2324], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2136, -0.3462, -0.3955, -1.3363], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3278,  0.7248,  0.7028, -1.3859], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.2797,  2.0030,  2.0283, -0.4114], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5743, 3.2225, 1.9802, 0.7895], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.9657, 3.2553, 4.6516, 1.9651], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.1464, 4.6426, 4.6687, 6.1418], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2717, 4.6698, 6.2048, 7.9494], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.7317, 6.1611, 8.0570, 9.9841], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2601, -1.3482, -1.3458, -2.2288], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2356, -0.3352, -0.3936, -1.3363], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3604,  0.7178,  0.7056, -1.3887], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3360,  2.0185,  2.0269, -0.3945], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4954, 3.2156, 1.9873, 0.8106], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8517, 3.2268, 4.6528, 1.9818], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.9440, 4.6266, 4.6142, 6.1488], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1302, 4.6446, 6.2066, 7.9854], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.5675,  6.1306,  8.0555, 10.0039], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2705, -1.3236, -1.3512, -2.2133], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3788, -0.3352, -0.3781, -2.1994], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4805,  0.7247,  0.7239, -1.2848], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.7191,  0.7358,  2.0059, -0.3132], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7464, 1.9496, 3.2288, 0.7129], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8447, 3.2297, 4.6244, 1.9887], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.9464, 4.6721, 4.6029, 6.2160], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0784, 4.6324, 6.1384, 7.9458], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.5216, 6.1203, 7.9613, 9.9725], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2431, -1.3117, -1.3152, -2.2066], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3427, -0.3414, -0.3315, -2.2020], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3230,  0.7501,  0.7273, -1.3541], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3368,  2.0043,  1.9808, -0.3780], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7393, 1.9164, 3.1499, 0.6881], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8418, 3.1782, 4.5208, 1.9721], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.9020, 4.5942, 4.5109, 6.1637], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1039, 4.6061, 6.0847, 8.0044], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.5333,  6.0633,  7.8812, 10.0187], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2167, -1.3312, -1.2747, -2.2053], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1834, -0.3480, -0.3538, -1.3115], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2831,  0.7709,  0.7598, -1.3326], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3310,  1.9171,  1.9824, -0.4187], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4900, 3.1474, 1.9472, 0.7995], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.9044, 3.1765, 4.5624, 2.0126], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.9033, 4.5606, 4.5279, 6.1272], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1308, 4.6030, 6.1253, 7.9928], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.5611, 6.0645, 7.9371, 9.9899], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2038, -1.3196, -1.2893, -2.1966], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1727, -0.3384, -0.3508, -1.3032], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2440,  0.8177,  0.7855, -1.3060], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3209,  1.9092,  1.9735, -0.4262], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5150, 3.1775, 1.9598, 0.8236], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.9421, 3.1975, 4.5838, 2.0373], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.9421, 3.1971, 4.5743, 2.0418], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.9420, 4.5935, 4.5574, 6.1755], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1326, 4.6039, 6.1301, 7.9795], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.5711, 6.0728, 7.9452, 9.9864], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2105, -1.3100, -1.3140, -2.1918], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3301, -0.3211, -0.3770, -2.1955], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.3974,  0.7504,  0.7851, -1.2634], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3626,  1.8985,  1.9093, -0.4429], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4493, 3.1750, 1.9122, 0.7983], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8481, 3.1733, 4.5389, 1.9859], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.7450, 4.5626, 4.4672, 6.0802], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0176, 4.6064, 6.1310, 8.0176], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.4269,  6.0855,  7.9674, 10.0646], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2289, -1.3032, -1.2980, -2.1933], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1999, -0.3393, -0.3470, -1.3099], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2935,  0.8341,  0.7984, -1.3150], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3807,  1.9203,  1.9387, -0.4156], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4237, 3.1916, 1.9472, 0.8283], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8283, 3.1898, 4.5915, 2.0023], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.7145, 4.5967, 4.5021, 6.1215], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.9849, 4.6089, 6.1551, 7.9921], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.3750,  6.0736,  7.9736, 10.0054], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2336, -1.3054, -1.2939, -2.1903], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1930, -0.3508, -0.3483, -1.3011], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3648,  0.7467, -0.3404, -0.3341], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3867,  1.9189,  0.7279, -0.3161], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.3992, 3.1630, 1.8666, 0.8231], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7716, 3.1243, 4.4535, 1.9392], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.7595, 4.6428, 4.4554, 6.2307], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.9639, 4.6139, 6.0640, 7.9800], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.3327, 6.0752, 7.8456, 9.9655], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2013, -1.2909, -1.2957, -2.1714], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3134, -0.3117, -0.3274, -2.1728], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4172,  0.7498,  0.7638, -1.2591], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3889,  1.8797,  1.8675, -0.4270], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7552, 1.8839, 3.1353, 0.7151], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7964, 3.1112, 4.4949, 1.9142], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.8045, 4.6252, 4.5222, 6.2101], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1041, 4.6353, 6.1860, 8.0481], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.4710, 6.0811, 7.9685, 9.9881], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1784, -1.2837, -1.2877, -2.1696], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.2813, -0.2855, -0.2746, -2.1705], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3088,  0.7424,  0.7516, -1.3467], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3600,  1.8922,  0.7635, -0.3391], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4021, 3.1104, 1.8854, 0.7481], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8179, 3.1040, 4.5306, 1.8933], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.8266, 4.5857, 4.5370, 6.1719], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1565, 4.6055, 6.2054, 8.0532], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.5265,  6.0469,  7.9820, 10.0084], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1837, -1.2892, -1.3110, -2.1684], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4252,  0.7477,  0.7371, -1.2688], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.7275,  0.7508,  1.8591, -0.3252], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7561, 1.8660, 3.1314, 0.6788], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8123, 3.0861, 4.4997, 1.8776], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.8782, 4.5846, 4.5494, 6.2118], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1668, 4.5754, 6.1870, 8.0048], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.5703, 6.0436, 8.0004, 9.9969], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1802, -1.3142, -1.3118, -2.1714], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1500, -0.2670, -0.3154, -1.2771], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3054,  0.7322,  0.7343, -1.3307], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3438,  1.8855,  0.7586, -0.3162], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4151, 3.1199, 1.8733, 0.7623], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8321, 3.1106, 4.5216, 1.8941], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.8950, 4.5973, 4.5603, 6.2128], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1898, 4.5957, 6.1986, 8.0120], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.5948,  6.0710,  8.0086, 10.0072], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1734, -1.3091, -1.3066, -2.1664], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1474, -0.2581, -0.3183, -1.2716], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3005,  0.7397,  0.7274, -1.3154], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3553,  1.9222,  1.8751, -0.3754], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7643, 1.8811, 3.1227, 0.7020], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8314, 3.1082, 4.5016, 1.9090], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.8845, 4.5790, 4.5429, 6.1844], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2004, 4.5869, 6.1903, 8.0087], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.6012, 6.0468, 7.9908, 9.9999], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1424, -1.2712, -1.2252, -2.1485], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1361, -0.2860, -0.3139, -1.2596], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2680,  0.7081,  0.7251, -1.2748], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3313,  1.8213,  0.7495, -0.3007], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4547, 3.0684, 1.8894, 0.8139], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8733, 3.0592, 4.5349, 1.9625], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8786, 3.0469, 4.5343, 1.9658], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.9236, 4.4979, 4.5652, 6.2075], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2054, 4.5066, 6.1990, 7.9883], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.6208, 5.9538, 8.0204, 9.9937], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1474, -1.2399, -1.2752, -2.1288], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.2749, -0.3381, -0.3356, -2.1371], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2431,  0.6591,  0.6904, -1.2464], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3342,  1.7767,  0.6843, -0.3061], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5018, 3.0544, 1.8743, 0.8645], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8919, 3.0471, 4.5630, 1.9870], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.9298, 4.5059, 4.5713, 6.2160], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2178, 4.5185, 6.2478, 8.0007], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.6265, 5.9727, 8.0854, 9.9943], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1479, -1.2244, -1.2689, -2.1392], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.2823, -0.3312, -0.3687, -2.1371], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4002,  0.6832,  0.6987, -1.2302], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3379,  1.8407,  1.8435, -0.3499], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5139, 3.1083, 1.9181, 0.8711], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8774, 3.1244, 4.5997, 1.9727], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.9085, 4.6029, 4.5846, 6.1876], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2080, 4.6477, 6.2558, 7.9972], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.6165,  6.1607,  8.0932, 10.0027], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1461, -1.2439, -1.2136, -2.1696], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1589, -0.3526, -0.3567, -1.2782], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2445,  0.6915,  0.7041, -1.2446], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3471,  1.8131,  0.6986, -0.3186], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4759, 3.0769, 1.8729, 0.8344], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8337, 3.1053, 4.5513, 1.9414], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.9074, 4.6078, 4.5758, 6.2129], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1908, 4.6411, 6.2354, 7.9996], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.5969,  6.1449,  8.0799, 10.0023], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1586, -1.2647, -1.2043, -2.1841], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1675, -0.3504, -0.3529, -1.2782], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2479,  0.6929,  0.7132, -1.2451], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3502,  1.8088,  0.7117, -0.3189], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4554, 3.0465, 1.8704, 0.8085], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8318, 3.0973, 4.5848, 1.9357], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.9067, 4.5992, 4.5934, 6.2053], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1941, 4.6241, 6.2631, 7.9855], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.5915, 6.1239, 8.1110, 9.9711], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1894, -1.2703, -1.2601, -2.1967], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1852, -0.3548, -0.3647, -1.2923], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2731,  0.6671,  0.6914, -1.2691], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3722,  1.8011,  0.6704, -0.3379], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4437, 3.0394, 1.8441, 0.7980], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8011, 3.0493, 4.5974, 1.8764], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.8974, 4.5654, 4.5839, 6.2023], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1983, 4.5969, 6.2971, 8.0044], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.6071,  6.0990,  8.1631, 10.0114], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2244, -1.3339, -1.3366, -2.2144], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3274, -0.3908, -0.3996, -2.2071], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4544,  0.6309,  0.6418, -1.3023], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4208,  1.7454,  1.7287, -0.4452], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7395, 1.7902, 3.1382, 0.6855], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7817, 3.0424, 4.5991, 1.8464], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.8923, 4.5773, 4.5797, 6.1924], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1904, 4.6148, 6.3019, 7.9958], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.5953,  6.1302,  8.1652, 10.0032], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2264, -1.3454, -1.3291, -2.2136], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1906, -0.3677, -0.3653, -1.2959], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3410,  0.6590, -0.3665, -0.3493], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4167,  1.7535,  0.6054, -0.3907], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4528, 3.1004, 1.8229, 0.8241], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7720, 3.0729, 4.5888, 1.8408], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.8995, 4.6204, 4.5736, 6.2025], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1918, 4.6584, 6.2925, 7.9983], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.5965,  6.1953,  8.1530, 10.0041], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1772, -1.3207, -1.2856, -2.1821], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1660, -0.3464, -0.3461, -1.2749], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3362,  0.6659, -0.3637, -0.3469], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4462,  1.7640,  0.5806, -0.3991], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.3997, 3.1351, 1.7920, 0.8306], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7171, 3.1067, 4.5939, 1.8583], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.6964, 4.6126, 4.4833, 6.1361], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0675, 4.6629, 6.2802, 8.0429], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.4255,  6.1878,  8.1509, 10.0679], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1370, -1.2992, -1.2762, -2.1322], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1376, -0.3451, -0.3347, -1.2490], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3234,  0.6456, -0.3511, -0.3353], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4087,  1.7659,  0.6306, -0.3296], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.3717, 3.1093, 1.7753, 0.8521], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6927, 3.0806, 4.5894, 1.8769], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.6324, 4.6010, 4.4606, 6.1886], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.9393, 4.5968, 6.2145, 7.9731], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.2570, 6.1340, 8.0874, 9.9917], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1200, -1.3039, -1.3322, -2.0842], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.2573, -0.3842, -0.3682, -2.1086], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3160,  0.6264,  0.5919, -1.2622], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4824,  1.7645,  1.6839, -0.3913], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6174, 1.8027, 3.1085, 0.6802], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6100, 3.0887, 4.5777, 1.8701], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.4599, 4.6135, 4.3905, 6.1711], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.8832, 4.6213, 6.2072, 8.0190], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.2675,  6.1823,  8.1316, 10.0754], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1228, -1.3255, -1.3261, -2.0982], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.2874, -0.4198, -0.4048, -2.1377], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3269,  0.6115,  0.6054, -1.3006], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4615,  1.7854,  1.7532, -0.4055], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6785, 1.8230, 3.1575, 0.6794], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7000, 3.1002, 4.6162, 1.8507], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.5562, 4.6219, 4.4073, 6.1542], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0408, 4.6174, 6.2362, 7.9856], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.4869,  6.1946,  8.1806, 10.0663], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1433, -1.3570, -1.3141, -2.1483], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1805, -0.4474, -0.4196, -1.3321], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3622,  0.5957, -0.3885, -0.3833], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4132,  1.7288,  0.6118, -0.3788], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4519, 3.1233, 1.8289, 0.9122], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7403, 3.0446, 4.5393, 1.8033], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.6581, 4.6095, 4.3807, 6.1768], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1529, 4.6030, 6.2129, 7.9669], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.5770, 6.1279, 8.0962, 9.9747], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1625, -1.3633, -1.3202, -2.1841], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1969, -0.4716, -0.4495, -1.3574], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3690,  0.5799, -0.4126, -0.3933], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3786,  1.7379,  0.6188, -0.3588], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5202, 3.1341, 1.8612, 0.9540], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7816, 3.0485, 4.5490, 1.8004], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.6945, 4.6180, 4.3882, 6.1603], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2111, 4.6176, 6.2422, 7.9603], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.6230, 6.1354, 8.1114, 9.9237], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1767, -1.3281, -1.3114, -2.2153], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2003, -0.4533, -0.4375, -1.3782], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3728,  0.5934, -0.4145, -0.4111], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3732,  1.7811,  0.6403, -0.3790], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4568, 3.1171, 1.8262, 0.8245], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7930, 3.0748, 4.5737, 1.7890], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.7158, 4.6396, 4.4065, 6.1807], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2233, 4.6201, 6.2471, 7.9673], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.6656, 6.1635, 8.1404, 9.9698], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1884, -1.3269, -1.3160, -2.2330], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2069, -0.4482, -0.4320, -1.3949], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3880,  0.5948, -0.4227, -0.4344], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4112,  1.7835,  0.6140, -0.4358], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.3870, 3.0864, 1.7702, 0.7176], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7934, 3.0761, 4.5821, 1.7913], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.7688, 4.6616, 4.4402, 6.3022], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2759, 4.6442, 6.2902, 8.1224], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.7411,  6.1862,  8.1900, 10.1704], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1911, -1.3093, -1.3275, -2.2181], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3358, -0.4446, -0.4482, -2.2353], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4696,  0.5689,  0.5797, -1.3865], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3927,  1.7974,  1.7993, -0.4989], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.3994, 3.0953, 1.7874, 0.6776], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7676, 3.0780, 4.5698, 1.7242], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.6398, 4.6083, 4.4014, 6.1187], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2553, 4.6764, 6.3041, 8.1477], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.5636,  6.1282,  8.0835, 10.0453], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2138, -1.4009, -1.3458, -2.2361], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2238, -0.4797, -0.4395, -1.4047], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4198,  0.6054, -0.4231, -0.4398], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4608,  1.7998,  0.6275, -0.4850], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.3751, 3.1208, 1.8428, 0.7118], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6831, 3.0943, 4.5793, 1.6957], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.4561, 4.6394, 4.4211, 6.0953], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0370, 4.6511, 6.2471, 8.0291], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.4125,  6.1946,  8.1176, 10.1225], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2708, -1.4489, -1.4017, -2.2451], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2643, -0.4957, -0.4704, -1.4200], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4738,  0.6050, -0.4514, -0.4471], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.5026,  1.7892,  0.6078, -0.4861], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.3593, 3.1252, 1.8461, 0.7559], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6811, 3.1063, 4.5833, 1.7345], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.4247, 4.6669, 4.4574, 6.1245], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.9355, 4.5883, 6.1875, 7.8519], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.2776, 6.1177, 8.0503, 9.8499], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2838, -1.3963, -1.4177, -2.2243], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4494, -0.4653, -0.5184, -2.2221], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5219,  0.5715,  0.6174, -1.3903], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4721,  1.8092,  1.7995, -0.4598], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6586, 1.8093, 3.1266, 0.6119], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7289, 3.1125, 4.6256, 1.8527], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.3573, 4.6170, 4.4623, 6.2138], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0103, 4.6051, 6.2974, 8.1286], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.2654,  6.0494,  8.0574, 10.0200], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2343, -1.3928, -1.3886, -2.1969], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2034, -0.5129, -0.4631, -1.3909], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4554,  0.5928, -0.4566, -0.4406], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4980,  1.7574,  0.5946, -0.4877], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.3681, 3.0903, 1.8408, 0.7926], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7676, 3.0996, 4.6035, 1.8813], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.3703, 4.5891, 4.5073, 6.2280], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0202, 4.5887, 6.3135, 8.1324], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.2547, 6.0201, 8.0614, 9.9700], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1984, -1.4078, -1.3729, -2.1816], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1671, -0.5215, -0.4526, -1.3790], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4325,  0.5984, -0.4464, -0.4371], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4711,  1.7867,  0.6174, -0.4620], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.3788, 3.1114, 1.8435, 0.7987], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7768, 3.1108, 4.5830, 1.8694], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.4249, 4.6357, 4.5710, 6.2930], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.9401, 4.5376, 6.2097, 7.9886], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.3212,  6.0734,  8.0749, 10.0441], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2086, -1.4315, -1.3871, -2.1941], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1925, -0.5254, -0.4626, -1.3950], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4344,  0.6067, -0.4496, -0.4434], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4534,  1.8118,  0.6242, -0.4618], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4060, 3.1515, 1.8284, 0.7848], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7527, 1.8714, 3.1397, 0.6303], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8480, 3.1713, 4.5737, 1.8523], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.5366, 4.7092, 4.5697, 6.2615], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1182, 4.6376, 6.1931, 7.9501], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.5453,  6.2334,  8.0821, 10.0303], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2466, -1.4566, -1.4117, -2.2516], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2210, -0.5167, -0.4683, -1.4294], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4491,  0.6041, -0.4633, -0.4706], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4457,  1.8128,  0.6080, -0.4824], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4285, 3.1491, 1.8000, 0.7656], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8872, 3.1659, 4.5770, 1.8339], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.6122, 4.6756, 4.5727, 6.2731], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2172, 4.6339, 6.2211, 7.9751], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.6173,  6.2009,  8.0809, 10.0319], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2530, -1.4670, -1.4119, -2.2539], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2212, -0.5072, -0.4666, -1.4154], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4485,  0.5962, -0.4675, -0.4692], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4531,  1.7822,  0.5905, -0.5028], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4428, 3.1206, 1.7962, 0.7629], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8669, 3.1384, 4.5603, 1.8017], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.5968, 4.6200, 4.5459, 6.2076], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2278, 4.6138, 6.2148, 7.9572], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.6126,  6.1776,  8.0697, 10.0004], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2552, -1.4575, -1.4027, -2.2596], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2322, -0.4940, -0.4613, -1.4151], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4619,  0.5963, -0.4669, -0.4796], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4847,  1.7728,  0.5858, -0.5334], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4109, 3.1176, 1.7982, 0.7335], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8128, 3.1331, 4.5801, 1.7679], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.5923, 4.6272, 4.5753, 6.2318], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1932, 4.5906, 6.2190, 7.9310], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.5965,  6.1687,  8.1096, 10.0158], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2761, -1.4451, -1.4061, -2.2667], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2499, -0.4757, -0.4536, -1.4124], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4730,  0.6029, -0.4602, -0.4805], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4894,  1.7848,  0.6102, -0.5308], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.3880, 3.1147, 1.8027, 0.7050], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7876, 3.1246, 4.5862, 1.7419], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.5802, 4.5978, 4.5757, 6.1837], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2042, 4.5788, 6.2350, 7.8953], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.5864, 6.1410, 8.1097, 9.9226], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2567, -1.3866, -1.3975, -2.2561], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3909, -0.3915, -0.4226, -2.2451], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4799,  0.6539,  0.6565, -1.4310], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4252,  1.8157,  1.8316, -0.5167], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.3972, 3.1377, 1.8194, 0.6847], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8235, 3.1567, 4.6321, 1.7805], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.5331, 4.5575, 4.5165, 6.0983], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3002, 4.6303, 6.2747, 8.0503], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.5922, 6.1146, 8.0201, 9.9503], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2557, -1.3658, -1.4347, -2.2450], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3860, -0.3999, -0.4581, -2.2448], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4568,  0.6435,  0.6321, -1.4041], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6477,  0.6429,  1.7950, -0.4694], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7057, 1.8287, 3.1307, 0.5658], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8365, 3.1473, 4.6245, 1.8291], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.4711, 4.5420, 4.4470, 6.1362], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2251, 4.5935, 6.2111, 8.0529], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.4938, 6.0823, 7.9690, 9.9837], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2726, -1.3955, -1.4314, -2.2558], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4072, -0.4326, -0.4640, -2.2538], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4676,  0.6277,  0.6317, -1.3986], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4437,  1.8010,  1.7995, -0.4788], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6680, 1.8142, 3.1369, 0.5500], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8083, 3.1569, 4.6602, 1.8382], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.4040, 4.5593, 4.4233, 6.1607], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1298, 4.5801, 6.1823, 8.0181], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.4259,  6.1079,  7.9902, 10.0109], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2934, -1.3978, -1.4190, -2.2891], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4211, -0.4374, -0.4512, -2.2843], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4794,  0.6338,  0.6314, -1.4084], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6137,  0.6294,  1.8100, -0.4871], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6573, 1.8329, 3.1435, 0.5554], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7853, 3.1736, 4.6605, 1.8306], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.4184, 4.6128, 4.5351, 6.1841], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1184, 4.6249, 6.2312, 8.0221], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.4066, 6.1430, 8.0363, 9.9990], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2947, -1.3839, -1.4137, -2.2926], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4071, -0.4322, -0.4387, -2.2825], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4758,  0.6368,  0.6205, -1.3981], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6331,  0.6441,  1.8155, -0.4468], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6741, 1.8368, 3.1409, 0.5722], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7950, 3.1634, 4.6441, 1.8230], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.4472, 4.6176, 4.6056, 6.1552], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1493, 4.6475, 6.2661, 8.0233], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.4397, 6.1590, 8.0615, 9.9877], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2881, -1.3825, -1.4299, -2.2935], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3872, -0.4370, -0.4476, -2.2841], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4576,  0.6305,  0.5995, -1.3956], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6464,  0.6343,  1.7853, -0.4465], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6864, 1.8271, 3.0950, 0.5703], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7908, 3.1305, 4.5538, 1.7971], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.5070, 4.6238, 4.6083, 6.2157], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1716, 4.6325, 6.2116, 8.0259], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.5133,  6.1694,  8.0291, 10.0541], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2774, -1.3774, -1.4277, -2.2779], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3730, -0.4317, -0.4430, -2.2695], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4388,  0.6368,  0.6064, -1.3851], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6685,  0.6437,  1.7930, -0.4340], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7543, 1.8774, 3.1554, 0.6251], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8023, 3.1219, 4.5224, 1.7533], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.5845, 4.6486, 4.6525, 6.2401], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2119, 4.6366, 6.2100, 7.9842], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.5622,  6.1770,  8.0316, 10.0202], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2537, -1.3805, -1.3792, -2.2603], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2481, -0.4422, -0.4407, -1.4041], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4099,  0.6209, -0.4357, -0.4362], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3699,  1.8376,  0.6593, -0.4424], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4836, 3.1738, 1.8509, 0.6858], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8157, 3.1233, 4.5151, 1.7628], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.5700, 4.6187, 4.6306, 6.1821], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2324, 4.6405, 6.2058, 7.9838], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.5616, 6.1637, 7.9988, 9.9923], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2673, -1.4057, -1.4070, -2.2612], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3940, -0.4517, -0.4523, -2.2730], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4403,  0.6264,  0.6215, -1.3911], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6697,  0.6296,  1.8003, -0.4377], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7595, 1.8841, 3.1667, 0.6314], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8196, 3.1376, 4.5215, 1.7779], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.5740, 4.6314, 4.6353, 6.1984], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2196, 4.6428, 6.2030, 7.9589], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.5585, 6.1817, 8.0181, 9.9602], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2758, -1.3885, -1.4259, -2.2577], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4061, -0.4350, -0.4670, -2.2745], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4465,  0.6384,  0.6284, -1.4010], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6602,  0.6347,  1.8086, -0.4550], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7228, 1.8781, 3.1561, 0.5887], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8121, 3.1562, 4.5382, 1.7933], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.5627, 4.6419, 4.6132, 6.2375], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2225, 4.6565, 6.1999, 8.0308], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.5556,  6.1925,  8.0039, 10.0563], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2664, -1.3792, -1.4010, -2.2476], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4114, -0.4342, -0.4546, -2.2660], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4575,  0.6291,  0.6414, -1.3981], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4199,  1.8080,  1.8264, -0.4886], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4054, 3.1531, 1.8198, 0.6239], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7822, 3.1433, 4.5365, 1.7792], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.5098, 4.6017, 4.5734, 6.1539], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1619, 4.6140, 6.1526, 7.9197], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.5099, 6.1537, 7.9779, 9.9676], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2532, -1.3983, -1.3690, -2.2506], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2501, -0.4511, -0.4286, -1.4024], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4292,  0.6103, -0.4378, -0.4434], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4504,  1.8010,  0.6353, -0.4913], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.3718, 3.0855, 1.8031, 0.5963], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7848, 3.1160, 4.5504, 1.8234], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.4787, 4.5253, 4.5461, 6.1392], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1485, 4.5414, 6.1357, 7.9364], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.5025, 6.0831, 7.9686, 9.9823], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2460, -1.3875, -1.3776, -2.2401], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2476, -0.4271, -0.4415, -1.3894], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4071,  0.6482,  0.6240, -1.4129], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4338,  1.8024,  1.8346, -0.4811], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.3638, 3.0778, 1.7951, 0.5839], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8232, 3.1537, 4.5848, 1.8667], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.5081, 4.5644, 4.5697, 6.1806], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1868, 4.5658, 6.1698, 7.9714], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.5282, 6.0978, 7.9809, 9.9847], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2344, -1.3851, -1.4019, -2.2326], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3845, -0.3918, -0.4542, -2.2341], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4376,  0.6479,  0.6393, -1.3793], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6665,  0.6431,  1.8300, -0.4259], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6372, 1.7955, 3.0692, 0.4878], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8528, 3.1537, 4.5538, 1.8578], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.5288, 4.5544, 4.5402, 6.1526], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2188, 4.5692, 6.1434, 7.9930], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.5601,  6.0932,  7.9422, 10.0169], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2380, -1.4153, -1.4124, -2.2428], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2386, -0.4338, -0.4539, -1.3885], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3955,  0.6379,  0.6241, -1.4190], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4099,  1.8090,  1.8419, -0.4750], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4610, 3.0791, 1.8640, 0.6624], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7963, 3.1249, 4.5524, 1.8423], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.3869, 4.5232, 4.4883, 6.1098], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1092, 4.5379, 6.1337, 7.9877], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.4228,  6.0751,  7.9545, 10.0494], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2587, -1.4217, -1.4087, -2.2476], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2605, -0.4452, -0.4478, -1.3985], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4269,  0.6409,  0.6296, -1.4165], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4441,  1.8229,  1.8431, -0.4578], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4285, 3.0859, 1.8703, 0.6809], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7506, 3.1286, 4.5606, 1.8311], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.3330, 4.5591, 4.4949, 6.1337], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0292, 4.5244, 6.1175, 7.9211], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.3262, 6.0531, 7.9310, 9.9665], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2540, -1.3905, -1.4033, -2.2372], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4031, -0.4062, -0.4443, -2.2325], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4583,  0.6403,  0.6393, -1.3692], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6225,  0.6318,  1.8165, -0.4173], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5799, 1.7809, 3.0813, 0.5076], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7348, 3.0994, 4.5460, 1.8472], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.3171, 4.5332, 4.5001, 6.1709], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0314, 4.5068, 6.1369, 7.9906], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.3105,  6.0117,  7.9367, 10.0071], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2438, -1.3671, -1.3888, -2.2357], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3869, -0.4166, -0.4322, -2.2422], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4470,  0.6274,  0.6424, -1.3749], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4546,  1.7770,  1.8061, -0.4730], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4154, 3.0500, 1.8509, 0.6611], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7480, 3.0795, 4.5537, 1.8262], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.3266, 4.5114, 4.4986, 6.1412], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0951, 4.5168, 6.1806, 8.0175], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.3629,  6.0107,  7.9653, 10.0050], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2376, -1.3451, -1.4013, -2.2357], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3671, -0.4116, -0.4302, -2.2481], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4350,  0.6353,  0.6416, -1.3765], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4475,  1.7760,  1.7925, -0.4808], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4413, 3.0839, 1.8708, 0.6747], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7662, 3.0943, 4.5725, 1.8139], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.3619, 4.5396, 4.5229, 6.1627], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1124, 4.5308, 6.1849, 7.9967], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.3883, 6.0313, 7.9702, 9.9920], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2418, -1.3584, -1.3972, -2.2592], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3542, -0.4306, -0.4101, -2.2709], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3581,  0.6534,  0.6571, -1.4041], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4510,  1.7773,  0.6013, -0.5028], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4575, 3.0924, 1.8729, 0.6819], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7925, 3.0889, 4.5553, 1.8123], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.4382, 4.5398, 4.5388, 6.1822], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2403, 4.5497, 6.1985, 8.0223], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.6545,  6.0910,  8.0471, 10.0826], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2419, -1.4052, -1.4041, -2.2703], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2055, -0.4328, -0.4333, -1.3938], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3368,  0.6449,  0.6497, -1.3946], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3801,  1.7864,  0.6170, -0.4726], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5746, 3.1213, 1.8923, 0.7192], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.9252, 3.1125, 4.5509, 1.8332], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.5873, 4.5477, 4.5463, 6.1219], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3901, 4.5727, 6.1989, 7.9667], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.8058,  6.0969,  8.0339, 10.0263], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2498, -1.4114, -1.4124, -2.2689], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3469, -0.4384, -0.4377, -2.2467], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3565,  0.6371,  0.6242, -1.3850], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3863,  1.7565,  1.7413, -0.4382], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7263, 1.7880, 3.0727, 0.6258], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8461, 3.0739, 4.5048, 1.8339], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.4885, 4.5336, 4.5207, 6.1549], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1957, 4.5231, 6.1405, 7.9340], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.5759, 6.0365, 7.9743, 9.9770], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2651, -1.4048, -1.3851, -2.2733], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2326, -0.4256, -0.4283, -1.3856], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3951,  0.6201,  0.6282, -1.4031], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4235,  1.7578,  0.6022, -0.4620], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4674, 3.0644, 1.8262, 0.7037], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7672, 3.0618, 4.5394, 1.8340], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.3141, 4.4895, 4.4756, 6.0694], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0911, 4.5362, 6.1801, 8.0088], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.3868, 6.0346, 7.9704, 9.9870], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2929, -1.3974, -1.3782, -2.2711], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2628, -0.4380, -0.4208, -1.3966], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4269,  0.6279, -0.4312, -0.4283], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4887,  1.7504,  0.5676, -0.4704], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.3824, 3.0559, 1.7817, 0.6974], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6978, 3.0831, 4.5535, 1.8515], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2049, 4.5668, 4.4460, 6.1277], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.9797, 4.5782, 6.1666, 8.0071], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.2840,  6.1059,  7.9829, 10.0000], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3158, -1.3891, -1.4096, -2.2489], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4621, -0.4419, -0.4551, -2.2535], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5078,  0.5982,  0.5721, -1.3573], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.5353,  0.6028,  1.7251, -0.4380], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5864, 1.7464, 3.0504, 0.5939], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6883, 3.0876, 4.5606, 1.8554], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1805, 4.6005, 4.4423, 6.1777], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.9815, 4.5845, 6.1628, 8.0010], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.3168,  6.1293,  8.0005, 10.0384], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3041, -1.3980, -1.3943, -2.2420], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2598, -0.4557, -0.4157, -1.3920], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4164,  0.6035, -0.4239, -0.4180], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4427,  1.7695,  0.5922, -0.4075], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4477, 3.0614, 1.7990, 0.7559], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7521, 3.0757, 4.5448, 1.8623], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2132, 4.5790, 4.4229, 6.1421], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0667, 4.5981, 6.1659, 8.0148], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.4011,  6.1260,  7.9759, 10.0182], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2755, -1.3846, -1.3812, -2.2478], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2426, -0.4445, -0.4173, -1.3962], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3967,  0.6024, -0.4263, -0.4235], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4125,  1.7663,  0.5945, -0.4268], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4628, 3.0415, 1.7822, 0.6911], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7781, 3.0596, 4.5361, 1.8123], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2447, 4.5649, 4.4234, 6.1233], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1055, 4.5764, 6.1700, 7.9998], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.4542,  6.1058,  7.9962, 10.0231], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2638, -1.3793, -1.3950, -2.2578], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4191, -0.4311, -0.4414, -2.2748], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4455,  0.5779,  0.5881, -1.3939], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4154,  1.7609,  1.7756, -0.4576], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4322, 3.0277, 1.7525, 0.6197], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4280, 3.0298, 1.7427, 0.6127], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7686, 3.0473, 4.5270, 1.7694], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.3036, 4.5898, 4.4457, 6.2282], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1204, 4.5656, 6.1674, 8.0168], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.4552,  6.0810,  7.9720, 10.0139], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2570, -1.3883, -1.4029, -2.2612], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4220, -0.4468, -0.4757, -2.2746], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4431,  0.5830,  0.5765, -1.4029], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6047,  0.6029,  1.7478, -0.4868], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6737, 1.7747, 3.0827, 0.5519], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7725, 3.0956, 4.5386, 1.7431], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.3099, 4.6390, 4.4535, 6.1862], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1633, 4.6496, 6.1993, 8.0149], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.4824, 6.1977, 8.0022, 9.9826], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2352, -1.3971, -1.3678, -2.2586], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2410, -0.4681, -0.4391, -1.4119], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4028,  0.5821, -0.4511, -0.4437], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4254,  1.7561,  0.5584, -0.4860], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4319, 3.0939, 1.7466, 0.5974], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7514, 3.0865, 4.5103, 1.7308], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.3040, 4.6199, 4.4282, 6.1786], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1602, 4.6455, 6.1767, 8.0191], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.4846,  6.1925,  7.9816, 10.0042], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4016, -0.4840, -0.4838, -2.2420], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3902,  0.5698,  0.5791, -1.4190], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4334,  1.7437,  0.5442, -0.4760], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4051, 3.0766, 1.7246, 0.5847], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7327, 3.0770, 4.5208, 1.7282], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.3033, 4.5967, 4.4383, 6.1949], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1437, 4.6251, 6.1874, 8.0036], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.4748, 6.1794, 8.0120, 9.9994], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2389, -1.3829, -1.3711, -2.2352], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2548, -0.4669, -0.4652, -1.3825], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4236,  0.5846, -0.4634, -0.4460], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4537,  1.7453,  0.5414, -0.4872], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.3750, 3.0775, 1.7221, 0.5725], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7004, 3.0629, 4.5272, 1.7196], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2107, 4.5398, 4.3935, 6.1521], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0869, 4.5486, 6.1708, 8.0043], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.4031,  6.0882,  8.0005, 10.0123], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2567, -1.3896, -1.3943, -2.2359], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4171, -0.4760, -0.4864, -2.2398], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4675,  0.5892,  0.5784, -1.3851], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.5647,  0.5840,  1.7396, -0.4706], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6306, 1.7619, 3.0972, 0.5774], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6922, 3.0560, 4.5403, 1.7464], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2001, 4.5475, 4.4115, 6.2169], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2059, 4.5779, 4.4280, 6.2301], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0745, 4.5687, 6.2097, 8.0212], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.3767,  6.1503,  8.0648, 10.0036], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2515, -1.3993, -1.3851, -2.2391], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2612, -0.4757, -0.4569, -1.3941], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4284,  0.5909, -0.4461, -0.4411], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4468,  1.7589,  0.5904, -0.4668], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.3942, 3.1175, 1.7971, 0.6197], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7348, 3.1317, 4.6108, 1.7782], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2834, 4.6333, 4.5534, 6.2484], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1946, 4.6378, 6.2855, 8.0460], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.6025,  6.2439,  8.1591, 10.0594], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2627, -1.4515, -1.4357, -2.2599], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2573, -0.4909, -0.4929, -1.4076], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3892,  0.5453,  0.5531, -1.4409], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4003,  1.7501,  0.5508, -0.4842], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4817, 3.1001, 1.7541, 0.5887], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8972, 3.1400, 4.5654, 1.7639], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.5003, 4.6558, 4.5820, 6.2079], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.4333, 4.6810, 6.3111, 7.9993], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.9164,  6.3034,  8.2242, 10.0374], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2601, -1.4361, -1.4529, -2.2552], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4082, -0.4652, -0.5200, -2.2370], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4187,  0.5787,  0.5753, -1.3940], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6717,  0.6044,  1.7776, -0.4862], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7304, 1.7811, 3.0689, 0.4985], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.9559, 3.1578, 4.5952, 1.7562], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.5945, 4.6769, 4.6562, 6.2081], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.5376, 4.7019, 6.3699, 8.0147], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.9929,  6.2796,  8.2477, 10.0054], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2524, -1.4294, -1.3964, -2.2631], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2459, -0.4655, -0.4410, -1.4210], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4227,  0.5710, -0.4606, -0.4776], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3634,  1.7460,  0.6118, -0.4864], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5099, 3.0390, 1.7941, 0.5501], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.9365, 3.1055, 4.5727, 1.7452], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.5995, 4.6305, 4.6587, 6.2021], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.5534, 4.6872, 6.3687, 8.0251], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([8.0026, 6.2468, 8.2411, 9.9936], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2720, -1.4350, -1.4063, -2.2653], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2705, -0.4728, -0.4452, -1.4281], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4564,  0.5620, -0.4718, -0.4900], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4105,  1.7617,  0.5828, -0.4776], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4671, 3.0717, 1.7956, 0.5836], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8698, 3.1112, 4.5998, 1.7661], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.4989, 4.6322, 4.6338, 6.2365], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3827, 4.6103, 6.2881, 7.9986], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.8142, 6.1970, 8.1401, 9.9943], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3034, -1.4175, -1.4365, -2.2548], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4964, -0.4595, -0.4914, -2.2629], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5094,  0.5785,  0.5665, -1.4058], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.5536,  0.5756,  1.7678, -0.5172], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6263, 1.7754, 3.1224, 0.4923], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8466, 3.1495, 4.6297, 1.7935], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.4195, 4.6197, 4.5834, 6.1965], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3322, 4.6071, 6.2459, 8.0272], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.7520,  6.1907,  8.0967, 10.0078], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2829, -1.4550, -1.3887, -2.2629], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2922, -0.5041, -0.4453, -1.4345], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4513,  0.5635, -0.4530, -0.4782], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4159,  1.7874,  0.6123, -0.4560], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5001, 3.1162, 1.8449, 0.6517], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8769, 3.1530, 4.6254, 1.8400], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.4308, 4.6017, 4.5337, 6.2079], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3073, 4.5873, 6.1563, 8.0100], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.7372,  6.1757,  7.9901, 10.0062], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2898, -1.4884, -1.4361, -2.2671], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.3016, -0.5331, -0.4990, -1.4344], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4436,  0.5629, -0.4755, -0.4703], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4311,  1.7709,  0.5580, -0.4653], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4946, 3.1104, 1.8074, 0.6601], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8244, 3.1377, 4.5725, 1.8161], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.3927, 4.6053, 4.5065, 6.1938], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2655, 4.5929, 6.1506, 8.0053], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.6802,  6.1893,  7.9988, 10.0047], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2951, -1.4444, -1.4123, -2.2717], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.3180, -0.5182, -0.4942, -1.4430], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4605,  0.5776, -0.4628, -0.4796], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4692,  1.7712,  0.5773, -0.4981], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4500, 3.1071, 1.8323, 0.6239], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7562, 3.1308, 4.6181, 1.7612], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.3622, 4.6145, 4.5454, 6.1856], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2422, 4.5984, 6.1918, 8.0086], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.6503,  6.1953,  8.0431, 10.0062], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3000, -1.4359, -1.4129, -2.2745], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.3226, -0.5188, -0.5026, -1.4382], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4684,  0.5787, -0.4719, -0.4800], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4797,  1.7669,  0.5557, -0.4974], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4157, 3.0852, 1.7838, 0.5950], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7315, 3.1125, 4.5912, 1.7480], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.3636, 4.6181, 4.5289, 6.2013], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2326, 4.5980, 6.1766, 8.0045], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.6367, 6.1964, 8.0235, 9.9978], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2802, -1.4244, -1.4107, -2.2757], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2982, -0.5139, -0.4934, -1.4322], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4501,  0.5879, -0.4694, -0.4752], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4653,  1.7844,  0.5627, -0.4855], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4174, 3.1018, 1.7832, 0.5929], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7294, 3.1273, 4.5967, 1.7464], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.3604, 4.6401, 4.5300, 6.1952], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2231, 4.6190, 6.1771, 8.0032], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.6264,  6.2305,  8.0277, 10.0025], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2700, -1.4139, -1.4304, -2.2801], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4395, -0.4904, -0.5137, -2.2905], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5013,  0.5938,  0.5880, -1.4254], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.5424,  0.5814,  1.7702, -0.5148], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6137, 1.8254, 3.1391, 0.5722], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7119, 3.1430, 4.5827, 1.7273], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.3637, 4.6749, 4.5443, 6.2084], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2093, 4.6428, 6.1802, 7.9979], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.6084, 6.2516, 8.0285, 9.9927], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2729, -1.4455, -1.4409, -2.2924], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2554, -0.4703, -0.4522, -1.4238], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4255,  0.6146, -0.4530, -0.4701], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4569,  1.8007,  0.5848, -0.4924], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4287, 3.1379, 1.8087, 0.6038], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6997, 3.1283, 4.5796, 1.7151], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.3647, 4.6601, 4.5699, 6.2099], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2070, 4.6252, 6.1995, 8.0072], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.6015, 6.2194, 8.0461, 9.9963], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2780, -1.4614, -1.4495, -2.2978], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2667, -0.4600, -0.4585, -1.4251], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4362,  0.6097, -0.4648, -0.4735], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4666,  1.7855,  0.5702, -0.4959], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4236, 3.1300, 1.8026, 0.6107], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6946, 3.1117, 4.5799, 1.7217], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.3569, 4.6429, 4.5762, 6.2155], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1977, 4.5986, 6.2100, 8.0021], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.6004, 6.2022, 8.0698, 9.9985], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2794, -1.4617, -1.4621, -2.2794], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4395, -0.4490, -0.4855, -2.2575], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4982,  0.5983,  0.5905, -1.4118], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.5487,  0.5852,  1.7949, -0.5022], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6183, 1.8079, 3.1439, 0.5763], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7371, 3.1364, 4.5958, 1.7545], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.3716, 4.6418, 4.5723, 6.2053], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2186, 4.6005, 6.2032, 7.9993], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.6108, 6.1992, 8.0465, 9.9892], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2729, -1.4551, -1.4557, -2.2710], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4470, -0.4640, -0.4774, -2.2671], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4989,  0.5882,  0.5925, -1.4172], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4943,  1.8026,  1.7833, -0.4705], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6161, 1.8034, 3.1307, 0.5823], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7396, 3.1358, 4.5767, 1.7767], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2942, 4.6380, 4.5256, 6.2083], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1744, 4.5957, 6.1815, 8.0133], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.5633,  6.1942,  8.0261, 10.0053], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2700, -1.4561, -1.4370, -2.2698], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2884, -0.4760, -0.4603, -1.4415], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4412,  0.6068, -0.4627, -0.4637], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4204,  1.8066,  0.5956, -0.4441], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4890, 3.1598, 1.8171, 0.6529], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8446, 3.1703, 4.5989, 1.7996], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.4196, 4.6697, 4.5536, 6.2074], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3367, 4.6094, 6.1568, 7.9650], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.8392, 6.2292, 8.0158, 9.9922], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2419, -1.4044, -1.3996, -2.2686], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2617, -0.4535, -0.4534, -1.4420], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4030,  0.6232, -0.4526, -0.4598], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3745,  1.8279,  0.6089, -0.4409], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5284, 3.1524, 1.8092, 0.6271], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8903, 3.1975, 4.5995, 1.8103], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.5010, 4.6714, 4.5604, 6.2080], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.4566, 4.6211, 6.1772, 8.0152], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.9416, 6.2034, 8.0043, 9.9946], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2489, -1.4188, -1.4295, -2.2725], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4259, -0.4518, -0.4578, -2.2752], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4665,  0.6169,  0.6207, -1.4255], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4175,  1.8369,  1.8349, -0.4597], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7069, 1.8098, 3.1286, 0.5676], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8586, 3.1885, 4.6021, 1.8007], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.5229, 4.6499, 4.5855, 6.2179], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.4750, 4.5983, 6.1879, 8.0090], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.9577, 6.1782, 8.0169, 9.9862], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2595, -1.4535, -1.4456, -2.2784], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2664, -0.4620, -0.4636, -1.4432], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4399,  0.6311,  0.6150, -1.4311], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4382,  1.8457,  1.8377, -0.4630], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7022, 1.8279, 3.1358, 0.5749], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8274, 3.2046, 4.5949, 1.7916], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.5263, 4.6644, 4.5956, 6.2154], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.4778, 4.6026, 6.1783, 7.9998], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.9841,  6.2020,  8.0251, 10.0040], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2703, -1.4411, -1.4386, -2.2816], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2683, -0.4578, -0.4617, -1.4417], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4355,  0.6269,  0.6167, -1.4368], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4398,  1.8391,  1.8335, -0.4729], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6950, 1.8245, 3.1291, 0.5686], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8019, 3.1947, 4.5858, 1.7792], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.4698, 4.6375, 4.5906, 6.1897], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3730, 4.5684, 6.1480, 7.9808], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.8814,  6.1786,  8.0190, 10.0297], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2897, -1.4177, -1.4135, -2.2886], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2802, -0.4568, -0.4531, -1.4407], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4158,  0.6135, -0.4519, -0.4585], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3887,  1.8272,  0.6309, -0.4416], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5437, 3.1373, 1.8659, 0.6600], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8064, 3.1976, 4.5941, 1.8136], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.4334, 4.6383, 4.5780, 6.1918], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3438, 4.5793, 6.1488, 8.0103], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.8110, 6.1558, 7.9685, 9.9865], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2976, -1.3865, -1.4075, -2.2806], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4539, -0.4436, -0.4614, -2.2750], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4831,  0.6400,  0.6204, -1.4139], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6114,  0.6313,  1.8387, -0.4673], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7202, 1.8479, 3.1444, 0.6482], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8460, 3.1929, 4.5807, 1.8385], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.4629, 4.6382, 4.5825, 6.2036], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3855, 4.5886, 6.1479, 8.0269], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.8985,  6.1698,  7.9784, 10.0057], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2829, -1.4068, -1.3924, -2.2835], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2547, -0.4337, -0.4418, -1.4198], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3707,  0.6352,  0.6425, -1.3996], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3566,  1.8244,  0.6337, -0.4480], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5944, 3.1384, 1.8614, 0.6696], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8872, 3.1958, 4.5736, 1.8306], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.5368, 4.6529, 4.5945, 6.2226], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.4461, 4.5949, 6.1596, 8.0188], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.9873,  6.1879,  8.0108, 10.0143], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2897, -1.4172, -1.4229, -2.2815], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4263, -0.4297, -0.4584, -2.2604], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4570,  0.6537,  0.6387, -1.4114], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6486,  0.6573,  1.8432, -0.4391], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7516, 1.8490, 3.1332, 0.6330], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8849, 3.1887, 4.5745, 1.8115], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.5543, 4.6502, 4.6059, 6.2100], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.4765, 4.5943, 6.1921, 8.0177], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.0195,  6.1768,  8.0427, 10.0188], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2785, -1.4048, -1.4056, -2.2750], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4177, -0.4259, -0.4459, -2.2555], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4481,  0.6489,  0.6561, -1.4110], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4166,  1.8173,  1.8184, -0.4682], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6241, 3.1344, 1.8584, 0.6791], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8686, 3.1537, 4.5753, 1.7859], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.5573, 4.6308, 4.6146, 6.2021], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.4774, 4.5765, 6.2043, 8.0133], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.0172,  6.1549,  8.0563, 10.0183], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2750, -1.4070, -1.4012, -2.2708], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2520, -0.4320, -0.4445, -1.4138], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3535,  0.6509,  0.6461, -1.3818], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4316,  1.8249,  1.8132, -0.4696], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7149, 1.8243, 3.1346, 0.5956], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8278, 3.1542, 4.5783, 1.7639], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.5443, 4.6368, 4.6077, 6.2023], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.4710, 4.5798, 6.2031, 8.0200], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.9926,  6.1493,  8.0384, 10.0013], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2806, -1.4104, -1.4137, -2.2690], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4158, -0.4281, -0.4426, -2.2553], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4565,  0.6611,  0.6570, -1.4104], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6293,  0.6545,  1.8395, -0.4442], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7127, 1.8299, 3.1476, 0.5934], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8209, 3.1596, 4.5874, 1.7611], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.5002, 4.6333, 4.5835, 6.1893], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.4060, 4.5596, 6.1649, 7.9900], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.8979, 6.1294, 7.9933, 9.9868], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2736, -1.3718, -1.3880, -2.2527], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4088, -0.4055, -0.4211, -2.2338], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4730,  0.6653,  0.6523, -1.3972], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6001,  0.6527,  1.8317, -0.4250], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6397, 1.8033, 3.1179, 0.5767], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7712, 3.1601, 4.6002, 1.8243], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.3131, 4.6059, 4.5142, 6.1901], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3141, 4.5379, 6.1652, 8.0265], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.7901,  6.0990,  7.9941, 10.0118], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2865, -1.3972, -1.4054, -2.2378], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4148, -0.4088, -0.4195, -2.2086], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4819,  0.6574,  0.6496, -1.3814], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.5892,  0.6520,  1.8325, -0.4088], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6336, 1.8114, 3.1381, 0.5951], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7676, 3.1499, 4.6019, 1.8437], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2875, 4.6095, 4.5043, 6.2467], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2679, 4.5082, 6.1378, 8.0101], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.7455,  6.0651,  7.9604, 10.0131], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2961, -1.4036, -1.4243, -2.2419], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4180, -0.4160, -0.4197, -2.2306], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4795,  0.6508,  0.6449, -1.3877], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6023,  0.6495,  1.8353, -0.4079], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6344, 1.8154, 3.1349, 0.6006], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7615, 3.1321, 4.5801, 1.8310], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2519, 4.5850, 4.4807, 6.2072], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2504, 4.4959, 6.1276, 7.9936], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.7298,  6.0575,  7.9612, 10.0014], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2892, -1.4014, -1.4016, -2.2565], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4193, -0.4151, -0.4012, -2.2586], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4020,  0.6522,  0.6465, -1.3819], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4945,  1.8421,  1.8364, -0.4272], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6367, 1.8247, 3.1445, 0.6041], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7652, 3.1361, 4.5834, 1.8364], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2443, 4.5849, 4.4746, 6.2093], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2411, 4.4907, 6.1152, 7.9866], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.7262, 6.0532, 7.9486, 9.9948], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2878, -1.4011, -1.4016, -2.2622], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4199, -0.4125, -0.4154, -2.2655], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4711,  0.6530,  0.6482, -1.3995], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6175,  0.6524,  1.8399, -0.4182], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6481, 1.8208, 3.1345, 0.6019], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8061, 3.1338, 4.5830, 1.8436], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2448, 4.5706, 4.4625, 6.1748], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2567, 4.4877, 6.1230, 7.9713], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.7439, 6.0472, 7.9702, 9.9713], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2631, -1.3708, -1.3695, -2.2654], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2617, -0.4081, -0.4157, -1.4121], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3708,  0.6475,  0.6515, -1.3865], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3830,  1.8228,  0.6483, -0.4253], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5695, 3.1244, 1.8598, 0.7196], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8299, 3.1233, 4.5994, 1.8577], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2378, 4.5606, 4.5307, 6.1675], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2460, 4.4813, 6.1582, 7.9893], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.7200, 6.0371, 8.0078, 9.9911], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2638, -1.3531, -1.3654, -2.2659], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3905, -0.4046, -0.4097, -2.2736], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4594,  0.6552,  0.6490, -1.4046], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6162,  0.6474,  1.8228, -0.4327], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6523, 1.8198, 3.1377, 0.6037], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8327, 3.1407, 4.6150, 1.8743], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2523, 4.6199, 4.6155, 6.2395], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2377, 4.5089, 6.1960, 7.9930], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.7099, 6.0775, 8.0470, 9.9814], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2573, -1.3630, -1.3788, -2.2593], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3769, -0.4086, -0.4237, -2.2633], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4366,  0.6554,  0.6430, -1.3939], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6337,  0.6528,  1.8132, -0.4187], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6836, 1.8363, 3.1376, 0.6418], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8522, 3.1427, 4.5948, 1.8902], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2361, 4.6131, 4.6072, 6.2064], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2468, 4.5183, 6.1999, 8.0160], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.7211,  6.0812,  8.0501, 10.0164], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2455, -1.3765, -1.3718, -2.2562], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2434, -0.4211, -0.4235, -1.4001], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3421,  0.6497,  0.6405, -1.3609], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4387,  1.8155,  1.8163, -0.4051], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5793, 3.1219, 1.8260, 0.7237], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8368, 3.1233, 4.5848, 1.8784], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2241, 4.6055, 4.6133, 6.1906], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2347, 4.5087, 6.1990, 8.0016], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.7033, 6.0702, 8.0427, 9.9996], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2470, -1.3656, -1.3729, -2.2502], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3631, -0.4085, -0.4123, -2.2518], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4195,  0.6598,  0.6526, -1.3744], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6359,  0.6573,  1.8179, -0.4105], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6761, 1.8321, 3.1253, 0.6370], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8185, 3.1378, 4.5915, 1.8453], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2232, 4.6457, 4.6023, 6.2070], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2371, 4.5628, 6.1882, 8.0134], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.6981, 6.1299, 8.0207, 9.9983], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2557, -1.3757, -1.3809, -2.2575], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3711, -0.4156, -0.4095, -2.2641], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3633,  0.6497,  0.6481, -1.3800], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4438,  1.8188,  1.8242, -0.4217], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5580, 3.1230, 1.8127, 0.6961], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8058, 3.1265, 4.5837, 1.8136], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2293, 4.6531, 4.5979, 6.2170], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2282, 4.5728, 6.1667, 8.0025], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.7107,  6.1504,  8.0130, 10.0218], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2508, -1.3816, -1.3696, -2.2548], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2514, -0.4186, -0.4133, -1.4048], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3750,  0.6452, -0.4195, -0.4132], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4034,  1.8147,  0.6121, -0.4354], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5631, 3.1339, 1.8120, 0.7032], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7985, 3.1199, 4.5684, 1.7921], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2248, 4.6669, 4.5889, 6.2075], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2312, 4.5913, 6.1654, 7.9975], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.7054, 6.1659, 8.0037, 9.9987], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2468, -1.3682, -1.3707, -2.2396], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3739, -0.4010, -0.4183, -2.2434], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4154,  0.6506,  0.6487, -1.3694], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6369,  0.6545,  1.8149, -0.4163], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6844, 1.8313, 3.1412, 0.6382], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7958, 3.1084, 4.5739, 1.7928], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1918, 4.6328, 4.5730, 6.1821], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2222, 4.5710, 6.1759, 8.0086], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.6812,  6.1206,  8.0037, 10.0062], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2444, -1.3759, -1.3727, -2.2322], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2486, -0.4186, -0.4130, -1.3912], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3736,  0.6345, -0.4185, -0.4062], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3989,  1.8132,  0.6196, -0.4116], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5593, 3.1244, 1.8133, 0.7225], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7925, 3.0938, 4.5718, 1.7972], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1735, 4.6028, 4.5570, 6.1926], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1999, 4.5442, 6.1566, 7.9880], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.6585, 6.0966, 7.9841, 9.9823], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2430, -1.3669, -1.3725, -2.2305], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3679, -0.4033, -0.4098, -2.2371], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4140,  0.6472,  0.6451, -1.3629], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6346,  0.6483,  1.8103, -0.4107], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6777, 1.8195, 3.1360, 0.6359], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8004, 3.1125, 4.5817, 1.8109], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1578, 4.6083, 4.5507, 6.2052], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1903, 4.5504, 6.1515, 7.9964], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.6498,  6.1111,  7.9807, 10.0006], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2437, -1.3778, -1.3736, -2.2337], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2456, -0.4096, -0.4130, -1.3877], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3745,  0.6442,  0.6324, -1.3694], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4490,  1.8149,  1.8114, -0.4063], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6727, 1.8163, 3.1245, 0.6309], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7993, 3.1146, 4.5708, 1.8154], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1302, 4.5847, 4.5252, 6.1901], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1753, 4.5354, 6.1339, 7.9948], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.6359,  6.0883,  7.9619, 10.0070], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2449, -1.3853, -1.3727, -2.2371], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2469, -0.4144, -0.4206, -1.3825], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3740,  0.6337,  0.6331, -1.3752], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4504,  1.8098,  1.8131, -0.4081], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5494, 3.1326, 1.8114, 0.7204], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7953, 3.1337, 4.5728, 1.8188], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1215, 4.6122, 4.5275, 6.1875], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1728, 4.5932, 6.1420, 7.9979], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.6281,  6.1780,  7.9748, 10.0018], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2477, -1.3812, -1.3734, -2.2357], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2541, -0.4210, -0.4337, -1.3789], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3778,  0.6271,  0.6307, -1.3769], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4061,  1.8144,  0.6255, -0.4182], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5347, 3.1160, 1.8015, 0.6998], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7935, 3.1478, 4.5748, 1.8206], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1245, 4.6365, 4.5349, 6.1947], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1755, 4.6258, 6.1476, 8.0018], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.6267, 6.2168, 7.9771, 9.9979], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2388, -1.3699, -1.3637, -2.2319], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2471, -0.4220, -0.4284, -1.3777], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3710,  0.6308,  0.6341, -1.3739], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3938,  1.8257,  0.6400, -0.4117], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5375, 3.1125, 1.7992, 0.6909], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8085, 3.1528, 4.5698, 1.8206], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1318, 4.6442, 4.5353, 6.1911], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1838, 4.6384, 6.1446, 8.0011], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.6390, 6.2307, 7.9754, 9.9969], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2302, -1.3689, -1.3640, -2.2330], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2401, -0.4284, -0.4265, -1.3835], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3855,  0.6311, -0.4268, -0.4213], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3957,  1.8168,  0.6404, -0.4315], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5338, 3.0982, 1.8034, 0.6658], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8166, 3.1253, 4.5701, 1.8043], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1413, 4.6319, 4.5484, 6.1991], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1866, 4.6108, 6.1554, 8.0095], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.6438,  6.1837,  7.9923, 10.0121], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2338, -1.3724, -1.3754, -2.2369], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3758, -0.4215, -0.4376, -2.2462], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4155,  0.6271,  0.6305, -1.3783], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4392,  1.8037,  1.8340, -0.4256], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5311, 3.1012, 1.8145, 0.6634], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8107, 3.1026, 4.5741, 1.7934], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1347, 4.5853, 4.5452, 6.1958], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1771, 4.5610, 6.1532, 7.9990], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.6366,  6.1418,  7.9962, 10.0066], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2345, -1.3657, -1.3737, -2.2369], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3739, -0.4167, -0.4326, -2.2458], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4146,  0.6342,  0.6303, -1.3775], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6417,  0.6378,  1.8166, -0.4206], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6594, 1.8056, 3.1212, 0.5937], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8326, 3.1066, 4.5805, 1.8014], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1535, 4.5704, 4.5515, 6.1956], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1981, 4.5523, 6.1595, 8.0055], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.6567,  6.1232,  7.9942, 10.0020], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2258, -1.3922, -1.3799, -2.2317], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2337, -0.4354, -0.4313, -1.3807], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3816,  0.6339, -0.4305, -0.4258], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3835,  1.7993,  0.6275, -0.4436], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6055, 3.1155, 1.8354, 0.7187], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8447, 3.0951, 4.5622, 1.8092], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1642, 4.5564, 4.5298, 6.2010], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2075, 4.5554, 6.1502, 8.0114], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.6688,  6.1296,  7.9817, 10.0058], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2253, -1.3905, -1.3905, -2.2253], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3483, -0.4284, -0.4209, -2.2312], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3398,  0.6499,  0.6460, -1.3599], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4164,  1.8119,  1.7975, -0.4141], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6809, 1.8072, 3.1224, 0.6037], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8544, 3.1171, 4.5863, 1.8144], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1617, 4.5721, 4.5375, 6.1855], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1995, 4.5663, 6.1566, 7.9890], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.6602, 6.1422, 7.9932, 9.9929], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2300, -1.3870, -1.3762, -2.2318], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2382, -0.4151, -0.4202, -1.3742], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3749,  0.6414,  0.6462, -1.3732], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3647,  1.8065,  0.6477, -0.4209], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6338, 3.1162, 1.8346, 0.7344], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8664, 3.1121, 4.5743, 1.8182], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2389, 4.5698, 4.5323, 6.1963], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3197, 4.5758, 6.1512, 7.9882], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.8902,  6.1815,  8.0278, 10.0207], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2340, -1.3986, -1.3885, -2.2431], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2364, -0.4162, -0.4245, -1.3883], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3733,  0.6373,  0.6436, -1.3922], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3333,  1.8150,  0.6500, -0.4312], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6513, 3.1121, 1.8073, 0.6859], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8896, 3.1221, 4.5837, 1.7795], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.3245, 4.5935, 4.5522, 6.1919], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.4411, 4.6134, 6.2002, 8.0136], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.0022,  6.1988,  8.0544, 10.0195], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2373, -1.3921, -1.3872, -2.2353], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2429, -0.4177, -0.4214, -1.3874], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3994,  0.6368,  0.6447, -1.3957], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3467,  1.8029,  0.6460, -0.4457], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6219, 3.1041, 1.7923, 0.6577], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8088, 3.1046, 4.5632, 1.7242], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.3004, 4.6003, 4.5618, 6.1959], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3540, 4.5890, 6.1751, 7.9595], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.8871, 6.1706, 8.0337, 9.9559], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2450, -1.3875, -1.3808, -2.2324], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2546, -0.4234, -0.4162, -1.3858], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3878,  0.6460, -0.4178, -0.4283], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3710,  1.7984,  0.6551, -0.4307], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5843, 3.1045, 1.8014, 0.6682], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7563, 3.1068, 4.5857, 1.7443], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2248, 4.5818, 4.5633, 6.1781], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2616, 4.5637, 6.1635, 7.9572], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.8369,  6.1761,  8.0672, 10.0084], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2482, -1.3760, -1.3816, -2.2316], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4043, -0.4260, -0.4296, -2.2343], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4348,  0.6352,  0.6245, -1.3696], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6111,  0.6247,  1.7961, -0.4334], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6199, 1.7729, 3.0999, 0.5472], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7367, 3.1010, 4.5801, 1.7455], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1970, 4.5737, 4.5558, 6.1742], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1976, 4.5532, 6.1267, 7.9440], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.7409, 6.1526, 7.9888, 9.9710], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2531, -1.3887, -1.3798, -2.2325], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2541, -0.4253, -0.4158, -1.3691], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3931,  0.6302, -0.4205, -0.4206], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3744,  1.7982,  0.6468, -0.3921], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5706, 3.1271, 1.8002, 0.7036], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7335, 3.1500, 4.5974, 1.8042], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1665, 4.6160, 4.5381, 6.1861], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1701, 4.6163, 6.1107, 7.9870], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.6811, 6.2025, 7.9297, 9.9446], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2511, -1.3818, -1.3753, -2.2360], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2490, -0.4253, -0.4149, -1.3682], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3857,  0.6239, -0.4192, -0.4173], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3692,  1.7863,  0.6387, -0.3902], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5792, 3.1159, 1.7895, 0.7097], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7752, 3.1551, 4.5839, 1.8489], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1806, 4.6175, 4.5219, 6.2114], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1369, 4.5968, 6.0500, 7.9590], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.6865, 6.1994, 7.9010, 9.9834], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2503, -1.3736, -1.3791, -2.2430], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3843, -0.4257, -0.4204, -2.2249], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4406,  0.6224,  0.6175, -1.3773], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4488,  1.8014,  1.7917, -0.4017], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6554, 1.8100, 3.1046, 0.6431], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7855, 3.1605, 4.5806, 1.8585], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1675, 4.5964, 4.5066, 6.1893], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1395, 4.5965, 6.0485, 7.9729], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.6844, 6.1957, 7.8944, 9.9892], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2443, -1.3801, -1.3806, -2.2565], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3658, -0.4220, -0.4154, -2.2307], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3980,  0.6286,  0.6333, -1.3718], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3540,  1.8039,  0.6432, -0.3899], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5769, 3.1077, 1.7729, 0.6845], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7801, 3.1510, 4.5523, 1.8352], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1627, 4.5857, 4.4811, 6.1748], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1491, 4.6065, 6.0322, 8.0012], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.6819,  6.1985,  7.8502, 10.0042], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2500, -1.3872, -1.3906, -2.2707], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3702, -0.4287, -0.4273, -2.2569], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3925,  0.6262,  0.6229, -1.3846], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4439,  1.8007,  1.7859, -0.4257], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6670, 1.8283, 3.1168, 0.6469], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7804, 3.1571, 4.5550, 1.8313], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1603, 4.5958, 4.4927, 6.1769], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1495, 4.6182, 6.0452, 8.0135], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.6962,  6.2232,  7.8875, 10.0349], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2458, -1.3776, -1.3839, -2.2643], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3726, -0.4297, -0.4308, -2.2589], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4147,  0.6226,  0.6308, -1.3849], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4412,  1.7894,  1.7937, -0.4354], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5861, 3.1230, 1.7868, 0.6785], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7973, 3.1333, 4.5473, 1.8128], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2407, 4.5885, 4.5090, 6.2015], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2582, 4.6075, 6.0575, 7.9964], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.8854,  6.2129,  7.9146, 10.0273], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2313, -1.3716, -1.3798, -2.2506], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3554, -0.4328, -0.4377, -2.2547], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.3920,  0.6195,  0.6184, -1.3760], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6559,  0.6205,  1.7775, -0.4438], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7409, 1.8105, 3.1103, 0.6486], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.9102, 3.1332, 4.5409, 1.8300], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.3475, 4.5876, 4.4953, 6.1840], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3972, 4.6229, 6.0724, 7.9900], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([8.0235, 6.2039, 7.9066, 9.9799], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2407, -1.3763, -1.3808, -2.2473], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3713, -0.4349, -0.4282, -2.2591], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3557,  0.6126,  0.6310, -1.3868], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3117,  1.7956,  0.6387, -0.4359], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6940, 3.1185, 1.7996, 0.6943], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.9329, 3.1211, 4.5671, 1.8045], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.3822, 4.5684, 4.5321, 6.1832], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.4553, 4.6237, 6.1361, 8.0349], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.0762,  6.1967,  7.9821, 10.0194], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2576, -1.3776, -1.3802, -2.2510], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4023, -0.4311, -0.4278, -2.2661], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3928,  0.6163,  0.6265, -1.3986], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3632,  1.7829,  0.6229, -0.4714], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6437, 3.1173, 1.7997, 0.6807], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8119, 3.1004, 4.5601, 1.7509], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.3837, 4.5939, 4.5851, 6.2723], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.4123, 4.6087, 6.1657, 8.0304], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.0561,  6.2051,  8.0568, 10.0749], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2646, -1.3769, -1.3867, -2.2308], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4053, -0.4281, -0.4327, -2.2325], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4357,  0.6252,  0.6245, -1.3683], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6091,  0.6101,  1.8031, -0.4516], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6272, 1.7625, 3.0990, 0.5372], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7535, 3.0853, 4.5712, 1.7198], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2702, 4.5345, 4.5478, 6.1645], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3588, 4.5837, 6.1847, 8.0243], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.9541,  6.1579,  8.0549, 10.0555], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2447, -1.3886, -1.3840, -2.2277], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2691, -0.4356, -0.4477, -1.3827], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4471,  0.6290,  0.6035, -1.3804], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4971,  1.8084,  1.7929, -0.4272], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5678, 1.7675, 3.0877, 0.5224], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6958, 3.1129, 4.5938, 1.7509], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1680, 4.5770, 4.5276, 6.2317], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3175, 4.6213, 6.2201, 8.0929], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.8306, 6.1559, 8.0203, 9.9898], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2450, -1.3852, -1.3834, -2.2303], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2758, -0.4415, -0.4448, -1.3898], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4607,  0.6157,  0.6043, -1.3955], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.5107,  1.8113,  1.7968, -0.4226], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5526, 1.7931, 3.1068, 0.5512], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7102, 3.1136, 4.5999, 1.7681], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1389, 4.5662, 4.5160, 6.2039], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2953, 4.6014, 6.1918, 8.0550], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.9124,  6.1854,  8.0583, 10.0565], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2339, -1.3930, -1.3947, -2.2315], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3926, -0.4466, -0.4416, -2.2364], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3924,  0.6041,  0.6247, -1.3918], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3645,  1.8255,  0.6255, -0.4265], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6155, 3.1101, 1.7961, 0.6610], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8726, 3.1230, 4.5876, 1.8110], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2346, 4.5636, 4.5090, 6.2125], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3747, 4.5909, 6.1616, 8.0139], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.9955,  6.1660,  8.0187, 10.0049], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2111, -1.3789, -1.3865, -2.2294], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3687, -0.4391, -0.4330, -2.2292], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3710,  0.6140,  0.6281, -1.3804], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3643,  1.8017,  0.6126, -0.4432], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6243, 3.0930, 1.8106, 0.6611], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8863, 3.1032, 4.5887, 1.8189], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2042, 4.5375, 4.5116, 6.1922], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3563, 4.5842, 6.1946, 8.0500], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.9528, 6.1300, 8.0308, 9.9839], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2431, -1.3832, -1.3961, -2.2455], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4060, -0.4327, -0.4455, -2.2396], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4233,  0.6250,  0.6170, -1.3786], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6323,  0.6179,  1.7922, -0.4296], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6650, 1.7843, 3.0952, 0.5794], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8860, 3.1088, 4.5669, 1.8280], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2480, 4.5987, 4.5323, 6.3165], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3134, 4.5803, 6.1516, 8.0128], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.9374, 6.1478, 8.0240, 9.9529], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2607, -1.3939, -1.4211, -2.2453], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4012, -0.4441, -0.4370, -2.2433], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4155,  0.6306,  0.6160, -1.3963], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4191,  1.7825,  1.7930, -0.4153], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6488, 3.1305, 1.8362, 0.6975], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8852, 3.0897, 4.5484, 1.8088], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2536, 4.5653, 4.5481, 6.2557], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3343, 4.5740, 6.1465, 8.0121], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.0078,  6.1660,  8.0221, 10.0400], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2385, -1.4027, -1.3856, -2.2565], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2530, -0.4425, -0.4345, -1.4016], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4018,  0.6101, -0.4459, -0.4497], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3935,  1.7626,  0.5857, -0.4903], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6645, 3.1362, 1.8337, 0.7087], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.9061, 3.0965, 4.5424, 1.8282], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2764, 4.5765, 4.5251, 6.2785], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3379, 4.5688, 6.1044, 7.9950], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.0127,  6.1567,  7.9664, 10.0257], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2385, -1.3854, -1.4123, -2.2471], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3931, -0.4310, -0.4555, -2.2393], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4196,  0.6305,  0.6045, -1.3747], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6343,  0.6106,  1.7752, -0.4428], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6931, 1.7894, 3.1267, 0.6034], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.9069, 3.1121, 4.5769, 1.8302], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2347, 4.5513, 4.5221, 6.1966], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3360, 4.5738, 6.1354, 8.0007], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.9933,  6.1495,  7.9951, 10.0251], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2353, -1.3939, -1.3850, -2.2525], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2649, -0.4349, -0.4463, -1.3971], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4329,  0.6284,  0.6210, -1.4071], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4538,  1.7783,  1.7893, -0.4319], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6214, 3.1256, 1.8255, 0.6753], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8609, 3.0992, 4.5530, 1.8047], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2347, 4.5586, 4.5147, 6.2253], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3045, 4.5518, 6.1003, 7.9705], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.9571, 6.1279, 7.9490, 9.9962], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2477, -1.3952, -1.4035, -2.2504], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4162, -0.4417, -0.4486, -2.2479], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4527,  0.6311,  0.6120, -1.3822], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6020,  0.6198,  1.7849, -0.4281], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6263, 1.7947, 3.1369, 0.5837], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8195, 3.1136, 4.5678, 1.8017], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2085, 4.5675, 4.5453, 6.1985], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2731, 4.5598, 6.1397, 7.9274], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.9147, 6.1419, 8.0126, 9.9176], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2459, -1.3972, -1.3861, -2.2541], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2778, -0.4390, -0.4384, -1.4029], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4254,  0.6243, -0.4351, -0.4539], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4022,  1.8164,  0.6398, -0.4425], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5899, 3.1205, 1.8467, 0.6725], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8076, 3.1377, 4.5905, 1.8198], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1511, 4.5361, 4.5117, 6.1085], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2929, 4.5952, 6.1593, 7.9771], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.9027, 6.1580, 7.9820, 9.9438], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2455, -1.3941, -1.4100, -2.2493], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4060, -0.4459, -0.4512, -2.2492], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4454,  0.6231,  0.6189, -1.3810], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6151,  0.6370,  1.8248, -0.4039], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6138, 1.8021, 3.1232, 0.5963], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8262, 3.1634, 4.6213, 1.8542], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1980, 4.5785, 4.5603, 6.1946], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2783, 4.5908, 6.1580, 7.9600], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.9048, 6.1612, 8.0046, 9.9355], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2407, -1.3992, -1.3887, -2.2466], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2548, -0.4403, -0.4411, -1.3896], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3632,  0.6009,  0.6320, -1.3870], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3900,  1.8239,  0.6371, -0.4279], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5714, 3.1034, 1.8273, 0.6556], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7816, 3.1197, 4.5910, 1.8076], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1568, 4.5876, 4.5439, 6.1841], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2270, 4.5961, 6.1521, 7.9923], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.8545,  6.1830,  8.0204, 10.0155], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2553, -1.3911, -1.3983, -2.2526], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4161, -0.4390, -0.4440, -2.2430], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4611,  0.6179,  0.6158, -1.3817], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.5843,  0.6259,  1.8343, -0.4200], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5733, 1.7886, 3.1199, 0.5908], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7376, 3.1267, 4.6143, 1.8098], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.0889, 4.6079, 4.5421, 6.1694], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1318, 4.6026, 6.1481, 7.9764], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.7650, 6.1955, 8.0221, 9.9985], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2653, -1.4073, -1.4111, -2.2606], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4244, -0.4323, -0.4557, -2.2342], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4673,  0.6215,  0.6031, -1.3775], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.5809,  0.6257,  1.8252, -0.4164], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5602, 1.7856, 3.1021, 0.5828], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7135, 3.1145, 4.5901, 1.7915], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.0781, 4.6153, 4.5392, 6.1838], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1143, 4.6080, 6.1497, 8.0035], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.7358,  6.1861,  8.0128, 10.0170], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2616, -1.4050, -1.3987, -2.2652], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2709, -0.4371, -0.4330, -1.3963], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4295,  0.6192, -0.4423, -0.4439], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4450,  1.8377,  0.6221, -0.4439], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4950, 3.0985, 1.7888, 0.6108], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7217, 3.1212, 4.5770, 1.7970], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.0936, 4.6387, 4.5290, 6.2174], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1109, 4.6222, 6.1227, 7.9978], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.7158, 6.1932, 7.9526, 9.9712], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2513, -1.3743, -1.3927, -2.2570], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4041, -0.4255, -0.4372, -2.2405], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4538,  0.6284,  0.6099, -1.3900], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.5908,  0.6268,  1.7998, -0.4186], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6139, 1.8363, 3.1417, 0.6465], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7129, 3.1116, 4.5300, 1.7654], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1097, 4.6643, 4.5239, 6.2292], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1218, 4.6541, 6.1174, 8.0012], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.7267, 6.2345, 7.9447, 9.9752], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2436, -1.3909, -1.3841, -2.2669], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2649, -0.4490, -0.4399, -1.4034], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4163,  0.6346, -0.4392, -0.4363], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4608,  1.7767,  0.5874, -0.5107], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5421, 3.1572, 1.8013, 0.6347], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7477, 3.1215, 4.5207, 1.7688], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1085, 4.6545, 4.4990, 6.1869], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1424, 4.6686, 6.1017, 8.0037], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.7727,  6.2643,  7.9384, 10.0010], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2471, -1.3996, -1.3955, -2.2809], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2673, -0.4586, -0.4525, -1.4141], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4143,  0.6322, -0.4478, -0.4424], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4627,  1.7385,  0.5656, -0.5455], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5568, 3.1329, 1.7873, 0.6246], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8078, 3.1066, 4.5357, 1.8100], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1397, 4.5645, 4.4989, 6.2237], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1645, 4.5697, 6.1009, 8.0235], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.7807, 6.1111, 7.9218, 9.9928], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2330, -1.3754, -1.3818, -2.2656], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3796, -0.4336, -0.4581, -2.2553], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4107,  0.6573,  0.6415, -1.4097], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6572,  0.6299,  1.8123, -0.4291], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6753, 1.8304, 3.1387, 0.6597], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8299, 3.1041, 4.5317, 1.8218], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1504, 4.5096, 4.4854, 6.2273], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1776, 4.5263, 6.0903, 8.0265], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.7897, 6.0547, 7.9030, 9.9936], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2434, -1.4009, -1.3967, -2.2654], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2476, -0.4470, -0.4367, -1.3925], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4085,  0.6469, -0.4381, -0.4368], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4051,  1.8149,  0.6337, -0.4744], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5712, 3.1479, 1.7979, 0.6313], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8134, 3.0881, 4.5247, 1.7829], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1550, 4.4847, 4.4904, 6.2158], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1892, 4.5084, 6.0978, 8.0263], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.8145,  6.0404,  7.9455, 10.0220], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2444, -1.3886, -1.3785, -2.2619], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2520, -0.4457, -0.4211, -1.3926], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4125,  0.6427, -0.4226, -0.4416], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3958,  1.8049,  0.6773, -0.4693], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5495, 3.0993, 1.8071, 0.6005], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8333, 3.0999, 4.5945, 1.8171], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1705, 4.4536, 4.5263, 6.2433], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1918, 4.4656, 6.1213, 8.0373], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.8022,  5.9715,  7.9365, 10.0161], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2590, -1.3997, -1.4133, -2.2585], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3887, -0.4150, -0.4299, -2.2476], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4170,  0.6611,  0.6339, -1.3800], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6551,  0.6425,  1.8299, -0.4130], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5906, 1.7438, 3.0394, 0.5355], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8545, 3.1142, 4.6164, 1.8472], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1706, 4.4425, 4.5336, 6.2386], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1952, 4.4606, 6.1395, 8.0263], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.8142,  5.9793,  7.9712, 10.0076], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2496, -1.4087, -1.4026, -2.2587], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2316, -0.4020, -0.4120, -1.3678], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3955,  0.6279,  0.6351, -1.4215], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3535,  1.8570,  0.7031, -0.4154], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5574, 3.0828, 1.8062, 0.5921], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8474, 3.1364, 4.5798, 1.8241], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1764, 4.4717, 4.5282, 6.2221], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2037, 4.4977, 6.1139, 8.0260], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.8169,  6.0184,  7.9300, 10.0073], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2358, -1.4141, -1.3694, -2.2614], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2238, -0.4058, -0.4042, -1.3643], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3871,  0.6392, -0.4182, -0.4319], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3333,  1.8426,  0.7096, -0.4101], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6122, 3.0822, 1.8437, 0.6240], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8962, 3.1271, 4.5827, 1.8321], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2485, 4.4507, 4.5394, 6.2078], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3173, 4.4893, 6.1378, 8.0026], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.0062,  6.0278,  7.9930, 10.0115], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2443, -1.4129, -1.4093, -2.2449], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2237, -0.4174, -0.4231, -1.3658], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3796,  0.6191,  0.6192, -1.3896], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3541,  1.8257,  0.6591, -0.4448], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6292, 3.1120, 1.8463, 0.6269], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.9021, 3.2050, 4.6013, 1.8337], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2968, 4.6257, 4.5908, 6.2320], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3817, 4.7185, 6.1957, 8.0269], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.0734,  6.3073,  8.0387, 10.0021], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2576, -1.3963, -1.4100, -2.2563], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3904, -0.4062, -0.4209, -2.2528], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4083,  0.6337,  0.6186, -1.3764], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6780,  0.6593,  1.8454, -0.4329], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6675, 1.8560, 3.0903, 0.6003], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8751, 3.2133, 4.6159, 1.7975], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.3172, 4.6930, 4.6577, 6.2176], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3893, 4.7896, 6.2366, 7.9941], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([8.0972, 6.3606, 8.0986, 9.9872], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2530, -1.3951, -1.3713, -2.2585], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2661, -0.4358, -0.4090, -1.4092], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3903,  0.6136, -0.4141, -0.4384], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3923,  1.7685,  0.6391, -0.4937], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5983, 3.0523, 1.8329, 0.5775], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8498, 3.1305, 4.5600, 1.7633], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2888, 4.5699, 4.5774, 6.1636], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3808, 4.7146, 6.1768, 7.9734], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.1184,  6.3031,  8.0451, 10.0212], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2622, -1.3416, -1.4130, -2.2355], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3838,  0.6760,  0.6302, -1.3498], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4503,  1.7708,  1.7291, -0.4427], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6832, 1.8646, 3.1178, 0.6572], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7998, 3.1285, 4.5448, 1.7667], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2270, 4.5547, 4.5685, 6.2328], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2048, 4.6404, 6.1588, 7.9757], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.8985,  6.1744,  8.0417, 10.0181], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2518, -1.3467, -1.3603, -2.2428], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4035, -0.4429, -0.4099, -2.2591], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4005,  0.6962,  0.6655, -1.3333], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4846,  1.7308,  1.7626, -0.4525], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5485, 3.1288, 1.8271, 0.6408], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7596, 3.0989, 4.6262, 1.7970], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1172, 4.4722, 4.5639, 6.1796], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1011, 4.5941, 6.1694, 8.0096], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "randomly selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2746, -1.3772, -1.4122, -2.2577], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4378, -0.4247, -0.4841, -2.2501], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4447,  0.6575,  0.5934, -1.3644], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.5897,  0.6153,  1.7256, -0.4358], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6122, 1.8354, 3.1004, 0.6395], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7367, 3.1319, 4.5883, 1.7977], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1362, 4.5832, 4.6032, 6.2317], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0465, 4.6271, 6.1454, 7.9648], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.7223, 6.1470, 8.0053, 9.9595], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2601, -1.3739, -1.3663, -2.2737], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2762, -0.4396, -0.4088, -1.3988], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4113,  0.6252, -0.4112, -0.4322], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4292,  1.7769,  0.6366, -0.4778], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5325, 3.1010, 1.8197, 0.6125], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7773, 3.1125, 4.6241, 1.8130], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1894, 4.5778, 4.6720, 6.1908], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1313, 4.6340, 6.1626, 7.9704], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.8964,  6.1844,  8.0383, 10.0193], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2452, -1.3615, -1.3861, -2.2711], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3771, -0.4236, -0.4547, -2.2667], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.3947,  0.6573,  0.6159, -1.3735], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6587,  0.6353,  1.7477, -0.4357], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7129, 1.8534, 3.1283, 0.6635], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8518, 3.1381, 4.5492, 1.7684], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2954, 4.6445, 4.6663, 6.1726], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3124, 4.7307, 6.2008, 8.0273], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([8.0381, 6.2381, 8.0114, 9.9670], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2284, -1.3668, -1.3966, -2.2510], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3359, -0.4371, -0.4200, -2.2616], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3711,  0.6006,  0.6235, -1.3960], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3293,  1.7956,  0.6483, -0.4260], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6475, 3.1140, 1.8108, 0.6254], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8698, 3.0733, 4.5319, 1.7516], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.3562, 4.5901, 4.6963, 6.1944], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3547, 4.6713, 6.2006, 8.0368], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.1103,  6.1820,  8.0316, 10.0102], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2300, -1.4014, -1.3741, -2.2577], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2535, -0.4467, -0.4431, -1.3812], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3761,  0.6097, -0.4349, -0.4327], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3369,  1.7973,  0.6362, -0.4333], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6532, 3.1457, 1.8055, 0.6272], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8947, 3.1311, 4.5617, 1.7718], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.3464, 4.6444, 4.6802, 6.1686], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3601, 4.7225, 6.1986, 8.0055], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.1412,  6.2620,  8.0495, 10.0008], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2620, -1.3806, -1.4102, -2.2582], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4239, -0.4198, -0.4775, -2.2601], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.3994,  0.6149,  0.5937, -1.3778], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6760,  0.6241,  1.7738, -0.4290], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7250, 1.8254, 3.1182, 0.6515], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8876, 3.1292, 4.5554, 1.7808], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.3538, 4.6409, 4.6768, 6.1938], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3636, 4.7047, 6.2103, 8.0163], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([8.1221, 6.2117, 8.0461, 9.9799], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2427, -1.4142, -1.3634, -2.2429], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2667, -0.4568, -0.4116, -1.3865], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4012,  0.5960, -0.4346, -0.4480], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3592,  1.7851,  0.6246, -0.4383], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6210, 3.1303, 1.7811, 0.6232], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8712, 3.1189, 4.5658, 1.7722], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.3274, 4.6261, 4.6679, 6.1527], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3470, 4.7075, 6.1830, 8.0089], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([8.0963, 6.2245, 7.9941, 9.9817], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2464, -1.3812, -1.3909, -2.2229], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4254, -0.4129, -0.4477, -2.2290], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4349,  0.5960,  0.5418, -1.3502], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6408,  0.6019,  1.7696, -0.4187], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6557, 1.7978, 3.1188, 0.6276], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7898, 3.0859, 4.5517, 1.7611], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.3050, 4.5718, 4.6680, 6.2204], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2862, 4.5579, 6.1809, 8.0179], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.0356,  6.0439,  8.0412, 10.0090], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2434, -1.3943, -1.3922, -2.2223], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2769, -0.4735, -0.4463, -1.3877], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4282,  0.6006, -0.4475, -0.4447], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4839,  1.7604,  0.5546, -0.5041], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4955, 3.0559, 1.7118, 0.5727], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7151, 3.0334, 4.5392, 1.7324], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2042, 4.4882, 4.6118, 6.2133], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2208, 4.4842, 6.1520, 8.0615], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.9064,  5.9461,  7.9669, 10.0071], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2515, -1.3459, -1.3816, -2.2144], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4690, -0.4442, -0.4813, -2.2403], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4741,  0.6007,  0.5829, -1.3727], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.5969,  0.5810,  1.8127, -0.4356], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4834, 1.6977, 3.0311, 0.4393], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7099, 3.0505, 4.5877, 1.7530], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1000, 4.4833, 4.5809, 6.2271], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1512, 4.4622, 6.1140, 8.0532], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.8380,  5.9583,  7.9136, 10.0245], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2565, -1.3261, -1.4078, -2.1980], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4577, -0.4718, -0.4911, -2.2349], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4662,  0.5878,  0.5778, -1.3574], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6029,  0.5771,  1.8000, -0.4173], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5516, 1.7538, 3.0816, 0.5619], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7207, 3.0828, 4.5712, 1.7757], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.0830, 4.5864, 4.5984, 6.2106], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1405, 4.5880, 6.1334, 8.0185], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.8569,  6.0985,  7.9603, 10.0365], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2238, -1.4526, -1.4049, -2.2260], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2191, -0.5231, -0.4344, -1.3863], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4145,  0.5886, -0.4427, -0.4305], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4516,  1.8002,  0.5953, -0.4349], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4797, 3.0411, 1.6996, 0.5697], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7985, 3.1042, 4.5408, 1.8150], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.0864, 4.6163, 4.5685, 6.1610], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1986, 4.6701, 6.1682, 8.0557], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.9247,  6.1918,  8.0079, 10.0585], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1957, -1.3734, -1.3996, -2.2185], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3861, -0.4675, -0.5039, -2.2232], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4044,  0.5897,  0.5625, -1.3482], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6566,  0.5978,  1.7711, -0.4177], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6450, 1.7861, 3.0451, 0.6156], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8849, 3.1486, 4.5878, 1.8617], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1794, 4.6561, 4.6754, 6.2718], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2551, 4.6714, 6.2585, 8.1152], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.9697,  6.1474,  8.0891, 10.0850], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1785, -1.3598, -1.3926, -2.2263], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3512, -0.4766, -0.4926, -2.2217], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.3775,  0.5604,  0.5877, -1.3528], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4078,  1.7658,  1.8253, -0.4362], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6529, 3.0717, 1.8629, 0.6534], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.9633, 3.1894, 4.7123, 1.9245], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2863, 4.7146, 4.8168, 6.3970], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3406, 4.8121, 6.3972, 8.1637], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.0294,  6.3490,  8.2323, 10.0151], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2225, -1.4836, -1.4444, -2.2786], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2049, -0.5040, -0.4596, -1.4232], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3825,  0.5999, -0.4555, -0.4461], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3901,  1.7871,  0.5880, -0.4562], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6897, 3.1488, 1.8568, 0.6978], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.9815, 3.2420, 4.6537, 1.9250], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2847, 4.7806, 4.7439, 6.3597], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3653, 4.9008, 6.3344, 8.1239], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.0631,  6.4641,  8.1619, 10.0209], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2330, -1.4049, -1.3663, -2.2795], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2487, -0.4687, -0.4465, -1.4360], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4071,  0.6167, -0.4492, -0.4639], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4510,  1.7551,  0.5592, -0.5371], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6548, 3.1623, 1.8708, 0.6549], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.9907, 3.2775, 4.7574, 1.9301], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2655, 4.7965, 4.7893, 6.3554], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3749, 4.9346, 6.4032, 8.1530], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.0388,  6.4808,  8.2081, 10.0113], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3154, -1.4233, -1.4332, -2.2576], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4649, -0.4733, -0.4835, -2.2445], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5052,  0.6010,  0.5628, -1.3780], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.5527,  0.5805,  1.6953, -0.4480], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5738, 1.8225, 3.1319, 0.5790], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8135, 3.2283, 4.7008, 1.8504], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2100, 4.8150, 4.7848, 6.3885], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2668, 4.9088, 6.3942, 8.0793], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.9468, 6.4896, 8.2697, 9.9917], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3110, -1.4297, -1.4196, -2.1989], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.3199, -0.4594, -0.4458, -1.4121], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4684,  0.6160, -0.4288, -0.4529], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.5106,  1.7878,  0.5902, -0.4534], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4917, 3.1463, 1.8265, 0.6118], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7194, 3.2062, 4.6929, 1.7813], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1493, 4.7861, 4.7539, 6.2991], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2343, 4.9101, 6.3553, 8.0833], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.9132,  6.4965,  8.2057, 10.0324], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2647, -1.4550, -1.4027, -2.1799], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2912, -0.4646, -0.4520, -1.3950], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4498,  0.6165, -0.4424, -0.4419], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.5034,  1.7796,  0.5506, -0.4485], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.5624,  1.7411,  1.7362, -0.4905], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5065, 1.7959, 3.1219, 0.5003], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7587, 3.2395, 4.7265, 1.7806], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1551, 4.7906, 4.7470, 6.2261], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2334, 4.8994, 6.3400, 7.9764], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.9679, 6.5207, 8.2640, 9.9981], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2429, -1.4144, -1.4054, -2.1975], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2893, -0.4449, -0.4401, -1.4244], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4512,  0.6270, -0.4391, -0.4658], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4767,  1.8037,  0.5899, -0.4463], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4878, 3.1209, 1.7456, 0.5637], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7874, 3.2123, 4.6552, 1.7914], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2031, 4.7817, 4.6515, 6.3164], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2148, 4.8307, 6.1595, 7.9631], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.9821, 6.4763, 8.0738, 9.9958], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2523, -1.3687, -1.4109, -2.2097], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3232, -0.4736, -0.4516, -2.2947], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4563,  0.5176,  0.6221, -1.4902], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3988,  1.8542,  0.6730, -0.3781], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5541, 3.1508, 1.8497, 0.6047], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8626, 3.2559, 4.8072, 1.8298], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2593, 4.8150, 4.7812, 6.3447], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3185, 4.9121, 6.3263, 8.0670], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.9947, 6.4814, 8.1574, 9.9952], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3082, -1.3888, -1.4260, -2.2259], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3461, -0.4858, -0.4666, -2.3077], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4491,  0.5220,  0.6128, -1.4776], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4779,  1.7914,  0.5287, -0.4734], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5206, 3.1058, 1.7326, 0.5690], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8075, 3.1670, 4.6077, 1.7751], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1849, 4.7311, 4.5950, 6.2545], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2620, 4.8459, 6.1410, 8.0173], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.0292,  6.4863,  8.0365, 10.0762], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3314, -1.3889, -1.4060, -2.2430], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3501, -0.4807, -0.4529, -2.3104], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4438,  0.5665,  0.6268, -1.4461], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4865,  1.8273,  0.5353, -0.4645], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5367, 3.1479, 1.7741, 0.6046], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8659, 3.2512, 4.7161, 1.8641], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2088, 4.7970, 4.6385, 6.3370], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2366, 4.8552, 6.1296, 7.9838], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.9516, 6.4534, 7.9657, 9.9815], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3291, -1.3894, -1.3711, -2.2588], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.3099, -0.4525, -0.4433, -1.4302], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4475,  0.6069, -0.4619, -0.4546], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4856,  1.8247,  0.5188, -0.4619], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5504, 3.1315, 1.7803, 0.6149], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8895, 3.2525, 4.7161, 1.9108], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1623, 4.7495, 4.5954, 6.2570], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2585, 4.8543, 6.1327, 8.0001], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.9739, 6.4538, 7.9630, 9.9965], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3582, -1.3949, -1.4345, -2.2779], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3544, -0.4816, -0.4807, -2.3082], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4468,  0.5646,  0.5856, -1.4357], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4590,  1.8627,  0.5282, -0.4424], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6027, 3.2143, 1.8253, 0.6600], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8806, 3.3015, 4.6681, 1.8799], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1098, 4.7678, 4.5473, 6.1506], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2843, 4.9104, 6.1281, 8.0179], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.0019,  6.5097,  7.9605, 10.0132], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3112, -1.4352, -1.3687, -2.3155], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2792, -0.4429, -0.4098, -1.4269], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4034,  0.6514, -0.4076, -0.4342], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4112,  1.8662,  0.6345, -0.4306], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6939, 3.2659, 1.9567, 0.7567], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8073, 3.1736, 4.5641, 1.7834], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1642, 4.6300, 4.5633, 6.2196], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2494, 4.7693, 6.0527, 7.9495], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.9499, 6.3292, 7.8393, 9.9621], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2778, -1.4302, -1.4330, -2.2374], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3491, -0.4954, -0.5271, -2.3025], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4689,  0.5966,  0.5731, -1.3963], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6371,  0.5670,  1.7867, -0.4827], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6776, 1.8939, 3.2482, 0.7206], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7907, 3.1712, 4.6051, 1.8041], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.0801, 4.5252, 4.5187, 6.1228], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2212, 4.7244, 6.0745, 7.9562], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.9832,  6.3492,  7.9684, 10.0536], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2515, -1.4228, -1.3953, -2.2141], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2686, -0.4879, -0.4135, -1.4115], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4340,  0.6335, -0.4244, -0.4383], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4917,  1.8097,  0.6014, -0.4899], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5907, 3.2479, 1.8869, 0.6900], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7914, 3.2236, 4.6336, 1.8071], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1204, 4.6379, 4.5374, 6.2094], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2540, 4.8113, 6.0829, 8.0060], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.9592, 6.3897, 7.8778, 9.9992], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2287, -1.4225, -1.3672, -2.2247], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2545, -0.5182, -0.4193, -1.4300], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4290,  0.6329, -0.4479, -0.4547], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4533,  1.8224,  0.6039, -0.4959], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6490, 3.2647, 1.8943, 0.6863], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8413, 3.2531, 4.6576, 1.8014], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2012, 4.6647, 4.5926, 6.1515], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3423, 4.8632, 6.1553, 8.0250], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.0558,  6.4502,  7.9748, 10.0069], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2566, -1.4142, -1.4090, -2.2408], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2817, -0.5184, -0.4546, -1.4410], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4471,  0.6464, -0.4602, -0.4666], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4518,  0.6445, -0.4704, -0.4696], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4991,  1.8075,  0.5476, -0.5285], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5828, 3.1942, 1.7776, 0.6211], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8147, 3.2067, 4.5662, 1.7787], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2660, 4.6782, 4.5539, 6.2082], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3392, 4.8291, 6.0610, 7.9947], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([8.0724, 6.4295, 7.8885, 9.9881], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2811, -1.3905, -1.4018, -2.2591], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3561, -0.3828, -0.4954, -2.2334], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5063,  0.6259,  0.6886, -1.4869], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4712,  1.8538,  1.9515, -0.5151], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5452, 3.1370, 1.8033, 0.5455], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8203, 3.2233, 4.6298, 1.7917], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2753, 4.6098, 4.6050, 6.1920], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3675, 4.7946, 6.0891, 8.0509], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([8.0721, 6.3494, 7.8663, 9.9954], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3246, -1.4906, -1.4249, -2.2904], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2937, -0.4187, -0.4146, -1.4134], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4687,  0.6741, -0.4541, -0.4655], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4284,  1.9375,  0.6758, -0.4122], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5512, 3.1408, 1.7741, 0.5885], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8748, 3.2644, 4.6400, 1.9092], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2976, 4.6120, 4.5918, 6.2051], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3434, 4.7632, 6.0593, 7.9538], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([8.0901, 6.3395, 7.8830, 9.9353], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3663, -1.4289, -1.4698, -2.2683], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4333, -0.3939, -0.5065, -2.2199], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4844,  0.5603,  0.6491, -1.4298], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4110,  1.9027,  1.9356, -0.3867], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6696, 3.1641, 1.8054, 0.7056], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.9400, 3.2590, 4.6051, 1.9760], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.3030, 4.5609, 4.5456, 6.1559], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3548, 4.7412, 6.0229, 7.9414], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([8.0794, 6.2979, 7.8077, 9.9417], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3494, -1.4649, -1.4399, -2.2686], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2631, -0.4337, -0.3865, -1.3810], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4266,  0.6304, -0.4375, -0.4242], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4083,  1.9062,  0.6791, -0.4005], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6440, 3.1748, 1.8643, 0.6881], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8153, 3.2631, 4.6130, 1.8990], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1713, 4.5702, 4.5629, 6.0974], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1553, 4.7298, 6.0093, 7.8881], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.9217, 6.3556, 7.8900, 9.9797], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3539, -1.4655, -1.4236, -2.2808], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2857, -0.4247, -0.3933, -1.3879], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4437,  0.6369, -0.4259, -0.4305], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4587,  1.8919,  0.6713, -0.4300], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5898, 3.1345, 1.8637, 0.6728], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7113, 3.1800, 4.5748, 1.8484], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.0988, 4.5258, 4.5579, 6.1165], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0982, 4.6884, 6.0364, 7.9662], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.7773, 6.2146, 7.8148, 9.9297], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3600, -1.3771, -1.4226, -2.2666], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4414, -0.3582, -0.4377, -2.2504], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5113,  0.6421,  0.6079, -1.3945], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6007,  0.6384,  1.8190, -0.3945], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5647, 1.8294, 3.0853, 0.6561], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7421, 3.1316, 4.4648, 1.8977], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1057, 4.5200, 4.4716, 6.1298], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0475, 4.6028, 5.9111, 7.8699], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.8399, 6.1829, 7.7902, 9.9726], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3445, -1.2592, -1.3760, -2.2873], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4102, -0.3537, -0.4198, -2.2782], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4741,  0.6633,  0.6397, -1.3965], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6523,  0.6397,  1.8401, -0.3946], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6732, 1.8181, 3.1367, 0.7260], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8939, 3.1227, 4.5062, 1.9746], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1612, 4.4603, 4.4822, 6.0970], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0994, 4.5846, 5.8907, 7.8636], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.9081, 6.1813, 7.7444, 9.9850], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3332, -1.3885, -1.3667, -2.3568], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2791, -0.4670, -0.4346, -1.4410], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3997,  0.6698, -0.4204, -0.4258], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4284,  1.8172,  0.5953, -0.4707], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6823, 3.1258, 1.8395, 0.6989], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8826, 3.1472, 4.4864, 1.9483], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1631, 4.4925, 4.4916, 6.1164], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1037, 4.6185, 5.9176, 7.9117], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.8436, 6.1701, 7.7264, 9.9494], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3345, -1.3739, -1.3917, -2.3110], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4082, -0.4327, -0.4574, -2.2928], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4759,  0.7130,  0.6813, -1.4018], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6485,  0.6521,  1.8278, -0.3705], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6387, 1.8205, 3.1213, 0.7065], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8011, 3.1095, 4.5154, 1.9286], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.0878, 4.4396, 4.4945, 6.0706], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0489, 4.5706, 5.9335, 7.8930], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.8312,  6.1535,  7.8024, 10.0084], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3625, -1.3694, -1.3879, -2.2888], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4516, -0.4864, -0.4434, -2.3542], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4563,  0.6878,  0.6863, -1.4032], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.5159,  1.8005,  1.8129, -0.4411], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6019, 3.0979, 1.8263, 0.6790], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7551, 3.1066, 4.4952, 1.8858], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1254, 4.5075, 4.5200, 6.1750], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0654, 4.6283, 5.9438, 7.9514], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.8144,  6.2060,  7.7755, 10.0134], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3654, -1.3315, -1.3733, -2.2643], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4640, -0.4519, -0.4392, -2.3639], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4585,  0.7001,  0.6988, -1.4238], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.5113,  1.8139,  1.8270, -0.4618], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6068, 3.0955, 1.8400, 0.6677], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7307, 3.0674, 4.4898, 1.8330], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1055, 4.4406, 4.5132, 6.1441], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0856, 4.5870, 5.9655, 7.9920], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.8049, 6.1135, 7.7665, 9.9944], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3698, -1.4209, -1.3700, -2.3302], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.3305, -0.4294, -0.3893, -1.4166], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4423,  0.6630, -0.4033, -0.4422], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4596,  1.8293,  0.6342, -0.4781], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5886, 3.0524, 1.8201, 0.6272], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7511, 3.1014, 4.5291, 1.8405], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1304, 4.4836, 4.5311, 6.1622], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1129, 4.6416, 6.0346, 7.9959], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.8239, 6.1831, 7.8809, 9.9794], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3548, -1.3499, -1.4152, -2.2647], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4016, -0.3546, -0.3785, -2.3170], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4678,  0.7474,  0.7039, -1.4140], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6711,  0.7257,  1.8174, -0.3365], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5698, 1.7107, 2.9931, 0.5858], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7349, 3.0137, 4.4818, 1.8210], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1245, 4.3775, 4.5837, 6.1412], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1019, 4.5179, 6.1301, 7.9619], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.8517, 6.0435, 8.0412, 9.9829], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3178, -1.4338, -1.4448, -2.1918], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4250,  0.6793,  0.6990, -1.3535], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4320,  1.7444,  1.8428, -0.3581], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6119, 2.9320, 1.8253, 0.6492], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7215, 2.9329, 4.4924, 1.7814], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1298, 4.3426, 4.6477, 6.1056], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1253, 4.5193, 6.2147, 7.9561], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.8568, 6.0490, 8.0946, 9.9574], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2708, -1.3872, -1.3532, -2.2154], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2472, -0.3798, -0.3456, -1.3820], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3200,  0.6521, -0.3805, -0.3497], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3614,  1.8507,  0.6223, -0.3938], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6673, 3.0542, 1.8547, 0.6956], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7539, 3.0405, 4.5068, 1.8009], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1538, 4.4493, 4.6800, 6.1176], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1473, 4.6228, 6.2715, 7.9652], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.9059, 6.1842, 8.1998, 9.9967], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2685, -1.4366, -1.4025, -2.2376], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2632, -0.4101, -0.3877, -1.4261], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3191,  0.6263, -0.3922, -0.3580], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3619,  1.8099,  0.5995, -0.4040], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6915, 3.0428, 1.8563, 0.7258], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7701, 3.0145, 4.4977, 1.8243], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1651, 4.4277, 4.6602, 6.1322], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1434, 4.6075, 6.2475, 7.9660], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.9053,  6.1718,  8.1761, 10.0140], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2468, -1.3308, -1.3742, -2.2105], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3930,  0.6329,  0.5740, -1.3487], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4399,  1.8059,  1.7480, -0.3448], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6420, 1.7659, 3.0376, 0.6919], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7374, 3.0012, 4.4970, 1.8301], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1456, 4.3943, 4.6520, 6.1583], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.8488, 6.0506, 8.1778, 9.9924], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2507, -1.3343, -1.3624, -2.2072], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4095, -0.4742, -0.4478, -2.3409], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4071,  0.5785,  0.5816, -1.3622], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4180,  1.7343,  0.5536, -0.4442], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6468, 3.0079, 1.8472, 0.7242], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7097, 2.9652, 4.5313, 1.8118], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1230, 4.3449, 4.6653, 6.1572], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1187, 4.4967, 6.2730, 8.0224], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.8380,  6.0244,  8.1780, 10.0249], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2320, -1.3523, -1.3516, -2.2252], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2755, -0.4487, -0.4671, -1.4313], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3892,  0.6297,  0.6191, -1.3753], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.5272,  1.7366,  1.6800, -0.4529], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6029, 1.7580, 3.0415, 0.6465], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6918, 3.0218, 4.5238, 1.7955], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.0905, 4.3818, 4.6338, 6.1700], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1050, 4.5183, 6.2488, 8.0126], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.8166,  6.0518,  8.1457, 10.0065], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2205, -1.4325, -1.3632, -2.2352], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2149, -0.4452, -0.4314, -1.3898], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3507,  0.6045, -0.4065, -0.3981], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4710,  1.7434,  0.5343, -0.5208], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6254, 3.0448, 1.8136, 0.6807], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7282, 3.0413, 4.5403, 1.8060], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1176, 4.3784, 4.6470, 6.1759], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1218, 4.5204, 6.2265, 7.9977], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.8324, 6.0593, 8.1098, 9.9845], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1897, -1.3819, -1.3311, -2.2298], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2013, -0.4135, -0.4376, -1.3751], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3422,  0.6244,  0.6283, -1.3702], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4333,  1.7594,  0.5560, -0.5031], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6100, 3.0346, 1.7684, 0.6263], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7237, 3.0403, 4.5124, 1.7819], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1115, 4.3895, 4.6336, 6.1598], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1164, 4.5351, 6.2293, 7.9919], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.8306, 6.0849, 8.1359, 9.9952], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2163, -1.3692, -1.3475, -2.2340], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2459, -0.4446, -0.4644, -1.3829], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3611,  0.6052,  0.6162, -1.3727], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4334,  1.7554,  0.5658, -0.4979], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6112, 3.0711, 1.7699, 0.6432], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7400, 3.1119, 4.5502, 1.8310], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1439, 4.5270, 4.6342, 6.2411], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1068, 4.6700, 6.1589, 7.9851], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.8141, 6.2845, 8.0205, 9.9712], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2354, -1.3974, -1.4082, -2.2107], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3849, -0.4239, -0.5297, -2.2449], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4180,  0.5926,  0.5823, -1.3852], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6391,  0.6377,  1.6775, -0.3992], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6611, 1.8196, 3.0724, 0.7055], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7627, 3.1546, 4.5620, 1.8655], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1149, 4.5893, 4.5460, 6.1946], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1248, 4.7564, 6.0925, 8.0288], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.7910, 6.2948, 7.9098, 9.9651], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2312, -1.4018, -1.3969, -2.2059], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2283, -0.4797, -0.4378, -1.3746], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4095,  0.5632, -0.4413, -0.4614], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4377,  1.6964,  0.5620, -0.4886], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5812, 3.0090, 1.7176, 0.6365], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7368, 3.0479, 4.5478, 1.8631], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.0701, 4.4362, 4.5012, 6.1695], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0514, 4.5000, 5.9866, 7.9760], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.7531, 6.0900, 7.8558, 9.9825], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2508, -1.4600, -1.4225, -2.2498], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2288, -0.4586, -0.3981, -1.4247], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4242,  0.6032, -0.4309, -0.4763], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4318,  1.7722,  0.6049, -0.4776], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5685, 3.0768, 1.7333, 0.6287], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7124, 3.0991, 4.5549, 1.8457], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.0714, 4.5019, 4.5191, 6.2313], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0426, 4.5090, 5.9783, 8.0069], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.7069, 6.0794, 7.8097, 9.9632], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2413, -1.4187, -1.4330, -2.2706], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3350, -0.3887, -0.4270, -2.2529], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4535,  0.5815,  0.5636, -1.4156], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.5927,  0.5447,  1.7273, -0.4829], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5739, 1.7231, 3.0212, 0.6021], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6470, 3.0075, 4.5238, 1.7474], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.0361, 4.4011, 4.5177, 6.1710], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0241, 4.4177, 5.9990, 8.0071], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.7200,  6.0045,  7.8934, 10.0226], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1996, -1.3829, -1.4256, -2.2436], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3239, -0.4779, -0.4265, -2.2905], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3758,  0.5472,  0.5827, -1.4051], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4218,  1.7462,  0.6141, -0.4686], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5843, 3.0749, 1.7728, 0.6258], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6686, 3.0065, 4.5585, 1.7332], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1044, 4.4230, 4.5475, 6.1999], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1455, 4.4591, 6.0389, 8.0346], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.8381, 6.0281, 7.8809, 9.9773], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1973, -1.4441, -1.4840, -2.2382], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3403, -0.5095, -0.4628, -2.3066], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3773,  0.5558,  0.5682, -1.4091], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4115,  1.7668,  0.6110, -0.4706], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6464, 3.1381, 1.8056, 0.6620], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7296, 3.0490, 4.5847, 1.7229], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2445, 4.4823, 4.6006, 6.2302], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2729, 4.4660, 6.0461, 7.9296], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.9968, 6.0611, 7.9336, 9.9241], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2249, -1.4082, -1.3895, -2.2626], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2922, -0.4539, -0.4871, -1.4553], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4143,  0.5891,  0.5850, -1.4109], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4554,  1.7562,  1.7953, -0.4842], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6117, 3.0851, 1.8086, 0.6017], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7054, 2.9964, 4.5570, 1.6861], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2629, 4.4296, 4.6708, 6.2123], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.4012, 4.4748, 6.2515, 8.0374], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([8.0993, 6.0285, 8.1234, 9.9771], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3240, -1.4265, -1.4356, -2.2672], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4981, -0.5419, -0.5497, -2.3070], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5107,  0.5751,  0.5445, -1.4193], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.5992,  0.5372,  1.7384, -0.4553], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6691, 1.7534, 3.1067, 0.6673], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8158, 3.0588, 4.6265, 1.7872], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.3167, 4.4702, 4.7734, 6.1971], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.4383, 4.4991, 6.3641, 7.9678], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.2059,  6.1145,  8.3184, 10.0191], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3598, -1.4812, -1.4603, -2.3157], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.3023, -0.5429, -0.5013, -1.4448], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4608,  0.5565, -0.4874, -0.4871], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4078,  1.7791,  0.5991, -0.4743], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6338, 3.0912, 1.7713, 0.5867], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8150, 3.0204, 4.5444, 1.7496], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.3851, 4.4863, 4.7742, 6.2761], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.4559, 4.4916, 6.3091, 7.9842], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([8.1818, 6.0747, 8.2102, 9.9715], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3161, -1.3480, -1.4277, -2.2890], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3851, -0.4862, -0.4981, -2.2928], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4662,  0.6083,  0.5972, -1.4447], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6215,  0.5765,  1.7593, -0.4888], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6484, 1.7508, 3.1018, 0.5919], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8276, 3.0374, 4.5823, 1.7752], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.3287, 4.4544, 4.7191, 6.2046], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.4539, 4.5070, 6.2985, 8.0105], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([8.1649, 6.1065, 8.1995, 9.9798], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2940, -1.3770, -1.4720, -2.2993], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3778, -0.5447, -0.5595, -2.3023], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4601,  0.5642,  0.5985, -1.4549], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4440,  1.7549,  1.7898, -0.4929], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6185, 3.1005, 1.7799, 0.5824], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8028, 3.0925, 4.6261, 1.7817], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.3472, 4.5433, 4.7835, 6.2313], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.4414, 4.6037, 6.3347, 7.9958], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.1665,  6.2467,  8.2792, 10.0011], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2725, -1.5156, -1.4369, -2.3326], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2396, -0.5333, -0.4656, -1.4348], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4503,  0.5932, -0.4761, -0.4940], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4412,  1.8209,  0.5873, -0.5002], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6012, 3.1588, 1.7854, 0.5727], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7866, 3.1697, 4.6260, 1.8072], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2763, 4.5779, 4.7159, 6.1724], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.4296, 4.6525, 6.3078, 8.0175], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([8.1308, 6.2699, 8.2178, 9.9987], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3673, -1.5073, -1.4788, -2.3446], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2969, -0.5013, -0.4618, -1.4330], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4972,  0.5864, -0.4812, -0.5083], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4902,  1.7667,  0.5720, -0.5305], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5692, 3.1179, 1.7984, 0.5587], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7617, 3.1485, 4.6749, 1.7906], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2927, 4.5812, 4.7726, 6.2177], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.4130, 4.6444, 6.3256, 8.0044], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.1414,  6.2729,  8.2541, 10.0137], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3629, -1.4965, -1.4598, -2.3483], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.3224, -0.4672, -0.4893, -1.4241], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4632,  0.5871,  0.5623, -1.4171], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.5015,  1.8048,  1.7675, -0.5064], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5979, 1.8130, 3.1134, 0.5942], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7724, 3.1544, 4.6246, 1.7709], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2785, 4.6108, 4.6866, 6.1805], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.4461, 4.6971, 6.3065, 8.0123], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.1659,  6.3308,  8.2210, 10.0012], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3636, -1.5216, -1.5037, -2.3526], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.3412, -0.4759, -0.5149, -1.4458], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4940,  0.5640,  0.5597, -1.4442], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4599,  1.8181,  1.8020, -0.4804], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6241, 1.8230, 3.1509, 0.6070], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7919, 3.1468, 4.6410, 1.7631], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2990, 4.6380, 4.6652, 6.2109], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.4530, 4.7055, 6.2678, 8.0258], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.1867,  6.3394,  8.1644, 10.0357], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3435, -1.4878, -1.4758, -2.3340], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.3369, -0.4614, -0.4778, -1.4748], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4907,  0.5841,  0.5775, -1.4474], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4508,  1.8157,  1.7900, -0.4794], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6176, 1.8136, 3.1245, 0.5893], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8130, 3.1552, 4.6556, 1.7892], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.3044, 4.6605, 4.6604, 6.2409], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.4868, 4.7372, 6.3089, 8.0862], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.1740,  6.3373,  8.1712, 10.0100], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3456, -1.4935, -1.5239, -2.3174], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.5549, -0.4672, -0.4515, -2.3982], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4786,  0.5938,  0.5910, -1.4366], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4419,  1.8203,  1.7871, -0.4558], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6320, 1.8487, 3.1294, 0.6252], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7938, 3.1773, 4.5636, 1.7780], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.3763, 4.7804, 4.6182, 6.3739], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.4454, 4.7702, 6.1746, 7.9950], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([8.1651, 6.4306, 8.0477, 9.9721], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2731, -1.4019, -1.3648, -2.2899], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.3049, -0.4480, -0.4416, -1.4614], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4596,  0.6208, -0.4636, -0.4661], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4586,  1.8048,  0.5916, -0.4917], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6420, 3.2019, 1.8306, 0.6463], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7369, 3.1489, 4.5487, 1.7398], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1580, 4.6545, 4.5172, 6.1408], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2492, 4.7100, 6.1238, 7.9600], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.9938,  6.3822,  8.0738, 10.0643], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2902, -1.4706, -1.4398, -2.4124], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.3179, -0.4897, -0.5056, -1.5189], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4507,  0.6110,  0.5889, -1.4378], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4565,  1.8077,  1.7837, -0.4742], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6708, 1.9109, 3.2331, 0.7376], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7409, 3.1535, 4.5643, 1.8126], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1788, 4.6061, 4.5160, 6.2369], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2686, 4.6183, 6.1457, 8.0619], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.9208, 6.1958, 7.9660, 9.9672], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2516, -1.3887, -1.4181, -2.3438], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3839, -0.4676, -0.4360, -2.3476], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4154,  0.6370,  0.6117, -1.3969], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4202,  1.8068,  1.8063, -0.4610], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6367, 1.8159, 3.1358, 0.6414], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8193, 3.1495, 4.5790, 1.8631], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2695, 4.5504, 4.5192, 6.2632], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2868, 4.4959, 6.0496, 7.9896], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.0613,  6.1676,  7.9592, 10.0498], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2657, -1.3948, -1.4377, -2.3207], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3693, -0.4884, -0.4114, -2.3460], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4170,  0.6364,  0.6163, -1.3824], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4141,  1.8265,  1.8187, -0.4383], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6590, 1.8454, 3.1895, 0.6662], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8136, 3.1713, 4.6052, 1.8131], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2652, 4.5841, 4.5478, 6.1811], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2817, 4.5419, 6.0780, 7.8996], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.0818,  6.2263,  8.0183, 10.0315], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3585, -1.4206, -1.4646, -2.3292], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4071, -0.4828, -0.4073, -2.3240], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4540,  0.6204,  0.6086, -1.3923], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4726,  1.8001,  1.7681, -0.4753], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6451, 1.8425, 3.1637, 0.6667], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7628, 3.1354, 4.5062, 1.7766], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1875, 4.5264, 4.4558, 6.0839], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2916, 4.5425, 6.0504, 7.9560], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.1019,  6.2141,  7.9950, 10.0796], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3762, -1.3835, -1.4002, -2.3636], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4222, -0.4644, -0.3718, -2.3455], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4722,  0.6053,  0.6203, -1.4156], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4528,  1.8020,  0.6386, -0.4477], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5322, 3.0020, 1.7524, 0.4847], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6882, 3.0503, 4.4639, 1.6877], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1640, 4.4552, 4.4695, 6.0524], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2033, 4.4593, 5.9703, 7.9083], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.0189,  6.1477,  7.9193, 10.0905], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3958, -1.3592, -1.4638, -2.2980], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4752, -0.4314, -0.5127, -2.2527], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4748,  0.6176,  0.5895, -1.3710], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6224,  0.6163,  1.8509, -0.4406], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4736, 1.7318, 3.0288, 0.4467], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6511, 3.0741, 4.4819, 1.6646], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1751, 4.5308, 4.5373, 6.1486], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2114, 4.5604, 6.0768, 8.0270], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.9579,  6.1873,  7.9542, 10.1290], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3625, -1.4638, -1.3883, -2.3143], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.3464, -0.4582, -0.4162, -1.4422], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4975,  0.6220, -0.4536, -0.4554], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4327,  1.8867,  0.6704, -0.3794], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5329, 3.0630, 1.7637, 0.5615], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7043, 3.1030, 4.4962, 1.8252], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1551, 4.5189, 4.5085, 6.1687], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0870, 4.4795, 5.9586, 7.8449], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.9018, 6.1533, 7.9331, 9.9563], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3581, -1.4353, -1.4045, -2.2821], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.3277, -0.4608, -0.4192, -1.4084], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4980,  0.6113, -0.4624, -0.4490], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4443,  1.8631,  0.6604, -0.3761], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5585, 3.0726, 1.7888, 0.6350], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7503, 3.0771, 4.5185, 1.9356], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1088, 4.4855, 4.5177, 6.1587], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0799, 4.4765, 5.9977, 7.8892], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.7808, 6.0668, 7.8477, 9.8475], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3484, -1.4213, -1.4132, -2.2882], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.3458, -0.4689, -0.4999, -1.4021], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4511,  0.5883,  0.5976, -1.4008], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4470,  1.8551,  0.6309, -0.4211], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5720, 3.0669, 1.7859, 0.5951], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7698, 3.0781, 4.5155, 1.9074], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.0830, 4.4742, 4.5237, 6.0873], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0101, 4.4279, 5.9466, 7.7885], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.8690, 6.1238, 7.9426, 9.9836], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3470, -1.4061, -1.4401, -2.3091], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3713, -0.4225, -0.4089, -2.2946], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4113,  0.6128,  0.6209, -1.4137], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4385,  1.8626,  0.6238, -0.4473], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6104, 3.0853, 1.8140, 0.6205], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8695, 3.1339, 4.6145, 2.0302], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.0371, 4.4076, 4.4789, 6.0029], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2493, 4.5227, 6.1144, 8.0869], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.9502,  6.0715,  7.9284, 10.0683], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3464, -1.3531, -1.4269, -2.3221], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3634, -0.4260, -0.3794, -2.3343], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3782,  0.6174,  0.6789, -1.4179], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4753,  1.7595,  0.5839, -0.5211], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5779, 3.0042, 1.7748, 0.5636], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8818, 3.0708, 4.6379, 1.9737], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2093, 4.4534, 4.6685, 6.1700], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2347, 4.4639, 6.2072, 7.9407], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.9131, 6.0230, 8.0714, 9.8909], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3314, -1.3969, -1.4951, -2.3319], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4203, -0.4407, -0.4652, -2.3758], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4594,  0.6345,  0.5898, -1.4050], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.5767,  0.5755,  1.6750, -0.4628], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5343, 1.7307, 3.0602, 0.5606], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7140, 3.0555, 4.5577, 1.8783], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.9791, 4.3816, 4.5168, 5.8997], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2368, 4.5668, 6.2475, 8.0624], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.8568, 6.1215, 8.0625, 9.9399], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2832, -1.3497, -1.4465, -2.2615], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4589, -0.4485, -0.4763, -2.3568], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4570,  0.6359,  0.6760, -1.3947], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.5363,  1.7463,  1.7935, -0.4733], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5148, 3.0503, 1.7931, 0.5992], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6588, 3.0460, 4.5309, 1.8619], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1179, 4.5890, 4.6412, 6.1262], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1532, 4.5977, 6.1592, 7.9331], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.8558, 6.2180, 8.0722, 9.9434], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2784, -1.4243, -1.4696, -2.2399], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4674, -0.5642, -0.4796, -2.4026], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4345,  0.6434,  0.7239, -1.4271], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4925,  1.7650,  0.6187, -0.4345], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4791, 3.0537, 1.7611, 0.5477], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6812, 3.1313, 4.5657, 1.8951], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.9961, 4.6201, 4.4996, 5.9360], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1862, 4.6963, 6.0866, 8.0175], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.8688,  6.2875,  7.8988, 10.0124], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1895, -1.3486, -1.4014, -2.2322], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3143, -0.4926, -0.4363, -2.3474], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3873,  0.6541,  0.7027, -1.4139], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4248,  1.7662,  0.6188, -0.3796], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5260, 3.0389, 1.7484, 0.5893], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7237, 3.0573, 4.5665, 1.9187], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1334, 4.6183, 4.5304, 6.1216], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2058, 4.6056, 6.0045, 8.0447], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.8992,  6.1891,  7.8121, 10.0605], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1811, -1.3810, -1.3843, -2.2561], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3465, -0.4795, -0.4787, -2.3349], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4375,  0.6275,  0.6381, -1.4275], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4389,  1.7653,  0.6011, -0.3764], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5424, 3.1219, 1.7814, 0.6543], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6668, 3.0751, 4.5451, 1.8858], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1398, 4.7493, 4.6365, 6.2519], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1326, 4.6709, 6.1117, 7.9861], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.8363,  6.3007,  8.0075, 10.0008], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2558, -1.4099, -1.3741, -2.2568], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2521, -0.4175, -0.3825, -1.3957], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4061,  0.5896, -0.4305, -0.4317], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4793,  1.6905,  0.5576, -0.3986], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5490, 3.1207, 1.8020, 0.7004], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.5776, 2.9851, 4.4296, 1.8077], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.0919, 4.7047, 4.5962, 6.2455], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1367, 4.6816, 6.1483, 8.0498], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.8010, 6.2595, 8.0037, 9.9986], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2493, -1.4442, -1.4088, -2.2190], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2278, -0.3846, -0.3771, -1.3366], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4025,  0.5492, -0.4452, -0.4279], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4763,  1.6916,  0.5548, -0.3789], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5136, 3.1153, 1.7331, 0.6960], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6135, 3.0028, 4.5026, 1.8564], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.0306, 4.6380, 4.4763, 6.2091], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0597, 4.5310, 6.0711, 7.9355], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.7888,  6.1400,  8.0108, 10.0006], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2225, -1.4157, -1.4159, -2.1798], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3930, -0.4042, -0.3784, -2.2445], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4467,  0.5361,  0.5829, -1.4039], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4278,  1.7441,  0.5883, -0.3099], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5051, 3.0943, 1.7108, 0.6904], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.5904, 2.9674, 4.4788, 1.8361], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.0145, 4.5919, 4.4916, 6.1912], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1085, 4.5717, 6.2150, 8.0138], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.7928,  6.1795,  8.0972, 10.0314], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1990, -1.4656, -1.3630, -2.2300], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1947, -0.3685, -0.3793, -1.3112], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4181,  0.5641,  0.5909, -1.3962], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4269,  1.7642,  0.5861, -0.3162], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5095, 3.1429, 1.7134, 0.6975], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.5764, 3.0123, 4.4588, 1.8185], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.0086, 4.7027, 4.5044, 6.1897], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0929, 4.6741, 6.2468, 7.9683], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.7877, 6.2972, 8.1593, 9.9917], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2464, -1.3390, -1.4331, -2.2221], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4121, -0.4178, -0.4342, -2.2785], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4521,  0.4974,  0.5244, -1.3472], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.5098,  1.7900,  1.7627, -0.4216], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4871, 1.7467, 3.0983, 0.5304], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6719, 3.0978, 4.6142, 1.8777], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.0393, 4.7069, 4.5993, 6.1605], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1328, 4.7307, 6.3670, 7.9713], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.8286, 6.3634, 8.2948, 9.9821], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2071, -1.4276, -1.3405, -2.2561], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2733, -0.5035, -0.4881, -1.3946], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4026,  0.5124, -0.4761, -0.4437], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4443,  1.6628,  0.5361, -0.3835], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4948, 2.9894, 1.6774, 0.5801], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6898, 3.1345, 4.5377, 1.9187], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.0568, 4.7257, 4.4708, 6.2104], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1327, 4.7894, 6.1566, 8.0395], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.7758,  6.4181,  7.9655, 10.0029], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2223, -1.3921, -1.2986, -2.2946], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.3063, -0.4533, -0.4893, -1.4140], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4159,  0.5386,  0.5900, -1.4451], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4369,  1.8397,  0.6256, -0.3473], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4803, 3.0951, 1.7180, 0.5691], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6851, 3.2334, 4.5724, 1.9298], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.0563, 4.7332, 4.4572, 6.2182], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1079, 4.7273, 6.0276, 8.0549], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.8530,  6.3120,  7.8313, 10.1257], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2717, -1.3950, -1.3866, -2.3141], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1596, -0.4554, -0.4562, -1.3696], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3252,  0.5524,  0.5780, -1.4152], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4001,  1.7919,  0.5970, -0.3845], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5440, 2.9823, 1.7311, 0.5027], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8142, 3.1452, 4.5884, 1.9295], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2480, 4.5239, 4.5513, 6.1625], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.4152, 4.6353, 6.1239, 8.1150], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.1651,  6.1923,  7.9110, 10.0436], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2233, -1.3833, -1.3870, -2.3199], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3377, -0.5212, -0.5199, -2.2787], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3410,  0.5927,  0.5044, -1.3967], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4534,  1.7767,  1.7677, -0.4829], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5604, 1.7328, 3.0248, 0.5023], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.9183, 3.1731, 4.6282, 2.0170], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.3900, 4.4991, 4.5908, 6.2355], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.4911, 4.5901, 6.1118, 8.0406], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([8.2020, 6.1077, 7.8522, 9.9043], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2898, -1.3912, -1.3829, -2.3551], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1905, -0.5285, -0.5960, -1.4083], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4828,  0.5965,  0.4743, -1.4557], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.5457,  1.7800,  1.7836, -0.5284], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5932, 3.0729, 1.8510, 0.6332], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7820, 3.1502, 4.6317, 2.0190], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2263, 4.4622, 4.5877, 6.2579], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1951, 4.5490, 6.0124, 7.9988], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.9331, 6.1916, 7.8458, 9.9806], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.4414, -1.4055, -1.3848, -2.3518], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.3424, -0.5253, -0.5756, -1.4056], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.5792,  0.6240,  0.5275, -1.4369], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.6137,  1.8167,  1.8602, -0.5010], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6061, 3.2493, 1.9858, 0.7725], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6405, 3.2055, 4.6310, 1.9257], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1487, 4.7124, 4.6386, 6.3916], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2425, 4.8973, 6.2008, 8.2869], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.6451, 6.2601, 7.6596, 9.8451], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.5114, -1.4805, -1.5244, -2.2381], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.5689, -0.4415, -0.4529, -2.1877], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.6404,  0.6260,  0.6101, -1.3654], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.4665,  0.5610,  1.8194, -0.4066], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.4669, 1.7639, 3.2099, 0.6212], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6783, 3.1066, 4.7083, 2.0120], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.0471, 4.5743, 4.6370, 6.2338], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1828, 4.8185, 6.2322, 8.1941], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.7273, 6.2997, 7.8320, 9.8802], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.4454, -1.4727, -1.5310, -2.1160], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4824, -0.4134, -0.4741, -2.0959], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5565,  0.6345,  0.5593, -1.2880], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.5374,  0.5854,  1.6896, -0.3404], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6547, 1.8216, 3.1383, 0.7873], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8281, 3.1868, 4.6286, 2.0968], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1160, 4.6377, 4.5843, 6.2481], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1778, 4.8151, 6.1186, 8.1028], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.8467, 6.3622, 7.8596, 9.9366], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3925, -1.4916, -1.4520, -2.1511], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2703, -0.6111, -0.4339, -1.3925], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4773,  0.6092, -0.4611, -0.5281], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.5118,  1.7747,  0.5687, -0.5633], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6980, 3.0939, 1.8954, 0.6426], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6946, 3.0290, 4.5816, 1.5391], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2749, 4.6476, 4.5903, 6.2279], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.4532, 4.8497, 6.1964, 8.1301], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([8.1735, 6.3796, 7.8911, 9.9054], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.4491, -1.4006, -1.5683, -2.1668], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4638, -0.4313, -0.3599, -2.3515], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4287,  0.5876,  0.7002, -1.6973], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.5249,  1.7999,  0.5816, -0.6773], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5916, 3.1198, 1.7783, 0.4467], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7067, 2.9945, 4.6809, 1.3300], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.4302, 4.5734, 4.6494, 6.2776], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.6205, 4.6266, 6.2260, 8.1600], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.3993,  6.1145,  7.9201, 10.0059], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2506, -1.4267, -1.4875, -2.1147], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4040, -0.4420, -0.5105, -2.2451], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5075,  0.6486,  0.4915, -1.4499], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.5997,  0.5794,  1.5929, -0.5464], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6992, 1.7796, 3.1729, 0.3406], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8611, 3.1817, 4.8151, 1.5228], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.6289, 4.7966, 4.8194, 6.5281], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.6647, 4.8267, 6.3218, 8.1161], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([8.5130, 6.5605, 8.1313, 9.9553], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1005, -1.3989, -1.4138, -1.9740], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3751, -0.4169, -0.5729, -2.0084], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4748,  0.5675,  0.5402, -1.3488], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.5651,  0.5608,  1.7041, -0.5076], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6050, 1.6683, 3.0528, 0.3452], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6683, 3.0506, 4.6004, 1.4927], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.3123, 4.7131, 4.6561, 6.4117], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.4497, 4.8931, 6.2502, 8.1508], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([8.1218, 6.5777, 8.0538, 9.9130], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.6580, -1.4710, -1.4814, -2.0561], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.7587, -0.5383, -0.4570, -2.0661], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.5936,  0.4565,  0.6201, -1.4391], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.6649,  1.8835,  0.6836, -0.4446], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.3086, 3.1664, 1.7644, 0.5611], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.4281, 3.2956, 4.7177, 1.5422], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.8819, 5.0216, 4.5816, 6.4312], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1354, 5.1189, 6.0995, 8.0152], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.8438, 6.8785, 7.8550, 9.7933], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.7054, -1.4939, -1.3891, -2.0620], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.5065, -0.4734, -0.4043, -1.2708], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.6714,  0.6017, -0.4694, -0.3899], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.5903,  1.8492,  0.6802, -0.3103], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.3916, 3.0706, 1.7687, 0.6221], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6654, 3.1634, 4.7673, 1.6119], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.7713, 4.7300, 4.5345, 6.0935], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2044, 4.9742, 6.2829, 8.0686], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.1032,  6.7306,  8.2349, 10.1707], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2655, -1.4102, -1.3502, -1.9296], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.3080, -0.4211, -0.4245, -1.1940], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3708,  0.5055,  0.5795, -1.3230], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4817,  1.7976,  0.6264, -0.3302], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5338, 3.0214, 1.6827, 0.6299], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8886, 2.9888, 4.5751, 1.5991], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1825, 4.6428, 4.6784, 6.4156], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.4362, 4.6545, 6.3507, 7.9841], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.4540,  6.3982,  8.4164, 10.1723], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1510, -1.3891, -1.5106, -1.8000], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3775, -0.4321, -0.4986, -1.8483], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5090,  0.4666,  0.4707, -1.2015], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3604,  1.7374,  1.8540, -0.3457], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6021, 3.0627, 1.7643, 0.6947], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.9903, 2.9393, 4.6392, 1.7005], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2215, 4.5281, 4.7538, 6.4527], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.5319, 4.6258, 6.4786, 8.0964], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.4943,  6.3543,  8.5262, 10.1087], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2169, -1.4404, -1.4639, -1.9091], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3533, -0.4677, -0.2925, -2.0115], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3219,  0.6117,  0.7207, -1.4173], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.6029,  1.7722,  0.5824, -0.5623], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5301, 3.2181, 1.8400, 0.6165], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.1042, 3.2269, 4.8358, 1.8587], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.0680, 4.5590, 4.6692, 6.1617], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.5565, 4.7288, 6.4818, 8.1008], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.5326,  6.4997,  8.4687, 10.1146], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1733, -1.3666, -1.3384, -1.8993], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.3351, -0.4773, -0.6399, -1.2428], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3027,  0.6041,  0.5773, -1.3345], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4699,  1.7623,  1.6103, -0.4085], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6918, 1.7956, 3.1760, 0.4827], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.1934, 3.2950, 4.9055, 1.9835], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2044, 4.6050, 4.8087, 6.3343], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.5990, 4.7728, 6.6483, 8.1966], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.5803,  6.5055,  8.7078, 10.2265], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3145, -1.3607, -1.3721, -2.1094], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.5055, -0.6066, -0.3936, -2.3536], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3856,  0.5772,  0.6030, -1.5333], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.5161,  1.6715,  0.4532, -0.6514], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.3255, 3.0968, 1.5272, 0.2918], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.0021, 3.2635, 4.5997, 1.6447], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.8660, 4.4706, 4.2176, 5.7235], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.4950, 4.7615, 6.0898, 7.9774], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([8.3415, 6.4370, 7.8829, 9.9523], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3530, -1.3217, -1.3070, -2.3815], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3563,  0.5748,  0.6743, -1.6010], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4958,  1.6951,  0.5287, -0.6415], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.2747, 3.0216, 1.4989, 0.2905], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8667, 3.1808, 4.5984, 1.6140], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.8894, 4.3766, 4.3017, 5.8045], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1589, 4.4595, 5.9764, 7.6388], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.2912,  6.3472,  8.2161, 10.1645], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2677, -1.3937, -1.3593, -2.4412], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2191, -0.3744, -0.4571, -1.4692], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3021,  0.5514,  0.6081, -1.4914], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4619,  1.7577,  0.5998, -0.4650], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5392, 3.1813, 1.8531, 0.7744], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6411, 3.1346, 4.5882, 1.6605], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.0789, 4.5520, 4.9057, 6.0470], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2524, 4.6254, 6.3681, 7.9263], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.1805,  6.3808,  8.3675, 10.2045], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1922, -1.3718, -1.3913, -2.3685], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.2639, -0.5631, -0.4745, -2.3795], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2358,  0.5707,  0.5766, -1.3507], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3196,  1.8694,  0.7386, -0.1938], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7723, 3.3001, 2.0920, 1.1760], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7198, 3.2883, 4.8654, 1.9623], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1931, 4.8761, 5.3047, 6.2502], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.4170, 4.9984, 6.7858, 8.1834], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.8859, 6.5767, 8.3935, 9.7963], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1595, -1.3834, -1.3847, -2.2988], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.2701, -0.4876, -0.5196, -2.3022], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5498,  0.5804,  0.5316, -1.3413], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.4996,  0.5808,  1.8200, -0.4646], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6074, 1.9529, 3.1808, 0.8894], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.5370, 3.2656, 4.6585, 1.8420], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1794, 5.0523, 5.3623, 6.3122], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.4787, 5.1674, 6.9363, 8.3480], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.7410, 6.5774, 8.4079, 9.6595], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2054, -1.5449, -1.4638, -2.3528], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1886, -0.5493, -0.4508, -1.5200], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4512,  0.6481, -0.4400, -0.4934], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3973,  1.9835,  0.8318, -0.2175], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.2390, 3.0829, 1.7371, 0.5879], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.5419, 3.3491, 4.8922, 1.8838], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2986, 5.2526, 5.6119, 6.5790], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3773, 5.2015, 6.8968, 8.2512], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.8214, 6.8221, 8.5103, 9.8001], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2606, -1.4837, -1.5132, -2.3298], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3931, -0.4319, -0.5683, -2.3233], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.6440,  0.6143,  0.5451, -1.4264], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.4363,  0.6045,  1.8726, -0.5324], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6421, 2.0885, 3.3899, 0.9490], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6491, 3.4397, 4.8397, 1.9799], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.4499, 5.4058, 5.6834, 6.7263], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.4883, 5.3262, 6.8938, 8.3006], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.9923, 6.9494, 8.5355, 9.8476], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2219, -1.5721, -1.4126, -2.3074], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2339, -0.5587, -0.4951, -1.4348], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.5390,  0.6015, -0.5089, -0.5220], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4283,  1.8583,  0.6925, -0.3646], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5129, 3.2843, 1.9843, 0.7629], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.9016, 3.3709, 4.8183, 2.0850], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.4962, 5.2308, 5.6032, 6.6116], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.6426, 5.2836, 6.9415, 8.4205], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([8.0926, 6.7978, 8.5041, 9.9232], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2397, -1.3872, -1.3967, -2.2151], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3239, -0.4784, -0.4658, -2.2264], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4698,  0.5621,  0.6523, -1.5092], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4456,  1.8981,  0.7006, -0.3771], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5120, 3.2851, 2.0194, 0.7235], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8941, 3.3621, 4.7805, 2.0209], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.4208, 5.2034, 5.4637, 6.4572], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.5209, 5.1808, 6.6359, 8.1993], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([8.0960, 6.8105, 8.2560, 9.8564], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2955, -1.3682, -1.3972, -2.2052], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4483, -0.4592, -0.4111, -2.2624], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.5212,  0.7357,  0.7935, -1.5116], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4172,  2.0252,  0.7932, -0.3054], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6177, 3.3183, 2.0657, 0.8232], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.0016, 3.4234, 4.8687, 2.1386], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.4625, 5.1538, 5.1156, 6.6874], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3101, 4.8850, 6.0009, 8.0415], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.1541,  6.6379,  7.7670, 10.0859], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3085, -1.5202, -1.3189, -2.2922], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.3707, -0.3941, -0.4868, -1.3685], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.5571,  0.8301,  0.8129, -1.5264], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4811,  1.8740,  1.9593, -0.3438], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5543, 3.1722, 1.8229, 0.7075], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.9920, 3.3548, 4.8059, 2.0863], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2295, 4.9619, 4.5992, 6.4755], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3231, 4.8524, 5.7988, 8.1157], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.0619,  6.5421,  7.5002, 10.0589], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3247, -1.4289, -1.3247, -2.3193], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2916, -0.3993, -0.4427, -1.3702], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.5617,  0.8926,  0.8185, -1.5096], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4876,  1.9161,  1.9873, -0.3514], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5538, 3.2281, 1.7987, 0.7443], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8553, 3.2848, 4.7571, 1.9213], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1751, 4.9320, 4.4469, 6.4809], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2428, 4.7861, 5.6546, 8.0429], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.0143,  6.4850,  7.3493, 10.0324], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3373, -1.4902, -1.4310, -2.2883], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1756, -0.4731, -0.3186, -1.3650], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4367,  0.6991, -0.3122, -0.4385], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4458,  1.9578,  0.7229, -0.4057], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6922, 3.2799, 1.8795, 0.8529], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.9040, 3.2928, 4.7973, 1.9150], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1836, 4.9336, 4.4001, 6.4229], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2409, 4.7889, 5.6059, 7.9447], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([8.0660, 6.5223, 7.3385, 9.9782], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3493, -1.3723, -1.4932, -2.2298], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.5677, -0.4397, -0.4990, -2.2492], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5800,  0.6694,  0.7183, -1.4296], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4205,  1.9236,  1.9747, -0.3032], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7119, 3.2175, 1.8247, 0.7887], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.9722, 3.2626, 4.7983, 1.9517], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2425, 4.8273, 4.3877, 6.4545], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3064, 4.7246, 5.6470, 8.0118], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([8.0631, 6.3431, 7.3196, 9.9264], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1023, -1.4756, -1.3982, -2.1763], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.0494, -0.5297, -0.3520, -1.3129], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3287,  0.6982, -0.2971, -0.3900], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.2868,  1.9865,  0.8056, -0.2677], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.8553, 3.3186, 2.0076, 0.9062], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.0381, 3.3584, 4.9553, 1.9965], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2956, 4.9004, 4.6123, 6.4441], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3282, 4.7982, 5.8266, 7.9551], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([8.1368, 6.5336, 7.6260, 9.9055], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1116, -1.3907, -1.3102, -2.1711], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.3271, -0.4377, -0.4134, -1.3731], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4080,  0.7891, -0.2582, -0.4482], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3686,  1.9557,  0.7741, -0.3814], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.8801, 3.3988, 2.1034, 0.9073], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.9068, 3.2625, 4.7523, 1.8864], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2122, 4.8219, 4.8051, 6.1895], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3180, 4.7544, 6.0299, 7.9450], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.2500,  6.5285,  8.0663, 10.0704], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1518, -1.5286, -1.3797, -2.1979], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.4463, -0.4860, -0.4120, -1.4399], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4159,  0.7384, -0.2168, -0.4287], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3798,  1.8576,  0.8078, -0.3989], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.8728, 3.3352, 2.2286, 0.8724], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8994, 3.2751, 4.8832, 1.8703], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2463, 4.8976, 5.1582, 6.1513], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3917, 4.8618, 6.3529, 7.9836], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([8.2069, 6.6149, 8.2574, 9.9657], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1618, -1.5194, -1.4133, -2.2020], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.4827, -0.3939, -0.4379, -1.4193], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4566,  0.7650,  0.7034, -1.3240], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.5282,  1.9877,  1.8617, -0.2955], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7376, 1.9854, 3.3334, 0.8224], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8941, 3.2929, 4.8920, 1.8774], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2977, 4.9375, 5.3519, 6.1878], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.4148, 4.8708, 6.5613, 7.9968], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([8.2281, 6.6053, 8.5498, 9.9685], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2257, -1.3722, -1.4859, -2.2218], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4771, -0.4749, -0.3942, -2.3784], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4203,  0.7306,  0.7538, -1.3505], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3582,  1.8787,  0.8185, -0.3691], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.8762, 3.3232, 2.2376, 0.8883], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8756, 3.2172, 4.8418, 1.8401], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.3404, 4.9327, 5.3640, 6.2285], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3622, 4.8323, 6.4898, 7.9137], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([8.2623, 6.6395, 8.5551, 9.9787], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1864, -1.2861, -1.5008, -2.2283], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3203, -0.4701, -0.3667, -2.3735], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3570,  0.7240,  0.7559, -1.3658], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3189,  1.9001,  0.7986, -0.3708], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.8616, 3.3313, 2.1433, 0.8347], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.9483, 3.2818, 4.8868, 1.8733], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.3764, 4.8694, 5.3460, 6.2766], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.4208, 4.8216, 6.4999, 7.9934], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([8.2471, 6.5631, 8.4284, 9.9658], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1639, -1.3997, -1.5052, -2.2897], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3343, -0.4222, -0.4084, -2.3952], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3269,  0.7913,  0.7047, -1.3934], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4064,  2.0331,  1.7722, -0.3452], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7924, 2.0528, 3.2530, 0.7250], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8829, 3.2717, 4.5266, 1.7180], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.3395, 4.8151, 5.1039, 6.2023], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.4297, 4.8321, 6.3391, 8.0753], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.1332,  6.4950,  8.1981, 10.0062], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.9847, -1.2961, -1.1261, -2.3324], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.3051, -0.3295, -0.2282, -1.5177], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3489,  0.7554, -0.3145, -0.3862], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.2482,  1.9730,  0.7834, -0.3058], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.8076, 3.3625, 2.0130, 0.8005], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8192, 3.2673, 4.7136, 1.8239], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1516, 4.7594, 5.1759, 6.2352], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0577, 4.7669, 6.2401, 7.9736], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.7772, 6.5045, 8.0459, 9.9870], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1130, -1.5084, -1.4252, -2.3474], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.3498, -0.3729, -0.4581, -1.3741], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3483,  0.7352,  0.6643, -1.3305], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.2768,  2.0244,  1.8899, -0.1859], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.8777, 2.1120, 3.3811, 0.9499], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.9446, 3.3941, 4.6076, 1.9547], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1555, 4.7618, 4.9291, 6.2039], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1179, 4.8073, 5.9770, 7.9789], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.8340, 6.4961, 7.7452, 9.9738], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2663, -1.3815, -1.2896, -2.3550], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.3591, -0.3279, -0.2782, -1.3445], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3694,  0.6723, -0.3013, -0.3357], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.2090,  1.9239,  0.8060, -0.2268], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.8681, 3.2966, 1.9569, 0.8494], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.0250, 3.2924, 4.5992, 2.0011], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1690, 4.6072, 4.8053, 6.1780], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1215, 4.6941, 5.8171, 7.9792], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.8305, 6.3381, 7.5424, 9.9876], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.4136, -1.3569, -1.3808, -2.3779], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4201, -0.3302, -0.2312, -2.3276], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4354,  0.6819,  0.7572, -1.3837], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.2131,  2.0207,  0.8863, -0.1898], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7951, 3.4145, 2.0195, 0.8119], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.0087, 3.4053, 4.6947, 1.9585], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1266, 4.7141, 4.8437, 6.1604], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0949, 4.7781, 5.8173, 7.9493], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.7561, 6.4476, 7.4883, 9.9065], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.4492, -1.3460, -1.3604, -2.4009], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.5367, -0.4199, -0.4038, -2.3652], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4864,  0.6512,  0.6701, -1.3962], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3310,  1.9506,  0.7534, -0.3390], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6851, 3.3158, 1.8993, 0.7303], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.9570, 3.2607, 4.5472, 1.9413], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.0853, 4.4845, 4.7121, 6.1403], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0725, 4.5935, 5.7544, 8.0111], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.7157, 6.1582, 7.4176, 9.9864], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.4149, -1.3384, -1.2927, -2.3733], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.4493, -0.3491, -0.4139, -1.3108], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4590,  0.7416,  0.6953, -1.3440], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.2555,  2.0854,  2.0733, -0.1746], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5822, 1.8330, 3.1214, 0.4373], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.1000, 3.5126, 4.8777, 2.0918], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2618, 4.6876, 5.1341, 6.3953], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1497, 4.6650, 6.1770, 8.0398], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.7244, 6.2596, 7.9390, 9.8557], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2772, -1.4218, -1.4132, -2.2473], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1844, -0.2941, -0.4268, -1.1831], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3085,  0.8918,  0.6757, -1.2712], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.1555,  2.3156,  2.0646, -0.0438], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6844, 1.9227, 2.9749, 0.6569], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.0063, 3.3361, 4.4538, 2.0337], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2108, 4.5157, 5.0666, 6.2276], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1880, 4.5544, 6.2815, 8.0786], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.8483, 6.1325, 8.1695, 9.9939], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1174, -1.4293, -1.2783, -2.1482], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.0338, -0.4152, -0.3141, -1.2140], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2702,  0.7778, -0.2928, -0.3164], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.1356,  2.0686,  0.9623, -0.2105], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.8057, 3.0159, 2.1095, 0.6915], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.1330, 3.2106, 4.6280, 2.0489], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.4035, 4.4726, 5.2013, 6.2469], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.4144, 4.5687, 6.2867, 8.0847], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([8.0891, 6.1313, 7.9592, 9.9569], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1388, -1.3170, -1.3330, -2.1750], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.2109, -0.3156, -0.4531, -2.1378], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.2690,  0.7289,  0.7039, -1.2498], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.8645,  0.9125,  2.1547, -0.1528], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7521, 1.9312, 3.0818, 0.8225], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.1802, 3.3453, 4.7769, 2.2122], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.4397, 4.4978, 5.2391, 6.3038], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.4129, 4.5223, 6.3316, 8.0385], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([8.1048, 6.0583, 8.1684, 9.9173], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1871, -1.4373, -1.3293, -2.2514], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1494, -0.4532, -0.3705, -1.4867], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3586,  0.7569, -0.3426, -0.3860], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3264,  1.9882,  0.8327, -0.3211], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6713, 3.0189, 2.0350, 0.7385], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.9125, 3.0627, 4.5282, 2.0830], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.3545, 4.3643, 5.1035, 6.3889], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2726, 4.3700, 6.1943, 8.1379], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.9326,  5.8779,  8.0154, 10.0337], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1997, -1.3609, -1.2825, -2.2585], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1769, -0.3383, -0.2899, -1.5031], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4336,  0.8237, -0.2902, -0.4455], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4690,  2.0402,  0.8922, -0.4616], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6211, 3.1608, 2.2263, 0.7366], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7937, 3.1228, 4.7040, 2.0123], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2742, 4.3996, 5.2595, 6.3819], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1466, 4.3134, 6.3033, 8.1051], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.8430,  5.9075,  8.1589, 10.0690], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1580, -1.1891, -1.3144, -2.1873], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.1923, -0.2532, -0.3541, -2.1260], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4655,  0.8250,  0.6897, -1.3374], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.5494,  0.7339,  1.7799, -0.3088], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5854, 1.8264, 3.0375, 0.7986], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8588, 3.0655, 4.5638, 2.0789], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2561, 4.4654, 5.1491, 6.2650], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2020, 4.6507, 6.3809, 8.0982], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.9498,  6.2617,  8.2968, 10.1019], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.0984, -1.2716, -1.3808, -2.0961], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.0085, -0.3647, -0.2819, -2.0408], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.0962,  0.7372,  0.7988, -1.2006], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.2815,  1.7575,  0.7438, -0.2605], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.9926, 3.1861, 2.2675, 1.0864], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.9691, 3.1239, 4.5972, 2.1397], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2861, 4.5750, 5.1504, 6.2403], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2051, 4.7640, 6.3324, 8.0152], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.9718,  6.3889,  8.2449, 10.0515], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.0070, -1.4250, -1.3052, -2.1141], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.7983, -0.2587, -0.1146, -1.1842], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.1877,  0.8770, -0.2815, -0.2494], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.2099,  1.7815,  0.8206, -0.1750], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.0287, 3.1388, 2.2949, 1.0798], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.0523, 3.1454, 4.6967, 2.1613], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.3309, 4.5556, 5.1699, 6.2343], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2879, 4.7463, 6.3085, 8.0173], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.0649,  6.3159,  8.1240, 10.0013], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.0093, -1.4386, -1.3593, -2.1370], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8886, -0.2707, -0.3286, -1.2064], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.0334,  0.8271,  0.8015, -1.1698], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3921,  1.7580,  1.6467, -0.2515], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.8089, 1.8180, 3.1061, 0.9096], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.0346, 3.1212, 4.6724, 2.1265], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.3188, 4.5667, 5.1390, 6.2697], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2418, 4.7680, 6.2812, 8.0248], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.9619, 6.3607, 8.1366, 9.9699], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2026, -1.1678, -1.3171, -2.1416], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.0496, -0.2841, -0.3251, -2.0972], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.3556,  0.9090,  0.8295, -1.3793], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6551,  0.8237,  1.8734, -0.3375], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6656, 1.8236, 3.0839, 0.8105], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7792, 3.1092, 4.6158, 1.9413], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2413, 4.6432, 5.1162, 6.2611], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1498, 4.8538, 6.1990, 8.0094], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.8904, 6.4792, 8.0509, 9.9938], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3341, -1.2776, -1.2847, -2.1981], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.0095, -0.4503, -0.1680, -2.1474], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.1028,  0.6859,  0.8499, -1.2085], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.1768,  1.6955,  0.8479, -0.1309], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.9315, 3.1214, 2.1365, 0.9838], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7765, 3.0252, 4.6655, 1.9359], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2422, 4.6313, 5.1610, 6.2341], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1309, 4.9087, 6.2271, 7.9807], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.8892, 6.5662, 8.0757, 9.9900], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3466, -1.2803, -1.2921, -2.0965], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-0.9780, -0.4451, -0.1657, -2.0832], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.0833,  0.6169,  0.7378, -1.1743], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.1439,  1.7702,  0.7591, -0.1088], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.9745, 3.1973, 2.0932, 1.0154], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7109, 3.0037, 4.6315, 1.8008], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2217, 4.5475, 5.1130, 6.2246], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0985, 4.7306, 6.2034, 7.9652], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.8533, 6.3404, 8.0629, 9.9639], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3650, -1.1792, -1.3143, -2.0536], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-0.9399, -0.1903, -0.1844, -1.9438], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.0882,  0.6389,  0.6459, -1.1567], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.1442,  1.8087,  0.6989, -0.1212], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.9427, 3.1491, 2.0062, 0.9687], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7540, 3.1025, 4.6678, 1.8469], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2071, 4.4328, 5.0379, 6.2396], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0785, 4.5760, 6.1181, 8.0113], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.8181,  6.1375,  7.9437, 10.0005], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.4196, -1.3367, -1.3454, -2.1289], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.0684, -0.2143, -0.2386, -2.0355], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.2790,  0.6838,  0.6549, -1.3627], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6569,  0.7412,  1.7958, -0.4187], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6779, 1.8093, 3.1481, 0.7425], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6966, 3.1347, 4.6462, 1.7248], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2155, 4.5737, 5.0792, 6.2572], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.0577, 4.6830, 6.0819, 8.0082], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.8001,  6.3326,  7.8924, 10.0002], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.9480, -1.2798, -1.1644, -2.0536], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8960, -0.2180, -0.2064, -1.2580], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2053,  0.6255, -0.2483, -0.2740], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.2400,  1.7799,  0.6364, -0.2787], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.8570, 3.1332, 1.8765, 0.7815], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7523, 3.1760, 4.6292, 1.6584], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.3031, 4.6704, 5.0623, 6.2307], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2276, 4.7664, 6.1009, 8.0137], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([8.0073, 6.4259, 7.9359, 9.9876], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.8101, -1.2906, -1.3210, -2.0279], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-0.8889, -0.2135, -0.3074, -2.1231], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.1789,  0.6523,  0.6708, -1.4642], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3852,  1.8317,  1.7832, -0.4034], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6634, 1.8664, 3.1490, 0.5018], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7741, 3.2278, 4.5841, 1.5850], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.3722, 4.7391, 5.0069, 6.2195], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3104, 4.8168, 6.0002, 7.9935], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.1257,  6.5229,  7.8262, 10.0170], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.7002, -1.1973, -1.1904, -1.9890], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9307, -0.3002, -0.4793, -1.2593], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.1292,  0.6267,  0.5372, -1.2321], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3437,  1.8750,  1.7755, -0.3102], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7998, 1.9128, 3.2203, 0.7547], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.9171, 3.2454, 4.6090, 1.7941], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.4208, 4.6290, 4.9978, 6.1976], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3546, 4.6803, 6.0011, 7.9521], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([8.1849, 6.2972, 7.8520, 9.9751], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.7317, -1.2632, -1.1941, -2.0748], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9544, -0.4198, -0.5098, -1.2758], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2174,  0.5559,  0.5425, -1.2955], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3363,  1.8214,  1.8059, -0.2959], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.8461, 1.8489, 3.2707, 0.8425], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.9666, 3.1299, 4.5705, 1.9058], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.4331, 4.4507, 4.9402, 6.2268], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3516, 4.5181, 5.9420, 8.0218], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.1084,  6.1359,  7.7433, 10.0146], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.9460, -1.2744, -1.2732, -2.2677], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1049, -0.3655, -0.5059, -1.3414], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4133,  0.6879,  0.5811, -1.3858], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4813,  1.9287,  1.8133, -0.3459], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5993, 1.9001, 3.2160, 0.6953], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7813, 3.2468, 4.6273, 1.9129], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1652, 4.5404, 4.9546, 6.2217], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1774, 4.5697, 5.9773, 8.0320], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.8978,  6.1993,  7.7782, 10.0204], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.4064, -1.1557, -1.2002, -2.2820], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.3705, -0.3352, -0.3490, -2.2534], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4637,  0.6505,  0.6314, -1.4719], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.5239,  0.6378,  1.8304, -0.5252], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.5447, 1.8430, 3.1865, 0.7222], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7259, 3.1906, 4.5832, 1.9493], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1224, 4.5264, 4.9431, 6.2647], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1033, 4.5608, 5.9333, 7.9760], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 7.8587,  6.2295,  7.7721, 10.0074], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.6065, -1.2566, -1.2776, -2.2769], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.5186, -0.4516, -0.4634, -2.2806], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.5084,  0.6293,  0.6024, -1.4833], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.5631,  0.6525,  1.8483, -0.4928], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6041, 1.8867, 3.2464, 0.7764], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7941, 3.2508, 4.6322, 1.9675], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.0943, 4.5524, 4.9218, 6.1842], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.1542, 4.6302, 5.9400, 7.9972], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.9066, 6.2987, 7.7187, 9.9913], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.6651, -1.1394, -1.3316, -2.1732], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.4890, -0.3989, -0.4704, -2.1988], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4621,  0.6862,  0.6028, -1.4312], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([ 0.6363,  0.6856,  1.8386, -0.4284], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.6439, 1.8460, 3.1358, 0.7881], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8515, 3.2220, 4.5808, 1.9766], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.1771, 4.5223, 4.9221, 6.2396], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2492, 4.5754, 5.9756, 8.0351], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([7.9846, 6.1689, 7.7692, 9.9957], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.6842, -1.2169, -1.4419, -2.0816], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.2960, -0.4641, -0.3391, -2.1459], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4435,  0.6449,  0.7177, -1.3460], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3573,  1.9149,  0.7167, -0.3689], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7490, 3.1423, 1.8884, 0.6436], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8927, 3.2012, 4.6050, 1.9074], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2338, 4.5064, 4.9387, 6.2075], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.3124, 4.5779, 5.9540, 8.0442], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.0581,  6.2020,  7.7285, 10.0129], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.0193, -1.5119, -1.3678, -1.9797], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9852, -0.5265, -0.4935, -1.0699], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4634,  0.6095, -0.4201, -0.3493], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.2869,  1.9930,  0.7284, -0.3369], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7575, 3.1678, 1.8824, 0.6229], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8979, 3.2202, 4.5526, 1.8925], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2343, 4.5282, 4.9297, 6.1944], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2996, 4.5768, 5.9560, 8.0316], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.0384,  6.2173,  7.7712, 10.0215], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.8503, -1.4881, -1.3715, -2.0453], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9364, -0.4677, -0.5263, -1.1283], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2796,  0.6678,  0.6889, -1.3646], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3321,  1.9200,  0.7234, -0.4139], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7020, 3.1097, 1.9207, 0.5825], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8671, 3.1335, 4.5456, 1.8750], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2306, 4.4760, 4.9471, 6.2015], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2866, 4.5435, 5.9687, 8.0387], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.0065,  6.1415,  7.7634, 10.0062], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.7453, -1.3150, -1.2664, -2.0165], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8515, -0.4075, -0.4447, -1.1225], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2587,  0.6205,  0.6630, -1.3766], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3111,  1.9702,  0.7300, -0.4067], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.7488, 3.2272, 1.9771, 0.6401], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.8977, 3.2300, 4.5656, 1.9049], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([3.2256, 4.5674, 4.9538, 6.1982], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.2988, 4.6373, 5.9820, 8.0396], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.0215,  6.2860,  7.7875, 10.0209], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import gym \n",
    "import gym_examples\n",
    "from doubledqn import Agent\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('gym_examples/GridWorld-v0', size=4)\n",
    "agent = Agent(gamma=0.9, epsilon=0.5, batch_size=10, n_actions=4, eps_end=0.01, input_dims=[16], lr=0.003, update_freq=50)\n",
    "scores, eps_hist,avg_scores_ddqn = [], [], []\n",
    "n_games = 500\n",
    "\n",
    "for i in range(n_games):\n",
    "    score = 0\n",
    "    done=False\n",
    "    observation = env.reset()[0]['agent']\n",
    "    counter = 0\n",
    "\n",
    "    while not done:\n",
    "        agent_location = env.get_agent_location()\n",
    "        print(f\"Agent's Location: {agent_location}\")\n",
    "\n",
    "        # choose an action based on current state of env\n",
    "        action = agent.choose_action(observation)\n",
    "\n",
    "        observation_, reward, done, truncated, info = env.step(action)\n",
    "        observation_ = observation_['agent'] # since one hot encoded state is nested in dictionary\n",
    "\n",
    "        score += reward\n",
    "\n",
    "        # store transition and update weights\n",
    "        agent.store_transitions(observation, action, reward, observation_, done)\n",
    "        agent.learn()\n",
    "\n",
    "        #update state\n",
    "        observation = observation_\n",
    "        \n",
    "        # end of an episode\n",
    "        counter += 1\n",
    "        print('---')\n",
    "\n",
    "    scores.append(score)\n",
    "    eps_hist.append(agent.epsilon)\n",
    "    print('end game')\n",
    "\n",
    "    avg_score = np.mean(scores)\n",
    "    \n",
    "    avg_scores_ddqn.append(avg_score)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.7832460396332905"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(avg_scores_ddqn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you've run both your DQN and Double DQN for the same number of episodes\n",
    "episodes = list(range(1, n_games + 1))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot for regular DQN\n",
    "plt.plot(episodes, avg_scores_dqn, marker='o', linestyle='-', color='b', label='DQN Average Score per Episode')\n",
    "\n",
    "# Plot for Double DQN\n",
    "plt.plot(episodes, avg_scores_ddqn, marker='x', linestyle='--', color='r', label='Double DQN Average Score per Episode')\n",
    "\n",
    "plt.title('Average Scores Over Episodes for DQN and Double DQN')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Score')\n",
    "plt.legend()  # This ensures the label for each line is shown\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-188, -111, -182, -222, -64, -101, -138, -60, -20, -23]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
