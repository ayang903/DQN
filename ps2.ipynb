{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete random movements on environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent's INITIAL Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [1 3], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [1 3], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [1 3], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 3], Target Location: [0 0]\n",
      "Agent's Location: [0 3], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 3], Target Location: [0 0]\n",
      "Agent's Location: [0 3], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 3], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 1], Target Location: [0 0]\n",
      "Agent's Location: [0 1], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 1], Target Location: [0 0]\n",
      "Agent's Location: [0 1], Target Location: [0 0]\n",
      "Agent's Location: [0 1], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 1], Target Location: [0 0]\n",
      "Agent's Location: [0 1], Target Location: [0 0]\n",
      "Agent's Location: [0 1], Target Location: [0 0]\n",
      "Agent's Location: [0 1], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 3], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 1], Target Location: [0 0]\n",
      "Agent's Location: [0 0], Target Location: [0 0]\n",
      "Episode finished\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import gym_examples\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('gym_examples/GridWorld-v0', size=4)\n",
    "\n",
    "state = env.reset()\n",
    "agent_location = env.get_agent_location()\n",
    "target_location = env.get_target_location()\n",
    "print(f\"Agent's INITIAL Location: {agent_location}, Target Location: {target_location}\")\n",
    "\n",
    "done = False\n",
    "\n",
    "\n",
    "while not done:\n",
    "    # Select action randomly\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # take step, returns transition\n",
    "    next_state, reward, done, truncated, info = env.step(action)\n",
    "    \n",
    "    agent_location = env.get_agent_location()\n",
    "    target_location = env.get_target_location()\n",
    "    print(f\"Agent's Location: {agent_location}, Target Location: {target_location}\")\n",
    "\n",
    "    #update state\n",
    "    state = next_state\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(\"Episode finished\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-0.0801,  0.0645,  0.1087, -0.0889], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([ 0.1087,  0.1150, -0.0784,  0.0924, -0.0813,  0.0837,  0.0974,  0.1230,\n",
      "         0.0814,  0.1150], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.8906, -0.9073, -0.8965, -0.8857, -0.8893, -0.8965, -0.8873, -0.8893,\n",
      "        -0.9022, -0.9073], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.9290246963500977\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-0.1548,  0.1522,  0.0233, -0.1180,  0.0082,  0.0194,  0.1428,  0.2358,\n",
      "         0.1522,  0.1260], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.7878, -0.8217, -0.8159, -0.8630, -0.8630, -0.8715, -0.8866, -0.7878,\n",
      "        -0.8217, -0.8172], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.8106781244277954\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([ 0.3445,  0.1806,  0.1914, -0.0509, -0.2068,  0.1943,  0.1943,  0.3445,\n",
      "        -0.0521, -0.1578], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6900, -0.7516, -0.8375, -0.8278, -0.6900, -0.7404, -0.7404, -0.6900,\n",
      "        -0.7518, -0.8251], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.7586825489997864\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([ 0.2232, -0.2670, -0.2670,  0.2364, -0.2077, -0.1262, -0.0953, -0.1238,\n",
      "         0.2358, -0.1234], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6578, -0.6047, -0.6047, -0.7877, -0.7991, -0.7873, -0.6047, -0.7991,\n",
      "        -0.6853, -0.6881], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4722633361816406\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3372, -0.1668,  0.5479, -0.0509], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.2007, -0.3372,  0.5479, -0.1952,  0.2780, -0.0976, -0.3372,  0.5479,\n",
      "         0.2780, -0.2646], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6122, -0.5069, -0.5069, -0.7498, -0.5733, -0.5069, -0.5069, -0.5069,\n",
      "        -0.5733, -0.7498], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.46121469140052795\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4148, -0.2625,  0.6654, -0.0600], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.4148, -0.2911,  0.3285, -0.3030,  0.3770, -0.1124, -0.2787,  0.6654,\n",
      "         0.3285,  0.6654], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4011, -0.5288, -0.4804, -0.6832, -0.5303, -0.4011, -0.7044, -0.4011,\n",
      "        -0.4804, -0.4011], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.48727408051490784\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-0.4863, -0.4054,  0.7822,  0.3648, -0.3981,  0.4278, -0.1412, -0.3920,\n",
      "        -0.4863, -0.4863], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.2960, -0.6277, -0.2960, -0.3811, -0.6717, -0.4449, -0.2960, -0.4455,\n",
      "        -0.2960, -0.2960], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.274029016494751\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-0.5322, -0.4657,  0.4539,  0.4659, -0.4949, -0.1838, -0.4717, -0.5106,\n",
      "        -0.4523,  0.8777], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.2101, -0.6521, -0.3613, -0.5915, -0.3726, -0.2101, -0.2896, -0.5807,\n",
      "        -0.6521, -0.2101], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3197955787181854\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([ 0.9446,  0.9446, -0.6080,  0.4687, -0.5865,  0.4009, -0.2318, -0.5588,\n",
      "        -0.5557,  0.4009], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.1498, -0.1498, -0.5648, -0.2870, -0.3197, -0.2181, -0.1498, -0.2181,\n",
      "        -0.6392, -0.2181], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3935837745666504\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3825, -0.5899,  0.3802, -0.2542], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.6481,  0.4772, -0.6785, -0.5964,  0.9815, -0.5352, -0.5964, -0.6074,\n",
      "         0.9815, -0.6212], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.2914, -0.5920, -0.5705, -0.1167, -0.1167, -0.6578, -0.1167, -0.1606,\n",
      "        -0.1167, -0.6578], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4370252192020416\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-0.5386, -0.6089, -0.5790,  0.4218, -0.6669,  0.3280, -0.7107, -0.6529,\n",
      "         0.3280,  0.9551], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.7048, -0.1512, -0.1404, -0.6232, -0.3083, -0.1512, -0.6204, -0.7048,\n",
      "        -0.1512, -0.1404], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3320290744304657\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([ 0.9061, -0.5323, -0.7176, -0.3416,  0.2546, -0.6615,  0.9061,  0.3430,\n",
      "        -0.5323,  0.2546], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.1845, -0.7708, -0.6913, -0.1845, -0.1591, -0.7708, -0.1845, -0.6579,\n",
      "        -0.7708, -0.1591], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.38742583990097046\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3367, -0.5916,  0.1856, -0.3216], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.5444, -0.5444, -0.5444, -0.7232, -0.5383,  0.3565,  0.8607, -0.5383,\n",
      "        -0.6531, -0.5383], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.8329, -0.8329, -0.8329, -0.7638, -0.2253, -0.2357, -0.2253, -0.2253,\n",
      "        -0.3799, -0.2253], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.21502578258514404\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-0.3778,  0.1853, -0.5565, -0.5200, -0.5565, -0.5390,  0.1294, -0.5200,\n",
      "        -0.5816, -0.7220], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.2799, -0.7127, -0.8836, -0.2799, -0.8836, -0.1667, -0.1667, -0.2799,\n",
      "        -0.8836, -0.8332], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14750051498413086\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([ 0.0881,  0.7406, -0.5783, -0.5744, -0.3877, -0.5783,  0.0881,  0.0881,\n",
      "        -0.6725, -0.5032], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.1777, -0.3335, -0.9207, -0.9207, -0.3335, -0.9207, -0.1777, -0.1777,\n",
      "        -0.9207, -0.3335], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1813460737466812\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([ 0.6981, -0.4921,  0.0589,  0.0663,  0.0663, -0.7453,  0.2891,  0.0663,\n",
      "        -0.6021, -0.5988], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3717, -0.3717, -0.7398, -0.1707, -0.1707, -0.9470, -0.2526, -0.1707,\n",
      "        -0.9403, -0.9403], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2530592679977417\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3243, -0.6378,  0.0486, -0.3450], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.0486,  0.6690, -0.5101, -0.7794, -0.7229, -0.4873,  0.0486, -0.6304,\n",
      "        -0.6378, -0.4010], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.1490, -0.3979, -0.1490, -0.9925, -0.9563, -0.3979, -0.1490, -0.9563,\n",
      "        -0.9563, -0.3979], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16622304916381836\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.6696, -0.5702,  0.9736, -0.4084], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.6696, -0.6828, -0.7612,  0.6466, -0.4878,  0.6466, -0.4878,  0.0439,\n",
      "        -0.6828, -0.4084], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9605, -0.9605, -0.9605, -0.4180, -0.4180, -0.4180, -0.4180, -0.1238,\n",
      "        -0.9605, -0.4180], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.25834307074546814\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-0.7426, -0.7144,  0.0576, -0.8791, -0.4916,  0.6305, -0.0501,  1.0087,\n",
      "        -0.7144,  0.6305], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9482, -0.9482, -0.0922, -1.0450, -0.4325, -0.4325, -0.7180, -0.0922,\n",
      "        -0.9482, -0.4325], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.41233357787132263\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.7719, -0.6454,  1.0485, -0.4303], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.7719, -0.8077,  1.0485, -0.7330, -0.7719, -0.5050,  0.6194, -0.8077,\n",
      "        -0.8679, -0.7719], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9269, -0.9269, -0.0563, -0.5097, -0.9269, -0.4426, -0.4426, -0.9269,\n",
      "        -0.9269, -0.9269], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.25060826539993286\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-0.8415,  0.6103, -0.8415, -0.6843, -0.8727,  1.0865, -0.9225, -0.5267,\n",
      "         0.1165, -0.5794], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.8952, -0.4507, -0.8952, -0.0221, -0.8952, -0.0221, -0.8952, -0.4507,\n",
      "        -0.0221, -0.0221], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.31359046697616577\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([ 0.5770, -0.5392, -0.9089, -0.5392, -0.8985, -0.5392, -0.8985,  0.5770,\n",
      "        -0.8985, -0.6819], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4807, -0.4807, -0.8850, -0.4807, -0.8850, -0.4807, -0.8850, -0.4807,\n",
      "        -0.8850, -0.0291], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2674992084503174\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4663, -0.9298,  0.1215, -0.3833], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 1.0444, -0.6580,  0.5261, -0.5370, -0.9373,  0.1215,  0.1215, -0.9373,\n",
      "         0.5261, -0.9373], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.0600, -0.0600, -0.5265, -0.5265, -0.8907, -0.0600, -0.0600, -0.8907,\n",
      "        -0.5265, -0.8907], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3865845203399658\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-0.6188, -0.9405, -0.1997,  0.0927, -0.9405, -0.4521,  0.4622,  0.0927,\n",
      "        -0.9599, -0.9609], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.1075, -0.9166, -0.6991, -0.1075, -0.9166, -0.5840, -0.5840, -0.1075,\n",
      "        -0.9166, -0.9166], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17080004513263702\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.5133, -0.4925,  0.3976, -0.3320], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.9409,  0.3976,  0.9368, -0.9409, -0.9748, -0.9748, -0.9748,  0.0499,\n",
      "         0.0499,  0.3976], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9551, -0.6421, -0.1569, -0.9551, -0.9551, -0.9551, -0.9551, -0.1569,\n",
      "        -0.1569, -0.6421], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.34454554319381714\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([ 0.8855, -0.0027, -0.4788, -0.6481,  0.8855,  0.2702,  0.3310, -0.4665,\n",
      "        -0.9837, -0.4986], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.2030, -0.2030, -0.7021, -0.7021, -0.2030, -0.2324, -0.7021, -0.2030,\n",
      "        -1.0024, -0.7021], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.38939380645751953\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9972, -0.4823,  0.8353, -0.5053], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.9972, -0.4374, -0.6170,  0.8353, -0.9972,  0.8353, -0.4946, -0.3845,\n",
      "        -0.4946, -0.4823], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.0525, -0.2483, -0.7490, -0.2483, -1.0525, -0.2483, -0.7539, -0.7952,\n",
      "        -0.7539, -0.2483], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.27652376890182495\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([ 0.7871, -0.4384,  0.2324, -0.5276, -1.0261, -0.9452, -0.4384, -0.1047,\n",
      "        -0.4023, -1.0261], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.2916, -0.2916, -0.7909, -0.7909, -1.0942, -1.0942, -0.2916, -0.2916,\n",
      "        -0.2916, -1.0942], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.24017027020454407\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.0562, -0.3938,  0.7387, -0.5651], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.9444, -0.3651, -0.5492, -0.1483,  0.1959, -0.5651,  0.7387, -0.9444,\n",
      "        -0.1483, -0.5343], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.1334, -0.3351, -0.8203, -0.3351, -0.8237, -0.8237, -0.3351, -1.1334,\n",
      "        -0.3351, -0.8237], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2559008002281189\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-0.5352, -0.1770, -0.1770, -0.3654, -0.5652,  0.7133, -0.9650,  0.7133,\n",
      "        -0.1770, -0.6184], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.8583, -0.3580, -0.3580, -0.3580, -0.8361, -0.3580, -1.1593, -0.3580,\n",
      "        -0.3580, -0.8361], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2656809687614441\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-0.2072, -0.6051, -1.0224, -0.3406, -0.2072,  0.7042,  0.1819,  0.1819,\n",
      "         0.1819,  0.1585], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3662, -0.8363, -1.4475, -0.3662, -0.3662, -0.3662, -0.8363, -0.8363,\n",
      "        -0.8363, -0.3058], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4756891131401062\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.1765, -0.3287,  0.6934, -0.7434], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.6934,  0.1746,  0.1746,  0.6934, -1.0386,  0.6934,  0.1746, -1.1765,\n",
      "        -0.2490, -0.6224], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3759, -0.8429, -0.8429, -0.3759, -1.2241, -0.3759, -0.8429, -1.2241,\n",
      "        -0.3759, -0.8711], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.665074348449707\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2133, -0.3277,  0.6702, -0.8028], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.2133,  0.1550,  0.1550, -0.6693, -1.0911,  0.6702,  0.6702, -0.2967,\n",
      "        -0.2967, -1.0911], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.2670, -0.8605, -0.8605, -0.8605, -1.2670, -0.3968, -0.3968, -0.3968,\n",
      "        -0.3968, -1.2670], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.446093887090683\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2562, -0.3360,  0.6485, -0.8609], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.3395, -0.3395,  0.1373, -1.2562, -0.3395, -0.3395,  0.6485, -0.3329,\n",
      "         0.6485,  0.6485], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4163, -0.4163, -0.8764, -1.3056, -0.4163, -0.4163, -0.4163, -0.4163,\n",
      "        -0.4163, -0.4163], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4462377429008484\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-1.2261, -1.2985,  0.6207, -0.7901, -1.2985, -0.5472, -0.3525, -0.3525,\n",
      "         0.6207, -0.3503], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3530, -1.3530, -0.4414, -0.9287, -1.3530, -0.9263, -0.4414, -0.4414,\n",
      "        -0.4414, -0.4414], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.24650327861309052\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.7414, -1.3092, -0.4274, -0.8797], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.4274,  0.6040, -1.3092,  0.0938,  0.0938,  0.6040, -1.3092, -0.4274,\n",
      "         0.6040, -0.3900], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4564, -0.4564, -1.3846, -0.9156, -0.9156, -0.4564, -1.3846, -0.4564,\n",
      "        -0.4564, -0.4564], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5428512692451477\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([ 0.0644, -1.0252, -1.3864, -1.3864,  0.5738, -0.8886, -0.4378, -0.4681,\n",
      "        -1.3864,  0.5738], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9420, -0.9420, -1.4213, -1.4213, -0.4836, -0.9583, -0.4836, -0.4836,\n",
      "        -1.4213, -0.4836], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3266794979572296\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-1.4292, -0.9430,  0.5359,  0.0282, -0.8516, -0.5103, -1.4902,  0.0282,\n",
      "         0.5359,  0.0282], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4593, -0.9806, -0.5176, -0.9746, -0.9746, -0.5176, -1.4593, -0.9746,\n",
      "        -0.5176, -0.9746], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5255523920059204\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-1.1074, -1.2141, -0.5593, -0.5593, -0.5259, -0.0174,  0.4861, -1.4751,\n",
      "        -1.4751, -0.7436], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.0157, -1.5034, -0.5625, -0.5625, -0.5625, -1.0157, -0.5625, -1.5034,\n",
      "        -1.5034, -0.9848], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.22493776679039001\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-1.5213, -1.6694,  0.4407, -0.0601, -1.5213, -0.0374, -0.9297, -0.0601,\n",
      "         0.4407,  0.4407], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.5320, -1.5320, -0.6034, -1.0541, -1.5320, -0.4021, -1.0541, -1.0541,\n",
      "        -0.6034, -0.6034], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5413859486579895\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9728, -0.6711, -0.1139, -0.8448], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.1139, -0.1139, -0.6546, -1.1669, -1.5693, -0.6371, -0.6711, -1.1669,\n",
      "        -0.9728, -0.8448], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.1025, -1.1025, -0.6581, -1.1025, -1.5734, -0.6581, -0.6581, -1.1025,\n",
      "        -1.1025, -1.1513], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2074379175901413\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-1.6213, -1.6213, -0.1643, -0.7325,  0.3253, -1.7370,  0.3253, -1.0217,\n",
      "        -1.6213, -0.1643], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.6098, -1.6098, -1.1479, -0.7073, -0.7073, -1.6813, -0.7073, -1.1479,\n",
      "        -1.6098, -1.1479], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.40870943665504456\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-0.1770, -0.2211, -1.2434,  0.2600, -1.4744, -1.0753, -0.7906, -1.0410,\n",
      "        -0.7721, -1.6752], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4867, -1.1990, -1.1990, -0.7660, -1.6531, -1.1990, -0.7660, -1.1932,\n",
      "        -0.7660, -1.6531], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.21782949566841125\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.1101, -1.0930, -0.2327, -0.9968], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.1991, -1.7370, -0.8501, -1.7370, -1.7370, -1.1395, -0.2280, -1.9682,\n",
      "        -0.7638, -1.5450], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.8208, -1.6874, -0.8208, -1.6874, -1.6874, -1.2458, -0.5067, -1.6874,\n",
      "        -0.8208, -1.6874], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12397996336221695\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([ 0.1324, -0.8172, -1.7874, -0.3285,  0.1324, -2.0160, -1.1979, -1.6103,\n",
      "         0.1324, -0.8172], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.8808, -0.8808, -1.7355, -1.2957, -0.8808, -1.7355, -1.2957, -1.7355,\n",
      "        -0.8808, -0.8808], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4130035936832428\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2273, -1.1657, -0.3038, -1.0629], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.1417, -1.8355, -0.3038, -0.3954, -0.8894, -0.8894,  0.0512, -0.9284,\n",
      "         0.0512,  0.0512], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.1846, -1.8005, -1.2734, -1.3559, -0.9539, -0.9539, -0.9539, -0.9539,\n",
      "        -0.9539, -0.9539], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4905472695827484\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-0.0483, -0.3678, -0.9799, -0.9799, -1.5135, -1.7377, -0.9802, -1.8772,\n",
      "        -0.0483, -2.0826], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.0435, -1.3310, -1.0435, -1.0435, -1.4174, -1.8819, -1.0435, -1.8819,\n",
      "        -1.0435, -1.8819], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2990918755531311\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3581, -1.0290, -0.5653, -1.1105], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.4446, -2.1114, -1.9173, -0.1532, -1.3581, -1.0330, -0.1532, -1.0330,\n",
      "        -2.1114, -1.3871], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4002, -1.9696, -1.9696, -1.1379, -1.5087, -1.1379, -1.1379, -1.1379,\n",
      "        -1.9696, -1.5087], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.295465886592865\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4223, -1.0677, -0.6455, -1.1623], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.0876, -1.9748, -0.2562, -1.9748, -0.2562, -2.1219, -1.8674, -0.5255,\n",
      "        -1.1739, -0.6455], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.2306, -2.0565, -1.2306, -2.0565, -1.2306, -2.0565, -2.0565, -1.4730,\n",
      "        -1.2306, -1.5809], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.37485942244529724\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-1.9499, -0.3581, -1.5009, -2.1436, -1.1601, -1.2565, -0.3581, -0.7448,\n",
      "        -1.1214, -1.2159], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.1309, -1.3223, -1.6537, -2.1309, -1.3223, -1.3223, -1.3223, -0.7790,\n",
      "        -1.3223, -1.5486], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.20984220504760742\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-1.2049, -1.5265, -1.2049, -0.7807, -0.7807, -1.3034, -1.3034, -0.8030,\n",
      "        -2.1970, -0.4262], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3836, -1.5826, -1.3836, -1.7026, -1.7026, -1.3836, -1.3836, -0.8094,\n",
      "        -2.1730, -1.3836], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2697005867958069\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-1.6953, -1.3654, -1.5054, -0.8396, -0.4806, -0.6823, -2.2122, -0.8396,\n",
      "        -2.2699, -0.8396], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7556, -1.4325, -1.7556, -1.7556, -1.4325, -1.6140, -2.2195, -1.7556,\n",
      "        -2.2195, -1.7556], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.43651705980300903\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-2.3269, -0.9059, -1.4155, -0.7373, -0.5455, -2.2978, -2.2978, -0.5455,\n",
      "        -0.9059, -1.6863], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2740, -1.8153, -1.4909, -1.6636, -1.4909, -2.2740, -2.2740, -1.4909,\n",
      "        -1.8153, -1.6636], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4309898912906647\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.5103, -2.1422, -2.0416, -1.8185], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-1.5078, -1.6730, -1.5886, -1.8255, -1.6137, -2.5382, -1.5078, -1.0013,\n",
      "        -0.6385, -0.8231], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.5746, -1.6723, 10.0000, -1.9012, -1.7407, -2.3593, -1.5746, -1.9012,\n",
      "        -1.5746, -1.7407], grad_fn=<AddBackward0>)\n",
      "LOSS: 13.688581466674805\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-1.5543, -1.5659, -1.5008, -1.8485, -1.0510, -2.3866, -1.0423, -2.3866,\n",
      "        -1.5490, -1.0423], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.3989, -1.6098, -1.7502, -1.9381, -0.9520, -2.3941, -1.9381, -2.3941,\n",
      "        -1.6098, -1.9381], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.24037912487983704\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-2.0902, -2.1413, -0.9313, -1.5970], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.5185, -0.9313, -1.6660, -0.8314, -1.0774, -2.5094, -0.7062, -2.4171,\n",
      "        -1.6892, -0.7062], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.4501, -1.6559, -1.9697, -1.7482, -1.9697, -2.4201, -1.6355, -2.4201,\n",
      "        -1.7316, -1.6355], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3995949327945709\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-2.1418, -2.4470, -1.6146, -1.6571, -0.8392, -1.1178, -0.8392, -0.7494,\n",
      "        -1.1178, -1.6234], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.9988, -2.3478, -1.6745, -2.0060, -1.7552, -2.0060, -1.7552, -1.6745,\n",
      "        -2.0060, -1.6745], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4270261228084564\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-1.6368, -0.7918, -2.4796, -1.0630, -1.6647, -1.1551, -1.1551, -0.7918,\n",
      "        -2.4796, -1.5022], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.9567, -1.7127, -2.2865, -1.6597, -1.7127, -2.0396, -2.0396, -1.7127,\n",
      "        -2.2865, -2.0396], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.40844541788101196\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-1.2178, -0.8279, -1.1728, -1.7748, -2.4993, -0.8279, -1.7748, -1.1772,\n",
      "        -0.9645, -1.1772], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7688, -1.7451, -1.0633, -2.2847, -2.2847, -1.7451, -2.2847, -2.0595,\n",
      "        10.0000, -2.0595], grad_fn=<AddBackward0>)\n",
      "LOSS: 12.434054374694824\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-1.6911, -2.2167, -0.8280, -1.3324, -0.8502, -2.4549, -0.8280, -1.7185,\n",
      "        -2.5212, -1.7185], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7250, -2.0351, -1.7250, -1.9939, -1.7652, -2.1889, -1.7250, -1.7652,\n",
      "        -2.1495, -1.7652], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.31316936016082764\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.9177, -2.1858, -2.2385, -1.2275], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.2313, -2.5284, -1.2228, -1.7273, -1.9177, -1.2313, -0.8529, -0.9177,\n",
      "        -0.8529, -0.9177], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.8955, -2.0620, -1.8955, -1.8657, -2.1047, -1.8955, -1.5944, -1.8259,\n",
      "        -1.5944, -1.8259], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4356060028076172\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-2.4039, -1.1182, -1.6868, -1.6288, -2.0328, -1.0376, -0.9198, -1.0376,\n",
      "        -1.3338, -1.3338], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.9775, -1.7924, -1.8827, -1.4703, -1.7924, -1.9338, -1.4703, -1.9338,\n",
      "        -1.7924, -1.7924], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3087722659111023\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-1.1835, -1.1835, -2.3866, -2.0554, -0.9968, -1.6038, -1.4611, -2.0320,\n",
      "        -2.0320, -1.4254], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.9223, -1.9223, -1.9009, -1.9456, -1.3669, -1.3669, -1.6923, -1.9223,\n",
      "        -1.9223, -1.6632], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16666415333747864\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-1.6109, -1.5702, -2.0989, -1.6109, -2.1519, -1.3755, -0.7435, -1.3755,\n",
      "        -2.5265, -2.2879], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.5952, -1.2730, -1.8727, -1.5952, -1.8727, -1.8295, -1.8727, -1.8295,\n",
      "        -1.8276, -1.8276], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.26055559515953064\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-1.8631, -1.1795, -1.1795, -1.7540, -2.4465, -2.0693, -1.5740, -2.4465,\n",
      "        -0.5760, -1.9630], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4098, -1.1985, -1.1985, -1.5184, -1.7773, -1.8103, -1.7483, -1.7773,\n",
      "        -1.1985, -1.5925], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1779526025056839\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-1.7131, -1.8558, -1.8558, -2.2857, -1.8319, -1.5826, -0.5232, -1.9207,\n",
      "        -1.2458, -1.3890], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.6862, -1.4709, -1.4709, -1.7605, -1.5471, -1.6862, -1.1455, -1.7753,\n",
      "        -1.4709, -1.1455], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11831215769052505\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.8626, -1.7501, -2.8350, -0.8459], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.9132, -1.8068, -1.9132, -2.5191, -0.4986, -1.6811, -1.8626, -1.7125,\n",
      "        -2.8350, -2.1065], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4487, -1.6435, -1.4487, -1.6435, -1.1083, -1.5186, -1.7613, -1.4487,\n",
      "        -1.4826, -1.7618], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.36506861448287964\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-1.2358, -1.8685, -1.5213, -1.3782, -1.4255, -1.9191, -1.7969, -1.8685,\n",
      "        -1.1555, -1.4255], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.0939, -1.4518, -1.5128, -1.5128, -1.6335, -1.7751, -1.6335, -1.4518,\n",
      "        -1.0939, -1.6335], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05234517529606819\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.3813, -1.5701, -1.6695, -0.5637], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.3813, -1.7518, -1.0550, -1.1955, -1.4415, -1.7518, -1.7977, -1.4415,\n",
      "        -1.7518, -0.8405], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.5074, -1.7894, -1.4551, -1.5592, -1.4551, -1.7894, -1.4551, -1.4551,\n",
      "        -1.7894, -1.5074], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08749519288539886\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-1.7251, -1.7021, -1.6211, -1.3112, -0.8411, -2.3127, -1.3388, -0.9677,\n",
      "        -0.6846, -1.7251], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.6162, -1.4177, -1.7974, -1.4296, -1.4924, -1.6162, -1.4510, -1.0718,\n",
      "        -1.4510, -1.6162], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16696612536907196\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-0.9840, -1.2666, -0.8488, -1.8375, -0.9021, -1.6351, -1.1825, -0.5321,\n",
      "        -1.6381, -2.2095], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4477, -1.6210, -1.4789, -1.3312, -1.0640, -1.4216, -1.4789, -1.4789,\n",
      "        -1.4477, -1.6210], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.24325580894947052\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-1.5062, -1.1113, -1.3238, -2.0813, -0.4912, -1.4482, -0.6950, -2.0813,\n",
      "        -1.0939, -0.4912], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.8032, -1.4618, -1.5570, -1.6255, -1.0561, -1.8032, -1.4421, -1.6255,\n",
      "        -1.4618, -1.0561], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.21384552121162415\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-1.0464, -1.4505, -1.4141, -0.4886, -1.5144, -1.4505, -1.4141, -1.4505,\n",
      "        -1.3291, -1.4623], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.6394, -1.4397, -1.7946, -1.0456, -1.7946, -1.4397, -1.7946, -1.4397,\n",
      "        -1.7736, -1.4327], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12287871539592743\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-1.3129, -1.4417, -1.5427, -0.5946], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.8223, -1.8223, -1.5090, -0.7132, -1.0516, -0.4815, -1.3843, -1.8223,\n",
      "        -0.9348, -0.7132], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.6419, -1.6419, -1.7715, -1.4333, -1.4175, -1.0285, -1.4333, -1.6419,\n",
      "        -1.0285, -1.4333], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16479863226413727\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.2636, -1.3382, -2.0142, -0.8354], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.0717, -1.3042, -1.3215, -1.4649, -1.4138, -0.4743, -1.1084, -1.2636,\n",
      "        -1.4639, -0.5820], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.6577, -1.6577, -1.4269, -1.3528, -1.7505, -1.0127, -1.4269, -1.7518,\n",
      "        -1.6577, -1.7518], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.26410096883773804\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-0.5715, -1.3069, -1.1150, -1.4391, -1.3879, -1.0361, -1.2821, -1.2821,\n",
      "        -1.1221, -1.3879], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7254, -1.3742, -1.6604, -1.6604, -1.7254, -1.4175, -1.4175, -1.4175,\n",
      "        -1.3713, -1.7254], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2154460847377777\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.1639, -1.2959, -1.2478, -0.4755], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.0705, -1.3829, -1.4278, -0.5603, -1.2618, -1.4278, -1.8690, -0.7216,\n",
      "        -1.1078, -0.7577], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.5043, -1.3455, -1.6494, -1.6819, -1.4005, -1.6494, -1.4761, -1.4005,\n",
      "        -1.4280, -1.3421], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2624492645263672\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-1.1120, -0.4249, -0.3487, -0.3487, -1.2493, -1.5459, -1.2493, -1.4971,\n",
      "        -1.4323, -1.2493], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4998, -0.9429, -1.3138, -1.3138, -1.3824, -1.6431, -1.3824, -1.4998,\n",
      "        -1.6431, -1.3824], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2388693392276764\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.1432, -1.5925, -1.0902, -0.3194], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.2512, -0.6631, -1.1432, -1.5530, -1.3330, -0.8298, -1.1656, -1.1432,\n",
      "        -1.4479, -0.3194], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3655, -1.2874, -1.2874, -1.4907, -1.6332, -0.9178, -1.4907, -1.2874,\n",
      "        -1.6332, -1.2874], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16232731938362122\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.1853, -1.6674, -1.0927, -0.2837], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.7360, -1.2624, -1.2624, -1.2316, -1.6320, -1.6674, -1.7205, -1.2624,\n",
      "        -0.6218, -1.4151], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.6175, -1.3463, -1.3463, -1.4753, -1.5844, -1.5597, -1.5844, -1.3463,\n",
      "        -1.2553, -1.6175], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05691887065768242\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-1.0930, -0.5100, -0.2453, -1.8221, -1.4933, -0.2453, -1.4921, -1.4921,\n",
      "        -1.2784, -0.6680], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3344, -1.5324, -1.2208, -1.6012, -1.6012, -1.2208, -1.6012, -1.6012,\n",
      "        -1.3277, -1.3277], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3528479039669037\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3968, -1.6386, -1.2250, -0.3459], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.4328, -1.3746, -0.8464, -1.1056, -1.7430, -0.5050, -1.3746, -1.3458,\n",
      "        -1.2995, -1.5217], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.5038, -1.5923, -0.8421, -1.3113, -1.5367, -1.5038, -1.5923, -1.3109,\n",
      "        -1.3109, -1.5923], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1188458651304245\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4347, -1.6977, -1.2406, -0.3179], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.4347, -0.9656, -1.9752, -0.6442, -1.8538, -0.5239, -1.4347, -0.3179,\n",
      "        -1.6360, -1.4256], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.1705, -0.8211, -1.5798, -1.2946, -1.4715, -1.1705, -1.1705, -1.2861,\n",
      "        -1.5798, -1.2946], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.22618432343006134\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4311, -1.7043, -1.2385, -0.3055], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.3799, -0.1750, -1.3799, -2.0067, -0.5091, -1.3455, -0.5091, -1.4653,\n",
      "        -1.6773, -0.3055], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.5847, -1.1575, -1.5847, -1.5847, -1.1575, -1.2874, -1.1575, -1.4582,\n",
      "        -1.3855, -1.2750], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.30965277552604675\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4291, -1.6999, -1.2376, -0.3020], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.3020, -1.3831, -0.3020, -1.4518, -1.3606, -1.4291, -1.8501, -1.4589,\n",
      "        -1.8758, -0.1582], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.2718, -1.5933, -1.2718, -1.2848, -1.2848, -1.1424, -1.5125, -1.3426,\n",
      "        -1.4571, -1.1424], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3312472403049469\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4110, -1.6648, -1.2291, -0.3108], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.3167, -0.3224, -1.9171, -1.6826, -1.6113, -1.8499, -1.7687, -1.9977,\n",
      "        -0.6760, -1.7421], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.1387, -0.8018, -1.5210, -1.6084, -1.6084, -1.5210, -1.2519, -1.6084,\n",
      "        -1.2901, -1.5063], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13835647702217102\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3807, -1.5937, -1.2122, -0.3333], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.8713, -1.7794, -0.9672, -1.6086, -0.1643, -1.6372, -0.8713, -1.7685,\n",
      "        -1.3731, -0.7114], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.8088, -1.4847, -0.8088, -1.6402, -1.1478, -1.6402, -0.8088, -1.5435,\n",
      "        -1.6402, -1.3066], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15644395351409912\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3516, -1.5216, -1.1974, -0.3577], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.1776, -1.7071, -1.3674, -1.6080, -1.1110, -0.8632, -1.3516, -1.3674,\n",
      "        -1.4135, -0.5636], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.1598, -1.5686, -1.6747, -1.6747, -1.3220, -0.8184, -1.1598, -1.6747,\n",
      "        -1.3224, -1.1598], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16243097186088562\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-0.8620, -0.3728, -1.1078, -1.2353, -1.3162, -1.6175, -0.9332, -0.3728,\n",
      "        -1.1078, -1.3782], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.8258, -1.3355, -1.3355, -1.4493, -1.1681, -1.7002, -0.8258, -1.3355,\n",
      "        -1.3355, -1.7002], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.21484115719795227\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-1.6691, -0.8697, -1.5970, -1.2666, -1.4041, -1.6397, -1.8049, -1.6397,\n",
      "        -1.5038, -0.6107], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.6119, -0.8312, -1.5497, -1.3441, -1.7178, -1.7178, -1.6119, -1.7178,\n",
      "        -1.7178, -1.1763], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05265878513455391\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.2067, -1.5604, -1.1406, -0.1982], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.4796, -0.3802, -1.3723, -0.7248, -0.6328, -1.4123, -1.4123, -1.4390,\n",
      "        -1.4123, -1.4123], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.6523, -1.3422, -1.3501, -1.5695, -1.1783, -1.3501, -1.3501, -1.7269,\n",
      "        -1.3501, -1.3501], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.20652928948402405\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.1886, -1.5296, -1.1521, -0.1947], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.6901, -1.6901, -1.4768, -1.4579, -1.5295, -1.4579, -1.6835, -1.5330,\n",
      "        -0.8176, -1.2197], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7358, -1.7358, -1.7358, -1.6778, -1.5877, -1.6778, -1.5280, -1.6778,\n",
      "        -1.3575, -1.3575], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05270785838365555\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.1771, -1.5167, -1.1685, -0.1872], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.7184, -1.4590, -1.5390, -1.5892, -1.5167, -0.8756, -1.7592, -0.3860,\n",
      "        -1.3514, -0.1872], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7391, -1.6916, -1.6916, -1.6626, -1.6006, -0.8338, -1.6626, -1.3474,\n",
      "        -1.3567, -1.1685], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.19885170459747314\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.1693, -1.5145, -1.1872, -0.1808], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.4766, -1.6781, -0.3871, -0.8220, -0.1808, -0.1808, -1.7455, -0.1808,\n",
      "        -1.5145, -1.7526], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3546, -1.7398, -1.3484, -1.3546, -1.1627, -1.1627, -1.7398, -1.1627,\n",
      "        -1.6113, -1.6740], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.41345423460006714\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.1627, -1.5185, -1.2098, -0.1837], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.1837, -1.4632, -1.1627, -0.8338, -1.4153, -1.7756, -0.1837, -0.1837,\n",
      "        -1.4934, -0.8604], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.1653, -1.7504, -1.1653, -1.3536, -1.6237, -1.7504, -1.1653, -1.1653,\n",
      "        -1.3536, -0.8315], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.33079057931900024\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.1615, -1.5285, -1.2331, -0.1907], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.1907, -0.8556, -0.1907, -1.3309, -1.2213, -1.7472, -1.3309, -1.8049,\n",
      "        -1.5705, -1.5086], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.1717, -0.8326, -1.1717, -1.3517, -1.3517, -1.7037, -1.3517, -1.7633,\n",
      "        -1.7037, -1.3517], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.19888177514076233\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.1612, -1.5414, -1.2547, -0.1996], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.1996, -0.8596, -1.6704, -1.7476, -1.4337, -1.4884, -0.4037, -0.4037,\n",
      "        -0.3887, -0.4037], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.1796, -1.3498, -1.7736, -1.7170, -1.6471, -1.7736, -1.3633, -1.3633,\n",
      "        -0.8335, -1.3633], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4299794137477875\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.1667, -1.5610, -1.2795, -0.2094], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.3289, -0.8280, -0.2094, -1.5610, -1.2457, -0.4162, -0.3894, -1.5610,\n",
      "         0.1859, -1.5367], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3505, -1.6593, -1.1884, -1.6593, -1.3505, -1.3746, -0.8327, -1.6593,\n",
      "        10.0000, -1.3505], grad_fn=<AddBackward0>)\n",
      "LOSS: 9.914567947387695\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.1688, -1.5927, -1.2940, -0.1719], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.8683, -1.7328, -1.2940, -1.7328, -1.5432, -1.7634, -0.3920, -1.2940,\n",
      "        -1.8683, -1.8683], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7663, -1.7663, -1.3528, -1.7663, -1.7663, -1.7663, -1.3528, -1.3528,\n",
      "        -1.7663, -1.7663], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10132815688848495\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.1720, -1.6272, -1.3085, -0.1374], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.4301, -0.6770, -1.3085, -0.1374, -1.3085, -0.8422, -0.8297, -1.8768,\n",
      "        -0.1374, -0.1374], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3672, -1.1236, -1.3320, -1.1236, -1.3320, -0.7240, -1.2840, -1.7468,\n",
      "        -1.1236, -1.1236], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3359917402267456\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.1734, -1.6577, -1.3211, -0.1087], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.7975, -1.4670, -0.1087, -0.2847, -0.2847, -0.1087, -1.4670, -1.1734,\n",
      "        -0.1087, -0.1087], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7021, -1.5947, -1.0979, -0.6773, -0.6773, -1.0979, -1.5947, -1.0979,\n",
      "        -1.0979, -1.0979], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4269247055053711\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.1802, -1.6852, -1.3359, -0.0900], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.6829, -1.3320, -1.6852, -1.4815, -0.8147, -1.5642, -1.5642, -1.8942,\n",
      "        -0.3392, -0.8313], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7089, -1.2399, -1.5854, -1.5854, -1.2399, -1.2399, -1.2399, -1.7332,\n",
      "        -1.3053, -0.6346], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14188553392887115\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-1.7793, -1.3664, -1.8052, -0.9334, -1.8890, -1.8890, -1.3363, -0.3374,\n",
      "        -0.3374, -0.2559], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7083, -1.2303, -1.7414, -0.6005, -1.7414, -1.7414, -1.3037, -1.3037,\n",
      "        -1.3037, -0.6005], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.21690888702869415\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.5000, -1.7162, -1.8959, -0.6527], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.8021, -1.7078, -1.8712, -0.8427, -0.9092, -0.8091, -1.7815, -1.8712,\n",
      "        -1.3260, -0.9092], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7584, -1.5874, -1.7584, -1.2314, -0.5735, -1.5874, -1.7208, -1.7584,\n",
      "        -1.3137, -0.5735], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10278169810771942\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-0.0797, -1.8426, -1.8743, -1.6420, -1.5019, -1.8426, -1.7026, -1.5003,\n",
      "        -1.4498, -0.0797], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.0717, -1.7862, -1.5717, -1.7862, -1.2380, -1.7862, -1.5931, -1.5931,\n",
      "        -1.3613, -1.0717], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.21851494908332825\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.1630, -1.6910, -1.2708, -0.0900], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.4643, -1.6910, -0.3868, -0.3868, -1.3467, -1.2708, -0.0900, -0.2765,\n",
      "        -1.8044, -1.3467], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.2488, -1.6013, -1.3481, -1.3481, -1.2488, -1.3481, -1.0810, -0.5416,\n",
      "        -1.8159, -1.2488], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2980422377586365\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-0.1071, -1.7828, -1.8636, -0.1071, -1.4298, -1.1591, -1.4298, -1.7423,\n",
      "        -0.4156, -0.9124], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.0964, -1.7838, -1.7838, -1.0964, -1.2680, -1.0964, -1.2680, -1.8489,\n",
      "        -1.3741, -1.6148], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3443319499492645\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.1559, -1.6567, -1.2170, -0.1306], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.1559, -1.7827, -1.7827, -0.4499, -0.7359, -1.7910, -1.7214, -1.7355,\n",
      "        -1.3946, -1.5955], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.1175, -1.8117, -1.8117, -1.4050, -0.5281, -1.8840, -1.8840, -1.8840,\n",
      "        -1.2916, -1.8840], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11093392223119736\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-0.1504, -0.3462, -1.5903, -1.7879, -1.4121, -1.7105, -1.7822, -1.1981,\n",
      "        -0.1504, -1.7105], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.1354, -0.5233, -1.9085, -1.9085, -1.5143, -1.9085, -1.8349, -1.4339,\n",
      "        -1.1354, -1.9085], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2234586924314499\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.1861, -1.3755, -1.1849, -0.5000], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.7025, -1.3652, -1.7049, -1.1861, -1.3652, -1.6596, -1.1559, -0.1682,\n",
      "        -0.1682, -1.1861], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5160, -1.3275, -1.9195, -1.1514, -1.3275, -1.6387, -1.1514, -1.1514,\n",
      "        -1.1514, -1.1514], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.20198340713977814\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-0.1900, -1.2026, -1.3625, -1.7853, -1.7031, -0.5171, -1.6932, -0.6895,\n",
      "        -1.2026, -1.1588], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.1710, -1.4654, -1.3440, -1.8711, -1.9302, -1.4654, -1.8711, -0.5115,\n",
      "        -1.4654, -1.1710], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2122558355331421\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.1863, -1.3953, -1.1876, -0.5181], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.2115,  0.5483, -1.1719, -1.7538, -1.7920, -1.7118, -1.7155, -1.1719,\n",
      "        -1.2952, -1.1719], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.1904, 10.0000, -1.1904, -1.9360, -1.8826, -1.9827, -1.9360, -1.1904,\n",
      "        -1.3564, -1.1904], grad_fn=<AddBackward0>)\n",
      "LOSS: 9.04610824584961\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.1871, -1.4183, -1.1905, -0.4776], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.7314, -0.1804, -0.1804, -0.4776, -1.7777, -1.3750, -1.7777, -1.1861,\n",
      "        -1.8074, -1.4554], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.9047, -1.1624, -1.1624, -1.4298, -1.9047, -1.3240, -1.9047, -1.1624,\n",
      "        -1.8638, -1.5040], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.29062697291374207\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-0.1544, -1.8122, -1.8122, -1.9019, -0.6526, -1.0542, -0.6486, -0.6621,\n",
      "        -1.3866, -0.1544], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.1389, -1.8727, -1.8727, -1.8727, -0.3778, -1.5837, -1.1389, -0.3778,\n",
      "        -1.2928, -1.1389], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.26328229904174805\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-0.1319, -0.1319, -1.0508, -1.7879, -0.2995, -1.8451, -1.7954, -0.4185,\n",
      "        -0.2995, -1.8122], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.1187, -1.1187, -1.5615, -1.6705, -0.3268, -1.8480, -1.5615, -1.3766,\n",
      "        -0.3268, -1.9457], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.32142654061317444\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4613, -1.4814, -0.7583, -0.4743], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.0596, -1.7940, -0.1184, -1.8487, -1.2194, -1.5307, -1.9527, -1.4021,\n",
      "        -1.8353, -1.7940], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.5431, -1.8310, -1.1065, -1.8309, -1.1065, -1.5431, -1.8310, -1.2534,\n",
      "        -1.8309, -1.8310], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12631066143512726\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4653, -1.8340, -1.5286, -0.4239], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.3162, -0.3953, -0.1116, -1.8086, -0.6271, -0.5834, -1.4018,  0.8415,\n",
      "        -1.2038, -0.1116], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3558, -1.3558, -1.1004, -1.8203, -0.2426, -1.1004, -1.2434, 10.0000,\n",
      "        -1.1004, -1.1004], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.720808029174805\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4654, -1.8582, -1.5307, -0.3765], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.0663, -1.3327, -0.3765, -1.4323, -1.5132, -1.8193, -1.9938, -1.0784,\n",
      "        -1.5316, -0.3585], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.0597, -1.2027, -1.3389, -1.2027, -1.6402, -1.7862, -1.7862, -1.4920,\n",
      "        -1.4920, -1.3227], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3145003020763397\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4785, -1.8766, -1.5459, -0.3678], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.4063, -1.9429, -0.3359, -0.5870, -1.9429, -0.3359, -2.0056, -0.5676,\n",
      "        -1.8628, -1.8270], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.1769, -1.7657, -1.3023, -0.0695, -1.7657, -1.3023, -1.7657, -0.0695,\n",
      "        -1.8028, -1.7657], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2564183175563812\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4783, -1.8749, -1.5409, -0.3760], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.7845, -0.0164, -1.8690, -1.1219, -1.7845, -1.3868, -1.8690,  1.1013,\n",
      "        -0.3760, -0.1853], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7645e+00, -1.0147e+00, -2.0097e+00, -1.4490e+00, -1.7645e+00,\n",
      "        -1.1668e+00, -2.0097e+00,  1.0000e+01, -1.3384e+00, -8.8650e-03],\n",
      "       grad_fn=<AddBackward0>)\n",
      "LOSS: 8.133739471435547\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4856, -1.8984, -1.5409, -0.3583], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.9036, -1.8440, -0.3583,  0.0385, -1.7989,  0.0385, -1.8803,  0.0385,\n",
      "        -1.3630, -0.3018], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7808, -1.7808, -1.3225, -0.9653, -1.7333, -0.9653, -1.4033, -0.9653,\n",
      "        -1.1197, -1.2717], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5203410983085632\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4983, -1.9008, -1.5407, -0.3714], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.2900, -1.8950, -1.8696, -0.2900, -1.9225, -0.4603, -1.1876, -1.9225,\n",
      "        -1.4811, -1.7695], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.2610, -1.7750, -1.3782, -1.2610, -1.7189,  0.2071, -0.9351, -1.7189,\n",
      "        -1.3782, -1.5718], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2783255875110626\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([ 0.0802, -1.5126, -1.7471, -1.1798, -0.3022, -0.4080, -1.8109, -1.2986,\n",
      "        -0.4080, -1.8267], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9278, -1.2583, -1.7302, -1.3816, -1.2719, -1.3672, -1.7856, -1.0846,\n",
      "        -1.3672, -1.3816], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.41469860076904297\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-1.8539,  0.0717, -1.7142, -1.7097, -0.4725, -0.4725, -1.4294, -1.8241,\n",
      "        -0.4480, -1.7142], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7586, -0.9355, -1.7586, -1.7586, -1.4252, -1.4252, -1.4032, -2.1105,\n",
      "        -0.9355, -1.7586], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3165690302848816\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-1.1534,  0.0684, -1.4524, -1.1859, -1.8264, -1.8558, -1.8264, -0.3394,\n",
      "        -0.4833, -1.6984], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3316, -0.9384, -1.3055, -1.1081, -1.7870, -1.4489, -1.7870, -1.4958,\n",
      "        -0.9384, -1.6341], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.27901798486709595\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([ 0.0638,  0.0638, -1.1109, -1.7883, -1.7601, -0.3991, -1.8124, -0.5612,\n",
      "        -1.6749,  0.0638], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9426, -0.9426, -1.3592, -1.8252, -1.8640, -1.3592, -1.4994, -1.5051,\n",
      "        -1.6678, -0.9426], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5022853016853333\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2370, -1.7183, -1.0603, -0.3935], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.3203, -1.4022, -0.1694, -1.7557, -1.6277, -1.1603,  0.0414, -1.6448,\n",
      "        -1.6448, -0.5727], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.4776, -1.5154,  0.4776, -1.8958, -1.8709, -1.1525, -0.9627, -1.5154,\n",
      "        -1.5154, -0.9627], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.23409247398376465\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([ 0.0250, -0.4522, -1.1149, -1.0442, -1.7184, -1.6007, -0.4522, -1.1034,\n",
      "        -0.6250, -0.4522], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9775, -1.4070, -1.1591, -1.4070, -1.9042, -1.5625, -1.4070, -0.9775,\n",
      "        -0.9775, -1.4070], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4049495756626129\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.5544, -1.5356, -1.3328, -0.8634], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.0029, -0.6250, -0.6848, -1.7665, -1.1037,  0.0029, -0.6250,  0.0029,\n",
      "         0.0029, -0.6848], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9974, -1.5625, -0.9974, -1.9331, -1.1669, -0.9974, -1.5625, -0.9974,\n",
      "        -0.9974, -0.9974], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5987280011177063\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.0885, -1.3346, -1.0285, -0.4993], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.6923, -0.9629, -0.7284, -0.0190, -0.9629, -1.5419, -1.9294, -0.6643,\n",
      "        -1.0927, -0.4993], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.9651, -1.0171, -1.5979, -1.0171, -1.0171, -1.6851, -1.9843, -1.5979,\n",
      "        -1.1804, -1.4494], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.36379313468933105\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.0952, -1.3278, -1.0251, -0.5301], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.0880, -0.0387, -1.7528, -1.4649, -1.5674, -0.4913, -0.0387, -0.0387,\n",
      "        -0.0387, -0.5301], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.1873, -1.0349, -1.9846, -1.7402, -1.9846, -1.5187, -1.0349, -1.0349,\n",
      "        -1.0349, -1.4771], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.623471736907959\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.1179, -1.3274, -1.0228, -0.5740], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.5293, -0.5740, -0.1746, -0.5740, -1.5622, -0.8840, -1.1179, -0.5740,\n",
      "        -0.5740, -1.1139], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7956, -1.5166,  0.5444, -1.5166, -2.0025, -1.0706, -1.0706, -1.5166,\n",
      "        -1.5166, -1.1996], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.43801528215408325\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.1517, -1.3346, -1.0337, -0.6413], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.5379, -0.6906, -0.9439, -1.3103, -1.0149, -1.5662, -0.1174, -0.1174,\n",
      "        -0.6413, -0.1174], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.8495, -1.6216, -1.1056, -1.4779, -1.2142, -2.0188, -1.1056, -1.1056,\n",
      "        -1.5772, -1.1056], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5068346261978149\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-0.7071, -1.3304, -1.8233, -1.5887, -1.0604, -0.1569, -1.0201, -1.7928,\n",
      "        -1.0162, -0.1102], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.6364, -1.4716, -2.0249, -2.0249, -1.6336, -1.1412, -1.6364, -2.4841,\n",
      "        -1.2223,  0.5417], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.37369126081466675\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-0.7258, -0.1832, -1.1044, -1.6354, -1.0337, -0.6627, -1.6354, -1.0651,\n",
      "        -1.7769, -1.6175], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.6532, -1.1649, -1.2023, -1.9940, -1.2023, -1.5964, -1.9940, -1.6532,\n",
      "        -1.9940, -1.9122], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3470545709133148\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-0.2027, -1.0716, -0.6101, -1.7098, -1.9801, -1.3961, -1.8643, -1.7098,\n",
      "        -0.7338, -0.2027], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.1824, -1.1671, -1.5491, -1.9403, -1.9403, -1.6604, -2.1010, -1.9403,\n",
      "        -1.5161, -1.1824], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3656389117240906\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2616, -1.5286, -1.2145, -0.6801], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.0786, -0.5720, -1.2616, -0.6801, -0.2302, -1.1094, -1.2216, -0.2302,\n",
      "        -1.2216, -0.5720], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.5724, -1.5148, -1.2072, -1.6120, -1.2072, -1.1387, -1.6120, -1.2072,\n",
      "        -1.6120, -1.5148], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.528788685798645\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-0.6398, -0.6398, -2.2909, -1.8660, -1.2446, -1.2535, -1.1232, -1.9260,\n",
      "        -0.2764, -0.6398], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.5758, -1.5758, -2.0851, -1.8611, -1.2488, -1.4977, -1.4977, -2.0851,\n",
      "        -1.2488, -1.5758], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.38414546847343445\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.5324, -1.9848, -1.5247, -0.5206], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.5206, -1.9476, -0.6288, -1.3096, -0.6288, -0.6288, -0.8365, -2.0906,\n",
      "        -2.2372, -0.6288], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4686, -1.8367, -1.5659, -1.4686, -1.5659, -1.5659, -1.5768, -1.8367,\n",
      "        -1.8367, -1.5659], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5221694111824036\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-1.3811, -0.6632, -1.4893, -2.1409, -0.3944, -0.5193, -0.5193, -0.9216,\n",
      "        -1.6706, -1.8533], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3550, -1.5969, -1.5969, -1.8536, -1.3550, -1.4673, -1.4673, -1.5634,\n",
      "        -1.9805, -1.9530], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.42047399282455444\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3817, -1.7157, -1.4114, -0.7068], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.7068, -0.1549, -1.3817, -0.7068, -0.1549, -0.4694, -0.5407, -1.0338,\n",
      "        -1.0985, -0.5407], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.6361,  0.5316, -1.4225, -1.6361,  0.5316, -1.4225, -1.4866, -1.5172,\n",
      "        -1.4225, -1.4866], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5708125829696655\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4290, -1.7536, -1.4498, -0.7592], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.7592, -0.5708, -0.5371, -1.5991, -2.0855, -0.5371, -0.5708, -2.1844,\n",
      "        -1.5391, -0.1535], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.6833, -1.5137, -1.4834, -1.6833, -1.9235, -1.4834, -1.5137, -1.9235,\n",
      "        -2.0408,  0.4947], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5196255445480347\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4649, -1.7996, -1.4807, -0.8163], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.1993, -0.6160, -2.1994, -1.9869, -0.8163, -0.1604, -2.1296,  1.6080,\n",
      "        -0.0264, -0.6189], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.5544, -1.5544, -2.1759, -2.0794, -1.7346,  0.4472, -2.1759, 10.0000,\n",
      "         0.4472, -1.5570], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.376020908355713\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4850, -1.8451, -1.4865, -0.8053], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.8053, -1.4865, -2.1765, -2.1765, -0.8053, -1.1983, -0.8053,  0.0246,\n",
      "        -0.5823, -1.1590], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7248, -1.5241, -1.9516, -1.9516, -1.7248, -1.5458, -1.7248,  0.5336,\n",
      "        -1.5241, -1.7248], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4225618243217468\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.5008, -1.8834, -1.4778, -0.8047], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.1127, -0.6074, -1.0670, -1.0670, -0.8047, -0.8047, -2.1779, -0.6074,\n",
      "        -1.8834, -0.5730], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.0088, -1.5466, -1.0088, -1.0088, -1.7242, -1.7242, -2.1445, -1.5466,\n",
      "        -2.0629, -1.5157], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4395101070404053\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-1.0616, -1.0616, -0.6191, -0.6191, -1.9411, -1.0616, -1.4326, -0.0249,\n",
      "        -0.6191, -1.1579], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9640, -0.9640, -1.5572, -1.5572, -1.9711, -0.9640, -1.5184,  0.6588,\n",
      "        -1.5572, -0.9640], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.31818801164627075\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.6312, -2.2133, -1.4729, -0.5904], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.4760, -2.3770, -2.6106, -1.6465, -1.1040, -0.8447,  1.8836, -2.0932,\n",
      "        -1.8347, -1.6312], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.5314, -2.4795, -2.4795, -1.7603, -0.9388, -1.7603, 10.0000, -1.9936,\n",
      "        -2.1103, -1.7603], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.688716888427734\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-0.9549, -0.6212, -2.0458, -0.9549, -0.8066, -1.4588, -2.4077, -0.6212,\n",
      "        -2.6515, -2.2120], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.8476, -1.5591, -1.9457, -0.8476, -1.7260, -1.0537, -2.4574, -1.5591,\n",
      "        -2.4574, -2.1180], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.28505873680114746\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.6245, -2.2801, -1.4173, -0.4871], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.6627, -0.7797, -1.0402, -2.1519, -2.2130, -0.9042,  2.1687, -0.4871,\n",
      "        -2.0428, -1.0063], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.5439, -1.7017, -0.7689, -2.1406, -2.0937, -0.7689, 10.0000, -1.4384,\n",
      "        -1.9057, -0.7689], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.327935218811035\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-0.5511, -0.7265, -0.5511,  0.3790, -1.6576, -2.4456, -0.5511, -1.6576,\n",
      "        -1.4391, -0.9139], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4960, -1.6539, -1.4960,  1.1432, -1.4960, -2.4196, -1.4960, -1.4960,\n",
      "        -1.3687, -0.6589], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4245362877845764\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9302, -1.7816,  0.0530, -1.4363], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.5131, -1.7816, -0.5131, -0.5131, -2.1258, -0.6819, -0.6819,  2.5723,\n",
      "        -1.5286, -1.8613], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4618, -2.2626, -1.4618, -1.4618, -2.0031, -1.6137, -1.6137, 10.0000,\n",
      "        -1.4618, -0.5435], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.159506320953369\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9484, -1.8529,  0.0771, -1.4142], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.6387, -2.5403, -0.6346, -2.5403, -0.6346, -2.1298, -0.7069, -1.6289,\n",
      "        -0.6346, -2.0773], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.9584, -2.0682, -1.5711, -2.0682, -1.5711, -2.0682, -0.4425, -1.4225,\n",
      "        -1.5711, -1.9584], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.36703968048095703\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9511, -1.8680,  0.1046, -1.4225], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.5728, -1.5046, -2.0100, -1.9099, -1.9435, -2.5031, -2.5031,  0.5728,\n",
      "        -0.4614, -1.9435], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.7458, -1.5815, -1.9658, -0.3800, -1.5675, -2.1038, -2.1038,  1.7458,\n",
      "        -1.4153, -1.5675], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6611955761909485\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9653, -1.8407,  0.1551, -1.4975], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.1551, -0.7289, -1.9861, -1.7775, -1.7775,  0.5109, -1.9653, -1.5358,\n",
      "         0.7063,  3.1807], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.8604, -1.6560, -2.0246, -1.6120, -1.6120,  1.8626, -1.9173, -1.6560,\n",
      "         1.8626, 10.0000], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.163112640380859\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-0.7721, -0.7721, -0.5649, -2.5964, -2.3252, -2.2532, -0.5649, -1.4583,\n",
      "        -0.7721,  0.2089], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.6949, -1.6949, -1.5084, -1.6254, -2.3011, -0.8120, -1.5084, -1.6949,\n",
      "        -1.6949, -0.8120], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.8453792333602905\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-1.4916, -0.8525, -1.3211, -1.5542, -1.9170, -2.2159, -0.8525,  0.7719,\n",
      "         0.5991,  0.1513], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.5776, -1.7672, -1.3027, -1.5776, -2.1027, -1.9959, -1.7672,  2.2101,\n",
      "         2.2101, -0.8638], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.7459079623222351\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8745, -1.9625, -1.3756, -1.1741], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.3663, -1.6902, -0.7009, -0.3633, -1.6902, -1.8843, -2.1278, -1.3146,\n",
      "        -1.6193, -0.7009], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.8434, -1.7607, -1.6308, -1.3269, -1.7607, -2.1295, -2.4810, -1.3269,\n",
      "        -1.7607, -1.6308], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3100753724575043\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.5072, -1.8225, -1.2958, -0.9618], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.3679, -0.4078, -1.5920, -1.9804, -0.9618, -1.7106, -0.7568,  1.0571,\n",
      "        -0.9618, -0.4101], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9535, -1.3670, -1.8016, -2.5396, -1.8657, -1.6811, -1.6811,  2.4272,\n",
      "        -1.8657, -0.4113], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5814382433891296\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.5187, -1.7654, -1.2929, -1.0009], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.7654, -1.3615, -1.5247, -1.8065, -1.5765, -1.2929, -1.7900, -1.6701,\n",
      "        -0.8228, -1.5280], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.1235, -0.3946, -1.8383, -1.4154, -1.8383, -1.4154, -1.0180, -2.0814,\n",
      "        -1.7405, -1.7405], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3050448000431061\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.5517, -1.7155, -1.3313, -1.0470], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.1976, -0.5503,  1.4549, -1.0470, -1.7012, -0.5210, -0.8796, -1.2293,\n",
      "        -1.0470, -1.0470], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.1064, -0.4131,  2.5678, -1.9423, -1.1231, -1.4689, -1.7916, -1.9423,\n",
      "        -1.9423, -1.9423], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6243270635604858\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([ 1.5618, -0.6130, -1.9405, -0.9696,  1.4944, -0.9971, -1.4630, -1.6068,\n",
      "        -1.5941, -0.3628], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 2.5971, -1.5517, -1.8973, -1.8727,  2.5971, -0.4603, -1.9775, -1.8973,\n",
      "        -1.8727, -0.4603], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.47105875611305237\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4677, -1.5223, -1.0991, -0.7004], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.3671, -1.8606, -1.0604,  3.9960, -0.4127, -0.7004, -0.4127, -1.6172,\n",
      "         0.5322, -1.0604], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.6303, -1.9544, -1.9544, 10.0000, -1.3714, -1.6303, -1.3714, -1.9015,\n",
      "         2.5964, -1.9544], grad_fn=<AddBackward0>)\n",
      "LOSS: 4.476935386657715\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4424, -1.4450, -1.1070, -0.6630], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.5240, -0.3230, -0.3230, -0.6630, -1.3713, -1.9261, -0.9928, -0.9928,\n",
      "        -0.6630,  0.6386], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.3265, -0.4252, -0.4252, -1.5967, -1.5967, -1.5967, -1.8935, -1.8935,\n",
      "        -1.5967,  2.7086], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.8475144505500793\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4144, -1.3894, -1.1013, -0.6174], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.2868,  1.9255, -1.9073, -0.9163, -1.5658, -0.9163, -0.9163, -0.6174,\n",
      "        -1.7423,  1.9255], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.6076,  2.7917, -2.6787, -1.8246, -1.6828, -1.8246, -1.8246, -1.5556,\n",
      "        -2.0506,  2.7917], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5662922859191895\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4140, -1.3512, -1.0940, -0.5968], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.8791, -0.1815, -0.8791, -2.2027, -0.1140, -0.8791, -2.3266, -1.6340,\n",
      "        -0.8791, -0.8791], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7912, -0.1378, -1.7912, -2.1743, -0.1378, -1.7912, -2.1743, -2.5805,\n",
      "        -1.7912, -1.7912], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5081917643547058\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-1.4162, -0.5503, -1.5482, -1.5482, -0.8806, -0.7539, -1.7147, -0.5801,\n",
      "        -1.0423, -1.6273], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7926, -0.0267, -1.4953, -1.4953, -1.7926, -1.6785, -1.9380, -1.5221,\n",
      "        -1.9380, -1.7926], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3874983489513397\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-2.3401, -1.7732, -1.3177, -1.0239, -1.7207, -1.5045, -1.5893, -1.0492,\n",
      "        -1.0492, -1.0492], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.4660, -1.9443, -1.7403, -1.3910, -1.8173, -2.1434, -1.8173, -1.9443,\n",
      "        -1.9443, -1.9443], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.32314568758010864\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-1.6172, -0.8509, -1.6596, -0.8084, -0.5904, -0.8084, -1.5242, -0.8084,\n",
      "         2.4525, -1.6618], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.2787, -1.7658, -2.4021, -1.7276, -1.5314, -1.7276, -1.7276, -1.7276,\n",
      "         2.9759, -2.1522], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5478678345680237\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-0.7981, -0.7981, -0.6310, -1.7792, -0.6310, -1.0691, -0.7981, -0.7981,\n",
      "        -1.4783, -1.6242], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7183, -1.7183, -1.5679, -2.3305, -1.5679, -1.9621, -1.7183, -1.7183,\n",
      "        -1.9621, -1.2091], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6650463938713074\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-1.7236, -0.8353, -1.7728, -1.5650,  0.2959, -1.2741, -0.8353, -0.8353,\n",
      "        -0.9985, -1.2741], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2818, -1.7518, -1.7518, -1.5995,  0.2523, -1.1789, -1.7518, -1.7518,\n",
      "        -1.1789, -1.1789], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.28855425119400024\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8422, -1.7034, -1.5504, -0.9000], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.2469, -1.2469, -0.9000, -1.5833, -1.0129, -1.5504, -0.7833, -0.9000,\n",
      "        -1.0129, -1.6526], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.1554, -1.1554, -1.8100, -1.5631, -1.9116, -1.7050, -1.7050, -1.8100,\n",
      "        -1.9116, -1.1554], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.44092756509780884\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-1.1532, -1.1532, -1.3358, -1.9441, -1.9441,  4.4901, -1.2470, -1.2470,\n",
      "        -1.5383, -1.1532], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.0379, -2.0379, -2.0379, -2.2022, -2.2022, 10.0000, -1.1736, -1.1736,\n",
      "        -1.9790, -2.0379], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.3538615703582764\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.0718, -1.6083, -1.3949, -0.9134], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.6556, -1.6083, -0.1439, -1.6374, -0.5109, -1.1873,  4.6400, -0.1439,\n",
      "         0.4482, -1.2074], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4923, -1.9901,  0.3665, -1.4923,  0.3665, -1.9901, 10.0000,  0.3665,\n",
      "         0.3665, -1.1295], grad_fn=<AddBackward0>)\n",
      "LOSS: 3.087124824523926\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1529, -1.6780, -1.3978, -0.9328], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.2234,  1.6028,  0.5119, -1.9060, -1.0505, -1.0840, -0.9328, -1.6840,\n",
      "        -2.1529, -2.6457], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.0010,  3.4042,  0.4425, -1.9176, -1.9454, -1.0010, -1.8395, -1.4286,\n",
      "        -1.9454, -1.9176], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5567919015884399\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([ 3.0339, -0.4141, -1.0748, -1.1476, -0.9342, -1.9699, -1.0748, -1.7167,\n",
      "        -1.0080, -0.9573], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.5881,  0.5561, -1.9673, -1.9673, -1.8408, -2.0329, -1.9673, -1.7998,\n",
      "        -1.9685, -0.8891], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5273353457450867\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-1.3739, -1.0538, -0.8624, -1.0538, -1.0538, -1.1510, -1.0538,  0.6870,\n",
      "        -0.9958, -2.0309], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.8963, -1.9484, -0.8277, -1.9484, -1.9484, -1.9484, -1.9484,  0.6047,\n",
      "        -1.8963, -2.0359], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4928819537162781\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2220, -2.1505, -1.3273, -1.2638], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.0233, -1.0616, -1.0233, -1.0962, -2.1739, -1.6966, -1.0616, -1.0616,\n",
      "        -0.7224, -0.2209], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.9210, -1.9555, -1.9210, -1.9865, -1.9210, -1.9865, -1.9555, -1.9555,\n",
      "        -1.6501,  0.6427], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6555840969085693\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2969, -2.0871, -0.6991, -1.2597], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.3345, -1.8761, -1.1177, -2.2049, -1.1177, -1.1177, -2.1613,  1.8106,\n",
      "        -0.7680, -1.1177], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.0059, -1.6292, -2.0059, -2.1460, -2.0059, -2.0059, -1.9593,  3.9639,\n",
      "        -0.7733, -2.0059], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.8348868489265442\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2342, -2.0795, -0.6822, -1.2513], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.7867, -1.0944, -2.2342,  0.4955, -1.9091, -2.0959, -0.6822,  3.5149,\n",
      "         0.7867, -2.2562], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.6786, -1.9850, -2.3282,  0.6786, -1.6140, -1.9956, -1.6140,  4.0267,\n",
      "         0.6786, -2.1983], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.20894308388233185\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-1.6580, -0.7069,  0.2891, -1.0817, -1.1566, -0.7118, -1.9263, -0.7118,\n",
      "        -0.7069, -2.0772], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3639, -0.7398,  0.7162, -1.9735, -2.0409, -1.6406, -1.6406, -1.6406,\n",
      "        -0.7398, -2.2732], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.36939746141433716\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2416, -2.2279, -1.4525, -1.5015], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.0893,  0.2623, -1.4332, -0.7857, -1.0893, -1.2180, -1.0893, -2.0885,\n",
      "        -1.9844, -1.2180], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.9804,  0.7326, -2.2899, -1.7072, -1.9804, -2.0962, -1.9804, -2.3073,\n",
      "        -2.0962, -2.0962], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5788928866386414\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2514, -2.2390, -1.4899, -1.5673], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.2390, -1.1533, -2.2564, -1.5451, -1.1533, -1.1073, -1.3027, -1.8808,\n",
      "        -0.6759, -1.1533], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4490, -2.0380, -2.3905, -2.1725, -2.0380, -0.7167, -2.1725, -2.0380,\n",
      "        -0.7167, -2.0380], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.43189746141433716\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-2.6436, -2.1406, -2.3379, -2.2656, -1.2516, -0.6971,  0.8090, -2.6436,\n",
      "        -1.8063, -1.2516], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.5034, -2.3606, -3.2170, -2.2493, -2.1265, -0.7657,  0.6781, -2.5034,\n",
      "        -2.4883, -2.1265], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2878180742263794\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2190, -2.2068, -1.5141, -1.6623], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.8011, -2.2016,  0.7919, -2.6972,  0.7115, -1.4636, -2.6972, -2.0104,\n",
      "        -2.0460, -1.4636], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.3172, -2.3627,  0.6477, -2.6210,  0.6477, -2.3172, -2.6210, -1.9374,\n",
      "        -2.9107, -2.3172], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2539310157299042\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-2.2659, -1.4756, -1.5231, -1.0955, -2.5030, -1.4892, -1.4756, -1.8115,\n",
      "        -0.9702,  3.9734], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.3708, -2.3280, -2.3708, -1.9859, -2.3280, -2.3403, -2.3280, -2.6303,\n",
      "        -1.7400,  4.1581], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5028088092803955\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4475, -1.7426, -1.2785, -0.8671], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.6177, -2.2087, -1.9098, -2.5397, -1.6177, -1.5923, -0.7623, -1.0372,\n",
      "        -1.5923, -2.3964], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.4560, -2.4330, -2.6977, -2.4560, -2.4560, -2.4330, -0.9407, -1.7804,\n",
      "        -2.4330, -2.3366], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.40849313139915466\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2469, -2.3148, -1.5177, -1.7708], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.8676, -1.2573, -2.7528, -2.0508, -2.6015, -1.2573,  4.0060,  0.7348,\n",
      "        -1.1765, -1.9240], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.0503, -2.1316, -3.2381, -2.5868, -2.5868, -2.1316,  4.1665,  0.5513,\n",
      "        -1.7696, -2.7316], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.314848929643631\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-1.3726, -1.9528, -1.5627, -1.6923, -1.7893, -2.8946, -3.1418, -1.8457,\n",
      "        -1.8457, -1.5627], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2353, -2.7576, -2.4064, -2.5231, -2.2353, -3.2703, -3.0817, -2.6611,\n",
      "        -2.6611, -2.4064], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5179436802864075\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([ 1.6534, -1.4521, -2.5565, -2.0111, -2.0111, -2.0997, -1.4521, -0.3277,\n",
      "        -2.0111,  5.7488], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.1739, -2.3069, -2.7597, -2.8100, -2.8100, -2.8100, -2.3069,  0.4881,\n",
      "        -2.8100, 10.0000], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.901322364807129\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.8111, -2.8103, -1.5107, -2.1297], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.9550, -2.7724, -2.2612, -3.4203, -1.9550, -1.7798, -2.6894, -2.2612,\n",
      "        -1.7509, -1.9550], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.7595, -3.2291, -2.7595, -3.2291, -2.7595, -1.2506, -3.1561, -2.7595,\n",
      "        -2.5758, -2.7595], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3861702084541321\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.8580, -2.9110, -1.5894, -2.1128], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.1655, -1.9282, -3.0740, -2.7224, -2.4507, -1.5894, -1.6225, -2.9110,\n",
      "        -1.9282, -1.7272], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.2237, -2.7354, -3.2057, -3.2329, -2.4305, -2.4305, -1.4813, -2.5545,\n",
      "        -2.7354, -2.5545], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3123595118522644\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-1.8305, -1.9122, -1.9191, -2.7069, -0.6164, -3.4393, -2.9763, -2.4999,\n",
      "        -2.4838,  1.9918], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4060, -2.7210, -2.7272, -3.2290, -1.1979, -3.3719, -2.6394, -2.7272,\n",
      "        -2.7272,  4.4541], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.8390178680419922\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-2.3750, -1.8069, -2.5846, -2.6163, -1.7860, -1.8503, -1.6152, -2.7880,\n",
      "        -1.8503, -2.8707], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.6653, -1.3126, -2.5637, -2.6647, -2.6074, -2.6653, -1.3126, -2.6074,\n",
      "        -2.6653, -3.1898], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2560428977012634\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([ 4.1246, -2.6312, -1.8096, -1.8261, -1.7933, -1.7933, -1.3043, -1.7933,\n",
      "        -2.8823, -1.7872], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.5318, -2.6435, -2.6287, -2.6435, -2.6140, -2.6140, -1.0184, -2.6140,\n",
      "        -3.1443, -2.6085], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4350261092185974\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([ 4.1474, -1.9259, -1.7535, -2.9986, -2.6634,  0.0444, -2.5406, -1.3112,\n",
      "        -3.0083, -2.9986], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.5510, -2.7333, -2.5781, -2.6418, -3.0886,  1.1810, -2.6171, -0.9600,\n",
      "        -2.6852, -2.6418], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.345572292804718\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.0074, -3.2082, -2.9487, -1.8629], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.9672, -1.7367, -2.8011, -2.2055, -1.9971, -2.2055, -3.0074, -1.7367,\n",
      "        -1.8629, -1.7367], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.2360, -2.5630, -1.2285, -2.7331, -2.7974, -2.7331, -2.6766, -2.5630,\n",
      "        -2.6766, -2.5630], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6562455892562866\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.9987, -3.1753, -3.0647, -1.9225], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.2867, -3.0647, -2.6103, -1.8095, -1.9225, -1.8095, -1.3350, -2.7078,\n",
      "        -2.0586,  0.2150], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.7302, -2.5821, -2.8527, -2.6286, -2.7302, -2.6286, -1.3179, -1.3179,\n",
      "        -2.6286,  1.2596], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5830652713775635\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-1.8247, -1.5092, -1.8247, -1.8247, -2.1985, -1.1550, -1.9770, -3.1284,\n",
      "        -2.8423, -3.1284], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.6422, -0.7330, -2.6422, -2.6422, -2.9264, -0.7330, -2.7793, -2.6422,\n",
      "        -2.6422, -2.6422], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4471873641014099\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.9179, -2.9611, -3.1395, -2.0431], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.2774, -2.4018, -1.9745, -1.8806, -2.2824, -2.0431, -2.7991, -2.7991,\n",
      "        -2.9259, -2.4102], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.0088, -3.6039, -2.7771, -2.6926, -2.7130, -2.8388, -2.7771, -2.7771,\n",
      "        -3.1781, -2.8388], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.43501797318458557\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-3.4244, -2.8597,  4.2848, -2.0670, -1.7674, -2.3273, -2.0670,  0.9649,\n",
      "        -3.0670, -1.1171], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2269, -2.8603,  4.4070, -2.8603, -0.6987, -2.6987, -2.8603,  1.1440,\n",
      "        -2.7112, -1.7920], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3206901550292969\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-2.1101,  0.2529, -3.0119, -2.1912, -2.1912, -2.3434, -2.9236, -2.1101,\n",
      "        -2.1101,  0.9540], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.8991,  1.0875, -2.8991, -2.9721, -2.9721, -2.6996, -2.9721, -2.8991,\n",
      "        -2.8991,  1.0875], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.394348680973053\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-2.0203,  4.3308, -2.1585, -3.5812, -2.5681, -2.2887, -1.8494, -2.2887,\n",
      "        -0.1187,  0.9224], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.8183,  4.3000, -2.9427, -3.3582, -2.6986, -3.0598, -0.8048, -3.0598,\n",
      "        -0.8048,  1.0155], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4079151153564453\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3417, -2.3430, -3.0161, -2.5295], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.9140, -3.1358, -3.1358, -2.2166, -2.0910, -2.0910,  5.8338, -1.0949,\n",
      "        -2.0910, -2.9140], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4277, -2.9949, -2.9949, -2.9949, -2.8819, -2.8819, 10.0000, -0.8549,\n",
      "        -2.8819, -3.4277], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.0464694499969482\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.2685, -2.0957, -2.5699, -1.9809], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.0373, -2.2653, -3.6063, -1.7692, -1.7692, -2.5037, -2.3348, -2.4574,\n",
      "        -1.0709, -3.2656], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.6887, -3.0388, -3.4445, -0.8545, -0.8545, -2.6887, -2.6887, -3.2117,\n",
      "        -0.8545, -3.2117], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5803026556968689\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-2.2343, -3.3442, -2.8379, -3.1189, -2.6021,  0.9463, -2.3325, -2.7693,\n",
      "        -2.5472, -1.6906], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.9497, -3.2415, -3.2415, -3.2925, -3.2925,  0.8996, -3.0993, -3.2321,\n",
      "        -3.2925, -0.9468], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.31052184104919434\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-3.0347, -2.7436, -3.2087, -2.6383,  0.7293, -2.3695, -2.6383, -3.2405,\n",
      "        -2.6383, -2.3695], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.9236, -2.9236, -3.2400, -3.2400,  0.8503, -2.9236, -3.2400, -2.9236,\n",
      "        -3.2400, -2.9236], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.18608513474464417\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-2.6003, -3.1830, -2.9469, -2.6003, -0.2942, -3.2205, -2.8170, -2.3300,\n",
      "        -0.9770,  1.5783], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.1376, -3.1376, -2.8535, -3.1376,  0.7655, -2.8535, -3.1280, -2.4632,\n",
      "        -1.2648,  0.7655], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.27038753032684326\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.2609, -2.2243, -3.1733, -2.9746], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-3.1733, -0.9007, -3.0650, -1.9613, -1.6209, -2.7757, -3.6600, -3.4737,\n",
      "        -2.4766, -0.9007], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.7763, -1.3679, -3.0019, -2.3428, -2.0409, -3.0260, -2.7743, -3.2289,\n",
      "        -2.9623, -1.3679], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.20629878342151642\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-2.8561, -2.9944, -3.5306, -3.4489, -2.9110, -0.4770,  1.0325, -1.8507,\n",
      "        -1.8507, -3.2736], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.9514, -2.7582, -3.1578, -2.9316, -2.9771,  0.6451,  0.6451, -2.2765,\n",
      "        -2.2765, -2.9771], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.23355846107006073\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.5382, -2.3506, -3.4718, -3.3105], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-3.1002, -3.4221, -1.8358, -0.8466, -3.1002,  1.0258, -3.0067,  1.0258,\n",
      "         1.0258, -3.1002], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.7410, -2.8900, -2.9263, -1.4222, -2.7410,  0.5851, -2.9474,  0.5851,\n",
      "         0.5851, -2.7410], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2776990830898285\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-3.5445, -2.0283, -3.0222, -3.6192], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.9183, -3.0573, -2.1576, -0.9150, -3.3236, -1.7888, -1.9183, -2.1576,\n",
      "        -3.1921, -3.1921], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.9029, -2.9419, -3.1030, -1.3780, -2.9419, -2.2341, -2.9029, -3.1030,\n",
      "        -2.8783, -2.8783], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.44949978590011597\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-1.9890, -1.4086, -1.8186, -1.6743], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-3.1725, -3.2369, -2.8251, -2.8905, -2.1954, -2.8251, -3.1114, -1.8112,\n",
      "        -2.8251,  0.5964], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.8826, -2.8826, -2.8767, -2.8767, -3.0912, -2.8767, -2.9759, -2.2546,\n",
      "        -2.8767,  0.5414], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1238335594534874\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-3.0466, -1.2705, -3.1440, -3.5646, -3.0947, -1.8956, -3.1616,  4.2213,\n",
      "        -3.5646, -2.2484], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.0236, -1.8602, -2.8965, -3.0806, -2.7877, -2.8717, -3.0236,  4.6720,\n",
      "        -3.0806, -3.0806], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.28396159410476685\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-3.0836, -0.9569,  0.8826, -3.0836, -3.0408, -2.7877, -3.1473, -0.2196,\n",
      "        -1.0260,  0.6266], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.9231, -1.1977,  0.5287, -2.9231, -2.9231, -2.8692, -3.0928,  0.5287,\n",
      "        -1.7753,  0.5287], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13892164826393127\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-2.8235, -2.3482, -3.1887, -2.9148], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.9891, -2.3482, -2.9308, -2.7630, -2.9562, -2.9891,  4.8970, -0.3066,\n",
      "        -2.8235,  0.8318], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.9625, -2.8666, -2.8752, -2.8839, -2.8752, -2.9625,  4.7341, -1.0815,\n",
      "        -3.1134,  0.5324], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10952283442020416\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-3.0139, -2.0906, -2.9065, -3.2116], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.9206, -2.8603,  1.7163, -3.0847, -2.7868, -0.9765, -2.8603, -2.8868,\n",
      "        -2.8603, -2.9264], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.9233, -2.9233,  4.7795, -3.2526, -3.2526, -1.5508, -2.9233, -3.0013,\n",
      "        -2.9233, -3.2526], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.00897216796875\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-2.7447, -2.8098, -2.7722, -2.7447, -2.9077, -3.0110, -0.6712,  1.8493,\n",
      "        -3.1985, -0.7379], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.9187, -2.9187, -2.9921, -2.9187, -2.8681, -2.9921, -0.7877,  4.7727,\n",
      "        -2.8485, -0.7877], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.8807443380355835\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2564, -1.4038, -1.3568, -0.8268], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.9306, -2.1832, -2.5170, -2.8780, -2.3397, -0.6242, -0.3275, -0.9171,\n",
      "        -2.5736, -2.9306], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2653, -2.8497, -3.1057, -2.9649, -2.8027, -0.6219, -0.6219, -0.6219,\n",
      "        -3.1057, -3.2653], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1693783551454544\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.0571, -1.9305, -2.6911, -2.4406], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.8767, -2.9007, -1.9465, -2.5143, -2.7779, -2.9007, -2.9505, -0.6777,\n",
      "        -2.9007, -2.5335], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2451, -3.2451, -2.4278, -2.8469, -2.9275, -3.2451, -2.8469, -1.1216,\n",
      "        -3.2451, -2.8469], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11621613800525665\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2011, -1.2948, -1.2711, -0.6836], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.6585, -2.5682, -2.7979, -2.5354, -3.3669, -2.9599, -0.6836, -2.5587,\n",
      "        -2.0552, -2.5682], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.6583, -2.6762, -2.7726, -2.8592, -2.8592, -3.1919, -2.6583, -2.9806,\n",
      "        -2.3748, -2.6762], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.46199360489845276\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-3.0468, -2.4993, -3.0468, -2.5814, -2.5767, -3.0468, -2.5767, -2.4363,\n",
      "        -2.6414, -2.8088], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.0877, -2.6654, -3.0877, -3.0877, -2.7647, -3.0877, -2.7647, -3.0877,\n",
      "        -2.8728, -2.6654], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08581385016441345\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-2.8552, -1.5561, -2.6082, -2.6962], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.6962, -2.8378, -3.2293, -0.6778, -2.5618, -2.1662, -2.1662, -2.7840,\n",
      "        -0.4236, -2.6665], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.7557, -2.5449, -2.9496, -0.9967, -2.5449, -2.7557, -2.7557, -2.7557,\n",
      "        -0.4264, -2.6562], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09653374552726746\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-1.7386, -1.1385, -1.6723, -1.2241], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.6139,  1.2961, -2.7273, -1.2575, -3.0482,  1.0871, -3.3385, -2.1500,\n",
      "         4.4112, -2.6139], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.4539,  1.2386, -2.5669, -0.9764, -2.3788,  1.2386, -2.8526, -2.1318,\n",
      "         4.4102, -2.4539], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08668075501918793\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-1.7501, -1.0881, -1.6719, -1.2513], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.3214, -1.2040, -0.3930, -2.7611, -2.7649,  1.1230, -1.9882, -2.5883,\n",
      "        -2.1592, -2.7611], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4280, -0.9657, -0.4280, -2.5089, -2.2689,  1.2728, -2.5883, -2.2689,\n",
      "        -2.0836, -2.5089], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09329818189144135\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-0.5475, -2.7627, -2.8594, -1.2035, -2.0284, -3.3415, -2.6228, -2.7852,\n",
      "        -2.7852, -2.7627], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9652, -2.4714, -2.3533, -2.0462, -2.2266, -2.7577, -2.3533, -2.3533,\n",
      "        -2.3533, -2.4714], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2136119306087494\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-1.7167, -1.0573, -1.5752, -1.2182], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.5997, -2.8411, -2.4339, -2.8150, -2.8422,  1.4419, -1.7167,  0.0431,\n",
      "        -2.4339, -2.5682], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.0860, -2.5379, -2.2274, -2.4627, -2.4627,  1.3627, -1.9516, -0.3599,\n",
      "        -2.2274, -2.3586], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09495148062705994\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-2.4865, -3.2676, -2.5388, -2.6032, -2.4865,  1.2787, -2.6032, -0.1956,\n",
      "        -2.8449, -2.4865], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.3701, -2.8027, -2.5483, -2.4740, -2.3701,  1.4172, -2.4740, -0.2892,\n",
      "        -2.2922, -2.3701], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.062371283769607544\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.0076, -1.2715, -1.8007, -1.6589], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.3762, -0.1044, -2.0548, -2.5011, -2.5011, -2.3762, -3.1863, -1.7904,\n",
      "         0.8327, -3.1863], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.3949, -0.2090, -2.5752, -2.5007, -2.5007, -2.3949, -2.8493, -2.0451,\n",
      "         1.4775, -2.8493], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09902218729257584\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9911, -1.3230, -1.7788, -1.6558], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.5618, -1.7002, -0.4961, -2.2733, -1.2721, -2.4843, -3.0586, -0.0601,\n",
      "        -0.0601,  1.5913], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.5302, -2.3556, -0.1411, -2.4227, -2.1907, -2.9014, -2.5302, -0.1411,\n",
      "        -0.1411,  1.5117], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1895255148410797\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9733, -1.3575, -1.7329, -1.6386], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.2915, -0.4984, -2.5626, -2.3460, -1.7431, -0.0234, -2.3460, -2.3460,\n",
      "        -2.3460, -2.2158], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.6230, -0.0892, -2.6230, -2.5688, -2.3789, -0.0892, -2.5688, -2.5688,\n",
      "        -2.5688, -2.4451], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09406668692827225\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9715, -1.3733, -1.7025, -1.6557], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.2279e+00, -4.8478e-01, -3.5450e-03, -2.9715e+00, -2.3507e+00,\n",
      "         4.1239e+00, -2.4532e+00, -2.2279e+00, -1.8186e+00, -2.4866e+00],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.4453, -0.0739, -0.0739, -2.9465, -2.5947,  4.1267, -2.5947, -2.4453,\n",
      "        -2.2970, -2.3770], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05892520025372505\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-1.3745, -2.6745, -2.3843, -2.3843, -2.3843, -0.5564,  3.7930, -1.7757,\n",
      "        -1.8387,  5.6546], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2371, -2.2804, -2.6087, -2.6087, -2.6087, -0.7479,  4.0891, -2.1674,\n",
      "        -2.2781, 10.0000], grad_fn=<AddBackward0>)\n",
      "LOSS: 2.04036283493042\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9643, -1.3629, -1.6733, -1.7018], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.0946,  0.1202,  0.0993, -2.4262, -2.3782, -2.3342, -2.1298, -1.7710,\n",
      "        -2.9406, -2.9406], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.4039, -0.0341, -0.0341, -2.6141, -2.6141, -2.6141, -2.5829, -2.1643,\n",
      "        -2.9169, -2.9169], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06677772104740143\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9648, -1.3440, -1.6716, -1.7271], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.4746, -2.9488, -2.0971, -2.7448, -2.4746, -1.3527, -1.5297, -2.4876,\n",
      "        -2.4746,  0.1283], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.6047, -2.8906, -2.3767, -2.6047, -2.6047, -2.2175, -2.2061, -2.8906,\n",
      "        -2.6047,  0.0068], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15344774723052979\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9826, -1.3210, -1.6725, -1.7625], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.2874, -2.1286, -1.9459, -2.9220, -0.3186, -0.4801, -2.0594, -1.7929,\n",
      "        -1.7929, -2.9617], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.1419, -2.3540, -2.1587, -2.8535, -0.6760, -0.6760, -2.5111, -2.1419,\n",
      "        -2.1419, -2.8535], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14562365412712097\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-2.1691, -2.9339,  4.1942, -1.8191, -2.9834, -2.1691, -1.2999, -2.6234,\n",
      "        -2.9339,  1.7052], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.3335, -2.8218,  4.4136, -2.1273, -2.8218, -2.3335, -2.1699, -2.5858,\n",
      "        -2.8218,  1.7115], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10067267715930939\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.0257, -1.2842, -1.7110, -1.8409], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.3137, -1.2398,  0.0987, -2.2747,  1.7115, -1.9936, -2.6905, -1.2842,\n",
      "        -2.4292, -1.1660], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.3182, -0.6179,  0.0564, -2.7942,  1.7348, -2.4401, -2.5776, -2.1558,\n",
      "        -2.5776, -2.1558], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.26325780153274536\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.0432, -1.2553, -1.7253, -1.8791], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.2776,  0.1338, -2.2832, -1.8791, -2.3490, -1.3349, -1.6877, -2.0916,\n",
      "        -2.7631, -2.4309], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2990,  0.0469, -2.2015, -2.0377, -2.2990, -2.0377, -2.0673, -2.0673,\n",
      "        -2.5604, -2.4067], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07225428521633148\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-2.4611, -2.8243, -1.5292, -3.0669, -1.9033, -1.7031, -1.7031, -2.7587,\n",
      "        -3.0669, -2.3498], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.5434, -2.5434, -2.1558, -2.7303, -2.0771, -2.0421, -2.0421, -2.5434,\n",
      "        -2.7303, -2.7303], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1156102791428566\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6248, -1.1026, -1.8507, -2.0306], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.0950,  0.0422, -3.0644, -1.7351, -2.3772, -2.8682, -0.3566, -2.5540,\n",
      "        -2.8682, -1.7009], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.0341,  0.0341, -2.6983, -2.0223, -2.2692, -2.5308, -0.6673, -2.3616,\n",
      "        -2.5308, -2.1724], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08155065029859543\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.0859, -1.2025, -1.7986, -1.9385], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.3880, -3.0245, -1.1288, -3.0245, -2.4917, -1.7681, -2.4680, -1.2025,\n",
      "        -1.2025, -3.0245], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2688, -2.6830, -2.0565, -2.6830, -2.5341, -2.0160, -2.2688, -2.0822,\n",
      "        -2.0822, -2.6830], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2875525951385498\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.0876, -1.2132, -1.8204, -1.9179], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.2132, -2.9055, -2.5557, -2.9468,  1.5729, -2.8482,  1.6658, -2.9468,\n",
      "        -1.9900, -2.7650], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.0919, -2.6848, -2.5525, -2.6848,  1.7897, -2.5525,  1.7897, -2.6848,\n",
      "        -2.0614, -2.6848], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11193990707397461\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.0707, -1.2356, -1.8362, -1.8845], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.5575,  4.1642, -2.8006, -2.3500,  0.0257, -2.8486,  0.0219,  1.1927,\n",
      "        -2.5120, -2.3500], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.1510,  4.7005, -2.5812, -2.3131,  0.0734, -2.6996,  0.0734,  1.8112,\n",
      "        -2.3131, -2.3131], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11401059478521347\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.0466, -1.2620, -1.8413, -1.8445], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.7877, -2.5108, -1.5973, -1.2178, -0.2049,  3.1299, -2.3392, -1.2620,\n",
      "        -2.7380, -2.5108], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.7202, -2.3465, -2.1676, -0.5554,  0.1181,  4.7215, -2.7202, -2.1358,\n",
      "        -2.6119, -2.3465], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.43844884634017944\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.0177, -1.2757, -1.8329, -1.8092], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.6660, -2.2867, -2.2867, -1.4317, -2.7209, -1.8766, -1.1674,  0.0257,\n",
      "         0.0480, -1.2757], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.7238, -2.3678, -2.3678, -2.2885, -2.7238, -2.1178, -2.1481,  0.1492,\n",
      "         0.1492, -2.1481], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.25572359561920166\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([ 3.2048,  0.3413, -1.8918,  1.2922,  1.2551, -2.4732, -2.2674, -2.6671,\n",
      "        -1.5448, -2.6264], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.6926,  0.1630, -2.1390,  1.8843,  1.8843, -2.3903, -2.2603, -2.7275,\n",
      "        -2.1390, -2.6382], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.341685950756073\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.5037, -1.2135, -1.9549, -1.8250], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.6554, -1.8944, -2.5713, -2.1225,  0.0206, -1.2750, -2.2788, -2.6147,\n",
      "        -1.7270, -0.2006], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.6422, -2.1451, -2.6422, -2.1574,  0.2089, -2.1475, -2.6422, -2.7223,\n",
      "        -2.0922, -0.5506], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12655724585056305\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9507, -1.2685, -1.8009, -1.7151], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.2704, -1.2685, -1.9644, -2.4443, -2.5370, -1.8958,  1.3581,  0.0314,\n",
      "        -2.1264, -0.1492], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.0905, -2.1417, -2.0905, -2.4196, -2.6331, -2.5273,  1.9369,  0.2223,\n",
      "        -2.1433,  0.2223], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.23692569136619568\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-2.2675, -2.5122, -2.5122,  0.0442, -2.2765, -2.4070, -2.5738, -1.9433,\n",
      "         0.0442, -1.9433], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2735, -2.6241, -2.6241,  0.2326, -2.4283, -2.6972, -2.6972, -2.1542,\n",
      "         0.2326, -2.1542], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.030758341774344444\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-1.2510, -1.9903, -1.2300,  1.4026,  1.7722, -1.2510,  6.1463, -1.2300,\n",
      "        -2.5137, -2.4429], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.1259, -2.1567, -2.3333,  1.9173,  1.9173, -2.1259, 10.0000, -2.3333,\n",
      "        -2.6058, -2.4262], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.913869857788086\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-2.3827, -1.2311, -2.0339, -2.1657], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.7611, -1.1504, -2.5863, -2.5450,  1.3906,  1.7963, -2.5072, -1.2362,\n",
      "        -2.5072, -2.5736], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2566, -0.4666, -2.6535, -2.4235,  1.9556,  1.9556, -2.5850, -2.1126,\n",
      "        -2.5850, -2.6535], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1863541454076767\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-2.3254, -1.3653, -2.1837, -2.0552], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.0746, -1.2235, -1.3653, -1.1658,  0.1552, -1.8616, -2.5598, -2.1332,\n",
      "        -2.5183, -2.5183], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.1513, -2.2288, -2.2288, -2.0969,  0.2941, -2.2360, -2.1012, -2.4120,\n",
      "        -2.5648, -2.5648], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.30809178948402405\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-2.5429,  1.6491, -2.3808, -2.3931, -1.7754, -2.3808,  4.4598, -2.6435,\n",
      "         1.8778, -2.6057], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.5474,  1.9752, -2.5978, -2.4031, -2.4702, -2.5978,  4.7334, -2.5474,\n",
      "         1.9752, -2.4702], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07954005897045135\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-2.4318, -2.4218, -1.2915, -1.1746, -2.4416, -1.1746, -2.4934, -2.5838,\n",
      "        -2.1745, -1.1746], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.5765, -2.5765, -1.9592, -2.0571, -2.3936, -2.0571, -2.3936, -2.5293,\n",
      "        -2.1623, -2.0571], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2842778265476227\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-2.3749, -1.0425, -1.9710,  0.0970, -1.3186,  1.8143, -2.4711, -2.4711,\n",
      "        -2.6292, -2.5751], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.4299, -0.5068, -1.9383,  0.3225, -2.0649,  1.9387, -2.5618, -2.5618,\n",
      "        -2.5165, -2.5618], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09437237679958344\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-2.5260, -1.2620, -2.2221, -2.4523], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.5780, -2.4927, -2.4523, -1.1488,  0.2202, -2.6801, -2.6015, -1.5652,\n",
      "        -2.6801, -2.6801], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.5406, -2.4976, -2.4087, -2.0339,  0.2968, -2.4976, -2.4087, -2.1358,\n",
      "        -2.4976, -2.4976], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1255396157503128\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-2.1252, -1.1059, -2.0036, -1.9399], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.1521, -1.2738,  4.5367, -2.5675, -1.1521, -1.7122, -2.6964,  0.2290,\n",
      "        -1.9527, -1.7122], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.0369, -1.9953,  4.8236, -2.5410, -2.0369, -2.4137, -2.4982,  0.2838,\n",
      "        -1.8958, -2.4137], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.31991177797317505\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-2.0781, -1.0791, -1.9569, -1.9016], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.7142, -1.3058,  0.0621,  6.4815, -2.6133, -2.3337, -2.5911,  1.8235,\n",
      "         4.7282, -1.9474], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.5093, -1.9712,  0.2699, 10.0000, -2.5668, -2.2243, -2.2060,  1.8564,\n",
      "         4.8333, -2.0751], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.309868574142456\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-2.0209, -1.0511, -1.8826, -1.8306], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.4820,  0.2641, -2.6950, -1.1937, -2.5549, -1.9125, -1.9614,  0.4484,\n",
      "        -2.6292, -1.9546], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.4282,  0.3190, -2.5285, -2.0743, -2.5967, -1.9001, -2.2602, -1.9001,\n",
      "        -2.5967, -2.0950], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6436567902565002\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-1.9784, -1.0037, -1.8223, -1.7964], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.4634, -2.3476, -1.5661, -2.6551, -2.3476,  0.3275, -2.5234, -2.7213,\n",
      "        -2.7008, -2.7008], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.4260, -2.2659, -2.2322, -2.5225, -2.2659,  0.3115, -2.2402, -2.5225,\n",
      "        -2.4260, -2.4260], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07470768690109253\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-1.9270, -0.9747, -1.7513, -1.7455], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.7230,  1.9294,  0.0847, -2.6733,  0.0847, -2.5754, -2.5887, -2.7077,\n",
      "         3.2553, -0.9747], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.5352,  1.9298, -0.8007, -2.5352, -0.8007, -2.6200, -2.6200, -2.4377,\n",
      "         5.1495, -1.8772], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6100338101387024\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-1.8179, -1.7172,  2.2568, -1.2239, -2.6595, -1.6042, -1.7752, -2.6909,\n",
      "        -2.6909, -1.2419], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.4234, -2.2793,  2.0076, -2.1177, -2.4438, -2.3027, -1.8458, -2.6361,\n",
      "        -2.6361, -2.1177], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.28561100363731384\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-2.6472, -2.3509, -2.6190, -1.2595, -1.4687, -2.6472, -2.2155, -2.0394,\n",
      "         0.3471, -2.6169], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.5695, -2.4680, -2.3003, -2.1335, -1.8445, -2.5695, -2.3218, -1.8375,\n",
      "         0.2969, -2.4680], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11093368381261826\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-2.6139, -1.2818, -2.5734,  2.0588, -2.5734,  3.5080,  0.3390, -1.2818,\n",
      "        -1.5067, -0.9276], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.3351, -2.1536, -2.4939,  2.1572, -2.4939,  5.3178,  0.2951, -2.1536,\n",
      "        -2.3351, -1.8348], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6406863927841187\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-1.2947, -1.5247, -1.6329, -2.5768, -0.9529, -2.5507, -1.6432,  2.2378,\n",
      "         3.6265, -2.6169], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.1652, -1.8436, -2.3554, -2.6247, -1.8576, -2.5090, -1.8436,  2.2639,\n",
      "         5.3344, -2.7427], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.517746090888977\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-1.8489, -0.9730, -1.5413, -1.5222], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.5437, -2.5908, -2.5604, -1.5142,  1.0412, -1.5970, -1.3459, -1.3039,\n",
      "        -2.5908,  1.4546], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.8339, -2.7699, -2.6446, -1.8757,  0.3091, -1.8339, -2.1735, -2.1735,\n",
      "        -2.7699,  2.3983], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3209896683692932\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-2.5630, -1.3040, -2.4717, -1.3806, -2.4635, -0.0772, -0.9828, -2.5099,\n",
      "         2.4637, -2.1934], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.7951, -2.1736, -2.3778, -2.1736, -2.7951, -0.6777, -1.8845, -2.3722,\n",
      "         2.5001, -2.4111], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2799108326435089\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-1.8450, -0.9935, -1.5213, -1.4926], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.2377,  4.8647, -0.9935, -1.2935, -1.5896,  2.2440,  2.5419, -2.3063,\n",
      "        -2.1908, -0.9935], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.5415,  5.3207, -1.8941, -2.1642, -1.8144,  2.5839,  2.5839, -2.8136,\n",
      "        -2.4306, -1.8941], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.316334068775177\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([ 0.4943, -2.5542, -0.2507, -2.5385, -1.5367, -2.4762,  2.5960, -1.5553,\n",
      "        -1.2815,  0.4943], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.4799, -2.6904, -0.6015, -2.8192, -1.9031, -2.6904,  2.6350, -2.3175,\n",
      "        -2.1534,  0.4799], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1743580549955368\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8643, -1.2716, -1.6620, -1.7143], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.4787,  2.6446,  0.1881, -2.5536,  2.6446, -2.5866,  0.1881, -2.4914,\n",
      "        -2.3362, -2.3362], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.1444,  2.6803,  0.5033, -2.8258,  2.6803, -2.8258,  0.5033, -2.7020,\n",
      "        -2.5433, -2.5433], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09058506786823273\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8598, -1.2461, -1.6455, -1.7101], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.6000,  2.6927,  1.6485, -1.0039, -2.5918, -2.5343, -2.5918, -1.2461,\n",
      "        -1.8986, -2.5707], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.8217,  2.7063,  2.7063, -1.9035, -2.7035, -2.7035, -2.7035, -2.1215,\n",
      "        -1.9035, -2.3891], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2830483317375183\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8511, -1.2208, -1.6353, -1.7104], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.2662,  6.9337, -1.7104, -2.6512,  0.5409,  2.7190, -2.6512, -1.9164,\n",
      "        -2.6424, -2.5423], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.5064, 10.0000, -2.3593, -2.8161,  0.5064,  2.7094, -2.8161, -1.8989,\n",
      "        -2.8161, -2.8161], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.0041903257369995\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8740, -1.1908, -1.6385, -1.7368], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 2.7766, -2.6272, -0.9764, -2.7099,  0.3273, -0.9764, -2.5852, -2.7099,\n",
      "        -2.6272, -2.4181], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 2.7753, -2.6767, -1.8787, -2.7870,  0.5613, -1.8787, -2.4936, -2.7870,\n",
      "        -2.6767, -2.5032], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17156943678855896\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8948, -1.1648, -1.6413, -1.7727], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.3673, -1.4441, -2.6905,  2.8290,  1.2334,  1.2334, -2.6580, -0.9615,\n",
      "        -1.5882, -2.4622], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.4803, -1.6877, -2.6584,  2.8295,  0.5917,  0.5917, -2.6584, -1.8653,\n",
      "        -1.6877, -2.4803], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17237532138824463\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([ 0.0454, -1.1443, -2.4974, -1.9342, -1.4247, -1.4247, -1.4795, -1.1443,\n",
      "        -1.4293, -2.7303], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3550, -2.0299, -2.4602, -2.4568, -1.6587, -1.6587, -1.8562, -2.0299,\n",
      "        -2.0299, -2.6441], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.26228103041648865\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 0.1316, -0.0416,  0.5134,  0.8015], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.8175, -1.7191, -2.7616, -2.7141, -2.4958,  0.7589, -2.6592, -1.1279,\n",
      "        -2.4277,  0.0689], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.7350, -2.3287, -2.6374, -2.6374, -2.4493,  0.7660, -2.6374, -2.0151,\n",
      "        -2.4493, -0.2786], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13107839226722717\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-2.7944, -2.3292,  0.7875, -2.7944, -2.7944, -1.5700, -1.6840, -2.8658,\n",
      "        -0.0124,  7.3834], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.6316, -2.2957,  0.8200, -2.6316, -2.6316, -1.6291, -2.0012, -2.7287,\n",
      "        -0.1972, 10.0000], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.7085556983947754\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.2499, 0.0227, 0.6620, 1.0417], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.7248, -2.6920, -2.7299, -1.5765,  4.5277, -2.6692, -1.4254, -2.7492,\n",
      "        -2.7767, -0.9629], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.6264, -2.4189, -2.4189, -2.3096,  5.8308, -2.4189, -1.6101, -2.4294,\n",
      "        -2.6264, -1.8666], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.34549015760421753\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.7726, -0.6674, -1.4003, -1.2992], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.4519, -1.0692, -2.6914, -2.7081,  0.9086, -2.7290, -0.6674, -1.0692,\n",
      "         0.1519, -1.0692], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.4409, -1.9623, -2.6379, -2.2992,  1.0569, -2.6379,  0.0779, -1.9623,\n",
      "         0.0779, -1.9623], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3154307007789612\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.3941, 0.0564, 0.8036, 1.2946], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.8456, -1.7993,  2.2262,  0.2018, -2.6896, -2.6128, -2.1345,  0.9650,\n",
      "        -1.0670, -1.0113], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.6523, -1.9603,  3.3256,  0.1651, -2.6523, -2.6523, -1.9102,  1.1542,\n",
      "        -1.9603, -1.9102], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.29683512449264526\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-2.8283, -2.6710,  1.0053, -1.5381,  0.4712, -1.3251, -2.4845, -2.8624,\n",
      "        -1.0670, -1.8361], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.7831, -2.6736,  1.2144, -2.3211,  1.2144, -1.5704, -2.4459, -2.6736,\n",
      "        -1.9603, -1.9603], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2121955156326294\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.6448, -0.6169, -1.3236, -1.1438], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.4201, -0.6169,  3.2042, -2.6922,  2.7440, -1.0937, -1.0684, -2.4951,\n",
      "        -1.8106, -1.4485], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.4759,  0.2500,  3.4058, -2.6870,  3.4058, -1.9844, -1.9615, -2.4471,\n",
      "        -1.9615, -1.9844], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3136516213417053\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-2.6323,  1.3280, -2.9324, -2.7290,  1.0188, -2.6140, -1.0848, -2.5055,\n",
      "        -2.4537, -2.7290], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.4782,  1.1983, -2.7890, -2.6973,  1.1983, -2.6973, -1.9763, -2.4457,\n",
      "        -2.4782, -2.6974], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09012896567583084\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-2.7709,  0.8465, -2.5773, -1.4308, -2.5931, -2.9630,  0.1310, -2.7552,\n",
      "         1.0380, -1.1431], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.7957,  1.1867, -2.4853, -1.9963, -2.7104, -2.7957,  0.1804, -2.7104,\n",
      "         1.1867, -2.0287], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1297384798526764\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.8041, 1.3977, 3.2925, 4.8803], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.7723, -1.0989,  1.0528, -2.7581, -0.5453, -1.2502, -2.5966,  1.0528,\n",
      "        -2.7723, -2.5121], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.7234, -2.4584,  1.1572, -2.8054,  0.1608, -1.4908, -2.5503,  1.1572,\n",
      "        -2.7234, -2.5003], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.24361145496368408\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.4124, 2.6801, 6.1304, 8.1447], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.5679, -2.8165, -2.5742, -2.8165, -1.7180,  1.0401, -1.3351,  0.8196,\n",
      "        -2.4762,  2.6801], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.4920, -2.7141, -2.4920, -2.7141, -2.2943,  1.0865, -1.5032,  1.0865,\n",
      "        -2.4258,  3.3525], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09219377487897873\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.7584, -1.9753, -2.8918, -3.0928], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 1.0143,  0.1590, -2.1846, -2.7584, -1.1124, -1.7286,  1.1031, -1.1874,\n",
      "         1.0143, -1.4184], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.9968, -0.0072, -2.2840, -2.7777, -2.0012, -2.2840, -1.5187, -2.0686,\n",
      "         0.9968, -2.0686], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.9209755063056946\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-2.6654, -1.7373, -2.6469, -2.9219], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.6654, -1.1022,  0.0679, -1.1828, -2.7811, -3.1876, -1.1828, -1.1828,\n",
      "        -2.9303, -1.9545], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.5636, -1.9920, -0.2268, -2.0645, -2.7590, -2.7590, -2.0645, -2.0645,\n",
      "        -2.6989, -2.5636], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.38299721479415894\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-2.6721, -1.3869, -2.3513, -2.7595], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 4.5559, -2.4521, -0.0293, -1.1129, -2.7711,  0.8804, -2.9782, -2.9285,\n",
      "        -2.4521, -1.5116], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 6.2054, -2.3605, -0.4048, -2.0016, -2.4877,  0.7301, -2.7071, -2.7071,\n",
      "        -2.3605, -1.4837], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3894776701927185\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-2.2635, -1.1288, -2.0534, -2.3891], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.5282,  0.5237, -2.9849, -1.1288, -2.3891, -2.5372,  2.6226, -2.9849,\n",
      "        -1.4744,  0.4502], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5287, -1.4754, -2.7209, -2.0159, -2.2425, -2.4118,  3.0745, -2.7209,\n",
      "        -1.4754,  0.6300], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5196367502212524\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-2.2726, -1.1361, -2.0538, -2.4298], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-3.3169, -2.9970, -2.8014, -1.9933, -2.0538, -1.4938, -2.9970, -1.1361,\n",
      "         0.7602, -3.3169], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.7939, -2.7318, -2.4981, -2.6304, -2.1160, -1.4420, -2.7318, -2.0225,\n",
      "         0.4976, -2.7939], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.20468473434448242\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([ 1.5680, -2.7972, -2.7972,  0.1135, -1.1691,  4.4293, -2.7972, -1.1691,\n",
      "        -2.7620, -1.1691], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 2.9863, -2.8360, -2.8360, -1.4285, -2.0522,  5.9910, -2.8360, -2.0522,\n",
      "        -2.5196, -2.0522], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.9231240153312683\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-2.2734, -1.1881, -2.0461, -2.4179], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.7851, -1.4764, -1.9716, -2.7062, -3.2537, -2.4764, -0.2896, -3.2537,\n",
      "         3.2692, -3.0653], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.8583, -2.3558, -2.4822, -2.5218, -2.8583, -2.4822, -0.9780, -2.8583,\n",
      "         2.9536, -2.8583], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.20024923980236053\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-2.2910, -1.2077, -2.0329, -2.3973], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.2695,  3.2665, -1.2077, -1.3941,  3.2665, -2.4559, -2.6337, -0.2826,\n",
      "        -3.0193, -2.3973], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3659,  2.9225, -2.0869, -2.0869,  2.9225, -2.5227, -2.5227, -1.0888,\n",
      "        -2.8782, -2.2547], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.33985403180122375\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([ 5.1114, -2.2044, -2.2969,  0.6452, -2.5716, -1.3677, -1.2162, -2.4295,\n",
      "         3.2388, -2.7972], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 5.7130, -2.3350, -2.0946,  0.3229, -2.5305, -2.2310, -2.0946, -2.5305,\n",
      "         2.8893, -2.9079], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.21868357062339783\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-2.2969, -1.2312, -1.9904, -2.3289], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.7685, -2.1771, -0.3833,  0.5854, -1.2312, -0.4601, -2.1534, -1.4070,\n",
      "        -0.3833, -0.6313], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.8733, -2.3340,  0.2855,  0.2855, -2.1081, -1.3450, -2.7629, -2.2663,\n",
      "         0.2855, -1.3226], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.41598111391067505\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([ 0.4598, -0.4651, -2.1914, -1.2418, -2.3307, -2.2752, -2.7700,  5.0592,\n",
      "        -1.4433, -0.7698], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.1448,  0.1448, -2.7799, -1.2971, -2.3362, -2.3340, -2.9075,  5.5345,\n",
      "        -2.2990, -1.2971], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.20791292190551758\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-2.3518, -1.2598, -1.9551, -2.3494], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.3494, -2.1190, -2.4760, -2.6594, -1.2598,  5.8418, -1.4772,  5.0538,\n",
      "        -1.4815, -1.2598], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.3559, -2.3333, -2.5529, -2.9378, -2.1338,  5.4279, -2.3295,  5.4279,\n",
      "        -1.2678, -2.1338], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2740578055381775\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-2.3904, -1.2716, -1.9534, -2.3756], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.8113,  3.2376, -0.9012, -1.3717, -2.5030, -1.5042, -2.6991, -1.1990,\n",
      "        -2.8113, -0.6195], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.9512,  2.6499, -1.5576, -2.3245, -2.5521, -2.3538, -2.9512, -1.2435,\n",
      "        -2.9512, -0.1783], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2707612216472626\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-2.4546, -1.2626, -1.9421, -2.4206], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.6995,  0.0130,  3.9845, -0.2601, -1.5066,  2.9427, -2.3717, -1.0964,\n",
      "        -2.8482, -2.5370], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 2.5860, -0.3704,  5.2342, -1.5844, -2.3559,  2.5860, -2.5319, -1.5844,\n",
      "        -2.9343, -2.5319], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.814136803150177\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-2.5103, -1.2589, -1.9088, -2.4288], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 6.8144, -2.0130,  2.3762, -3.0871, -1.5096, -1.9088, -1.2589, -2.8342,\n",
      "        -1.6170, -1.3779], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([10.0000, -2.2401,  2.5354, -3.0388, -2.3586, -2.3586, -2.1330, -2.9218,\n",
      "        -2.3586, -1.2459], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.2489466667175293\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-2.5740, -1.2572, -1.9058, -2.4376], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.5124, -2.5740, -1.4885,  3.9597, -0.0889, -1.4885,  3.1569, -2.5124,\n",
      "        -2.1068, -3.2177], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.4936, -2.1315, -2.1315,  5.1753, -0.4539, -2.1315,  2.5638, -2.4936,\n",
      "        -2.5205, -3.0316], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3191855549812317\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([ 2.9061, -2.4691, -2.2748, -2.4691, -3.1771, -1.6537, -3.1708,  2.4582,\n",
      "        -3.1708, -2.3909], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 2.6531, -2.4883, -2.4883, -2.4883, -3.0384, -2.1690, -3.0384,  2.6531,\n",
      "        -3.0384, -2.3611], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.046904947608709335\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-1.8920, -2.7635, -2.4346, -2.3709, -2.7635, -0.1490, -1.8924, -1.4575,\n",
      "        -2.2810, -2.9634], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.1420, -2.9069, -2.4964, -2.4914, -2.9069, -0.4084, -2.3849, -2.3117,\n",
      "        -2.7031, -2.7031], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14075107872486115\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.1680, -0.6755, -0.4376, -1.0216], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.4376, -2.1834, -3.1136, -2.9924, -2.5482, -3.1136, -0.1944, -2.7423,\n",
      "        -1.5542, -1.0216], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3918, -2.5073, -3.0750, -2.3988, -2.1111, -3.0750, -0.3918, -2.9195,\n",
      "        -2.1111, -1.2768], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1098998412489891\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-2.4834, -1.8664, -2.3402, -2.2931, -2.2935, -3.0985, -1.4777, -2.6150,\n",
      "        -3.0985, -1.4777], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.1119, -2.1034, -2.7219, -2.1119, -2.5229, -3.1062, -2.3300, -2.9387,\n",
      "        -3.1062, -2.3300], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1982784867286682\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-1.3126, -1.3314,  0.2440, -2.8497, -2.3857, -3.1005, -1.7311, -2.3342,\n",
      "        -1.4969, -2.8576], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3069, -1.3891, -0.3717, -2.9555, -2.7324, -3.1472, -1.3891, -2.5370,\n",
      "        -2.3472, -2.7324], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14127817749977112\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3085,  0.2105, -0.3628,  0.7203], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 4.3832, -1.7004, -2.7281, -3.1005, -2.8074, -3.1005, -1.7471, -2.9852,\n",
      "        -1.2643,  2.7931], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 5.3126, -2.1379, -2.9776, -3.1967, -3.1967, -3.1967, -1.3997, -3.1967,\n",
      "        -2.1379,  2.9449], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2239043414592743\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-2.7372, -2.7372, -1.9136, -1.7312, -2.4182, -2.1876, -2.7372, -0.7416,\n",
      "        -2.6974,  0.7383], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.9807, -2.9807, -2.0762, -2.1273, -2.5486, -2.0762, -2.9807, -1.4139,\n",
      "        -3.2103,  3.0048], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6242721080780029\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-2.5070,  4.4597, -1.6913, -2.4520, -2.7106, -2.7865, -1.2129, -2.4652,\n",
      "        -1.9110, -3.0981], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.5255,  5.3024, -2.0401, -2.5222, -2.7095, -2.9641, -2.0916, -2.5222,\n",
      "        -2.0401, -3.1903], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16690890491008759\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.3829, 2.7823, 5.0871, 6.9798], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.9471,  0.9471, -3.0943, -3.0943, -1.1722, -1.1722, -2.3202, -3.0943,\n",
      "        -2.7850, -2.3202], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.0389,  3.0389, -3.1632, -3.1632, -2.0550, -2.0550, -2.4981, -3.1632,\n",
      "        -2.9375, -2.4981], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.041010856628418\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.9150, -2.3494, -2.7834, -3.0558], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.6936, -1.4556, -1.3655, -2.7710, -3.0558,  2.9181,  2.8155, -2.6721,\n",
      "        -2.6468, -1.9261], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3872, -1.9298, -1.2717, -2.8915, -3.1144,  3.0000,  3.0000, -2.5473,\n",
      "        -3.1144, -2.4569], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12894856929779053\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-1.3543, -2.6329, -1.3543,  2.9062,  2.7253, -0.3979, -1.3441, -2.7954,\n",
      "        -0.3979, -2.6571], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.2443, -2.8381, -1.2443,  2.9543,  2.9543,  0.2408, -2.2097, -2.8381,\n",
      "         0.2408, -3.0516], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.18433396518230438\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.5560, -1.5054, -2.1332, -2.6653], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.3243, -1.0101, -1.8143, -1.2904, -2.7473, -1.3261, -1.0101, -2.9787,\n",
      "        -2.3024, -1.0101], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.3294, -1.9091, -1.8125, -2.1613, -2.7799, -1.2186, -1.9091, -2.5428,\n",
      "        -2.3438, -1.9091], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.38147231936454773\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-2.0586, -2.2502, -2.4332, -1.6947,  5.4032,  2.6976, -0.5785, -2.8871,\n",
      "        -1.6947, -2.4288], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.4644, -2.3249, -2.3249, -2.4644,  5.0915,  2.8206, -1.2095, -2.7417,\n",
      "        -2.4644, -2.3090], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1913069188594818\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.5163, -1.4492, -2.0310, -2.7198], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.2471, -2.3783,  2.6842, -2.2670, -2.8079, -2.9261, -1.9086, -2.3566,\n",
      "         2.7205, -2.9261], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.1987, -2.2901,  2.7609, -2.2286, -2.9102, -2.7177, -2.3043, -2.2901,\n",
      "         2.7609, -2.7177], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.027744850143790245\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([ 1.6524, -0.8696, -1.6550, -2.1056, -2.9471, -3.1250,  5.3632, -1.4627,\n",
      "        -2.7653, -2.3723], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 2.7076, -1.1978, -1.7151, -2.5709, -2.7114, -2.8950,  5.0026, -1.0396,\n",
      "        -2.8950, -2.2831], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.18835940957069397\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-2.8733, -1.5277, -2.1964, -2.9391], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.3954, -1.6017, -3.1084, -2.9404,  4.0560, -2.0222, -2.7150,  1.5033,\n",
      "        -1.9169, -2.6499], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9794, -1.6933, -2.8854, -2.7071,  4.9671, -1.8815, -2.7071,  0.5683,\n",
      "        -2.2738, -2.5892], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.230897456407547\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-2.0055, -0.9832, -1.5048, -2.1013], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.3175, -1.2011, -2.1718, -2.7027, -2.7027, -0.3387, -1.2011, -2.9077,\n",
      "         2.6817, -2.1718], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2610, -2.0810, -2.2624, -2.7061, -2.7061, -0.9029, -2.0810, -2.7061,\n",
      "         2.6179, -2.2624], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1931094378232956\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([ 2.6734, -1.3927,  2.6734, -2.0863, -2.2000, -1.1989, -2.8713, -2.0498,\n",
      "        -2.7119, -2.6299], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 2.5858, -1.6898,  2.5858, -2.6287, -2.2541, -2.0790, -2.7092, -2.2541,\n",
      "        -2.7092, -2.7092], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1249535083770752\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8488, -1.2017, -1.2075, -2.1547], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.7640, -2.8329, -1.8870, -2.0670, -2.8329, -2.7856, -0.7035, -2.7640,\n",
      "        -0.9978, -2.1031], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.8875, -2.7133, -2.2555, -2.3305, -2.7133, -2.8875, -1.1848, -2.8875,\n",
      "        -1.8980, -2.2555], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1339903324842453\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8690, -1.1906, -1.1726, -2.1461], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 0.6722, -2.8256, -2.0793, -2.7593,  2.6356, -2.2223, -1.4020, -2.2223,\n",
      "        -2.8179, -0.9938], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.8604, -2.8755, -2.2318, -2.7047,  2.5252, -2.2318, -2.0554, -2.2318,\n",
      "        -2.7047, -1.1688], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.054679274559020996\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4908, -0.5009,  0.3506, -0.6808], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 2.6154,  0.7072,  0.8268, -0.9788, -0.6808, -0.9788, -2.8038, -0.1682,\n",
      "        -1.1711,  6.4632], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 2.4961,  0.9029,  0.9029, -1.1513, -1.1513, -1.1513, -2.6959, -0.6845,\n",
      "        -2.0218, 10.0000], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.384994387626648\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([ 3.9334, -2.7820, -2.1009, -1.9428, -2.9489, -1.0121, -2.0645, -2.1583,\n",
      "        -1.3251, -2.7820], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.9183, -2.6971, -2.1082, -2.1926, -2.8580, -1.9109, -2.6461, -2.1829,\n",
      "        -2.1082, -2.6971], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.28150445222854614\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-2.8188,  0.4865, -2.1257, -1.8893,  0.8472, -1.8365,  0.4865, -0.3348,\n",
      "        -2.8170, -1.0135], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.6529,  1.0934, -2.2449, -2.1943,  1.0934, -2.2449,  1.0934, -0.5621,\n",
      "        -2.7003, -0.5621], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13677826523780823\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3574,  0.8868,  0.9006,  2.3606], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.9300, -1.1630, -2.9069, -2.0937, -2.0937,  4.9261, -2.1717, -1.8595,\n",
      "        -1.3693, -1.3693], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.8412, -1.8316, -2.8412, -2.6735, -2.6735,  5.0549, -1.9477, -2.1965,\n",
      "        -1.9477, -1.9477], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.19810301065444946\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([ 2.3899, -1.2073, -0.1404, -2.9590, -1.0922, -1.4196, -1.2073, -1.3398,\n",
      "        -0.5332,  0.9620], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 2.6536, -1.7332, -0.3724, -2.7647, -1.9830, -1.9830, -1.7332, -1.7130,\n",
      "        -1.1264,  1.1510], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.235194593667984\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.3423, 1.2769, 2.8362, 4.0653], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.7390, -2.6782,  2.8362, -2.0773,  5.2657, -2.7626, -1.2706, -2.0499,\n",
      "        -0.8380, -1.4909], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.1534, -2.4854,  2.6588, -2.2125,  5.1251, -2.5732, -1.6205, -2.1274,\n",
      "        -1.1534, -2.0187], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08208247274160385\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.9284, 2.4517, 5.3105, 6.8397], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.7705,  1.2496, -2.9852,  2.8713, -1.3652, -2.9203, -1.6666,  2.8713,\n",
      "        -2.3705, -0.3683], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.4999,  1.1939, -2.5966,  2.6677, -1.8024, -2.5966, -2.2698,  2.6677,\n",
      "        -2.4054, -0.2021], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09988945722579956\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-2.9085, -2.7609, -2.4355, -2.0644, -2.9680, -2.7374, -2.1040, -2.4767,\n",
      "        -2.1534, -3.0441], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.5982, -2.5045, -2.3829, -2.2751, -2.5982, -2.5045, -2.0223, -2.3804,\n",
      "        -2.3804, -2.5982], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06666647642850876\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.8677, -2.4814, -1.8396, -3.0200], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.5106,  2.5129, -2.1130, -3.0200, -3.0200, -0.3285,  2.3355, -1.4092,\n",
      "        -1.6441, -2.9132], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.1905,  2.7115, -2.4102, -2.6556, -2.6556, -0.1692,  2.7115, -1.8281,\n",
      "        -2.0458, -2.6556], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14255793392658234\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-2.8257, -0.2732, -1.4509, -1.5942, -1.4509, -2.6734, -1.4509, -2.9688,\n",
      "        -1.9673, -0.5501], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.7303, -0.1927, -1.4951, -2.1744, -1.4951, -2.6194, -1.4951, -2.7303,\n",
      "        -2.2770, -0.1927], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06413392722606659\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.5891, -1.4245, -1.7429, -1.9319], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 5.0029, -1.2134, -1.9319, -0.5602, -1.4245, -0.9189,  4.1547,  2.8082,\n",
      "        -1.2134, -1.4787], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 5.2813, -1.5042, -2.2820, -0.2277, -1.8271, -1.1821,  5.2813,  2.7392,\n",
      "        -1.5042, -1.5042], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1985550969839096\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2268, -0.9085, -1.3284, -1.8900], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.7814, -1.4350, -1.7358, -1.3826, -0.8115,  2.8032, -2.5811, -1.4902,\n",
      "        -1.4902,  1.0734], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2915, -1.8176, -2.0475, -2.0475, -1.1696,  2.7662, -2.7371, -1.4923,\n",
      "        -1.4923,  1.3786], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11929706484079361\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.7066, -0.1581, -0.8527, -1.3127], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.8239, -1.9756, -1.4905, -1.1238, -1.1238, -1.9239,  2.6677,  2.6677,\n",
      "         1.0622, -1.3127], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.9438, -2.5551, -1.5076, -2.0114, -2.0114, -2.2860,  2.7872,  2.7872,\n",
      "         1.4009, -1.7842], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.24229447543621063\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.1681, -0.7817,  0.7523, -0.5746], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.3516,  2.6762,  2.6762, -2.1525, -1.1005, -2.4925,  5.2418, -2.5715,\n",
      "        -1.1005, -2.7173], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7394,  2.7906,  2.7906, -2.5637, -1.9904, -2.5955,  5.2615, -2.5955,\n",
      "        -1.9904, -2.9858], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.20133526623249054\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.0214,  0.8609,  1.0970,  2.6856], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.0697, -2.2900, -1.4082, -0.7639, -1.7243, -1.3374, -2.0115, -2.0115,\n",
      "         2.8042, -2.5774], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2672, -2.2430, -1.6875, -1.1135, -1.9749, -1.6875, -2.2672, -2.2672,\n",
      "         2.7875, -2.8627], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0639243870973587\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-2.0848e+00,  2.8018e+00, -2.5233e+00, -1.4689e+00, -1.8354e+00,\n",
      "        -1.7010e+00, -2.7522e+00, -2.8745e+00, -2.6257e+00,  9.1875e-04],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2397,  2.7778, -2.6400, -1.4954, -2.2397, -1.9429, -3.0498, -3.0498,\n",
      "        -2.8990, -0.3626], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05870760604739189\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-2.7111,  1.2632, -2.8268, -2.7111, -2.5058,  5.1389, -1.6818, -1.6818,\n",
      "        -1.3531, -2.1754], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.9120,  1.3827, -3.0500, -2.9120, -2.6405,  5.2008, -1.9059, -1.9059,\n",
      "        -1.4770, -2.2026], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.028327947482466698\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.0456,  0.9423,  1.1145,  2.6043], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.4479, -0.4928, -2.2799, -1.3371, -0.9681, -0.0719, -2.8385, -2.2799,\n",
      "        -1.0059, -1.9517], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4435, -0.3578, -2.1762, -1.5549, -1.8713, -0.3578, -3.0250, -2.1762,\n",
      "        -1.0647, -2.1762], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1073315292596817\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.5271, 1.2881, 2.8218, 4.1175], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 1.1218,  2.5585, -1.3373, -0.8800, -0.9355, -2.9872,  0.7241,  2.5585,\n",
      "         0.7241,  2.5585], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.3027,  2.7058, -1.5250, -1.0605, -1.8420, -3.0050,  1.3027,  2.7058,\n",
      "         1.3027,  2.7058], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1657039225101471\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([ 2.4633, -1.7475, -3.0107, -1.4295, -2.4607, -2.0633, -3.0139, -2.6930,\n",
      "        -0.0551, -3.0717], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 2.6501, -2.1730, -2.5727, -1.3631, -2.1253, -2.1253, -2.8570, -2.5727,\n",
      "        -0.3025, -2.9664], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06398576498031616\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-0.5156, -1.4188, -1.4188, -3.0713, -0.8773, -2.9609,  2.5049, -0.1392,\n",
      "        -1.4188, -0.8773], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.0431, -1.3337, -1.3337, -2.8572, -1.7896, -2.9578,  2.6093, -0.2647,\n",
      "        -1.3337, -1.7896], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.20369699597358704\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-0.8580,  2.3459, -2.9957, -3.2456,  0.8496, -1.4074, -1.3561, -2.5422,\n",
      "        -0.0342, -0.8580], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7722,  2.5710, -2.9626, -2.9626,  1.1113, -1.3177, -1.4548, -2.0817,\n",
      "        -0.2354, -1.7722], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.21422520279884338\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.1999,  1.0115,  1.1157,  2.3025], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.2483, -1.6618, -2.4217, -1.4022, -0.3387,  1.0115, -0.8540, -3.2483,\n",
      "        -0.7520, -1.2437], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.9739, -1.7686, -2.6106, -1.3048, -0.1982,  1.0722, -1.7686, -2.9739,\n",
      "        -0.1982, -1.9381], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1855909824371338\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-2.8479,  2.8629, -3.2219, -0.8489,  2.8629, -3.1333, -2.5360, -3.1333,\n",
      "         0.8987, -2.5360], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.6457,  2.5061, -3.0123, -1.7640,  2.5061, -2.9242, -2.0634, -2.9242,\n",
      "         1.0387, -2.0634], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17306111752986908\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.2546,  0.9997,  1.0375,  2.2599], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.9997, -0.8571, -2.2083, -2.4152, -0.3731, -3.1416, -3.1416, -1.1600,\n",
      "        -2.3641, -3.0728], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.0339, -1.7714, -2.0658, -2.7020, -0.1997, -3.0759, -3.0759, -1.0281,\n",
      "        -2.5083, -2.9874], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10238854587078094\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([2.4222, 1.3074, 2.7710, 3.8949], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.0129, -1.3849, -1.1900,  2.2546, -0.9836, -1.3849,  0.9796,  0.8831,\n",
      "        -3.0703, -2.3794], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.0304, -1.3471, -1.0395,  2.5054, -1.3471, -1.3471,  1.0291,  1.0291,\n",
      "        -3.1247, -2.7370], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03755832836031914\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.1258, 2.5061, 5.2857, 6.5997], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.1001, -1.3893,  2.2491, -1.4596, -2.0615, -0.5620,  1.2977,  0.9653,\n",
      "        -0.9090, -2.2155], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.1449, -1.3334,  2.4950, -1.3334, -2.1947, -1.0578,  1.0242,  1.0242,\n",
      "        -1.8181, -1.8181], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1407785266637802\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-1.5718,  6.5924,  2.4931,  1.2731,  5.1232, -2.9184,  1.2731, -2.9184,\n",
      "        -0.3597, -2.0302], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.5490, 10.0000,  2.4829,  1.0399,  4.9332, -3.0566,  1.0399, -3.0566,\n",
      "        -0.1792, -1.9358], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.1836849451065063\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.1078, -2.5445, -2.4061, -2.8848], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.8603, -2.8848, -1.5778, -2.4836, -2.2839,  2.7031, -0.9886, -0.9886,\n",
      "         1.2557,  0.9480], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.0555, -3.1655, -1.5856, -3.0555, -2.1604,  2.5630, -1.8897, -1.8897,\n",
      "         1.1516,  1.1516], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.21552519500255585\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.1628, -2.4198, -2.2229, -2.8342], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.1917, -2.1707, -0.6412,  2.7068, -2.8342,  2.3905, -2.8567, -2.1917,\n",
      "         2.4847, -1.5599], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.9350, -2.1976, -1.1045,  2.6236, -3.0006,  2.6236, -3.1307, -1.9350,\n",
      "         2.6236, -1.6280], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.053504329174757004\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.8447, -1.3704, -2.0082, -2.1546], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.5347, -1.0957, -2.6521, -2.8609, -2.8352, -1.4475, -2.1496, -1.5355,\n",
      "        -1.3704, -0.1653], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.9346, -1.9861, -2.6379, -3.0822, -2.9346, -1.2083, -2.2333, -1.6678,\n",
      "        -1.6678, -0.0596], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11932529509067535\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.5004, -0.7734, -1.5070, -2.1797], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.8918,  0.8719, -1.3440, -2.8555, -2.8918, -3.0821, -2.8555, -2.8193,\n",
      "        -1.5070,  0.8719], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.0272,  1.3520, -2.0338, -2.8591, -3.0272, -3.0272, -2.8591, -2.8591,\n",
      "        -1.6961,  1.3520], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10138608515262604\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.0162, -0.1442, -1.0557, -1.8133], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 1.1067, -0.1015, -1.0557, -1.9293, -1.1584, -2.8940, -1.4667,  2.6214,\n",
      "        -2.1991,  2.7241], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.3593, -0.0040, -1.1298, -2.0599, -2.0425, -2.7936, -1.1395,  2.6952,\n",
      "        -2.7936,  2.6952], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1354498714208603\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.1543, -0.7890,  1.1242, -0.6724], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.0116, -3.0116,  2.7170, -3.0116, -2.2268,  2.4276, -2.3939, -3.0116,\n",
      "        -1.9294, -1.6554], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.9618, -2.9618,  2.6979, -2.9618, -2.2948,  2.6979, -2.3537, -2.9618,\n",
      "        -2.2948, -2.0537], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03816699981689453\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-3.0574, -1.9372, -2.7371, -0.1214, -2.9454, -0.0749, -0.1214,  7.3020,\n",
      "        -1.9926, -1.1614], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.9670, -2.0452, -2.4904,  0.0169, -2.7116,  0.0169,  0.0169, 10.0000,\n",
      "        -2.0296, -2.0452], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.8244048357009888\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.0256,  0.9969,  1.1916,  2.6958], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 2.6958,  7.4767, -1.1431, -2.2164, -2.9362, -0.7530, -2.9367, -2.9362,\n",
      "        -1.1431, -2.8908], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 2.7694, 10.0000, -2.0288, -2.7150, -2.7150, -1.0727, -2.5090, -2.7150,\n",
      "        -2.0288, -2.7150], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.8604032397270203\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-0.7194, -1.7627,  5.6339, -2.8320,  2.7421, -3.0273, -1.3657, -1.0510,\n",
      "        -3.1454, -2.6905], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.0188, -1.6633,  5.9824, -2.7476,  2.8949, -3.0648, -1.6633, -1.1749,\n",
      "        -3.0648, -2.5639], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08346368372440338\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([5.7987, 2.8420, 5.6559, 8.0050], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.0328,  5.7987, -2.3636, -1.4090, -2.9160, -2.8134, -2.8134, -2.9928,\n",
      "        -1.3464,  8.0050], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 2.0165e-03,  6.2045e+00, -2.4417e+00, -1.9928e+00, -3.1420e+00,\n",
      "        -2.7927e+00, -2.7927e+00, -3.1420e+00, -1.2034e+00,  1.0000e+01],\n",
      "       grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4587346911430359\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.1866, -2.6121, -2.4423, -2.9662], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.9662,  1.1114, -2.0336,  4.5995, -1.8070, -2.0985, -1.8070, -2.4423,\n",
      "        -1.0816, -0.6765], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.1980,  1.8038, -2.1804,  6.4863, -1.9524, -2.1804, -1.9524, -2.8211,\n",
      "        -0.9131, -0.9131], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4391433596611023\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([ 1.1484, -2.7032,  3.0266,  6.2155, -2.7032, -2.9601,  3.2286, -1.6330,\n",
      "         8.5562,  1.1484], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.9057, -2.8129,  3.2984,  6.7006, -2.8129, -3.2350,  3.2984, -1.5794,\n",
      "        10.0000,  1.9057], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3648031949996948\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-2.3639, -1.3168, -2.9949, -1.7383, -2.0698, -1.2032, -1.2032, -2.8901,\n",
      "         1.2599, -1.0035], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.4534, -1.2093, -3.2265, -1.8721, -2.1196, -1.2093, -1.2093, -2.6972,\n",
      "         1.9495, -0.8036], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06464210152626038\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.8091, -1.2191, -1.9871, -2.0860], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.2652, -2.0342,  1.4054,  1.3916,  0.4700, -1.7168, -2.8091,  5.0558,\n",
      "         3.0904,  3.2789], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.2525, -1.8395,  1.9510,  1.9510,  0.2525, -1.8395, -2.7061,  7.1178,\n",
      "         3.5503,  3.5503], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5258296728134155\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.5071, -0.5541, -1.4859, -2.1833], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.8983, -2.0804, -2.8916,  9.1441,  9.1441,  0.5152,  1.5671, -3.2218,\n",
      "        -2.6793,  1.6999], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.8085, -2.3415, -3.1377, 10.0000, 10.0000,  0.4104,  1.9263, -3.1377,\n",
      "        -2.6227,  1.9263], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2623835504055023\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.7527,  0.3570, -0.7509, -1.4529], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.5865, -0.7130, -2.0795, -2.8608,  1.3856, -1.6994, -2.9498, -2.9498,\n",
      "        -2.9254, -1.0873], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.5392, -0.9859, -2.0445, -2.5392,  1.9172, -2.0445, -3.0596, -3.0596,\n",
      "        -2.5294, -0.9859], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07741772383451462\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-2.6828, -2.8446, -1.0312, -2.6828, -3.1655, -2.6828, -2.1601, -2.6791,\n",
      "        -1.9289, -2.4835], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.4896, -2.5035, -0.8975, -2.4896, -2.9994, -2.4896, -1.9979, -2.4896,\n",
      "        -1.7528, -2.5035], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03673674911260605\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-2.2008, -1.6547, -0.9746,  9.6845, -2.4528,  2.0326, -1.9697, -3.1183,\n",
      "         0.5940, -2.9717], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.4892, -1.9528, -0.8409, 10.0000, -2.5041,  1.8788, -1.9528, -2.9808,\n",
      "         0.8294, -2.9808], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03904646262526512\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-0.7687,  9.8586,  9.8586, -0.3884,  3.2187, -2.6059, -0.7687, -1.6397,\n",
      "        -1.2342, -2.6059], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.6919, 10.0000, 10.0000, -0.5481,  4.2373, -2.5079, -1.6919, -2.0287,\n",
      "        -1.3495, -2.5079], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.29911530017852783\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-2.4046, -1.6978, -0.8604,  0.2550, 10.0304, -1.6901,  1.7808, -2.9974,\n",
      "        -1.7755, -0.5217], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.5280, -1.9742, -0.7705,  0.9949, 10.0000, -1.8376,  1.9653, -3.0100,\n",
      "        -1.8376, -0.5035], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07081885635852814\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.0408, -0.5879, -1.1026, -1.6655], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 7.3670,  1.8553, -3.1011, -1.6979, -1.7014, -1.9305, -0.7780, -1.4955,\n",
      "        -1.6806, -0.4617], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 8.1639,  2.0249, -3.0120, -1.7779, -1.6433, -1.7779, -0.6846, -1.6433,\n",
      "        -1.7779, -0.4586], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0744958147406578\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.8021, -0.7016,  0.4430, -1.2248], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([10.2913, -2.4822, -3.0862,  1.9663,  1.9663, -2.2367, -0.2635, -1.6126,\n",
      "         2.3198,  0.9583], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([10.0000, -2.5353, -3.0131,  2.0646,  2.0646, -2.5353, -0.4179, -1.7294,\n",
      "         2.0646,  1.1349], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.033521492034196854\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.2780, 0.2601, 2.4454, 0.8944], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.4326, -2.8686, -1.0661, -1.9258, -3.0822, -3.0822,  0.5275, -0.6363,\n",
      "        -2.0774, -0.9217], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.5208, -3.0272, -1.2096, -2.0416, -3.0272, -3.0272,  1.2009, -0.5252,\n",
      "        -2.5208, -0.5252], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0892433300614357\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.1494, 1.7915, 2.1971, 3.4968], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.6382, -2.3625, -2.3625, -2.8754, -0.6131, -2.3625,  4.5857,  2.5006,\n",
      "         0.7289, -1.8377], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.2505, -2.4562, -2.4562, -3.0105, -1.5518, -2.4562,  4.6296,  2.1471,\n",
      "         1.2505, -1.6706], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17276909947395325\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.6704, 2.4579, 4.6504, 6.3063], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.3046,  2.2998,  2.2998, -1.9831, -1.3652, -0.7150, 10.5064, -0.5680,\n",
      "         2.5043, -3.0626], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.3863,  2.1957,  2.1957, -1.9391, -1.5112, -1.1432, 10.0000, -1.5112,\n",
      "         2.1957, -2.9839], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14823779463768005\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-1.4866, -2.9071, -0.5126, -1.6127, -1.4866, -0.3810, -0.1053, -2.9071,\n",
      "        -1.3434, -0.3810], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.6417, -2.9704, -0.2016, -1.6417, -1.6417, -0.2016, -0.2470, -2.9704,\n",
      "        -1.4734, -0.2016], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02550141140818596\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.7151, -2.2300, -2.1958, -3.0389], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.2588, -2.2300, -3.0389, -3.0389, -0.4677, -1.8715, -2.2919,  1.9607,\n",
      "        -1.4964, -1.6403], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.3467, -2.5325, -2.9762, -2.9762, -1.4210, -1.9177, -2.9762,  2.2752,\n",
      "        -1.6143, -1.6143], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15996184945106506\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.3742, -1.6958, -1.5460, -2.3357], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 8.5461, -2.3357, -0.0589, -0.6629, -0.8494, -2.3357, -0.0589,  6.3618,\n",
      "        -2.1908,  2.4031], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 8.5030, -2.3914, -0.1557, -1.3739, -0.9875, -2.3914, -0.1557,  8.5030,\n",
      "        -2.5324,  2.2830], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5267286896705627\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([ 0.3699, -3.0278,  8.5390, -0.3820, -0.8162,  1.4597,  0.9270, -2.4129,\n",
      "        -0.3820, -2.4129], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.1272, -2.9076,  8.4527, -1.3438, -1.2908,  1.1272,  1.1272, -2.4197,\n",
      "        -1.3438, -2.4197], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2821453809738159\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.7773,  0.1457, -0.8008, -1.6089], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 8.4884, -1.8020,  1.0482, -0.3480, -0.3480, -0.1786, -3.0265, -2.4732,\n",
      "        -3.0265,  2.5120], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 8.4045, -1.8881,  1.0805, -1.3132, -1.3132, -0.1885, -2.8870, -2.4454,\n",
      "        -2.8870,  2.2905], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1967608481645584\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9834,  1.0957, -0.0535, -0.8217], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-3.0371, -2.0903, -0.6317, -2.5127,  4.9316, -2.1429,  0.6531, -0.1459,\n",
      "         4.8937, -1.3753], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.8636, -2.4767, -0.2159, -2.4767,  4.8424, -2.4767, -0.0139, -0.2159,\n",
      "         4.8424, -1.4843], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09372206777334213\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.0368, 0.4316, 2.1840, 0.5636], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.6035, -0.2942, -0.1327,  1.1625,  2.1840,  2.4946, -1.3432,  4.9017,\n",
      "        -0.9159, -0.5137], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.2659, -1.2648, -0.2659,  0.9656,  2.2877,  2.2877, -1.2648,  4.8652,\n",
      "        -1.0108, -0.7699], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12481703609228134\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.3486, 2.3408, 2.4734, 3.6414], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 3.6414, -0.5581, -2.5692, -3.0188,  2.1235, -0.7460,  2.9649, -1.8206,\n",
      "        -0.1485,  2.9649], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.8828, -0.3325, -2.4969, -2.8346,  2.2773, -0.9360,  2.2773, -1.4613,\n",
      "        -0.3325,  2.2773], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2799423635005951\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.9013, 2.9163, 4.8900, 6.5515], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.4865, -1.7813, -0.1890,  7.9773, -2.7718,  0.3460,  2.5638, -0.1890,\n",
      "        -0.2354, -1.7216], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.4224, -1.4657, -0.3723,  8.2870, -2.8422,  0.1194,  2.4109, -0.3723,\n",
      "        -1.2118, -1.8527], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13171759247779846\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([ 8.2120,  4.5236,  7.9857, 10.3115], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 4.9048,  0.0527, -1.7175, -0.6915,  2.3913, -0.2255, -0.5301, -2.7719,\n",
      "         0.0527,  2.6643], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.9090,  0.1438, -1.4771, -0.6841,  2.5351, -1.2029, -0.6841, -2.8433,\n",
      "         0.1438,  2.5351], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10961012542247772\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.7742, -2.0998, -2.0144, -2.8472], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.3842,  6.5801,  2.0792,  4.9167, -2.3528, -2.5318, -1.1831,  4.8735,\n",
      "        -1.6044, -0.2272], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.3417,  8.2758,  2.6464,  4.9221, -2.3979, -2.3979, -1.2045,  4.9221,\n",
      "        -1.7690, -1.2045], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.42040568590164185\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([ 0.3635, -0.3339,  0.3635, -2.5483, -1.5871, -2.5129,  2.8255, -1.9726,\n",
      "        -1.1760, -0.5684], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.1785, -0.3996,  0.1785, -2.7753, -1.5115, -2.3565,  2.7179, -2.2940,\n",
      "        -0.7767, -0.6728], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04396795108914375\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8322, -0.9049, -0.4718, -2.1561], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([10.2086,  4.2117,  0.3387, -1.3834,  0.2436,  0.3387,  0.3387, -2.2828,\n",
      "        -2.8425,  4.2117], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([10.0000,  4.9937,  0.1934, -1.5409,  0.1934,  0.1934,  0.1934, -2.2450,\n",
      "        -2.7472,  4.9937], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1367766559123993\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.7397,  0.2985, -0.5556, -1.2004], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.2232,  7.9728,  2.1362, -1.4444,  1.8237,  5.0266, -1.2457, -0.2879,\n",
      "        -0.3836, -0.3836], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2119,  8.1384,  2.9167, -1.5567,  0.9226,  4.9938, -1.1754, -1.1754,\n",
      "        -0.3427, -0.3427], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.22584009170532227\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-0.4154,  1.3693,  0.7873, -0.4154, -2.7712, -0.4154,  5.0667, -2.6579,\n",
      "        -1.6108, -1.0808], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.2914,  1.0003,  1.0003, -0.2914, -2.6843, -0.2914,  4.9669, -2.6843,\n",
      "        -1.5542, -0.8067], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03241782635450363\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-0.3674, -1.0128, -2.6202, -2.2981, -1.3576,  6.6069, -1.4474, -1.2938,\n",
      "        -0.6794, -1.9271], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.0529, -0.8376, -2.6479, -2.1815, -1.6115,  8.0119, -1.4992, -1.0529,\n",
      "        -0.8223, -1.4992], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.28174635767936707\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-2.7736,  3.1748,  4.4410, -2.1126, -0.3999,  2.3797,  2.3797,  0.1695,\n",
      "         0.5226, -2.5793], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.6135,  2.9969,  4.9520, -2.0681, -0.1493,  2.9969,  2.9969,  0.2322,\n",
      "         0.2322, -2.6135], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12345560640096664\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.6852,  1.3717,  0.5577, -0.0925], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.7663,  5.1917, -2.5521, -2.3247, -1.7943, -0.3727,  4.4210, -2.4064,\n",
      "        -1.0367, -0.4675], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.5851,  4.9267, -2.5851, -2.0897, -1.2341, -0.0820,  4.9267, -2.0897,\n",
      "        -0.9604, -0.8602], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10737546533346176\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.0628, 0.3557, 2.5425, 0.7868], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.2797, -0.4428,  2.5425, -2.0246, -2.2177,  2.5793,  1.0628, -1.1752,\n",
      "        -1.3210, -2.7340], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.0984, -0.9544,  2.9822, -2.0274, -2.0274,  2.9822, -0.0628, -1.6346,\n",
      "        -1.6346, -2.6055], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.22793202102184296\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6139, 2.3957, 3.1652, 4.3701], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.8061, -2.3678,  0.1918, -0.5364, -0.2236, -1.7935, -1.3485, -1.0576,\n",
      "        -2.2188, -0.7013], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4361, -2.1053,  0.2775, -0.8274,  0.0196, -2.0247, -1.6109, -0.8481,\n",
      "        -2.1053, -0.8481], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0557548813521862\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-2.7069,  0.8109,  2.4368, -0.4016,  4.3102,  4.0365, -0.9998, -0.4235,\n",
      "         0.5244, -1.9727], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.6633,  0.3103,  2.8792, -0.7860,  4.7994,  4.7994, -0.9324, -0.9324,\n",
      "         0.3103, -2.0438], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1731732338666916\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([ 5.1291,  3.0580, -1.3676, -2.4855,  2.6083, -0.4005, -2.7013,  0.4835,\n",
      "        -1.0979, -1.9756], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.7020,  2.8071, -1.5187, -2.7175,  2.8071, -0.9359, -2.7175,  0.3727,\n",
      "        -0.9359, -2.0716], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06961464881896973\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.3592, 2.6350, 2.9733, 4.1517], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.5166, -1.2135, -2.5166, -1.9922,  0.0724,  0.3957, -1.5233, -1.9578,\n",
      "         1.3592, -1.2135], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.7596, -1.4712, -2.7596, -2.0921,  0.0826,  0.4262, -1.4712, -2.2037,\n",
      "         1.3412, -1.4712], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03254510462284088\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.2822, 2.8958, 4.9860, 6.1519], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.4005,  0.6676,  2.5972,  2.5972, -1.9116,  2.8958,  4.9860, -2.3396,\n",
      "        -2.5424,  1.6365], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9307,  0.4728,  2.6646,  2.6646, -2.2161,  2.6646,  4.5367, -2.2161,\n",
      "        -2.7204,  1.3375], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08124925941228867\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-2.1162,  0.1988,  1.6641,  1.6641, -0.4270, -1.5119, -1.2793, -2.3696,\n",
      "         4.2014,  0.5118], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.1514,  0.1164,  1.3416,  1.3416, -0.9056, -1.3671, -1.3671, -2.1514,\n",
      "         4.4686,  0.4977], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.059303827583789825\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.6796, -1.9125, -2.0020, -2.7567], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.4815, -0.4798, -2.7567, -2.7567,  2.6384, -2.1928, -0.7836,  1.2830,\n",
      "        -2.6796,  0.2190], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.7212, -0.8592, -2.7212, -2.7212,  2.5399, -2.1680, -0.5242,  1.3746,\n",
      "        -2.7212,  0.1547], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.029584741219878197\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-2.2756, -1.4254, -1.2892, -2.6469], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.9410,  0.5333,  3.8813, -1.9731,  0.5333, -0.5407,  2.7039,  6.8001,\n",
      "         2.7039, -2.2672], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.6200,  0.4549,  4.3540, -2.1769,  0.4549, -0.7973,  2.4932,  7.0274,\n",
      "         2.4932, -2.1769], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05948331952095032\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([ 0.5292, -1.9508,  5.8902, -1.9659,  0.2343,  0.5292, -0.7592,  2.7047,\n",
      "        -0.6078, -2.8632], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.4338, -2.1402,  6.9680, -2.1849,  0.2467,  0.4338, -0.5238,  2.4733,\n",
      "        -0.7409, -2.7557], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14018487930297852\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([ 0.4977,  0.2249, -2.7071,  5.8634, -2.9082,  7.4850,  1.1418, -2.2000,\n",
      "        -2.4696, -2.6093], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.4115,  0.2951, -2.1636,  6.9020, -2.7564,  6.9020,  1.4598, -2.0998,\n",
      "        -2.7564, -2.7564], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.19644945859909058\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-2.1302, -1.0639, -0.6460, -2.3002], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 4.6295, -0.7994, -2.5901,  6.7865,  2.5989,  0.4738, -1.2041,  5.8740,\n",
      "         2.7233, -1.6777], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.2866, -0.6355, -2.7558,  6.8484,  2.4952,  0.4021, -1.6389,  6.8484,\n",
      "         2.4952, -1.3450], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14927363395690918\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.7604,  0.2974, -0.6192, -1.2313], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.3579,  0.2974,  1.5431,  0.2245,  0.2245,  0.2245, -1.4897,  1.4789,\n",
      "        -0.7816, -0.7816], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2253,  0.3310,  1.4372,  0.3310,  0.3310,  0.3310, -1.6103,  1.4372,\n",
      "        -0.6036, -0.6036], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.014361013658344746\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-2.3486,  4.5802, -1.4868,  4.5719,  0.2405, -0.4164,  3.9836, -1.2270,\n",
      "        -0.8056, -2.0939], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2679,  4.3625, -1.5845,  4.3625,  0.3238, -0.6203,  4.3625, -1.5939,\n",
      "        -0.5945, -2.1043], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04786276817321777\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([ 0.2632,  4.5384,  0.2989,  0.6346,  5.9965,  4.5304, -0.7327, -1.5094,\n",
      "        -0.6250, -1.9869], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.3099,  4.3968,  0.3770,  1.3918,  6.7167,  4.3968, -0.6158, -1.3302,\n",
      "        -0.6158, -2.3146], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1291387379169464\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-0.8100,  4.3908, -1.0537,  0.2712, -2.5313, -0.7330,  1.5297,  0.2712,\n",
      "         6.7968,  4.4804], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5985,  4.4451, -0.5985,  0.3767, -2.8236, -0.6067,  1.3517,  0.3767,\n",
      "         6.6794,  4.4451], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.042516566812992096\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.5137, -1.4992, -1.5319, -2.2589], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 1.5224, -1.3461, -2.7502,  8.4994,  0.4521, -2.2589, -2.0767, -2.2589,\n",
      "         0.3448,  4.1754], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 1.3179, -1.5191, -2.8490, 10.0000,  0.3701, -2.3493, -2.2115, -2.3493,\n",
      "         0.2665,  4.4909], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.24803395569324493\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.7512, -0.9575, -0.5233, -2.1782], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 4.2932, -0.8099, -2.5158,  0.3351, -2.2229,  0.3873,  0.3351,  1.6072,\n",
      "        -2.0730,  4.2932], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.5791, -0.6439, -2.8580,  0.2739, -2.3335,  0.2739,  0.2739,  1.3143,\n",
      "        -2.2255,  4.5791], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04497093707323074\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([ 0.4725, -2.0923, -2.2086, -2.2086, -0.8100, -0.8100,  1.6705, -2.1645,\n",
      "        -2.0759, -0.8995], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.3577, -2.3299, -2.3299, -2.3299, -0.6448, -0.6448,  1.3272, -2.2176,\n",
      "        -2.2176, -0.6448], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03592723608016968\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.6003,  0.4861, -0.6443, -1.0410], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.4704, -1.1850,  3.0657, -0.8012, -1.1850, -2.0339, -2.7003, -1.3455,\n",
      "         8.6463,  1.6844], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.3188, -1.2120,  3.0411, -0.6694, -1.2120, -2.2992, -2.8305, -1.3821,\n",
      "        10.0000,  1.3417], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2080993354320526\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([ 4.5962, -2.1893,  0.2982, -0.6228,  3.2721, -0.7822,  3.2721,  6.4452,\n",
      "        -1.1332, -2.1893], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 4.8007, -2.2500,  0.3825, -0.5394,  3.1366, -0.6868,  3.1366,  6.8840,\n",
      "        -1.1732, -2.2500], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.030323047190904617\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.1865, 0.9069, 2.6781, 0.8790], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([ 2.6781,  0.5351,  0.5738, -1.5435,  8.8608,  0.5738, -1.9574,  4.5226,\n",
      "         6.5476, -2.6679], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 3.2442,  0.3967,  0.3893, -1.1380, 10.0000,  0.3893, -2.2031,  4.8928,\n",
      "         6.9747, -2.8216], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2273346185684204\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.7008, 3.1773, 2.5206, 4.8244], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.8648, -1.0660,  6.6663,  0.6192, -2.7207, -0.1368, -0.5500, -0.4259,\n",
      "        -1.9947,  2.7562], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6871, -1.1232,  7.0947,  0.4604, -2.7952, -0.5055, -0.5055, -0.6027,\n",
      "        -2.1491,  3.3420], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0785188153386116\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-1.0844,  0.5840,  0.0554, -0.2406, -2.7340, -1.0844, -0.7239,  0.0554,\n",
      "        -2.1170,  2.8474], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.1261,  0.4361, -0.5660, -0.4744, -2.7693, -1.1261, -0.6758, -0.5660,\n",
      "        -2.0943,  3.4019], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1163860559463501\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([1.6363, 3.2426, 2.5670, 4.8906], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.6445, -1.1468, -0.6961,  9.1524, -2.4239, -2.7943,  0.6487,  0.6487,\n",
      "         6.8287,  0.6123], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.4738, -1.1133, -0.6690, 10.0000, -2.7467, -2.7467,  0.5355,  0.5355,\n",
      "         7.2372,  0.5355], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10542800277471542\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([4.6404, 3.4643, 4.6126, 6.9241], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.1170,  6.9241, -1.3178, -1.6827, -1.7814, -1.8868,  3.0405, -1.6827,\n",
      "         6.9241,  4.6126], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3872,  7.3248, -1.1053, -1.2573, -1.2573, -1.9793,  3.4230, -1.2573,\n",
      "         7.3248,  5.2317], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1613999307155609\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([6.8344, 4.7658, 6.7691, 9.3056], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([ 0.7271,  0.5750, -1.2621, -2.1412, -1.7745,  3.1466,  7.0054, -1.8742,\n",
      "         6.7691, -0.6235], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.5446,  0.5446, -1.1208, -1.9215, -2.0509,  3.4217,  7.3750, -2.0509,\n",
      "         7.3750, -0.6139], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07897654920816422\n",
      "---\n",
      "end game\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import gym \n",
    "import gym_examples\n",
    "from dqn import Agent\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('gym_examples/GridWorld-v0', size=4)\n",
    "agent = Agent(gamma=0.9, epsilon=0.5, batch_size=10, n_actions=4, eps_end=0.01, input_dims=[16], lr=0.003)\n",
    "scores, eps_hist,avg_scores_dqn = [], [], []\n",
    "n_games = 10\n",
    "\n",
    "for i in range(n_games):\n",
    "    score = 0\n",
    "    done=False\n",
    "    observation = env.reset()[0]['agent']\n",
    "    counter = 0\n",
    "\n",
    "    while not done:\n",
    "        agent_location = env.get_agent_location()\n",
    "        print(f\"Agent's Location: {agent_location}\")\n",
    "\n",
    "        # choose an action based on current state of env\n",
    "        action = agent.choose_action(observation)\n",
    "\n",
    "        observation_, reward, done, truncated, info = env.step(action)\n",
    "        observation_ = observation_['agent'] # since one hot encoded state is nested in dictionary\n",
    "\n",
    "        score += reward\n",
    "\n",
    "        # store transition and update weights\n",
    "        agent.store_transitions(observation, action, reward, observation_, done)\n",
    "        agent.learn()\n",
    "\n",
    "        #update state\n",
    "        observation = observation_\n",
    "        \n",
    "        # end of an episode\n",
    "        counter += 1\n",
    "        print('---')\n",
    "\n",
    "    scores.append(score)\n",
    "    eps_hist.append(agent.epsilon)\n",
    "    print('end game')\n",
    "\n",
    "    avg_score = np.mean(scores)\n",
    "    \n",
    "    avg_scores_dqn.append(avg_score)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the average score per episode\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "episodes = list(range(1, n_games + 1))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(episodes, avg_scores_dqn, marker='o', linestyle='-', color='b', label='Average Score per Episode')\n",
    "plt.title('Average Scores Over Episodes')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Score')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([ 0.0718, -0.0247, -0.0428,  0.0398], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([ 0.0645, -0.0381, -0.0419,  0.1146], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-0.0573, -0.2253, -0.1136, -0.0103], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.0917, -0.2476, -0.1451, -0.0928], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.2155, -0.4146, -0.2588, -0.2563], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.2942, -0.5106, -0.3325, -0.3485], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.3919, -0.6183, -0.4192, -0.4442], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-0.9026, -1.0861, -0.8791, -0.6014], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9643, -1.1422, -0.9081, -0.8804], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.0431, -1.1101, -0.9310, -1.0209], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2983, -1.2819, -1.0888, -1.1093], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2462, -1.2112, -1.0263, -1.0773], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.1142, -1.0516, -0.8653, -0.9824], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.7560, -0.8414, -0.6008, -0.7067], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.7198, -0.6956, -0.5968, -0.6573], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.7103, -0.7065, -0.6957, -0.6781], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.8316, -0.7121, -0.7390, -0.6935], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.6598, -0.7943, -0.6822, -0.6865], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-0.9007, -0.8352, -1.0011, -0.7443], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-0.7456, -0.8686, -0.8637, -0.7396], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-0.7808, -0.9031, -0.9325, -0.7851], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.0093, -0.9416, -1.2397, -0.9092], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-0.9650, -1.0460, -1.1496, -0.9794], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-0.9970, -1.0637, -1.1323, -0.9880], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.0059, -1.0656, -1.1138, -0.9865], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.0103, -1.0532, -1.0553, -0.9596], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.0072, -1.0413, -1.0148, -0.9441], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.0036, -1.0204, -0.9418, -0.9328], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-0.9979, -0.9970, -0.8765, -0.9314], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9378, -0.9782, -0.8284, -0.9901], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.0336, -1.0381, -0.8881, -0.9844], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.0220, -1.0216, -0.8680, -0.9741], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9198, -0.8624, -0.7216, -0.8728], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9099, -0.8541, -0.7161, -0.8661], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9883, -0.9904, -0.8626, -0.9536], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9884, -0.9920, -0.8852, -0.9567], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9906, -0.9955, -0.9081, -0.9628], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9925, -0.9989, -0.9350, -0.9692], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9951, -1.0026, -0.9605, -0.9742], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9974, -1.0056, -0.9842, -0.9772], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9320, -0.9207, -0.9506, -0.9452], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.0210, -1.0390, -1.0884, -1.0474], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9639, -1.0023, -1.0066, -1.0307], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.9113, -0.9096, -0.9051, -1.0245], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9840, -0.8879, -0.9820, -1.0040], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.8972, -0.9240, -0.9675, -0.9649], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-0.8974, -0.9044, -0.8933, -0.9464], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9021, -0.9289, -0.9706, -0.9600], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-0.9037, -0.9088, -0.9009, -0.9417], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9220, -0.9450, -0.9922, -0.9696], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-0.9224, -0.9205, -0.9225, -0.9549], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9924, -0.9873, -1.0593, -1.0317], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9963, -0.9885, -1.0640, -1.0229], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-0.9548, -0.9220, -0.9570, -0.9408], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-0.9508, -0.9181, -0.9527, -0.9380], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-0.9460, -0.9161, -0.9467, -0.9358], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-0.9414, -0.9172, -0.9339, -0.9496], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-0.9340, -0.9248, -0.9131, -0.9615], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-0.9277, -0.9277, -0.9104, -0.9463], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9181, -0.9566, -0.9691, -0.9642], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9275, -0.9701, -0.9870, -0.9780], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-0.9498, -0.9564, -0.9526, -0.9721], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9476, -0.9885, -1.0345, -0.9938], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-0.9863, -0.9859, -1.0080, -0.9937], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.9388, -0.9392, -0.9301, -0.9548], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9675, -0.9874, -1.0155, -0.9695], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9848, -0.9924, -1.0216, -1.0003], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.9497, -0.9402, -0.9237, -0.9663], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.9312, -0.9323, -0.9122, -0.9511], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9753, -0.9459, -0.9829, -0.9594], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9715, -1.0017, -1.0355, -0.9651], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9321, -0.9310, -0.9479, -0.9567], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.0087, -1.0273, -1.0664, -1.0089], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.0106, -1.0233, -1.0623, -1.0043], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9913, -0.9603, -0.9956, -0.9907], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.9368, -0.9495, -0.9148, -0.9642], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9540, -0.9290, -0.9558, -0.9514], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9883, -0.9975, -1.0331, -0.9697], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9382, -0.9673, -0.9865, -0.9651], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-0.9407, -0.9296, -0.9770, -0.9522], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.9403, -0.9604, -0.9234, -0.9638], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9423, -0.9334, -0.9503, -0.9397], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-1.0058, -1.0102, -1.0155, -0.9605], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-0.9520, -0.9391, -0.9835, -0.9612], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.9515, -0.9707, -0.9368, -0.9701], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9372, -0.9325, -0.9336, -0.9528], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.0097, -0.9635, -1.0145, -0.9737], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.0070, -0.9580, -1.0112, -0.9702], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.0050, -0.9534, -1.0072, -0.9670], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9973, -0.9389, -1.0043, -0.9407], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.0049, -0.9433, -1.0122, -0.9443], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.0162, -0.9498, -1.0225, -0.9641], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.0257, -0.9552, -1.0300, -0.9841], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.0171, -1.0107, -1.0561, -0.9949], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9587, -0.9706, -0.9557, -0.9751], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9299, -0.9298, -0.9159, -0.9658], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9071, -0.9212, -0.8905, -0.9420], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.0185, -0.9482, -0.9829, -0.9897], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9858, -1.0141, -0.9921, -0.9961], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.0403, -0.9609, -1.0017, -1.0082], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9320, -0.9200, -0.9011, -0.9580], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9254, -0.9114, -0.8977, -0.9446], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9194, -0.9030, -0.8949, -0.9302], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9160, -0.8984, -0.8964, -0.9201], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9061, -0.9058, -0.9354, -0.8577], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.0409, -1.0340, -1.0583, -1.0239], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.8554, -0.8253, -0.8826, -0.8237], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.0206, -0.9991, -1.0218, -0.9769], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.0131, -1.0021, -1.0080, -1.0000], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-0.9410, -0.9232, -0.9346, -0.8953], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-0.9462, -0.9498, -0.9368, -0.7888], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-0.9345, -0.9393, -0.9208, -0.7083], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-0.9396, -0.9471, -0.9169, -0.6608], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-0.9528, -0.9680, -0.9315, -0.6217], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-0.9803, -1.0006, -0.9601, -0.6177], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.0134, -1.0388, -0.9951, -0.6294], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.9795, -1.0881, -1.0184, -0.7677], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.0006, -1.0195, -1.0310, -0.9286], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.0455, -1.1348, -1.0761, -1.2910], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.0142, -1.1095, -1.0246, -1.3248], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-0.9801, -1.0773, -0.9740, -1.3282], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.8105, -0.9409, -0.8302, -1.0174], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-0.9158, -1.0183, -0.9405, -1.2607], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-0.9061, -0.9974, -0.9772, -1.1706], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-0.9770, -1.0184, -1.0934, -0.8971], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9423, -1.0009, -1.0663, -0.7188], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.0517, -1.0035, -1.1025, -0.8334], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.0690, -1.0041, -1.1030, -0.8908], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.0807, -0.9995, -1.0606, -1.0018], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.1140, -0.9444, -1.0403, -1.0439], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.0264, -0.9587, -0.9042, -0.9472], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9763, -0.8880, -0.8535, -0.9809], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9187, -0.8789, -0.8545, -0.8544], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.8881, -0.9330, -0.9048, -0.8451], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9847, -0.9944, -1.0756, -1.0073], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2945, -1.4003, -1.4973, -1.3760], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.8996, -1.9228, -2.1730, -1.8239], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1891, -2.1812, -2.4896, -2.0560], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3377, -2.3353, -2.5657, -2.2218], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-2.0699, -1.7799, -2.0176, -2.1308], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-1.7004, -1.4717, -1.6237, -1.6529], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-1.5916, -1.4165, -1.5523, -1.6965], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-1.4503, -1.2559, -1.3261, -1.4350], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-1.4355, -1.2753, -1.3116, -1.4231], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-1.4468, -1.3251, -1.3370, -1.4465], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.7329, -1.6802, -1.6382, -1.8464], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.7939, -1.7541, -1.8351, -1.7438], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9711, -1.9699, -2.1220, -1.9065], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1089, -2.0905, -2.2351, -2.2131], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8611, -1.9379, -2.0304, -1.9624], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.7226, -1.8296, -1.8159, -1.8355], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-1.5897, -1.6443, -1.6725, -1.6031], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-1.5827, -1.6367, -1.6241, -1.5862], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-1.6268, -1.7078, -1.6136, -1.6227], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.6649, -1.8009, -1.6139, -1.7302], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8449, -1.9306, -1.8229, -1.8130], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8603, -1.8762, -1.7476, -1.9016], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8906, -1.8641, -1.7766, -1.9155], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9169, -1.8501, -1.8076, -1.9280], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9273, -1.8281, -1.8310, -1.9341], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9128, -1.8280, -1.9341, -1.8615], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8718, -1.7867, -1.9271, -1.8426], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8309, -1.7502, -1.9122, -1.8218], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-1.8036, -1.7590, -1.8798, -1.7900], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-1.7897, -1.8087, -1.8491, -1.7617], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-1.7316, -1.8246, -1.8603, -1.7337], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-1.7372, -1.8355, -1.8592, -1.7361], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.8186, -1.8437, -1.8448, -1.9018], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.8348, -1.8484, -1.8464, -1.9014], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.8511, -1.8482, -1.8487, -1.9010], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-1.8093, -1.8563, -1.8644, -1.7540], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8430, -1.8433, -1.8277, -1.8507], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8348, -1.7589, -1.7867, -1.7726], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8983, -1.9264, -1.8820, -1.7928], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.7886, -1.8146, -1.8119, -1.5463], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4629, -0.4450, -0.5902,  0.1618], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.0020, -1.9918, -1.9007, -1.9811], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9368, -1.9473, -1.8757, -2.0155], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8839, -1.8781, -1.8280, -1.7167], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.7493, -1.7863, -1.7153, -1.5585], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.6807, -1.7407, -1.6674, -1.4777], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.6566, -1.7312, -1.6557, -1.4411], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.6433, -1.7256, -1.6513, -1.4116], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.6398, -1.7253, -1.6600, -1.3898], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.6561, -1.7421, -1.6867, -1.4069], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.6810, -1.7612, -1.7168, -1.4327], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.7840, -1.8433, -1.8392, -1.5323], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8159, -1.8517, -1.8755, -1.5702], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8788, -1.8292, -1.9265, -1.6664], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8834, -1.8188, -1.9155, -1.6935], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8844, -1.8101, -1.8941, -1.7172], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8680, -1.7991, -1.8320, -1.7521], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8408, -1.7973, -1.7777, -1.7553], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8377, -1.8081, -1.7757, -1.7614], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8472, -1.7671, -1.7050, -1.8737], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8394, -1.8154, -1.7899, -1.6780], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9060, -1.9094, -1.8444, -1.7928], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9197, -1.9284, -1.8618, -1.7909], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8752, -1.8660, -1.8482, -1.6496], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9308, -1.9378, -1.8912, -1.7640], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.7752, -1.8165, -1.8020, -1.8069], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.8266, -1.8234, -1.8600, -1.8457], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-1.8002, -1.8329, -1.8692, -1.9064], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-1.7974, -1.8318, -1.8649, -1.9238], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-1.7932, -1.8293, -1.8563, -1.9369], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.7909, -1.7945, -1.8091, -1.8731], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.7831, -1.7855, -1.7927, -1.8626], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.7760, -1.7756, -1.7781, -1.8507], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-1.7743, -1.7967, -1.7962, -1.9190], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-1.7786, -1.7987, -1.7923, -1.9134], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-1.7903, -1.8075, -1.7950, -1.9132], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-1.8034, -1.8192, -1.8001, -1.9154], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.7915, -1.8338, -1.7842, -1.8457], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8163, -1.8033, -1.7734, -1.8573], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9064, -1.8590, -1.7923, -1.8603], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9140, -1.8700, -1.8128, -1.8408], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9780, -1.9827, -1.9534, -1.8473], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9457, -1.9750, -1.9129, -1.8397], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9239, -1.9467, -1.8950, -1.7850], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9047, -1.9192, -1.8794, -1.7422], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8908, -1.8894, -1.8712, -1.7156], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8856, -1.8721, -1.8750, -1.7157], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8943, -1.8634, -1.8888, -1.6937], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9076, -1.8613, -1.9053, -1.6883], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9200, -1.8621, -1.9234, -1.6937], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9338, -1.8699, -1.9402, -1.7153], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9413, -1.8764, -1.9546, -1.7363], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8533, -1.8307, -1.8939, -1.9073], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.0117, -1.9108, -1.9609, -2.0151], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9212, -1.9116, -1.9768, -1.9516], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.7598, -1.7871, -1.7975, -1.7407], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8580, -1.8916, -1.9230, -1.8958], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-1.7620, -1.7964, -1.8247, -1.8866], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-1.7516, -1.7999, -1.8074, -1.8746], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-1.7513, -1.8066, -1.7951, -1.8652], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-1.7524, -1.8171, -1.7879, -1.8603], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-1.7614, -1.8297, -1.7843, -1.8656], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-1.7776, -1.8464, -1.7856, -1.8839], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-1.8369, -1.8880, -1.8251, -1.9224], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9310, -2.0001, -1.9271, -1.9067], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9737, -1.9531, -1.8807, -1.8615], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.8963, -1.9154, -1.8925, -1.8071], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8699, -1.7958, -1.8638, -1.6513], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8510, -1.8134, -1.8375, -1.6637], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8355, -1.8226, -1.8185, -1.6967], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8197, -1.8323, -1.8026, -1.7323], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8042, -1.8370, -1.7882, -1.7647], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.7746, -1.8319, -1.7525, -1.8058], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.7495, -1.8186, -1.7261, -1.7952], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.7452, -1.8163, -1.7218, -1.7848], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.7966, -1.8335, -1.7637, -1.8345], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8394, -1.8538, -1.8003, -1.8872], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8815, -1.8728, -1.8341, -1.9707], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9082, -1.8839, -1.8614, -2.0208], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9248, -1.8867, -1.8769, -2.0626], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9135, -1.9074, -1.9194, -1.8039], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8000, -1.8109, -1.8170, -1.7809], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.7521, -1.7846, -1.7814, -1.6735], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.7228, -1.7636, -1.7527, -1.5862], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.7482, -1.7571, -1.7587, -1.5596], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.7900, -1.7762, -1.7918, -1.7117], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8298, -1.8012, -1.8237, -1.8567], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9484, -1.9190, -1.9292, -2.0991], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8620, -1.7925, -1.8893, -2.0914], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8466, -1.8770, -1.8634, -1.9836], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-1.8035, -1.7686, -1.8453, -1.9323], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8081, -1.8493, -1.7985, -1.8019], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.7969, -1.7288, -1.7851, -1.5911], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8239, -1.7860, -1.8195, -1.7414], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8977, -1.9008, -1.8431, -1.7318], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9788, -1.9735, -1.9060, -1.8772], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9498, -1.9397, -1.8835, -1.8403], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9930, -1.9647, -1.8931, -2.0075], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9501, -1.9207, -1.8681, -1.9913], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.7644, -1.6735, -1.8097, -1.8623], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.7760, -1.6907, -1.8302, -1.8652], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8298, -1.7611, -1.8875, -1.9232], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8618, -1.7977, -1.8944, -1.9319], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8298, -1.9101, -1.8595, -2.0468], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9634, -1.9249, -1.9309, -1.7871], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9533, -1.9568, -1.9237, -1.6868], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9712, -2.0242, -1.8861, -1.7765], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9325, -2.0125, -1.8884, -1.7134], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8920, -1.9810, -1.8778, -1.7438], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.7727, -1.9204, -1.8398, -1.8503], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-1.8549, -1.8319, -1.9677, -1.6817], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.7040, -1.8093, -1.8230, -1.8459], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.7146, -1.8158, -1.8210, -1.8632], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.7474, -1.8423, -1.8260, -1.8745], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.8430, -1.9438, -1.8475, -1.8532], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.9128, -2.0232, -1.8613, -1.7499], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.9370, -2.0512, -1.8628, -1.6651], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.9466, -2.0681, -1.8689, -1.6162], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.9510, -2.0812, -1.8667, -1.6297], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.9470, -2.0593, -1.8640, -1.7021], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-2.0274, -1.8968, -1.9428, -1.8345], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-1.8983, -1.9231, -1.8476, -2.0102], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8915, -1.8881, -1.8557, -1.9658], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9103, -1.7572, -1.8343, -2.0226], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.8160, -1.6921, -1.8159, -2.1951], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.7499, -1.6199, -1.7733, -1.3257], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4034, -2.9692, -3.5926, -3.6521], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6297, -2.4267, -2.7841, -2.5191], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2110, -2.1817, -2.3242, -1.8489], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.3996, -2.1049, -2.6306,  6.6040], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-2.6227, -2.6187, -2.5523, -1.9604], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.0079, -3.0075, -2.9474, -3.0892], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.7826, -2.7429, -2.6928, -3.1622], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.7004, -2.3762, -2.5231, -2.9394], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-2.1688, -1.8884, -2.1273, -1.9238], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-2.4841, -2.1970, -2.4777, -2.2128], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-2.2174, -2.2437, -2.3542, -1.0462], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-1.9039, -2.0676, -2.0839, -0.7295], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-2.6836, -3.2318, -3.0914, -1.7544], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-2.6530, -3.1515, -3.0922, -1.8837], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.8730, -3.1569, -3.2960, -3.8967], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.8412, -2.9932, -3.2408, -4.0245], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.7657, -2.8013, -3.1691, -4.0929], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.6486, -2.6007, -3.0111, -4.0159], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-2.3211, -2.1827, -2.6612, -2.5197], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-2.2362, -2.0853, -2.5107, -2.6569], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.3621, -2.2401, -2.5352, -2.2446], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.3225, -2.2284, -2.4512, -2.0887], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9789, -1.9021, -2.1078, -2.1767], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.4040, -2.3601, -2.4223, -1.8922], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1777, -2.1614, -2.2091, -2.0788], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.3951, -2.3831, -2.3202, -1.6843], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.5290, -2.5948, -2.3758, -2.1756], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.5833, -2.6882, -2.4435, -2.2622], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6295, -2.7659, -2.5126, -2.3815], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6492, -2.7982, -2.5815, -2.4911], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6402, -2.7877, -2.6386, -2.5891], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.5766, -2.7351, -2.6412, -2.6500], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.9438, -3.1266, -3.0674, -3.0021], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.3740, -2.5490, -2.5746, -2.5814], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2884, -2.4323, -2.5271, -2.5413], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.2345, -2.3348, -2.4909, -2.4812], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1893, -2.2500, -2.4615, -2.4288], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.1680, -2.1885, -2.4470, -2.3959], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.5281, -2.4056, -2.7255, -2.6570], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.4045, -2.1996, -2.5542, -2.2244], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.5538, -2.3058, -2.7195, -2.6074], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.5294, -2.2621, -2.5443, -2.5094], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6035, -2.3585, -2.5632, -2.5166], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6949, -2.4900, -2.5992, -2.5359], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.7658, -2.6104, -2.6325, -2.5966], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.9241, -2.8142, -2.8499, -2.7586], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.8753, -2.8151, -2.7256, -2.4818], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.9414, -2.9722, -2.7216, -2.7459], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6975, -2.7597, -2.5041, -2.6289], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6754, -2.7663, -2.4649, -2.6251], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6348, -2.7477, -2.4270, -2.6123], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.5932, -2.7086, -2.3877, -2.5946], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.5598, -2.6662, -2.3598, -2.5469], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.5250, -2.6189, -2.3349, -2.4879], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.5110, -2.5738, -2.3310, -2.4446], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.5277, -2.5225, -2.3863, -2.4157], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.5627, -2.5151, -2.4323, -2.4320], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6092, -2.5130, -2.4866, -2.4604], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6607, -2.4986, -2.5852, -2.5536], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.7026, -2.6131, -2.6258, -2.5564], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6652, -2.5925, -2.5866, -2.5468], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6333, -2.5772, -2.5466, -2.5354], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.5118, -2.4833, -2.4584, -2.5702], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.5070, -2.4941, -2.4648, -2.5754], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6925, -2.6149, -2.5261, -2.5201], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6507, -2.6422, -2.5203, -2.6452], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6790, -2.6799, -2.5471, -2.6518], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6935, -2.7039, -2.5748, -2.6542], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6911, -2.7036, -2.6162, -2.6339], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6791, -2.6817, -2.6249, -2.6128], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6565, -2.6389, -2.6242, -2.5814], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6370, -2.5972, -2.6201, -2.5541], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6114, -2.5515, -2.6158, -2.5305], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.5885, -2.4955, -2.6063, -2.5091], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6504, -2.4439, -2.5759, -2.4810], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.4907, -2.1705, -2.5472, -2.8797], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6821, -2.2617, -2.7977, -2.4196], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6898, -2.2604, -2.7976, -2.4425], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.7039, -2.2729, -2.7931, -2.4702], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.7200, -2.3008, -2.7886, -2.4974], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.7319, -2.3290, -2.7844, -2.5217], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.7414, -2.3729, -2.7838, -2.5440], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.4856, -2.3221, -2.4806, -2.8237], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.7003, -2.4969, -2.7672, -2.5785], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6872, -2.5424, -2.7697, -2.5878], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6794, -2.5751, -2.7727, -2.5936], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6773, -2.6096, -2.7840, -2.5948], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.5395, -2.7446, -2.6224, -2.7364], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-2.4808, -2.7744, -2.5239, -2.4472], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-2.5287, -2.6549, -2.6400, -2.6645], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-2.5359, -2.6057, -2.6471, -2.6698], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-2.5712, -2.5392, -2.6789, -2.6955], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-2.5018, -2.5929, -2.5374, -2.5124], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-2.5219, -2.5848, -2.5324, -2.5194], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-2.5609, -2.5936, -2.5291, -2.5321], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6395, -2.5574, -2.6433, -2.6695], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.8924, -2.7648, -2.7475, -2.5745], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6724, -2.6007, -2.6322, -2.6413], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.8994, -2.7846, -2.7219, -2.5482], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6784, -2.6191, -2.6253, -2.6271], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6433, -2.6188, -2.6328, -2.6258], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-2.8742, -2.8805, -2.7543, -2.6463], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-2.5594, -2.6934, -2.5143, -2.6254], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6415, -2.6704, -2.6590, -2.6808], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-2.5887, -2.7299, -2.5412, -2.6790], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6617, -2.6893, -2.6716, -2.7198], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-2.6214, -2.7492, -2.5566, -2.7061], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6773, -2.6828, -2.6703, -2.7322], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.5496, -2.4321, -2.5628, -2.7996], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.5427, -2.4286, -2.5764, -2.7738], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.7759, -2.4845, -2.8919, -2.6349], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.7856, -2.7396, -2.6811, -2.5074], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6837, -2.7092, -2.6977, -2.6589], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6256, -2.5349, -2.5053, -2.6672], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6760, -2.5900, -2.5738, -2.4948], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.5703, -2.6085, -2.5093, -2.5023], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.5638, -2.6073, -2.5079, -2.5017], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.5713, -2.6131, -2.5088, -2.5298], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.5842, -2.6098, -2.4968, -2.5719], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.5907, -2.6049, -2.4964, -2.5952], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.5991, -2.6098, -2.5048, -2.6165], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6142, -2.6324, -2.5262, -2.6354], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6311, -2.6571, -2.5518, -2.6532], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6462, -2.6810, -2.5848, -2.6625], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6564, -2.6995, -2.6091, -2.6551], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.7642, -2.6647, -2.6867, -2.6338], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6385, -2.7049, -2.6185, -2.5688], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6052, -2.6801, -2.6049, -2.5327], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.5594, -2.6389, -2.5868, -2.4980], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.5106, -2.5940, -2.5554, -2.4702], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.4750, -2.5567, -2.5301, -2.4554], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.4613, -2.5340, -2.5126, -2.4604], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.4777, -2.5401, -2.5141, -2.4698], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.5079, -2.5507, -2.5204, -2.4955], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.5299, -2.5740, -2.5284, -2.5262], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.5594, -2.6016, -2.5425, -2.5597], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.5940, -2.6328, -2.5657, -2.5921], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6251, -2.6576, -2.5852, -2.6193], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6541, -2.6819, -2.6051, -2.6381], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6776, -2.6971, -2.6153, -2.6494], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6915, -2.6952, -2.6048, -2.6460], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6899, -2.6861, -2.5909, -2.6408], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6858, -2.6805, -2.5757, -2.6187], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6745, -2.6731, -2.5604, -2.5938], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6574, -2.6603, -2.5463, -2.5697], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6385, -2.6485, -2.5393, -2.5508], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6222, -2.6383, -2.5361, -2.5341], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6111, -2.6311, -2.5360, -2.5212], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6040, -2.6327, -2.5364, -2.5135], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.5973, -2.6364, -2.5378, -2.5096], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.5868, -2.6388, -2.5385, -2.5085], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.5828, -2.6458, -2.5410, -2.5119], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.5806, -2.6562, -2.5443, -2.5191], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.5794, -2.6612, -2.5475, -2.5251], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.5834, -2.6648, -2.5463, -2.5347], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.5894, -2.6688, -2.5442, -2.5414], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.5923, -2.6695, -2.5419, -2.5470], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6022, -2.6738, -2.5457, -2.5602], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6146, -2.6779, -2.5515, -2.5803], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6291, -2.6829, -2.5578, -2.5963], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6441, -2.6890, -2.5636, -2.6120], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6583, -2.6935, -2.5697, -2.6218], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6715, -2.6972, -2.5718, -2.6292], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6817, -2.6962, -2.5722, -2.6287], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6916, -2.6940, -2.5750, -2.6241], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.7016, -2.6958, -2.5850, -2.6168], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.7040, -2.6862, -2.5893, -2.6085], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6857, -2.6354, -2.5815, -2.5795], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6718, -2.6094, -2.5719, -2.5645], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6600, -2.5893, -2.5676, -2.5496], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6474, -2.5775, -2.5598, -2.5346], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6339, -2.5866, -2.5556, -2.5219], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6226, -2.5991, -2.5503, -2.5126], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6208, -2.6143, -2.5539, -2.5075], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6444, -2.6749, -2.5893, -2.6347], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6173, -2.6497, -2.5717, -2.5350], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6096, -2.6471, -2.5675, -2.5442], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6028, -2.6451, -2.5636, -2.5616], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.5988, -2.6532, -2.5583, -2.5761], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6001, -2.6619, -2.5569, -2.5919], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.5985, -2.6630, -2.5586, -2.6050], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.5996, -2.6549, -2.5598, -2.6097], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6023, -2.6474, -2.5620, -2.6129], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6028, -2.6464, -2.5664, -2.6104], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6021, -2.6422, -2.5590, -2.6033], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6031, -2.6327, -2.5525, -2.5920], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6684, -2.5383, -2.6000, -2.5667], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.5148, -2.4157, -2.5411, -2.5874], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.8055, -2.4570, -2.9409, -2.5949], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.7994, -2.4445, -2.9349, -2.5851], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.7876, -2.4325, -2.9282, -2.5734], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.7800, -2.4254, -2.9241, -2.5638], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.8812, -3.0515, -3.0269, -2.1900], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9420, -2.6783, -3.4383,  4.4705], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.7415, -4.3811, -5.6948, 10.0388], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-2.6160, -2.6059, -2.5661, -2.6498], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6331, -2.6781, -2.5752, -2.7257], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6034, -2.6407, -2.5524, -2.5694], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6211, -2.6541, -2.5665, -2.5866], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.7741, -2.7103, -2.6197, -2.8227], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.0115, -2.9227, -2.7647, -3.0701], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3100, -3.2402, -2.9560, -3.3331], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.6101, -3.5613, -3.1719, -3.5912], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.9146, -3.7529, -3.5075, -3.4340], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.9705, -3.8704, -3.5439, -3.4563], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.8419, -3.6717, -3.4137, -3.0641], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.7393, -3.4861, -3.3048, -2.8028], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.6248, -3.3045, -3.1757, -2.6685], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.5260, -3.1447, -3.0946, -2.5518], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4469, -3.0292, -3.0455, -2.4997], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3206, -2.9268, -3.0187, -2.6958], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.2829, -2.9278, -3.0753, -2.8411], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.2560, -2.9457, -3.1430, -3.0156], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4418, -3.1613, -3.4993, -3.6148], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.1857, -3.1678, -3.4253, -3.2919], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.6600, -3.6190, -3.8150, -3.7336], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.6708, -3.6877, -3.8778, -3.7996], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-3.4209, -3.4850, -3.7179, -3.3115], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-3.2806, -3.6333, -3.6541, -3.8944], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-3.2645, -3.6473, -3.5803, -3.8557], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-3.2293, -3.6337, -3.4117, -3.7932], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-3.1879, -3.6110, -3.2532, -3.6967], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-3.1600, -3.5928, -3.1148, -3.6083], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.1312, -3.4135, -3.0257, -3.1333], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.1859, -3.2889, -3.0558, -3.2439], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.2370, -3.3626, -3.1101, -3.2153], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.2956, -3.4254, -3.1624, -3.2120], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3667, -3.4383, -3.1566, -3.3417], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3896, -3.4740, -3.2372, -3.3750], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3764, -3.4674, -3.3031, -3.3943], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3381, -3.4062, -3.3537, -3.4142], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3810, -3.5273, -3.5490, -3.4684], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.2145, -3.3299, -3.5491, -3.4392], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.1599, -3.2666, -3.5145, -3.4353], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.1458, -3.2313, -3.4872, -3.4406], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.1543, -3.2197, -3.4671, -3.4443], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.1850, -3.2260, -3.4601, -3.4512], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.2436, -3.2488, -3.4502, -3.4862], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4008, -3.3684, -3.3367, -3.5298], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4191, -3.2906, -3.2654, -3.4766], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4438, -3.3019, -3.2429, -3.4720], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4023, -3.2846, -3.2734, -3.2917], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3966, -3.2773, -3.2678, -3.2384], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4939, -3.3252, -3.2403, -3.3060], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4976, -3.3310, -3.2570, -3.2767], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4951, -3.3362, -3.2694, -3.2647], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4330, -3.3225, -3.2844, -3.2046], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4494, -3.3474, -3.3219, -3.2599], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4538, -3.3683, -3.3480, -3.3195], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4491, -3.3849, -3.3699, -3.3760], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4382, -3.3872, -3.3694, -3.4234], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4183, -3.3726, -3.3589, -3.4329], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3886, -3.3226, -3.3295, -3.4227], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3713, -3.2579, -3.2568, -3.3839], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3464, -3.2180, -3.2315, -3.3508], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.2177, -3.1180, -3.2278, -3.3080], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3161, -3.1381, -3.3304, -3.2495], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3138, -3.1382, -3.3283, -3.2293], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3177, -3.1500, -3.3419, -3.2255], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.1333, -3.0863, -3.2453,  2.7023], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4154, -3.0013, -3.3370,  6.4330], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.2108, -4.3690, -4.7383, 11.1135], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.4089, -3.3362, -3.3955, -3.3737], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.3293, -3.2467, -3.4230, -3.2334], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.4099, -3.3570, -3.3949, -3.3666], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.3226, -3.2657, -3.3949, -3.2003], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.3731, -3.3536, -3.3104, -3.3368], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3216, -3.3660, -3.2405, -3.4582], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3042, -3.3549, -3.2157, -3.4470], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3815, -3.3628, -3.2955, -3.3004], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3815, -3.3724, -3.2979, -3.2983], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3824, -3.3806, -3.3020, -3.3012], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3905, -3.3942, -3.3160, -3.3076], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3987, -3.4070, -3.3426, -3.3144], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4057, -3.4142, -3.3686, -3.3221], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4102, -3.4191, -3.3949, -3.3375], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4085, -3.4144, -3.4100, -3.3470], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3984, -3.4078, -3.4142, -3.3561], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3889, -3.3988, -3.4188, -3.3423], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3831, -3.3934, -3.4187, -3.3237], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3746, -3.3877, -3.4125, -3.3101], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3625, -3.3391, -3.3887, -3.2944], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3560, -3.3138, -3.3619, -3.2931], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3546, -3.2913, -3.3375, -3.2904], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3708, -3.2801, -3.3202, -3.2985], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4247, -3.2850, -3.3028, -3.2754], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4077, -3.2942, -3.3216, -3.3327], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4345, -3.3452, -3.3475, -3.3826], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4683, -3.3750, -3.3382, -3.3203], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4449, -3.3966, -3.3640, -3.4084], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4334, -3.3980, -3.3560, -3.3926], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4111, -3.3836, -3.3382, -3.3687], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3810, -3.3574, -3.3108, -3.3384], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3552, -3.3311, -3.2885, -3.3028], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3393, -3.3089, -3.2724, -3.2720], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3326, -3.2948, -3.2685, -3.2558], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3430, -3.2944, -3.2814, -3.2640], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3613, -3.3009, -3.3055, -3.2877], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3878, -3.3150, -3.3387, -3.3155], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4417, -3.3367, -3.3572, -3.2930], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4400, -3.3421, -3.4086, -3.3901], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4726, -3.3399, -3.3802, -3.3841], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4015, -3.3047, -3.3913, -3.5156], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4770, -3.3134, -3.4453, -3.2597], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3661, -3.2693, -3.3456, -3.5151], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4159, -3.2670, -3.3852, -3.2558], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3048, -3.2272, -3.2985, -3.4828], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3533, -3.2299, -3.3451, -3.2302], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3200, -3.2114, -3.3139, -3.2049], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.2347, -3.2384, -3.2480, -3.4089], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.1957, -3.3267, -3.2113, -3.1735], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3262, -3.3980, -3.2998, -3.2822], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3452, -3.4585, -3.3376, -3.3782], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3751, -3.4252, -3.3727, -3.3249], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3902, -3.4421, -3.3898, -3.3311], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3941, -3.4424, -3.4015, -3.3374], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3902, -3.4168, -3.3991, -3.3378], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3846, -3.3851, -3.3923, -3.3247], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3840, -3.3449, -3.3850, -3.3155], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3790, -3.3013, -3.3496, -3.3044], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3398, -3.1972, -3.2304, -3.3474], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.2777, -3.1732, -3.2470, -3.4094], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4635, -3.3607, -3.2791, -3.4998], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3479, -3.1625, -3.2712, -3.2448], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3661, -3.2104, -3.2902, -3.2584], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3731, -3.2492, -3.3076, -3.2646], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3723, -3.2724, -3.3217, -3.2636], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3662, -3.3491, -3.3526, -3.4945], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3360, -3.2897, -3.3314, -3.2322], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3171, -3.3494, -3.3536, -3.4574], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3003, -3.4683, -3.3556, -3.2420], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3532, -3.4497, -3.3732, -3.4208], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.3114, -3.3630, -3.4386, -3.4137], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.3186, -3.3582, -3.4265, -3.3992], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.3412, -3.3614, -3.4231, -3.4072], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.3569, -3.3540, -3.4105, -3.4119], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-3.3123, -3.4652, -3.3376, -3.2505], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.3806, -3.3102, -3.3823, -3.4227], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-3.3456, -3.4398, -3.3302, -3.3072], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.3574, -3.2364, -3.3297, -3.4235], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-3.3515, -3.3924, -3.3104, -3.3586], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4090, -3.3784, -3.2959, -3.4283], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3759, -3.2198, -3.2623, -3.4286], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3854, -3.1898, -3.2620, -3.4411], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3769, -3.1856, -3.2690, -3.4321], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3738, -3.1875, -3.2822, -3.4258], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3737, -3.2059, -3.2926, -3.4224], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3779, -3.2343, -3.3147, -3.4221], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3828, -3.2624, -3.3377, -3.4234], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3863, -3.2835, -3.3576, -3.4239], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3931, -3.3049, -3.3799, -3.4307], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3869, -3.3136, -3.3751, -3.4253], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3772, -3.3149, -3.3584, -3.4105], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3662, -3.3055, -3.3379, -3.3954], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3527, -3.2834, -3.3213, -3.3825], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3439, -3.2528, -3.3094, -3.3749], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3334, -3.2072, -3.2724, -3.3720], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3329, -3.1665, -3.2398, -3.3823], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3351, -3.1281, -3.2110, -3.3843], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3272, -3.1303, -3.1914, -3.3790], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3191, -3.1438, -3.1904, -3.3870], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3070, -3.1593, -3.1971, -3.3947], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3071, -3.1846, -3.2114, -3.4176], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3140, -3.2101, -3.2247, -3.4364], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3034, -3.2284, -3.2220, -3.4088], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3217, -3.0593, -3.0809,  3.1595], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1588, -3.3054, -3.4456,  8.1371], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.1637, -3.9277, -4.0702, 10.4385], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.3995, -3.5513, -3.3539, -3.3078], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.3924, -3.5613, -3.3450, -3.2854], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.3842, -3.5716, -3.3278, -3.2652], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.3649, -3.5525, -3.3013, -3.2422], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4502, -3.6577, -3.3327, -3.4712], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4377, -3.5680, -3.3116, -3.3421], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4427, -3.5721, -3.3047, -3.3629], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4449, -3.5706, -3.2955, -3.3870], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4411, -3.5633, -3.2832, -3.3923], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4290, -3.5393, -3.2684, -3.3888], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4154, -3.5126, -3.2542, -3.3681], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4060, -3.4997, -3.2517, -3.3394], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3942, -3.4873, -3.2598, -3.2993], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3741, -3.4668, -3.2709, -3.2589], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3611, -3.4469, -3.2870, -3.2476], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3489, -3.4241, -3.2919, -3.2531], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3453, -3.3992, -3.3019, -3.2774], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3522, -3.3817, -3.3224, -3.3081], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3550, -3.3645, -3.3434, -3.3376], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3558, -3.3403, -3.3629, -3.3607], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3409, -3.2716, -3.3168, -3.3017], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3645, -3.3233, -3.4113, -3.3927], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3668, -3.2919, -3.3731, -3.3130], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3493, -3.2493, -3.3543, -3.2914], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3327, -3.2129, -3.3345, -3.2713], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3133, -3.1842, -3.2990, -3.2409], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.2954, -3.1608, -3.2680, -3.2043], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.2826, -3.1429, -3.2339, -3.1700], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.2724, -3.1270, -3.1974, -3.1328], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.2720, -3.1338, -3.1730, -3.1051], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3565, -3.2266, -3.2783, -3.2058], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3711, -3.1694, -3.2071, -3.3486], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4205, -3.2862, -3.3118, -3.1854], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4311, -3.2519, -3.2571, -3.3788], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4760, -3.3770, -3.3675, -3.2232], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4711, -3.3378, -3.3070, -3.4254], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4587, -3.3653, -3.3173, -3.4360], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4317, -3.3764, -3.2996, -3.4422], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3924, -3.3890, -3.2816, -3.4415], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3532, -3.3915, -3.2528, -3.4368], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3173, -3.3903, -3.2249, -3.4300], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.2837, -3.3808, -3.2047, -3.4247], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.2603, -3.3708, -3.1997, -3.4168], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.2381, -3.3495, -3.1997, -3.4138], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.2273, -3.3361, -3.2221, -3.4362], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.2722, -3.3448, -3.2709, -3.4738], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3272, -3.3566, -3.3305, -3.5169], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.2909, -3.3940, -3.3304, -3.4256], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.2929, -3.2845, -3.4345, -3.4505], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-3.3395, -3.5034, -3.4735, -3.4882], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-3.3511, -3.4880, -3.4708, -3.4657], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-3.3522, -3.4675, -3.4564, -3.4384], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-3.3496, -3.4457, -3.4258, -3.4122], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-3.3407, -3.4228, -3.3905, -3.3848], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-3.3261, -3.3986, -3.3512, -3.3629], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-3.3098, -3.3739, -3.2995, -3.3470], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.2583, -3.2578, -3.1564, -3.2534], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3249, -3.2779, -3.1959, -3.2981], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3496, -3.3664, -3.2142, -3.3284], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3797, -3.4660, -3.2384, -3.3517], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4200, -3.5802, -3.2732, -3.3779], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4463, -3.6692, -3.2944, -3.3959], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4548, -3.7319, -3.3292, -3.3975], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4477, -3.7493, -3.3507, -3.3882], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4333, -3.7227, -3.3789, -3.3748], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3293, -3.5117, -3.2807, -3.0672], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4237, -3.5602, -3.2790, -3.2871], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4730, -3.4562, -3.0735, -3.4194], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.6357, -3.4994, -3.1558, -3.6217], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.8802, -3.6492, -3.3119, -3.9313], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1648, -3.8541, -3.5384, -4.2699], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.4636, -4.0944, -3.8754, -4.6077], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7005, -4.2941, -4.1907, -4.7759], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8951, -4.4260, -4.4229, -4.8083], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9265, -4.4060, -4.2570, -4.6041], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7691, -4.2634, -4.0237, -4.2448], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5341, -4.1010, -3.8034, -3.9046], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3484, -3.9682, -3.6501, -3.6436], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2157, -3.8864, -3.5602, -3.4842], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0910, -3.8479, -3.4148, -3.4132], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0413, -3.8695, -3.3647, -3.4373], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0261, -3.9175, -3.4237, -3.5048], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0401, -3.9850, -3.5587, -3.6380], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0466, -4.0702, -3.6149, -3.8111], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0973, -4.2032, -3.7261, -4.0155], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1276, -4.2685, -3.8201, -4.2046], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1423, -4.2962, -3.9143, -4.3391], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0793, -4.2250, -3.8676, -4.3248], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0139, -4.1606, -3.8178, -4.2615], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.9837, -4.1337, -3.7796, -4.1530], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.9405, -4.0633, -3.7477, -4.0007], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.9259, -4.0100, -3.7579, -3.8695], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.9440, -4.0181, -3.8459, -3.7914], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.9818, -4.0354, -3.9499, -3.7433], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0471, -4.0768, -4.0718, -3.7565], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1416, -4.1905, -4.1903, -3.8033], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2746, -4.2955, -4.2945, -3.8806], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2952, -4.2587, -4.3350, -3.9163], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2592, -4.1888, -4.2323, -3.9376], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2718, -4.1720, -4.1390, -3.9720], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2565, -4.1187, -4.0258, -3.9794], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2707, -4.0726, -3.9133, -3.9803], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3048, -4.1111, -3.8373, -4.0050], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3513, -4.1740, -3.8103, -4.0536], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3631, -4.2423, -3.8340, -4.0996], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3092, -4.2162, -3.8634, -4.0963], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2303, -4.1842, -3.9085, -4.0767], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1453, -4.1468, -3.9660, -4.0211], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0667, -4.0741, -4.0135, -3.9433], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.9911, -3.9648, -4.0545, -3.8558], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.9327, -3.8853, -4.0957, -3.7852], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.8794, -3.8023, -4.1318, -3.7119], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.8668, -3.7474, -4.1568, -3.6588], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.8771, -3.7140, -4.1838, -3.6572], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.8947, -3.7148, -4.1949, -3.6809], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.9032, -3.7092, -4.1580, -3.7003], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.9224, -3.7379, -4.1004, -3.7408], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.8957, -3.8301, -3.5922, -3.6758], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.9713, -3.8844, -3.6130, -3.7444], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0365, -3.9344, -3.6346, -3.8162], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0730, -3.9802, -3.6597, -3.8995], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0926, -3.9918, -3.6883, -3.9512], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0946, -3.9779, -3.7178, -3.9568], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0546, -3.9612, -3.7471, -3.9932], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.9976, -3.9630, -3.7906, -4.0590], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.9572, -3.9588, -3.8482, -4.1150], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.9470, -3.9661, -3.9080, -4.1743], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.9566, -4.0304, -3.9824, -4.2532], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.8151, -3.9628, -3.9753, -4.1340], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.9531, -3.9856, -4.5570, -4.3756], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.9508, -4.0030, -4.5587, -4.2944], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.9370, -4.0122, -4.5430, -4.2115], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.9297, -4.0313, -4.5358, -4.1484], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.9402, -4.0491, -4.5292, -4.0927], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.9570, -4.0462, -4.5071, -4.0392], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.9612, -4.0354, -4.4611, -3.9993], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.9663, -4.0159, -4.4186, -3.9856], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.9813, -4.0061, -4.3953, -4.0238], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-4.0159, -3.9953, -4.3761, -4.0863], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-3.9209, -4.1736, -3.9871, -4.0437], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-3.9732, -4.1827, -4.0036, -4.1543], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-4.0214, -4.1935, -4.0048, -4.2288], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3274, -4.3108, -4.2478, -4.4704], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2265, -4.0718, -4.2232, -4.3483], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.6831, -4.0622, -2.9785, -4.1477], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3074,  0.9757, -0.2620,  5.7013], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.1431,  1.3845, -0.6372,  8.0703], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.5083,  1.8158, -0.3818, 10.0770], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.0731, -3.9485, -4.0476, -3.9453], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.0412, -3.9607, -4.0743, -3.9282], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.9845, -3.9588, -4.0868, -3.9110], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.9337, -3.9626, -4.0946, -3.9391], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.8802, -3.9439, -4.0718, -3.9446], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.8351, -3.9202, -4.0501, -3.9522], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.8426, -3.9124, -4.0404, -3.9700], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.8559, -3.9163, -4.0232, -3.9910], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.8350, -3.8836, -3.8831, -3.9856], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.8030, -3.8469, -3.6594, -3.9551], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.9382, -3.9633, -3.5771, -3.9208], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.7668, -3.8288, -3.5029, -4.0158], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.8221, -3.9142, -3.5117, -4.0708], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.9028, -4.0237, -3.6081, -4.1396], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.9989, -4.1382, -3.7418, -4.1983], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0905, -4.2424, -3.8977, -4.2406], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1455, -4.3221, -4.0381, -4.2322], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1640, -4.3425, -4.1404, -4.1284], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1118, -4.3084, -4.1898, -3.9962], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0424, -4.2450, -4.1920, -3.8526], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.9620, -4.1185, -4.1528, -3.6920], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.8812, -3.9795, -4.1130, -3.5997], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.8157, -3.8897, -4.0953, -3.5909], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.7400, -3.7960, -3.9927, -3.6231], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.6597, -3.7187, -3.8262, -3.6680], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.7061, -3.7565, -3.5617, -3.5319], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.6978, -3.7225, -3.4958, -3.5850], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.7167, -3.8285, -3.7469, -4.0568], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.8037, -3.9669, -3.5544, -3.8812], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.9036, -4.2182, -3.9683, -4.4674], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.9581, -4.2698, -3.7649, -4.1085], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0763, -4.4463, -4.1846, -4.5797], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0897, -4.3368, -3.9201, -4.1025], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1172, -4.2872, -4.2519, -4.3497], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1175, -4.1701, -3.9065, -3.8456], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1102, -4.0608, -3.8582, -3.7169], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1025, -3.9674, -3.8220, -3.6288], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1065, -3.9040, -3.7995, -3.5722], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1007, -3.8496, -3.7219, -3.5516], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0948, -3.8131, -3.6682, -3.5784], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1042, -3.8424, -3.5866, -3.6806], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0005, -3.7319, -3.7105, -3.8805], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0632, -3.8486, -3.7081, -4.0684], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1561, -3.9767, -3.7270, -4.2474], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2330, -4.0713, -3.7411, -4.3687], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2936, -4.1407, -3.8140, -4.4418], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3030, -4.1554, -3.9010, -4.4407], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2394, -4.1250, -3.9825, -4.3757], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1388, -4.0426, -4.0424, -4.2419], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0424, -3.9686, -4.1498, -4.1333], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2808, -4.1097, -4.2974, -4.1106], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.8404, -3.7143, -4.2847, -3.6780], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2193, -4.0784, -4.5253, -4.0898], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.7649, -3.6005, -4.2466, -3.5152], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2618, -4.1121, -4.5096, -4.1097], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.9162, -3.9851, -4.1179, -4.0261], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1557, -4.2524, -3.8400, -4.0697], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0325, -4.1415, -3.7987, -4.0848], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0869, -4.1785, -3.7015, -4.1187], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1550, -4.2343, -3.6371, -4.1693], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1895, -4.2398, -3.6568, -4.1856], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1844, -4.1930, -3.7305, -4.1676], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1495, -4.0808, -3.8225, -4.0946], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0394, -3.9616, -3.9487, -3.9919], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.9341, -3.8359, -4.0470, -3.9064], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2621, -3.8635, -4.2885, -3.9390], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.8136, -3.6280, -3.9848, -3.7996], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.7896, -3.8362,  0.8403, -3.8016], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0253,  1.2924,  0.9862,  6.0819], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.2863,  1.6926,  0.3492,  7.9442], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.5851,  2.0325,  1.4745,  9.9786], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.1355, -4.1170, -3.8773, -4.3049], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3343, -4.3613, -4.2794, -4.4585], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.9878, -4.1588, -4.2434, -4.2220], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0769, -4.2753, -4.0323, -4.1334], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.7444, -4.0674, -3.9624, -3.9513], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.8285, -4.1313, -3.8025, -3.8460], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.6211, -3.9855, -3.7819, -3.7761], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.7333, -4.0385, -3.7510, -3.6872], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.7304, -4.0147, -3.7494, -3.6710], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.7482, -3.9855, -3.7942, -3.6756], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.8069, -3.9750, -3.8528, -3.7067], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.9458, -4.0064, -3.7872, -3.9583], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0113, -3.9801, -3.9330, -3.8613], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0441, -3.9798, -3.9233, -3.9038], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0543, -3.9755, -3.8923, -3.9425], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0699, -4.0731, -4.0410, -4.2864], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0696, -4.0727, -4.0211, -4.2752], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0575, -4.0639, -3.9812, -4.2266], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0410, -4.0453, -3.9623, -4.1685], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0174, -4.0148, -3.9502, -4.1092], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.9889, -3.9718, -3.9888, -4.0484], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1426, -3.9302, -3.7637, -3.8518], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1031, -3.9161, -3.8251, -3.8559], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0829, -3.9163, -3.8980, -3.9162], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1049, -3.9294, -3.9582, -3.9784], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0187, -3.9606, -4.3630, -4.0412], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0263, -4.0692,  1.5043, -4.0357], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.9790,  1.4973,  1.0139,  6.1534], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.1665,  1.9039,  0.3810,  7.9042], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.3951,  2.3329,  1.5049,  9.8128], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.1322, -4.1107, -4.0250, -4.0961], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1615, -4.0792, -3.9598, -3.9560], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1664, -4.0771, -3.8853, -3.9183], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.9593, -3.9565, -3.8104, -3.9361], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.9055, -3.9052, -3.7410, -3.8595], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.8212, -3.8693, -3.7841, -3.7888], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.8009, -3.8907, -3.8574, -3.8021], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0460, -4.1396, -3.9692, -3.9239], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0522, -4.1999, -4.0314, -4.0115], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0600, -4.2591, -4.0808, -4.1005], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.9564, -4.2201, -4.1236, -4.1037], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.9505, -4.2389, -4.1461, -4.1300], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.9461, -4.2474, -4.1441, -4.1379], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.9557, -4.2527, -4.1403, -4.1264], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.9578, -4.2036, -4.1303, -4.0988], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.9497, -4.1370, -4.0898, -4.0480], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.9492, -4.0723, -3.9958, -3.9956], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.9541, -4.0133, -3.9119, -3.9487], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0878, -4.0496, -3.8311, -3.9853], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.9102, -3.8447, -3.8300, -3.9278], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.9899, -3.8643, -3.8284, -3.9486], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0563, -3.8992, -3.8414, -4.0119], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1184, -3.9388, -3.9055, -4.0750], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1541, -3.9746, -3.9732, -4.1339], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1700, -3.9993, -4.0649, -4.1718], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2668, -4.0093, -3.8753, -4.0750], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2268, -4.0261, -3.9872, -4.1202], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1792, -4.0362, -4.0913, -4.1641], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0654, -4.0453, -4.3947, -4.1188], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0925, -4.1308,  1.6331, -4.2307], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0025,  1.6303,  1.7359,  6.2463], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.1479,  1.9792,  1.1281,  7.9328], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.4213,  2.4511,  2.4641,  9.9701], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "end game\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import gym \n",
    "import gym_examples\n",
    "from doubledqn import Agent\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('gym_examples/GridWorld-v0', size=4)\n",
    "agent = Agent(gamma=0.9, epsilon=0.5, batch_size=10, n_actions=4, eps_end=0.01, input_dims=[16], lr=0.003, update_freq=50)\n",
    "scores, eps_hist,avg_scores_ddqn = [], [], []\n",
    "n_games = 10\n",
    "\n",
    "for i in range(10):\n",
    "    score = 0\n",
    "    done=False\n",
    "    observation = env.reset()[0]['agent']\n",
    "    counter = 0\n",
    "\n",
    "    while not done:\n",
    "        agent_location = env.get_agent_location()\n",
    "        print(f\"Agent's Location: {agent_location}\")\n",
    "\n",
    "        # choose an action based on current state of env\n",
    "        action = agent.choose_action(observation)\n",
    "\n",
    "        observation_, reward, done, truncated, info = env.step(action)\n",
    "        observation_ = observation_['agent'] # since one hot encoded state is nested in dictionary\n",
    "\n",
    "        score += reward\n",
    "\n",
    "        # store transition and update weights\n",
    "        agent.store_transitions(observation, action, reward, observation_, done)\n",
    "        agent.learn()\n",
    "\n",
    "        #update state\n",
    "        observation = observation_\n",
    "        \n",
    "        # end of an episode\n",
    "        counter += 1\n",
    "        print('---')\n",
    "\n",
    "    scores.append(score)\n",
    "    eps_hist.append(agent.epsilon)\n",
    "    print('end game')\n",
    "\n",
    "    avg_score = np.mean(scores)\n",
    "    \n",
    "    avg_scores_ddqn.append(avg_score)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-148.01809523809527"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(avg_scores_ddqn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you've run both your DQN and Double DQN for the same number of episodes\n",
    "episodes = list(range(1, n_games + 1))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot for regular DQN\n",
    "plt.plot(episodes, avg_scores_dqn, marker='o', linestyle='-', color='b', label='DQN Average Score per Episode')\n",
    "\n",
    "# Plot for Double DQN\n",
    "plt.plot(episodes, avg_scores_ddqn, marker='x', linestyle='--', color='r', label='Double DQN Average Score per Episode')\n",
    "\n",
    "plt.title('Average Scores Over Episodes for DQN and Double DQN')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Score')\n",
    "plt.legend()  # This ensures the label for each line is shown\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-188, -111, -182, -222, -64, -101, -138, -60, -20, -23]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
