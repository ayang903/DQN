{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent's INITIAL Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [1 3], Target Location: [0 0]\n",
      "Agent's Location: [0 3], Target Location: [0 0]\n",
      "Agent's Location: [0 3], Target Location: [0 0]\n",
      "Agent's Location: [0 3], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 3], Target Location: [0 0]\n",
      "Agent's Location: [0 3], Target Location: [0 0]\n",
      "Agent's Location: [1 3], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [1 3], Target Location: [0 0]\n",
      "Agent's Location: [0 3], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 3], Target Location: [0 0]\n",
      "Agent's Location: [0 3], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 3], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 1], Target Location: [0 0]\n",
      "Agent's Location: [0 0], Target Location: [0 0]\n",
      "Episode finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andyyang/Desktop/DS598 Reinforcement Learning/gym-examples/.env/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "# Complete random movements\n",
    "\n",
    "import gym\n",
    "import gym_examples\n",
    "# import numpy as np\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make('gym_examples/GridWorld-v0', size=4)\n",
    "\n",
    "# Reset the environment to start a new episode\n",
    "state = env.reset()\n",
    "agent_location = env.get_agent_location()\n",
    "target_location = env.get_target_location()\n",
    "print(f\"Agent's INITIAL Location: {agent_location}, Target Location: {target_location}\")\n",
    "# Initialize done to False\n",
    "done = False\n",
    "\n",
    "# Loop until the episode ends\n",
    "while not done:\n",
    "    # Select an action randomly\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # Apply the action to the environment\n",
    "    next_state, reward, done, truncated, info = env.step(action)\n",
    "    \n",
    "    # Optionally, render the environment to visualize it\n",
    "    # env.render()\n",
    "    agent_location = env.get_agent_location()\n",
    "    target_location = env.get_target_location()\n",
    "    print(f\"Agent's Location: {agent_location}, Target Location: {target_location}\")\n",
    "\n",
    "    # Update state\n",
    "    state = next_state\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "\n",
    "print(\"Episode finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0327, 0.0611, 0.0447, 0.0547], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([0.0283, 0.0731, 0.0763, 0.0709], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0766, 0.0389, 0.0202, 0.0806], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0327, 0.0611, 0.0447, 0.0547], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([0.0283, 0.0731, 0.0763, 0.0709], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0692, 0.0806, 0.0611, 0.0763, 0.0763, 0.0766, 0.0709, 0.0327, 0.0547,\n",
      "        0.0611], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9450, -0.9377, -0.9314, -0.9274, -0.9274, -0.9314, -0.9450, -0.9450,\n",
      "        -0.9450, -0.9314], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.0053720474243164\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([ 0.1371,  0.0005,  0.0013,  0.0013, -0.0707, -0.0324,  0.1371,  0.1580,\n",
      "         0.1580, -0.0370], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.8578, -0.8767, -0.8767, -0.8767, -0.8767, -0.8578, -0.8578, -0.9165,\n",
      "        -0.9165, -0.8767], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.8635309338569641\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-0.0690, -0.0577,  0.0245,  0.2089,  0.2086, -0.0577,  0.2089, -0.1332,\n",
      "        -0.1535, -0.1134], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.8120, -0.8120, -0.8383, -0.8123, -0.8570, -0.8120, -0.8123, -0.8120,\n",
      "        -0.8120, -0.8123], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.703802227973938\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.2305,  0.2217,  0.2487, -0.1080], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.2305,  0.1944, -0.1879, -0.0355, -0.1292,  0.2742,  0.2846,  0.2742,\n",
      "         0.2487,  0.2846], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.7439, -0.7762, -0.7532, -0.7762, -0.7439, -0.7975, -0.7532, -0.7975,\n",
      "        -0.8032, -0.7532], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.8009533882141113\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([ 0.2185, -0.2102, -0.1028,  0.3314,  0.3517,  0.2067, -0.3183, -0.2258,\n",
      "         0.3314, -0.2658], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.7242, -0.6835, -0.7242, -0.7316, -0.7017, -0.7284, -0.6835, -0.6835,\n",
      "        -0.7316, -0.7017], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6275495290756226\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3629,  0.3917,  0.2249, -0.2014], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.4302, -0.4324,  0.3917,  0.4302, -0.1766, -0.4011, -0.3158,  0.2215,\n",
      "         0.3552,  0.2249], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6803, -0.6129, -0.6859, -0.6803, -0.6623, -0.6129, -0.6129, -0.6623,\n",
      "        -0.6562, -0.6475], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6594301462173462\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4594,  0.4346,  0.2476, -0.3311], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.2100,  0.3723, -0.4813,  0.5017, -0.3904, -0.2546, -0.4116,  0.3772,\n",
      "         0.2167,  0.4721], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5788, -0.5751, -0.5484, -0.6290, -0.5484, -0.5788, -0.5484, -0.5807,\n",
      "        -0.5751, -0.6089], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5664698481559753\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([ 0.3587, -0.4765, -0.5544,  0.3522,  0.1782, -0.4676, -0.6269, -0.4765,\n",
      "         0.5829, -0.3337], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4978, -0.4754, -0.4754, -0.4965, -0.4965, -0.5316, -0.4754, -0.4754,\n",
      "        -0.5491, -0.4856], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3246987760066986\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.5181,  0.6155,  0.1774, -0.4491], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.6156,  0.1190,  0.6572,  0.6598,  0.6323,  0.3064, -0.5530,  0.1259,\n",
      "         0.3159, -0.5730], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4062, -0.3903, -0.4310, -0.4612, -0.4237, -0.4086, -0.4062, -0.4086,\n",
      "        -0.4111, -0.4612], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5218505263328552\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.5904,  0.6929,  0.1166, -0.5129], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.2414,  0.7299, -0.6604, -0.6119, -0.6314,  0.0600, -0.7725,  0.7307,\n",
      "         0.6929,  0.7299], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3217, -0.3674, -0.3431, -0.3431, -0.3674, -0.3217, -0.3431, -0.3257,\n",
      "        -0.3763, -0.3674], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5556877851486206\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.6093,  0.7462,  0.0601, -0.5482], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.1696,  0.7462, -0.7108, -0.6608, -0.6411, -0.0166,  0.7923,  0.7604,\n",
      "        -0.5321,  0.1891], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.2486, -0.3284, -0.3156, -0.3156, -0.3156, -0.2224, -0.2424, -0.2855,\n",
      "        -0.2224, -0.2586], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4214347302913666\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.6116,  0.7794,  0.0018, -0.5567], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.1169,  0.7660, -0.6464, -0.6390,  0.7794,  0.8411, -0.8124, -0.0753,\n",
      "        -0.6766,  0.7794], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.2083, -0.2153, -0.3106, -0.3106, -0.2985, -0.2985, -0.3106, -0.1867,\n",
      "        -0.2430, -0.2985], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.536433219909668\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.5982,  0.8051, -0.0571, -0.5605], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.9473, -0.1438,  0.8412,  0.7421,  0.0192, -0.5634, -0.6642,  0.8585,\n",
      "         0.8051,  0.0437], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.2429, -0.1474, -0.1249, -0.1656, -0.1474, -0.1409, -0.2273, -0.2754,\n",
      "        -0.2754, -0.1786], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6073626279830933\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.5774,  0.8236, -0.1152, -0.5552], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.5617, -0.2102,  0.8559,  0.8419, -0.7441, -0.5921, -0.7501, -0.0288,\n",
      "         0.7100,  0.8236], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3610, -0.1331, -0.2588, -0.0905, -0.3610, -0.3610, -0.3610, -0.1621,\n",
      "        -0.1258, -0.2588], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4397614896297455\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-0.0937,  0.8309,  0.8309, -0.1666, -0.5531,  0.8378, -0.2684, -0.2379,\n",
      "         0.6674, -0.5531], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.1587, -0.2522, -0.2522, -0.1164, -0.3994, -0.2522, -0.1310, -0.1339,\n",
      "        -0.0977, -0.3994], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.420341432094574\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-0.1500,  0.8117, -0.3152, -0.2096, -0.5789, -0.1500, -0.5124, -0.4672,\n",
      "        -0.5124, -0.5897], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.1621, -0.2483, -0.1360, -0.1253, -0.2483, -0.1621, -0.4417, -0.4417,\n",
      "        -0.4417, -0.0744], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15485996007919312\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-0.3501,  0.5708, -0.4720, -0.6892, -0.1969, -0.2038, -0.4996, -0.5427,\n",
      "         0.8288,  0.8288], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.1556, -0.0722, -0.4863, -0.4863, -0.1792, -0.1556, -0.1665, -0.3065,\n",
      "        -0.2541, -0.2541], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.30073824524879456\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-0.5476, -0.2348, -0.2987, -0.4450, -0.2741, -0.4321, -0.3759,  0.9068,\n",
      "         0.5211, -0.4951], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5310, -0.2045, -0.1977, -0.2045, -0.1784, -0.5310, -0.5310, -0.3484,\n",
      "        -0.0765, -0.3470], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.20668940246105194\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([ 0.7920, -0.2658, -0.4503,  0.7920, -0.3970,  0.4767, -0.4090, -0.4321,\n",
      "        -0.4974, -0.3381], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.2872, -0.2319, -0.3863, -0.2872, -0.5709, -0.0804, -0.2319, -0.2872,\n",
      "        -0.5709, -0.5709], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2787213623523712\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4543,  0.8216, -0.2744, -0.4246], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.6044,  0.7711,  0.7711, -0.3907,  0.7711,  0.7711, -0.4543,  0.8267,\n",
      "        -0.2930, -0.4116], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6068, -0.3060, -0.3060, -0.3060, -0.3060, -0.3060, -0.6068, -0.4042,\n",
      "        -0.2588, -0.4228], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6187528371810913\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-0.3112,  0.4006,  0.4006,  0.6013,  0.7440, -0.2895,  0.7782, -0.4189,\n",
      "         0.7440,  0.7920], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.2925, -0.0876, -0.0876, -0.3304, -0.3304, -0.2996, -0.4266, -0.6395,\n",
      "        -0.3304, -0.4588], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6718708276748657\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-0.3567, -0.3947, -0.5645,  0.7260,  0.7260,  0.7311, -0.4226, -0.3360,\n",
      "        -0.3221,  0.5677], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3202, -0.6693, -0.6693, -0.3466, -0.3466, -0.4394, -0.3420, -0.3197,\n",
      "        -0.6693, -0.3466], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.47220277786254883\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-0.3064,  0.5354,  0.7299,  0.7250,  0.6831, -0.3402, -0.3800, -0.3100,\n",
      "        -0.3127, -0.3486], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3554, -0.3554, -0.3475, -0.5182, -0.4435, -0.3431, -0.6914, -0.6914,\n",
      "        -0.3852, -0.3375], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5019186735153198\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.2196,  0.5104, -0.2726, -0.2900], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.5104,  0.7152,  0.6973, -0.3053, -0.3512,  0.6973,  0.7013,  0.6228,\n",
      "        -0.3253,  0.3254], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3563, -0.3563, -0.5406, -0.7072, -0.0779, -0.5406, -0.3725, -0.1497,\n",
      "        -0.5406, -0.0779], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.7158783674240112\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3391,  0.7153, -0.3714, -0.3694], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.3126, -0.3220,  0.7153, -0.3391,  0.3074,  0.6624,  0.4940, -0.3064,\n",
      "        -0.3714, -0.2833], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4012, -0.4012, -0.3563, -0.0741, -0.0741, -0.5554, -0.3563, -0.7233,\n",
      "        -0.3874, -0.3563], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3763420581817627\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3380,  0.7214, -0.3835, -0.3774], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.2851,  0.7214,  0.7214, -0.3190, -0.3246, -0.3504,  0.7214, -0.5602,\n",
      "        -0.2336,  0.5583], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4360, -0.3507, -0.3507, -0.4282, -0.0675, -0.7372, -0.3507, -0.7372,\n",
      "        -0.7372, -0.4420], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.49843257665634155\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3433,  0.7280, -0.3953, -0.3913], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.7280, -0.4175,  0.7280, -0.3424, -0.3171, -0.3253, -0.3230,  0.4722,\n",
      "        -0.3175, -0.5760], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3448, -0.4610, -0.3448, -0.5285, -0.5750, -0.7447, -0.4490, -0.3448,\n",
      "        -0.0608, -0.7447], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3358518183231354\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-0.4385, -0.3444,  0.7408, -0.6020, -0.3348,  0.5971, -0.3259,  0.6400,\n",
      "         0.7408, -0.2724], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5521, -0.7476, -0.3332, -0.7476, -0.4626, -0.4802, -0.4626, -0.1485,\n",
      "        -0.3332, -0.3332], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.43248850107192993\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([ 0.7540,  0.5674,  0.7540,  0.2811, -0.3340, -0.3098, -0.6381, -0.4466,\n",
      "        -0.3700,  0.5674], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3214, -0.5860, -0.3214, -0.0385, -0.5860, -0.0385, -0.7470, -0.5704,\n",
      "        -0.7470, -0.5860], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5382756590843201\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-0.2782, -0.4800, -0.3403,  0.7638, -0.3675, -0.3993, -0.4773, -0.3755,\n",
      "         0.4656,  0.7638], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3126, -0.5065, -0.5065, -0.3126, -0.3126, -0.7479, -0.5810, -0.4909,\n",
      "        -0.3126, -0.3126], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.310081422328949\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([ 0.7808, -0.5016,  0.4711,  0.4711, -0.3051,  0.7808, -0.2500,  0.6691,\n",
      "        -0.3673, -0.4655], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.2972, -0.5174, -0.2972, -0.2972, -0.0116, -0.2972, -0.7454, -0.0996,\n",
      "        -0.5174, -0.5961], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.44675788283348083\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3885,  0.8060, -0.4781, -0.5617], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.4663, -0.3043, -0.5238, -0.3680, -0.5238,  0.8060, -0.4265, -0.3832,\n",
      "         0.8060,  0.4415], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.7400,  0.0046, -0.5247, -0.5041, -0.5247, -0.2746, -0.5041, -0.2746,\n",
      "        -0.2746, -0.3889], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3231562376022339\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3957,  0.8243, -0.4950, -0.6040], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.8243, -0.4950,  0.8243, -0.2933,  0.4703, -0.3875,  0.8243, -0.4149,\n",
      "         0.5194, -0.2632], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.2582, -0.4382, -0.2582, -0.2582, -0.2582, -0.5768, -0.2582, -0.5325,\n",
      "        -0.5768, -0.7353], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5524190664291382\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4115,  0.8448, -0.5108, -0.6460], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.6460, -0.4396,  0.8448, -0.4115, -0.4281,  0.7025, -0.5629,  0.8448,\n",
      "         0.8448, -0.4778], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5753, -0.5422, -0.2397, -0.5832, -0.6249, -0.0315, -0.5422, -0.2397,\n",
      "        -0.2397, -0.5142], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4152641296386719\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4296,  0.8612, -0.5278, -0.6845], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.3146, -0.4420,  0.7096, -0.6845, -0.5020,  0.8612, -0.5678, -0.5678,\n",
      "         0.8612, -0.3840], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.2249, -0.6322, -0.0040, -0.5734, -0.5171, -0.2249, -0.7240, -0.7240,\n",
      "        -0.2249, -0.7240], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3089748024940491\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4494,  0.8753, -0.5461, -0.7206], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.4042, -0.6047, -0.5172, -0.4155, -0.4592,  0.8753, -0.4001,  0.7133,\n",
      "        -0.9533,  0.8753], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3580, -0.5550, -0.6362, -0.2122, -0.6362, -0.2122, -0.7159,  0.0303,\n",
      "        -0.7159, -0.2122], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.36583518981933594\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4712,  0.8852, -0.5623, -0.7465], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.8852, -0.6237, -0.4770, -0.3244,  0.3966, -0.5063, -0.9818,  0.8852,\n",
      "         0.4884,  0.8852], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.2033, -0.5604, -0.6431, -0.7086, -0.3548, -0.5604, -0.7086, -0.2033,\n",
      "        -0.5691, -0.2033], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5494408011436462\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-0.5772,  0.8909, -0.6562, -0.6397,  0.8909,  0.4848,  0.8909,  0.8909,\n",
      "        -0.4305,  0.3299], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4283, -0.1982, -0.7031, -0.5698, -0.1982, -0.1982, -0.1982, -0.1982,\n",
      "        -0.1982,  0.0682], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5362647771835327\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([ 0.8884,  0.4643, -0.5822, -0.5689, -0.6461, -0.3445, -0.3780,  0.4812,\n",
      "        -0.6461,  0.8884], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.2005, -0.5669, -0.4362, -0.5259, -0.5821,  0.0718, -0.2005, -0.2005,\n",
      "        -0.5821, -0.2005], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.41354164481163025\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-0.5457,  0.8747, -0.4200,  0.8747,  0.8747,  0.4705,  0.8747, -0.5417,\n",
      "         0.7119, -0.4592], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6868, -0.2127, -0.2127, -0.2127, -0.2127, -0.2127, -0.2127, -0.6047,\n",
      "         0.1338, -0.5426], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5605151653289795\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([ 0.8589, -0.5484,  0.8589, -0.7129, -0.5766, -0.5293,  0.8589, -0.5766,\n",
      "        -0.3867,  0.4148], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.2269, -0.6267, -0.2269, -0.7170, -0.4772, -0.6117, -0.2269, -0.4772,\n",
      "        -0.7170, -0.5907], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.46902018785476685\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([ 0.3891,  0.8397,  0.3054,  0.8397,  0.3054, -0.5402, -0.3456,  0.8397,\n",
      "        -0.6338,  0.8397], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6000, -0.2443,  0.0270, -0.2443,  0.0270, -0.6257,  0.0270, -0.2443,\n",
      "        -0.6498, -0.2443], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5979710221290588\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([ 0.2898,  0.8135,  0.4360, -0.7450,  0.3578, -0.4627,  0.8135,  0.4493,\n",
      "         0.8135,  0.8135], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.0093, -0.2679, -0.2679, -0.7392, -0.6076, -0.5956, -0.2679, -0.6780,\n",
      "        -0.2679, -0.2679], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.7471832036972046\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([ 0.7878,  0.3265, -0.5901, -0.7587,  0.7878,  0.7878, -0.5579, -0.7587,\n",
      "        -0.6578, -0.8129], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.2910, -0.6161, -0.6257, -0.7560, -0.2910, -0.2910, -0.5532, -0.7560,\n",
      "        -0.6161, -0.6161], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4421519637107849\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-0.6599,  1.0799, -0.6202, -1.0442], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.7559,  0.7559,  0.3805,  0.7559, -0.4283,  0.7559,  0.6048, -0.3759,\n",
      "        -0.5134,  0.7559], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3197, -0.3197, -0.7375, -0.3197, -0.7753, -0.3197,  0.1432, -0.3197,\n",
      "        -0.7753, -0.3197], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.7439773678779602\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4662,  0.4070, -0.5376, -0.7261], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.7145,  0.5695, -0.5470,  0.2067, -0.7786,  0.7145,  0.7145, -0.3191,\n",
      "        -0.5925,  0.7145], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3570,  0.1344, -0.6137, -0.4875, -0.7932, -0.3570, -0.3570, -0.6337,\n",
      "        -0.7652, -0.3570], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5396683216094971\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([ 0.6754, -0.4703,  0.3943,  0.6754,  0.2346,  0.3943, -0.7151, -0.5963,\n",
      "        -0.5471, -0.6155], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3921, -0.7338, -0.3921, -0.3921, -0.6452, -0.3921, -0.6452, -0.7338,\n",
      "        -0.6383, -0.7888], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4421781897544861\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.5851,  0.6587, -0.5554, -0.8191], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.6587, -0.5497,  0.2116,  0.3854, -0.7388, -0.6047, -0.3707,  0.6587,\n",
      "        -0.5851, -0.6260], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4071, -0.8192, -0.6531, -0.4071, -0.6531, -0.7572, -0.4071, -0.4071,\n",
      "        -0.7236, -0.8096], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3805457055568695\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([ 0.2521, -0.8326, -0.5747,  0.6550, -0.3770, -0.4242, -0.8262,  0.6550,\n",
      "         0.6550,  0.6550], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.8281, -0.6567, -0.8744, -0.4105, -0.4105, -0.4105, -0.8261, -0.4105,\n",
      "        -0.4105, -0.4105], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5829983949661255\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.6226,  0.6372, -0.5838, -0.8383], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.8383, -0.8404,  0.6372,  0.1816,  0.6372, -0.6443,  0.4983, -0.3876,\n",
      "         0.6372,  0.3759], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6673, -0.8365, -0.4265, -0.0920, -0.4265, -0.8458,  0.1728, -0.4265,\n",
      "        -0.4265, -0.7344], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4879376292228699\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.6365,  0.6134, -0.5994, -0.8408], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.6107, -0.6838,  0.1653, -0.6838, -0.3891, -0.6558,  0.3542, -0.8408,\n",
      "         0.6134,  0.6134], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9124, -0.8642, -0.1048, -0.8642, -0.1048, -0.8642, -0.4479, -0.6812,\n",
      "        -0.4479, -0.4479], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.32750681042671204\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.6472,  0.5958, -0.6248, -0.8430], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.0789, -0.4558, -0.5538,  0.5958,  0.5958, -0.5459, -0.6681,  0.5958,\n",
      "         0.5958, -0.8651], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5958, -0.4638, -0.8646, -0.4638, -0.4638, -0.8590, -0.8787, -0.4638,\n",
      "        -0.4638, -0.8646], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5185117721557617\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.6631,  0.5798, -0.6542, -0.8507], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.1409,  0.9711,  0.5798, -0.8829, -0.4381,  0.5798, -0.5749,  0.5798,\n",
      "         0.3179,  0.5798], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.1260, -0.7385, -0.4782, -0.8731, -0.4782, -0.4782, -0.8731, -0.4782,\n",
      "        -0.4782, -0.4782], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.8195810317993164\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.6822,  0.5532, -0.6839, -0.8571], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.4589,  0.5532, -0.6822,  0.1283, -0.6566, -0.4203, -0.6839, -0.9011,\n",
      "         0.0393, -0.8051], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5021, -0.5021, -0.7471, -0.1700, -0.8845, -0.1700, -0.7443, -0.8845,\n",
      "        -0.6255, -0.9024], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17787839472293854\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-0.7160,  0.1165,  0.5325,  0.5325,  0.0961,  0.5325, -0.6265,  0.5325,\n",
      "        -0.9157, -0.7038], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.7588, -0.2054, -0.5208, -0.5208, -0.7624, -0.5208, -0.9253, -0.5208,\n",
      "        -0.7624, -0.7489], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5394522547721863\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-0.7517, -0.9344,  0.5098,  0.5098,  0.5098,  0.5098,  0.5098, -0.0035,\n",
      "        -0.7213, -0.8724], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.7710, -0.9087, -0.5412, -0.5412, -0.5412, -0.5412, -0.5412, -0.6433,\n",
      "        -0.7511, -0.7790], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5942814350128174\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.7374,  0.4775, -0.7865, -0.8752], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.6911,  0.0584, -0.8669, -0.7924,  0.0825, -0.5320,  0.4775, -0.5320,\n",
      "         0.8147, -0.9547], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9683, -0.8000, -0.2668, -1.0287, -0.2668, -0.5702, -0.5702, -0.5702,\n",
      "        -0.7748, -0.8000], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5002539157867432\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.7473,  0.4325, -0.8256, -0.8621], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.9603, -0.7473,  0.4325,  0.4325, -0.8519,  0.4325,  0.4325,  0.4325,\n",
      "         0.4325, -0.6831], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.8250, -0.7694, -0.6108, -0.6108, -0.3341, -0.6108, -0.6108, -0.6108,\n",
      "        -0.6108, -0.7852], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6827419400215149\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-0.8366,  0.3253,  0.1546, -0.0056, -0.5134, -0.8366, -0.5861, -0.8640,\n",
      "        -0.8640,  0.3669], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.8609,  0.0542, -0.6698, -0.8609, -0.8082, -0.8609, -0.6698, -0.8440,\n",
      "        -0.8440, -0.6698], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2655196785926819\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.5290,  0.1244, -0.6872, -0.7171], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.9613,  0.3221, -0.8167,  0.3221, -0.6130, -0.9261,  0.3221,  0.3221,\n",
      "         0.3221, -0.8981], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.8881, -0.7101, -0.8881, -0.7101, -0.7101, -1.0101, -0.7101, -0.7101,\n",
      "        -0.7101, -0.8701], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.535498857498169\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.7759,  0.2702, -0.9336, -0.8037], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.2623,  0.2702,  0.2702, -0.9616, -0.4962,  0.2702,  0.1791, -1.1293,\n",
      "        -0.7759, -0.1087], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.0208, -0.7568, -0.7568, -0.9179, -0.5589, -0.7568, -0.8300, -1.0667,\n",
      "        -0.8300, -1.0667], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.519318699836731\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-0.5556, -0.1589,  0.2107, -0.9672, -0.7930, -0.2034, -0.9204,  0.2107,\n",
      "         0.0539,  0.1427], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.8716, -1.0980, -0.8104, -0.9345, -0.9515, -0.8023, -1.0761, -0.8104,\n",
      "        -0.8104, -0.8468], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5202175974845886\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([ 0.1633, -0.1221, -0.7474, -0.5230,  0.1633,  0.0162, -0.8061,  0.1633,\n",
      "        -0.6857,  0.1633], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.8530, -0.6986, -0.6986, -0.6986, -0.8530, -0.8530, -0.8537, -0.8530,\n",
      "        -0.8530, -0.8530], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5283147096633911\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([ 0.1286,  0.1286,  0.1286, -0.1606, -0.6198, -1.0484,  0.1286,  0.1286,\n",
      "         0.0683,  0.1286], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.8843, -0.8843, -0.8843, -0.7469, -0.8843, -1.2574, -0.8843, -0.8843,\n",
      "        -0.8591, -0.8843], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.747275173664093\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([ 0.0899, -0.7299, -0.6523, -0.6523,  0.0899, -1.0645, -0.8940, -0.6850,\n",
      "        -0.9456,  0.0899], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9191, -1.1783, -0.9191, -0.9191, -0.9191, -1.0294, -1.1853, -0.1473,\n",
      "        -1.1853, -0.9191], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.38302868604660034\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-0.9625, -1.0588, -0.8074,  0.0573,  0.0573, -1.0611,  0.0573, -0.6780,\n",
      "        -0.9078, -0.2209], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.2169, -1.0868, -1.0145, -0.9484, -0.9484, -1.2169, -0.9484, -1.3183,\n",
      "        -0.8625, -1.0868], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4329046607017517\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9187,  0.0914, -1.0637, -0.8623], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.1217, -0.7900,  0.0357, -1.1127, -0.1151, -0.7791, -1.3630,  0.0357,\n",
      "         0.0357,  0.0357], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3438, -0.9678, -0.9678, -1.1036, -0.9678, -1.2155, -1.2155, -0.9678,\n",
      "        -0.9678, -0.9678], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5049108862876892\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.1653,  0.8605, -1.3746, -1.2231], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.0230,  0.0230, -0.8031,  0.0230,  0.0230,  0.0805, -1.0078,  0.0230,\n",
      "         0.0230, -1.2231], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9793, -0.9793, -0.8741, -0.9793, -0.9793, -0.2255, -0.8524, -0.9793,\n",
      "        -0.9793, -0.9276], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6237843632698059\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.8479, -0.1319, -1.0845, -0.7788], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.0109, -0.0109, -0.0109,  0.0486, -0.0109, -1.2095, -0.0109, -1.0489,\n",
      "        -0.9121, -1.0845], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.0098, -1.0098, -1.0098, -0.2539, -1.0098, -1.1187, -1.0098, -0.8616,\n",
      "        -1.1527, -0.9685], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5195297002792358\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.8811, -0.1664, -1.1117, -0.8059], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.3080, -0.8867, -0.0571, -0.0571, -0.0571, -1.2314, -1.2031, -0.1322,\n",
      "        -0.0571, -0.8059], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.1863, -1.0514, -1.0514, -1.0514, -1.0514, -1.1863, -1.3117, -0.8803,\n",
      "        -1.0514, -0.2855], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.559728741645813\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9034, -0.2094, -1.1343, -0.8132], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.1128, -0.1161, -1.5090, -0.3921, -0.5064, -0.9254, -0.1161, -0.1161,\n",
      "        -0.9753, -0.2488], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9038, -1.1045, -1.3140, -0.9997, -1.0445, -1.3140, -1.1045, -1.1045,\n",
      "        -1.2239, -1.1045], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4616278111934662\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9195, -0.2474, -1.1488, -0.8338], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.0170, -1.1321, -0.2927, -0.1625, -0.1625, -0.1625, -0.1625, -0.5699,\n",
      "        -1.2189, -0.1625], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.2634, -0.9262, -1.1463, -1.1463, -1.1463, -1.1463, -1.1463, -1.3491,\n",
      "        -1.5129, -1.1463], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6364129185676575\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-0.9305, -0.2905, -0.9778, -1.2612, -1.5536, -1.3142, -0.2138, -0.2138,\n",
      "        -0.2138, -0.9406], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.1924, -1.2614, -1.1924, -1.4440, -1.3843, -1.4440, -1.1924, -1.1924,\n",
      "        -1.1924, -1.2684], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.411686509847641\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.1645, -0.2355, -1.3055, -1.1143], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.5860, -1.1252, -0.2517, -1.3687, -0.2517, -0.2517, -0.5341, -1.1740,\n",
      "        -0.2517, -1.3018], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.2954, -1.3517, -1.2266, -1.2954, -1.2266, -1.2266, -1.1141, -0.9826,\n",
      "        -1.2266, -1.6072], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.7863821983337402\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.1967, -0.2996, -1.3532, -1.1747], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.3920, -0.3099, -0.3099, -0.4440, -1.2305, -0.5869, -0.9452, -0.1807,\n",
      "        -0.4440, -1.0218], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.5282, -1.2789, -1.2789, -1.2789, -1.2696, -1.1626, -0.5692, -1.3559,\n",
      "        -1.2789, -1.6511], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5542082786560059\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-0.3550, -0.3550, -1.2234, -0.3550,  0.3673, -0.3550, -0.3550, -1.3515,\n",
      "        -0.3889, -0.3550], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3195, -1.3195, -1.4857, -1.3195, -1.3500, -1.3195, -1.3195, -1.5776,\n",
      "        -1.3500, -1.3195], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.9574346542358398\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2646, -0.4303, -1.4424, -1.3058], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.4945, -0.4256, -0.4256, -1.3079, -0.4256, -1.4945, -0.5870, -1.1745,\n",
      "        -1.1745, -0.4256], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3808, -1.3830, -1.3830, -1.5283, -1.3830, -1.3808, -1.5274, -1.3830,\n",
      "        -1.3830, -1.3830], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.47126397490501404\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3041, -0.4993, -1.4930, -1.3798], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.4887, -1.1810, -1.2319, -1.3955, -0.4887, -0.4887, -0.6382, -0.4887,\n",
      "        -0.4887, -0.4581], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4398, -1.4927, -1.7822, -1.5743, -1.4398, -1.4398, -1.5871, -1.4398,\n",
      "        -1.4398, -1.4123], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6766082644462585\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-1.5910, -0.7059, -0.4981, -0.5472, -0.4879, -0.5472, -1.3092, -0.5472,\n",
      "        -1.1406, -1.3325], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4390, -1.4925, -1.0559, -1.4925, -1.5308, -1.4925, -1.4925, -1.4925,\n",
      "        -1.0559, -1.8175], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4997236728668213\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.1291, -0.5203, -1.3681, -1.2097], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.7464, -0.6022, -0.7664, -0.6022, -0.5203, -1.2097, -0.6022, -0.6022,\n",
      "        -0.6155, -0.6022], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.6897, -1.5420, -1.5420, -1.5420, -1.4683, -1.1598, -1.5420, -1.5420,\n",
      "        -1.5539, -1.5420], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.7689195871353149\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-1.6758, -1.7871, -1.4503, -1.2218, -0.6929, -1.4024, -0.6644, -1.3813,\n",
      "        -0.6647, -0.6644], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7534, -1.8698, -1.5980, -1.5983, -1.6236, -1.6236, -1.5980, -1.6038,\n",
      "        -1.6038, -1.5980], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.37659531831741333\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-0.6017, -1.5397, -0.7523, -0.7188, -0.7479, -2.0901, -1.7863, -0.7188,\n",
      "        -0.7188, -0.8988], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.5415, -1.6469, -1.6297, -1.6469, -1.6731, -1.8983, -1.8089, -1.6469,\n",
      "        -1.6469, -1.6469], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5701689720153809\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-0.7839, -1.5765, -1.8678, -0.8646, -2.1868, -0.7839, -1.8678, -0.7788,\n",
      "        -1.8192, -1.0969], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7055, -1.5058, -1.8819, -1.6621, -1.9634, -1.7055, -1.8819, -1.4766,\n",
      "        -1.9629, -1.8685], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3492656350135803\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3404, -0.7112, -1.5775, -1.4676], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.1534, -2.2477, -0.7112, -1.5775, -1.9407, -1.0701, -1.9523, -0.8630,\n",
      "        -1.8958, -1.7332], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.8955, -1.9420, -1.6401, -1.8200, -1.9631, -1.7767, -2.0570, -1.7767,\n",
      "        -2.0266, -1.7767], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2930402159690857\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3981, -0.7692, -1.6483, -1.5276], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.1617, -1.9167, -1.2410, -1.7301, -0.9292, -0.9292, -0.7692, -1.8857,\n",
      "        -0.9292, -1.7301], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.8362, -2.1169, -1.9899, -1.6923, -1.8362, -1.8362, -1.6923, -2.1169,\n",
      "        -1.8362, -1.6923], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.44326916337013245\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-2.4244, -1.7909, -0.9893, -2.0098, -0.9893, -1.2579, -1.6640, -1.5851,\n",
      "        -0.8278, -0.9893], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.0950, -1.7450, -1.8903, -1.7450, -1.8903, -1.8903, -2.0706, -1.7671,\n",
      "        -1.7450, -1.8903], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4056207537651062\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-1.0434, -1.0434, -1.0434, -1.0434, -1.3621, -1.9910, -1.4265, -2.2135,\n",
      "        -2.3611, -2.0246], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.9391, -1.9391, -1.9391, -1.9391, -1.9391, -1.9391, -2.1424, -2.2170,\n",
      "        -2.1805, -1.8031], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4138595461845398\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9249, -1.1583, -2.0158, -2.1050], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.1036, -1.1036, -1.7999, -1.1036, -1.1036, -2.2426, -0.9687, -2.6050,\n",
      "        -1.1036, -2.4507], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.9933, -1.9933, -2.0425, -1.9933, -1.9933, -2.3867, -1.8718, -2.3867,\n",
      "        -1.9933, -2.2791], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.49296075105667114\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9853, -1.2233, -2.0117, -2.1135], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.0281, -1.1790, -2.5269, -2.6210, -1.1790, -2.1700, -1.6651, -1.1790,\n",
      "        -1.9677, -1.0587], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.9528, -2.0611, -2.3862, -2.4986, -2.0611, -2.4477, -2.2942, -2.0611,\n",
      "        -1.9814, -1.9528], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.36472970247268677\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-1.1543, -1.8918, -1.8359, -1.2654, -1.2929, -1.2654, -1.1543, -1.4375,\n",
      "        -1.2654, -1.2929], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.0389, -2.1636, -2.1388, -2.1388, -2.1636, -2.1388, -2.0389, -2.1492,\n",
      "        -2.1388, -2.1636], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6042226552963257\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-1.3529, -1.3529, -1.3529, -2.4579, -1.8594, -2.0728, -1.9590, -1.9336,\n",
      "        -1.2604, -1.3529], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2176, -2.2176, -2.2176, -2.0324, -2.2176, -2.1343, -2.2176, -2.0324,\n",
      "        -2.1343, -2.2176], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.41444700956344604\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1690, -1.4323, -2.0332, -2.1491], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.4459, -1.4459, -2.0457, -2.5560, -1.4323, -2.6564, -2.0882, -1.4459,\n",
      "        -2.1690, -2.0809], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.3014, -2.3014, -2.2890, -2.7347, -2.2890, -2.8793, -2.5186, -2.3014,\n",
      "        -2.2413, -2.3014], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.33091622591018677\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2409, -1.4900, -2.0625, -2.1887], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.5272, -1.4928, -1.9878, -1.5272, -1.8080, -3.0148, -1.5272, -1.4900,\n",
      "        -2.1560, -2.8181], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.3745, -2.3436, -2.5825, -2.3745, -2.4143, -3.0001, -2.3745, -2.3410,\n",
      "        -2.3436, -2.8984], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4364621639251709\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-2.8175, -2.9427, -2.0985, -2.3875, -2.3737, -2.0017, -2.2551, -2.3737,\n",
      "        -1.5929, -1.6073], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2487, -2.7334, -2.6247, -2.4466, -2.4466, -2.7334, -2.3924, -2.4466,\n",
      "        -2.4336, -2.4466], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2623790502548218\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.3733, -1.5907, -2.1595, -2.3361], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.6961, -1.5907, -1.6961, -1.6771, -1.6771, -1.6771, -2.2995, -2.7491,\n",
      "        -1.5907, -1.5907], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.5265, -2.4316, -2.5265, -2.5094, -2.5094, -2.5094, -2.5094, -2.6541,\n",
      "        -2.4316, -2.4316], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5631803274154663\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.4399, -1.6789, -2.2331, -2.4352], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.5618, -2.9008, -2.3752, -1.7719, -1.7719, -1.8246, -3.1306, -2.3937,\n",
      "        -1.7719, -2.6134], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.6147, -3.0765, -2.6422, -2.5947, -2.5947, -2.6422, -2.9330, -3.1732,\n",
      "        -2.5947, -2.5947], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.34513574838638306\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.5082, -1.7687, -2.3302, -2.5176], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.5368, -2.7473, -2.4908, -1.7687, -2.5082, -1.7687, -2.6454, -1.9478,\n",
      "        -1.8806, -2.3302], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.5370, -3.1044, -2.7530, -2.5919, -2.7530, -2.5919, -2.7135, -2.7530,\n",
      "        -2.6925, -2.5919], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.29919886589050293\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-3.0954, -3.2094, -1.9749, -1.8575, -1.8575, -2.6278, -2.9138, -1.9749,\n",
      "        -1.9749, -2.5931], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.6197, -3.3338, -2.7775, -2.6717, -2.6717, -2.8319, -3.1891, -2.7775,\n",
      "        -2.7775, -2.7775], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3651271164417267\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-2.0782, -2.7707, -2.9805, -2.0782, -2.9476, -3.0251, -2.9260, -2.6797,\n",
      "        -1.9679, -2.0782], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.8704, -3.4025, -2.8620, -2.8704, -2.8704, -2.8704, -3.4936, -2.7711,\n",
      "        -2.7711, -2.8704], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3301493525505066\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.8745, -2.0794, -2.8930, -2.8836], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.1890, -3.1898, -2.0794, -2.0794, -3.0952, -3.3761, -2.7312, -2.2121,\n",
      "        -2.1890, -2.1890], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.9701, -2.8135, -2.8715, -2.8715, -2.9302, -3.4581, -2.9701, -2.9909,\n",
      "        -2.9701, -2.9701], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.39241090416908264\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.9791, -2.2196, -3.1096, -3.0179], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.3221, -3.4088, -2.1515, -2.2512, -3.6831, -2.3251, -2.3221, -2.2196,\n",
      "        -3.5308, -3.2728], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.0899, -3.9071, -3.0408, -2.9364, -3.5082, -3.0926, -3.0899, -2.9976,\n",
      "        -3.4713, -3.7210], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.41169309616088867\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-2.4449, -2.4449, -2.3516, -3.5500, -3.1116, -2.3516, -2.4449, -2.8430,\n",
      "        -3.1794, -3.1794], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2004, -3.2004, -3.1164, -3.8105, -3.0526, -3.1164, -3.2004, -3.0326,\n",
      "        -3.1003, -3.1003], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3002191185951233\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.2604, -2.5011, -3.5937, -3.2928], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.5011, -3.7119, -4.2544, -4.3013, -3.2283, -3.5937, -3.6588, -2.5815,\n",
      "        -2.3661, -2.5815], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2510, -3.3234, -4.1089, -3.7402, -3.6802, -3.2510, -4.1089, -3.3234,\n",
      "        -3.2942, -3.3234], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.35355955362319946\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-3.7493, -3.4081, -2.7652, -3.5828, -3.3863, -2.6870, -3.4081, -2.6870,\n",
      "        -2.6771, -2.7652], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4887, -3.4094, -3.4887, -3.7884, -3.3340, -3.4183, -3.4094, -3.4183,\n",
      "        -3.4094, -3.4887], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.27655482292175293\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-2.9734, -2.8908, -2.8908, -2.9734, -3.9865, -2.8201, -2.9734, -3.9054,\n",
      "        -2.8908, -3.7581], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6761, -3.6017, -3.6017, -3.6761, -4.3638, -3.5381, -3.6761, -3.6017,\n",
      "        -3.6017, -4.1962], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.39394235610961914\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-4.1681, -3.1848, -4.3741, -3.5270, -3.7675, -3.9520, -3.1848, -3.1083,\n",
      "        -3.7775, -3.8727], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.1099, -3.8663, -4.1099, -3.6463, -3.8663, -3.7975, -3.8663, -3.7975,\n",
      "        -3.6135, -3.7537], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1565961092710495\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-3.1453, -3.3299, -4.0905, -3.5170, -3.4122, -3.9435, -3.4122, -3.3299,\n",
      "        -3.4122, -4.3775], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7890, -3.9969, -4.4778, -4.0710, -4.0710, -3.9969, -4.0710, -3.9969,\n",
      "        -4.0710, -4.6206], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.31248411536216736\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.9878, -3.4909, -4.0778, -4.0939], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.0091, -3.2267, -4.2172, -3.6222, -3.5494, -4.0091, -4.4191, -3.5494,\n",
      "        -4.4213, -3.5494], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3158, -3.9040, -3.9040, -4.2600, -4.1945, -4.3158, -4.6909, -4.1945,\n",
      "        -4.6909, -4.1945], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.25465232133865356\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.8845, -3.4991, -3.9256, -3.9484], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.1932, -4.3293, -3.8230, -3.7791, -4.2913, -4.6162, -3.8349, -3.8230,\n",
      "        -4.2550, -3.8230], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4545, -4.1492, -4.4407, -4.4012, -4.3127, -4.4545, -4.4407, -4.4407,\n",
      "        -4.4012, -4.4407], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.20473714172840118\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.4984, -3.9872, -4.4151, -4.4997], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.3944, -3.9872, -3.9815, -4.1267, -4.4151, -3.9979, -3.9979, -3.9979,\n",
      "        -3.9872, -3.9872], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9410, -4.5885, -4.5981, -4.5885, -4.5885, -4.5981, -4.5981, -4.5981,\n",
      "        -4.5885, -4.5885], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3087608516216278\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-3.6435, -4.1413, -4.1248, -4.1413, -4.1413, -4.7930, -4.1413, -4.1413,\n",
      "        -4.2961, -4.1312], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.2792, -4.7271, -4.7123, -4.7271, -4.7271, -4.7181, -4.7271, -4.7271,\n",
      "        -4.7271, -4.7123], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2994577884674072\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-4.4943, -4.2940, -4.2422, -4.7542, -4.2422, -4.3074, -4.6937, -4.2422,\n",
      "        -4.1061, -4.2422], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1735, -4.8646, -4.8180, -4.7854, -4.8180, -4.8180, -4.7854, -4.8180,\n",
      "        -4.6673, -4.8180], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2697974741458893\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-4.9331, -4.8248, -4.3694, -4.7178, -5.1103, -3.9726, -5.4304, -5.0008,\n",
      "        -4.7381, -5.1799], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5753, -4.9473, -4.9324, -4.7792, -5.0624, -4.5753, -5.2323, -4.5753,\n",
      "        -5.0189, -5.2323], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1131191998720169\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0472, -4.6119, -5.1536, -5.2266], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.6171, -5.6036, -5.0479, -4.6119, -4.5050, -5.2354, -4.5050, -5.0479,\n",
      "        -4.7529, -4.7152], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4913, -5.4547, -4.7827, -5.1507, -5.0545, -5.2641, -5.0545, -4.7827,\n",
      "        -4.9493, -5.1265], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12815479934215546\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-5.7212, -5.2282, -4.7659, -5.2282, -5.7212, -4.6308, -4.6308, -5.0906,\n",
      "        -4.9261, -5.3667], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.8167, -5.2893, -5.2893, -5.2893, -5.8167, -5.1677, -5.1677, -5.0172,\n",
      "        -5.1677, -5.3978], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09409352391958237\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-5.1341, -5.9589, -5.1278, -5.2895, -5.1693, -5.3683, -4.7287, -4.9009,\n",
      "        -4.7287, -5.2331], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2381, -5.9540, -5.2738, -5.4523, -5.2559, -5.5368, -5.2559, -5.4108,\n",
      "        -5.2559, -5.4565], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09601572155952454\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0459, -4.8296, -5.1995, -5.6660], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.0328, -6.2115, -5.6510, -5.3059, -5.5886, -4.8296, -4.8296, -4.9352,\n",
      "        -5.5382, -5.6660], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5295, -6.0859, -5.8726, -5.4417, -5.5859, -5.3466, -5.3466, -5.4417,\n",
      "        -5.9216, -5.6969], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12692196667194366\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.1825, -4.9434, -5.2773, -5.8107], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.1571, -5.6088, -5.3816, -4.9434, -5.1724, -5.1847, -6.0585, -5.9276,\n",
      "        -5.1571, -5.5383], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6414, -5.4490, -5.4030, -5.4490, -5.4490, -5.6663, -6.0814, -5.4030,\n",
      "        -5.6414, -5.6663], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13512063026428223\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.2868, -5.0768, -5.3888, -5.8743], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.0768, -5.8743, -5.3947, -5.0947, -5.5879, -6.0335, -5.3216, -6.5086,\n",
      "        -6.4609, -5.8175], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5691, -6.0425, -5.8552, -5.7894, -5.8552, -5.7894, -5.7894, -6.2144,\n",
      "        -6.1931, -5.5691], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15352973341941833\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.4215, -5.2656, -5.4974, -5.9146], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.6976, -5.4827, -5.2656, -6.5289, -5.4827, -5.9502, -5.9855, -5.4975,\n",
      "        -5.2656, -6.1582], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.0601, -5.9344, -5.7390, -6.4187, -5.9344, -6.0650, -5.7390, -6.0601,\n",
      "        -5.7390, -6.1855], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1391330510377884\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.6235, -5.4615, -5.6999, -5.9866], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.7548, -5.4615, -5.4615, -5.6235, -5.6417, -5.4615, -6.1247, -5.6417,\n",
      "        -5.7548, -5.5931], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.1793, -5.9154, -5.9154, -5.6944, -6.0775, -5.9154, -5.9154, -6.0775,\n",
      "        -6.1793, -6.0775], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16417047381401062\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.8217, -5.7066, -5.9199, -6.0854], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.7066, -5.7066, -6.6250, -6.6040, -6.4641, -6.1151, -5.7066, -6.0854,\n",
      "        -5.7066, -5.9199], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.1360, -6.1360, -6.7208, -6.6580, -6.2285, -6.2599, -6.1360, -6.5036,\n",
      "        -6.1360, -6.3269], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11664165556430817\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-5.9794, -5.9794, -6.0859, -6.3071, -6.1866, -6.0426, -5.9794, -6.2177,\n",
      "        -6.2706, -6.2706], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.3815, -6.3815, -6.3815, -6.6009, -6.4383, -6.4383, -6.3815, -6.4202,\n",
      "        -6.3815, -6.3815], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09442038834095001\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.3499, -6.1178, -6.7524, -6.2787], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.1045, -6.3499, -6.2700, -6.1045, -6.2034, -7.1056, -6.7888, -6.1045,\n",
      "        -6.7895, -6.2377], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.4940, -6.5830, -6.5332, -6.4940, -6.5830, -7.0520, -6.6140, -6.4940,\n",
      "        -7.0520, -6.5060], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08973373472690582\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.4456, -6.1972, -6.9281, -6.4383], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.3754, -6.6341, -6.6787, -6.1972, -6.2373, -6.2373, -6.4383, -7.1452,\n",
      "        -6.3754, -6.6837], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.5775, -6.7665, -6.7764, -6.5775, -6.6136, -6.6136, -6.7687, -7.1204,\n",
      "        -6.5775, -6.6136], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06512375175952911\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.6093, -6.2239, -7.1055, -6.6494], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.3463, -6.6093, -6.7033, -6.3463, -6.4978, -7.2241, -6.5408, -7.1126,\n",
      "        -7.1392, -6.3463], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.7117, -6.8867, -6.4839, -6.7117, -6.6804, -7.0035, -6.8867, -6.6015,\n",
      "        -7.2690, -6.7117], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1005263477563858\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.8406, -6.3045, -7.2552, -6.9086], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.3045, -6.8406, -7.1295, -6.3045, -6.6294, -7.2975, -7.4825, -6.5921,\n",
      "        -6.3707, -6.8987], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.6740, -6.9665, -6.9665, -6.6740, -6.9665, -7.5565, -7.3425, -6.6740,\n",
      "        -6.6098, -6.6740], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06301499903202057\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.0609, -6.4212, -7.4280, -7.1506], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.7398, -6.7398, -7.5502, -6.7398, -7.1170, -7.9333, -6.7398, -6.5904,\n",
      "        -6.7398, -6.4212], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.0659, -7.0659, -7.4053, -7.0659, -7.0466, -7.3802, -7.0659, -6.9314,\n",
      "        -7.0659, -6.7791], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11076198518276215\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.1694, -6.5930, -7.5925, -7.3826], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.9163, -6.9163, -7.5698, -7.4916, -7.5925, -6.7681, -6.7681, -7.2767,\n",
      "        -6.5930, -7.1774], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.2247, -7.2247, -7.4596, -7.0913, -7.0913, -7.0913, -7.0913, -7.2190,\n",
      "        -6.9337, -6.9708], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09847934544086456\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-7.1559, -7.0395, -7.8508, -7.1559, -6.7967, -7.0395, -8.5122, -7.1559,\n",
      "        -7.6024, -7.6399], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.4403, -7.3355, -7.8255, -7.4403, -7.1170, -7.3355, -7.9043, -7.4403,\n",
      "        -7.3355, -7.1170], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12354516983032227\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.4421, -7.3999, -7.5785, -7.9419], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-7.7966, -7.2572, -7.7741, -7.4357, -7.9800, -7.4357, -7.3999, -7.5359,\n",
      "        -7.4357, -7.0793], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.8140, -7.6005, -7.6005, -7.6005, -7.9078, -7.6005, -7.6599, -7.6599,\n",
      "        -7.6005, -7.3714], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.040322866290807724\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.5049, -7.7532, -7.5904, -8.0807], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-7.7625, -7.3933, -8.2353, -8.2720, -7.3000, -8.0677, -7.3933, -7.3159,\n",
      "        -8.3919, -7.3933], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.7746, -7.5700, -7.8094, -7.9516, -7.5843, -8.0538, -7.5700, -7.4428,\n",
      "        -7.7746, -7.5700], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08558806031942368\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-7.9263, -8.5356, -7.6736, -7.4094, -7.4229, -7.4094, -8.4713, -8.0429,\n",
      "        -8.0429, -8.2907], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.6685, -7.9135, -7.7249, -7.5458, -7.7249, -7.5458, -7.9607, -7.8317,\n",
      "        -7.8317, -8.2242], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09388784319162369\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-8.1830, -7.6656, -8.0290, -8.0892, -8.5172, -7.4265, -8.0290, -8.1830,\n",
      "        -8.1830, -8.0290], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.9427, -7.7586, -7.7586, -7.6787, -8.1154, -7.7586, -7.7586, -7.9427,\n",
      "        -7.9427, -7.7586], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08415436744689941\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-8.1484, -7.9248, -7.9248, -7.9424, -8.1484, -7.9248, -7.8690, -8.1484,\n",
      "        -7.6890, -7.5350], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.0557, -7.8968, -7.8968, -8.0557, -8.0557, -7.8968, -8.0981, -8.0557,\n",
      "        -8.3343, -8.4014], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12605223059654236\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.1536, -8.3245, -8.0309, -8.0458], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-7.9834, -7.7644, -7.7644, -7.8867, -7.7644, -8.6122, -8.0444, -7.7356,\n",
      "        -7.8867, -7.7256], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.1766, -7.9879, -7.9879, -7.7234, -7.9879, -8.5270, -8.0719, -7.8557,\n",
      "        -7.7234, -7.7234], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02630452811717987\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-8.3462, -7.9858, -7.6436, -7.9373, -7.6436, -7.7300, -7.6436, -8.2747,\n",
      "        -8.1293, -7.6436], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.8792, -8.1079, -7.8792, -7.9412, -7.8792, -7.9412, -7.8792, -8.1079,\n",
      "        -8.5865, -7.8792], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07365070283412933\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.5738, -8.6104, -8.8613, -8.0317], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-8.0356, -7.6477, -8.2187, -8.5703, -8.4056, -7.7729, -7.7729, -8.5703,\n",
      "        -8.0356, -7.6477], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.0989, -7.8829, -8.0183, -7.9252, -7.8829, -7.9252, -7.9252, -7.9252,\n",
      "        -8.0989, -7.8829], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13106317818164825\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.4085, -8.6605, -8.9461, -8.0826], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-8.0203, -8.6444, -8.0203, -7.8758, -8.1586, -8.0299, -8.0203, -8.0203,\n",
      "        -7.8758, -8.0203], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.0813, -8.2743, -8.0813, -8.0221, -8.2640, -8.3869, -8.0813, -8.0813,\n",
      "        -8.0221, -8.0813], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.033695753663778305\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.3025, -8.7871, -9.0447, -8.1449], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-8.1397, -8.0083, -8.0469, -7.7758, -8.0469, -8.0469, -7.7978, -7.7978,\n",
      "        -8.0469, -8.2851], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.0828, -8.0828, -8.0395, -7.8735, -8.0395, -8.0395, -7.9223, -7.9223,\n",
      "        -8.0395, -7.8735], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.021896103397011757\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.2127, -8.8940, -9.1291, -8.2122], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-8.4506, -8.1239, -8.2122, -8.4366, -8.2927, -7.9030, -7.9016, -8.3756,\n",
      "        -8.2386, -7.8737], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.9878, -8.1493, -8.3909, -8.4210, -7.7887, -8.1493, -7.9878, -7.8183,\n",
      "        -8.2225, -7.8183], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0882948487997055\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.2777, -8.9176, -9.1433, -8.2482], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-8.0183, -8.0183, -8.0992, -7.8839, -7.8941, -8.3155, -7.7505, -8.4240,\n",
      "        -7.8941, -8.3407], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.9755, -7.9755, -7.8699, -8.1703, -7.8699, -7.8699, -8.5561, -8.4684,\n",
      "        -7.8699, -8.2929], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0991213321685791\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.3182, -8.9220, -9.1089, -8.3475], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-8.0272, -8.0272, -8.2348, -8.8487, -7.9246, -8.0272, -7.8806, -7.9246,\n",
      "        -8.2196, -7.9246], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.0646, -8.0646, -8.0590, -7.9264, -7.9301, -8.0646, -8.0879, -7.9301,\n",
      "        -7.9301, -7.9301], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10124742984771729\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-7.9386, -7.9386, -8.0097, -8.4170, -7.9386, -7.9386, -8.9837, -8.1918,\n",
      "        -7.9386, -8.4170], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.0564, -8.0564, -8.1247, -8.5753, -8.0564, -8.0564, -8.5753, -8.0564,\n",
      "        -8.0564, -8.5753], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.031780000776052475\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-8.0434, -8.0287, -8.0434, -7.8342, -8.0665, -7.9954, -8.0287, -8.0434,\n",
      "        -7.9954, -8.1720], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.2390, -8.2085, -8.2390, -8.2390, -8.2390, -8.1477, -8.2085, -8.2390,\n",
      "        -8.1477, -8.0497], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.043459203094244\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.0238, -8.1660, -7.9757, -8.2002], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.0526, -8.1490, -8.1156, -8.1037, -8.0955, -8.2327, -8.1037, -8.1156,\n",
      "        -8.2009, -8.0526], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.2473, -8.1536, -8.2279, -8.2279, -8.1536, -7.9255, -8.2279, -8.2279,\n",
      "        -8.0506, -8.2473], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.025229236111044884\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-8.1386, -8.4333, -8.2073, -8.2073, -7.9480, -8.1818, -8.1820, -8.1230,\n",
      "        -8.1386, -8.0905], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.1090, -8.1532, -8.1090, -8.1090, -8.1532, -7.9662, -8.1532, -8.3107,\n",
      "        -8.1090, -7.6921], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03828844055533409\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.5479, -9.0641, -8.6166, -8.6285], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-8.2431, -7.9690, -8.2538, -8.3445, -8.1340, -8.1858, -8.2218, -8.2218,\n",
      "        -7.9175, -8.6365], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.1257, -8.1721, -7.6720, -7.9682, -7.9682, -7.9682, -8.1721, -8.1721,\n",
      "        -8.1689, -8.2453], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08310969173908234\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.9345, -7.9490, -7.6803, -7.7718], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.1663, -8.1798, -8.3408, -8.1741, -8.1663, -7.7103, -8.2227, -7.7095,\n",
      "        -8.1798, -8.1663], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.2842, -8.2360, -8.1449, -8.6527, -8.2842, -7.9123, -7.9386, -8.1738,\n",
      "        -8.2360, -8.2842], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06524769961833954\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.3405, -8.9065, -8.8007, -8.4991], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-8.4991, -7.8339, -7.9109, -8.1585, -8.2710, -8.1647, -8.1647, -7.8420,\n",
      "        -8.1647, -7.6644], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.5065, -7.9128, -8.0578, -8.2187, -8.0124, -8.1526, -8.1526, -8.1526,\n",
      "        -8.1526, -8.1526], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.043361056596040726\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.6132, -7.7995, -7.8156, -7.5691], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-7.8872, -8.1661, -7.8584, -8.1217, -7.9137, -7.8512, -8.1217, -8.1217,\n",
      "        -8.2588, -8.1217], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.9357, -8.1223, -8.0112, -8.0112, -7.9357, -7.9605, -8.0112, -8.0112,\n",
      "        -8.0112, -8.0112], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.015019851736724377\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.5000, -7.6996, -7.8933, -7.4765], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-8.0584, -8.3608, -7.8545, -7.8278, -8.0385, -8.3608, -8.0385, -8.0066,\n",
      "        -8.3046, -8.0905], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.7288, -8.0691, -7.8622, -7.8622, -7.9207, -8.0691, -7.9207, -7.9123,\n",
      "        -8.2526, -7.8654], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03701837360858917\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.4581, -7.5971, -7.9324, -7.4157], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-8.0836, -8.1313, -7.7476, -7.9259, -7.5642, -8.3292, -8.6574, -7.7178,\n",
      "        -7.9259, -8.4606], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.0852, -7.8781, -7.9044, -7.8781, -7.8781, -7.8807, -8.0852, -7.7991,\n",
      "        -7.8781, -7.8731], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10720570385456085\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.5094, -7.4659, -7.8474, -7.3678], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-7.9910, -7.9910, -7.9910, -7.6246, -7.7782, -7.7707, -7.9910, -7.9478,\n",
      "        -8.2987, -7.9128], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.1836, -8.1836, -8.1836, -7.9232, -7.9232, -7.7259, -8.1836, -7.9332,\n",
      "        -8.1836, -7.9332], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.027447020635008812\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-8.2942, -8.1459, -7.6897, -8.5879, -7.9416, -7.9749, -7.3293, -7.3293,\n",
      "        -8.2569, -7.5239], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.9156, -7.9110, -7.9110, -8.1774, -7.9110, -8.1774, -7.5964, -7.5964,\n",
      "        -7.9156, -7.7715], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.077840156853199\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.5544, -7.3476, -7.6452, -7.2977], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-7.8880, -7.9643, -7.9643, -7.6259, -7.6259, -7.6259, -8.4920, -7.8905,\n",
      "        -7.6259, -8.1636], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.8633, -8.1678, -8.1678, -7.8633, -7.8633, -7.8633, -8.1678, -7.5679,\n",
      "        -7.8633, -7.9672], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05567099526524544\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.6093, -7.3374, -7.5327, -7.2856], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-7.7994, -7.9775, -7.9919, -7.5948, -7.9919, -7.9775, -7.3374, -7.5327,\n",
      "        -7.7100, -7.4589], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.9314, -8.1797, -7.7130, -7.8353, -7.7130, -8.1797, -8.0502, -8.0150,\n",
      "        -7.9892, -7.7130], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11957564204931259\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.7299, -7.4163, -7.5032, -7.3297], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-7.8282, -7.8282, -7.3297, -7.9267, -8.0251, -7.8470, -7.8002, -7.8470,\n",
      "        -8.3022, -7.5687], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.8118, -7.8118, -7.5968, -7.8949, -8.2226, -7.7436, -8.0670, -7.7436,\n",
      "        -7.7436, -7.8118], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05756257846951485\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.7961, -7.5228, -7.4628, -7.3693], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-7.8832, -7.5732, -7.5732, -7.6537, -8.0759, -7.5228, -7.4628, -7.4074,\n",
      "        -7.7186, -7.8694], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.8159, -7.8159, -7.8159, -8.0314, -8.2025, -7.9630, -7.7743, -7.8703,\n",
      "        -7.6323, -7.8159], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0796385332942009\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.8963, -7.6987, -7.4758, -7.4266], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-7.4266, -8.0867, -7.8573, -7.6393, -8.3070, -7.9085, -7.7157, -7.9045,\n",
      "        -7.5664, -8.1686], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.6839, -8.1694, -7.6346, -7.8331, -7.8699, -7.7897, -7.8699, -7.8331,\n",
      "        -7.7897, -8.1694], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.044415779411792755\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-7.2832, -8.2448, -7.5849, -7.8615, -7.4935, -7.9536, -7.5823, -7.3806,\n",
      "        -7.4865, -7.7125], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.5549, -7.9206, -7.7983, -8.1470, -7.9206, -7.7442, -7.7442, -7.7643,\n",
      "        -7.7304, -7.7442], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07661378383636475\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-8.2967, -7.5437, -8.0011, -8.1028, -7.6414, -7.5057, -7.8299, -7.5658,\n",
      "        -7.5658, -7.3576], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.0707, -7.7093, -7.9432, -7.8153, -7.9432, -7.6214, -7.7093, -7.7359,\n",
      "        -7.7359, -7.6695], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04386374354362488\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-7.7486, -7.9134, -7.5871, -7.7570, -8.3291, -7.6822, -8.0196, -8.1013,\n",
      "        -8.1013, -7.6600], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.9140, -7.6733, -7.5965, -7.9813, -7.9813, -7.9813, -7.6733, -7.9140,\n",
      "        -7.9140, -7.7404], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05424480512738228\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-7.6140, -7.9208, -7.9402, -7.5008, -7.7444, -7.2504, -7.9588, -7.9674,\n",
      "        -7.5423, -7.7444], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.5253, -7.6963, -7.6963, -7.7881, -7.7881, -7.8408, -7.9413, -7.6963,\n",
      "        -7.4687, -7.7880], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06318958103656769\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-7.6196, -7.8499, -7.9551, -7.2268, -7.9551, -7.8794, -7.7837, -7.4990,\n",
      "        -8.1317, -7.9551], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.8576, -7.9723, -7.7491, -7.4340, -7.7491, -7.7491, -7.5460, -7.9723,\n",
      "        -7.9723, -7.7491], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05647805333137512\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-7.9416, -7.3479, -7.9416, -7.1899, -7.8670, -7.9416, -8.1109, -8.0561,\n",
      "        -7.0594, -8.0561], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.8288, -7.8535, -7.8288, -7.3535, -7.3535, -7.8288, -7.9379, -8.0141,\n",
      "        -7.9221, -8.0141], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13619521260261536\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-7.4026, -7.7559, -7.9545, -7.8912, -7.8912, -7.3482, -7.5430, -7.8054,\n",
      "        -7.8912, -7.8160], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.7979, -7.4315, -8.0249, -7.8880, -7.8880, -7.4315, -7.6169, -7.8880,\n",
      "        -7.8880, -7.9236], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.029729735106229782\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-7.5275, -7.8767, -7.8359, -7.5907, -7.8453, -7.8453, -7.5481, -7.8479,\n",
      "        -7.8767, -7.6423], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.6847, -8.0523, -7.9468, -7.9509, -7.9468, -7.9468, -7.9468, -7.9136,\n",
      "        -8.0523, -7.9072], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04825201630592346\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-7.7251, -7.8381, -7.8668, -7.8381, -7.8569, -7.8381, -7.8668, -7.8247,\n",
      "        -7.8668, -7.8097], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.9246, -7.9526, -8.0540, -7.9526, -7.8775, -7.9526, -8.0540, -7.9381,\n",
      "        -8.0540, -7.8775], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.020215366035699844\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-7.8016, -7.3511, -7.8016, -7.8849, -7.7124, -7.8016, -7.8383, -7.9066,\n",
      "        -7.8074, -7.8258], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.8416, -7.9276, -7.8416, -7.9381, -7.6855, -7.8416, -7.9276, -8.0284,\n",
      "        -7.5168, -8.0284], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04890051856637001\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.4186, -7.9336, -7.6605, -7.5540], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-7.5155, -7.9511, -7.8063, -7.9511, -7.4135, -7.9511, -7.9511, -7.9511,\n",
      "        -7.9487, -8.0407], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.8863, -7.9373, -7.7872, -7.9373, -7.8800, -7.9373, -7.9373, -7.9373,\n",
      "        -8.0003, -7.7872], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04233473911881447\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-8.0826, -7.5154, -7.8548, -7.8088, -7.8088, -7.9997, -7.9170, -7.9997,\n",
      "        -7.6346, -7.6037], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.5291, -7.9902, -7.9902, -7.7630, -7.7630, -7.9536, -7.8062, -7.9536,\n",
      "        -7.9536, -7.8913], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07553084194660187\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.7943, -7.9679, -7.4965, -7.8337], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-7.7412, -8.0204, -8.0204, -7.8337, -7.4965, -7.3210, -7.8337, -7.5247,\n",
      "        -7.9679, -7.4407], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.5985, -7.9508, -7.9508, -7.7468, -7.8170, -7.8170, -7.7468, -7.9633,\n",
      "        -7.8812, -7.7468], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06873705983161926\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-7.8826, -7.9546, -8.1410, -7.8565, -8.0540, -7.8826, -7.9546, -8.0949,\n",
      "        -7.8826, -7.8558], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.7753, -7.8938, -7.6415, -7.8114, -7.6415, -7.7753, -7.8938, -7.7753,\n",
      "        -7.7753, -7.6376], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.061350513249635696\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.6332, -8.1205, -7.8241, -7.8563], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-7.6603, -7.8202, -7.8559, -7.8991, -7.7908, -7.9809, -7.9515, -7.7243,\n",
      "        -7.8549, -7.9515], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.9472, -7.9439, -7.8228, -7.9124, -7.6994, -7.6994, -7.9472, -7.8047,\n",
      "        -7.8943, -7.9472], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01945403218269348\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.7716, -7.7762, -7.4512, -7.7247], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-7.8602, -7.7247, -7.8646, -7.8326, -7.8646, -7.2368, -7.8646, -7.6413,\n",
      "        -7.8602, -7.8646], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.8661, -7.7061, -7.9551, -7.8771, -7.9551, -7.9214, -7.9551, -7.8771,\n",
      "        -7.8661, -7.9551], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.055951911956071854\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-7.8402, -7.9488, -7.8402, -7.8175, -8.0362, -7.6375, -7.8402, -7.7060,\n",
      "        -7.8669, -7.8175], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.9354, -7.9234, -7.9354, -7.9234, -7.9357, -7.9234, -7.9354, -7.9354,\n",
      "        -7.8737, -7.9234], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01948315091431141\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-7.7378, -7.6465, -7.8905, -8.0697, -7.7401, -7.8763, -7.7782, -7.7937,\n",
      "        -7.7937, -7.8763], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.8544, -7.9173, -7.8669, -7.3545, -7.8544, -7.9660, -7.9660, -7.8818,\n",
      "        -7.8818, -7.9660], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06789882481098175\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.6041, -8.0326, -7.9074, -7.7500], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-7.6280, -7.9265, -7.8038, -7.9050, -7.4873, -7.7570, -7.9265, -7.8689,\n",
      "        -7.6041, -7.3148], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.0439, -7.9813, -7.8554, -8.0439, -7.8882, -7.8380, -7.9813, -8.0423,\n",
      "        -7.7438, -7.8437], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0697505921125412\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-7.9798, -7.5315, -8.0153, -7.9333, -8.0153, -7.8532, -7.9077, -8.1677,\n",
      "        -7.8532, -7.9403], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.0753, -7.7509, -7.9755, -7.4295, -7.9755, -7.8085, -7.7965, -7.9180,\n",
      "        -7.8085, -8.0753], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04111526161432266\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.6987, -7.9517, -7.7671, -7.8147], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-7.7977, -8.0794, -7.8786, -8.0747, -7.8786, -7.6034, -7.8786, -8.0794,\n",
      "        -7.8147, -7.8582], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.7724, -8.0092, -7.8026, -7.5182, -7.8026, -7.7313, -7.8026, -8.0092,\n",
      "        -7.7313, -8.0598], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04014357179403305\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-7.5953, -7.6174, -8.0918, -7.8635, -7.8868, -7.5746, -7.9166, -7.8635,\n",
      "        -7.5582, -7.8466], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.8343, -7.7636, -8.0705, -7.8343, -7.8358, -7.7095, -8.0705, -7.8343,\n",
      "        -7.7636, -7.8358], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01674250140786171\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.7617, -7.9269, -7.7548, -7.8211], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-7.8508, -7.8405, -7.4889, -7.7548, -8.1052, -8.1052, -7.8396, -7.8405,\n",
      "        -7.8405, -7.4889], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.8684, -7.8435, -7.6935, -7.7671, -8.1163, -8.1163, -8.1163, -7.8435,\n",
      "        -7.8435, -7.6935], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01610497757792473\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.6462, -7.7985, -7.6645, -7.8169], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-8.1098, -7.6462, -8.1514, -7.8257, -7.8257, -7.8584, -7.7924, -7.4969,\n",
      "        -8.1098, -7.5174], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.1474, -7.9771, -8.0057, -7.8594, -7.8594, -7.9172, -8.0057, -7.6628,\n",
      "        -8.1474, -7.9172], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03721678629517555\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-8.1365, -7.9982, -7.5307, -7.8992, -7.5900, -7.7980, -7.8529, -7.7357,\n",
      "        -7.5307, -8.1035], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.1356, -8.0192, -7.6112, -7.9622, -7.6112, -7.9527, -8.1356, -7.8938,\n",
      "        -7.6112, -8.1423], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.014813384041190147\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-7.9080, -7.8244, -8.1543, -7.9495, -7.9439, -7.9675, -7.6859, -7.6548,\n",
      "        -8.1998, -7.5929], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.5474, -7.9398, -8.1005, -8.0420, -8.2251, -7.9138, -7.8336, -7.8336,\n",
      "        -8.0255, -7.5474], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03230247274041176\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.7328, -7.6806, -7.2354, -7.6212], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.1767, -8.1820, -8.3406, -8.0068, -7.2354, -8.1820, -7.6642, -7.6546,\n",
      "        -7.6642, -8.0068], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.0751, -8.0716, -7.8963, -8.0759, -7.8610, -8.0716, -7.8610, -8.1999,\n",
      "        -7.8610, -8.0759], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10079684108495712\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.5911, -7.9718, -7.6783, -7.6795], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-7.7204, -7.6414, -7.9653, -7.8727, -7.8727, -7.8727, -7.8727, -7.9351,\n",
      "        -7.7741, -7.8727], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.9520, -7.5584, -7.8784, -7.9388, -7.9388, -7.9388, -7.9388, -8.1137,\n",
      "        -7.9388, -7.9388], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.014902813360095024\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-7.8126, -7.9226, -7.7425, -7.9498, -7.6588, -8.2556, -7.9468, -8.1353,\n",
      "        -7.6588, -7.5422], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.1303, -7.9009, -8.1134, -8.1358, -7.6184, -8.1521, -8.1521, -8.1134,\n",
      "        -7.6184, -7.6184], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03360286355018616\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-8.2381, -7.6270, -7.8358, -7.8999, -7.5139, -7.5139, -7.6847, -8.3098,\n",
      "        -8.2381, -7.9682], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.1356, -7.7625, -8.3973, -8.1755, -7.6855, -7.6855, -7.6855, -8.1898,\n",
      "        -8.1356, -7.8897], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05100112035870552\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.9403, -8.3182, -8.0044, -8.1926], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-7.4769, -8.0148, -7.6704, -8.0148, -7.9343, -8.1926, -8.0148, -8.3402,\n",
      "        -7.5230, -7.5230], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.7707, -7.9033, -7.8398, -7.9033, -8.1157, -8.1157, -7.9033, -8.2158,\n",
      "        -7.7292, -7.7292], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.029160764068365097\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-8.0751, -8.1577, -8.0512, -8.2440], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-7.7752, -7.7180, -8.0430, -8.1993, -7.9475, -8.1577, -8.0006, -8.6292,\n",
      "        -8.0492, -8.3297], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.7725, -7.8624, -8.2387, -8.0354, -8.2006, -7.8624, -7.9462, -8.0354,\n",
      "        -7.9462, -8.2006], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06200601905584335\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.9796, -8.2217, -7.9908, -8.1371], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-7.9731, -8.3114, -7.8532, -8.3681, -8.0370, -8.2811, -8.5587, -7.9670,\n",
      "        -8.3114, -8.0370], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.0432, -8.2683, -7.8840, -8.5313, -8.0432, -8.2662, -7.8840, -8.0978,\n",
      "        -8.2683, -8.0432], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05088391900062561\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-8.2029, -8.0225, -8.0774, -8.1448], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-8.2533, -8.2594, -7.7561, -8.0117, -8.2533, -8.2280, -7.9304, -8.0117,\n",
      "        -8.2280, -8.1075], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.2606, -8.3158, -7.8590, -8.1400, -8.2606, -8.3158, -7.9213, -8.1400,\n",
      "        -8.3158, -8.2606], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.008573567494750023\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-7.8801, -7.7709, -8.0595, -7.7688], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-8.0142, -8.0142, -8.2126, -8.0745, -8.2409, -8.0142, -8.2409, -8.0142,\n",
      "        -7.7205, -7.9051], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.1476, -8.1476, -8.1146, -8.2480, -8.2209, -8.1476, -8.2209, -8.1476,\n",
      "        -7.8986, -8.1146], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.018730217590928078\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-8.3093, -7.9792, -8.1002, -7.9956], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-8.2526, -8.2526, -8.0478, -8.1765, -7.9480, -8.2330, -8.2104, -7.8817,\n",
      "        -8.3579, -8.1731], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.1691, -8.1691, -8.1129, -8.1982, -8.2146, -8.2146, -8.1982, -8.2146,\n",
      "        -8.4502, -8.1691], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.020955612882971764\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-8.0167, -7.8121, -8.1329, -7.6513], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-8.3285, -8.1911, -7.6454, -8.1982, -8.1982, -8.2319, -8.4673, -8.2073,\n",
      "        -7.6190, -8.4805], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.0036, -7.8862, -7.8809, -8.3644, -8.3644, -8.1310, -8.1310, -8.1114,\n",
      "        -7.8571, -8.1422], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.061288021504879\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-8.2643, -8.3207, -7.7972, -7.9387, -7.8194, -8.1038, -7.8194, -8.3833,\n",
      "        -7.8037, -8.1038], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.1162, -8.0907, -8.0175, -7.9991, -8.1577, -8.0907, -8.1577, -8.1162,\n",
      "        -7.9024, -8.0907], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04373861104249954\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-7.9780, -8.1211, -8.1211, -8.3839, -8.2624, -8.3659, -7.7738, -8.0595,\n",
      "        -8.2624, -7.8408], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.9965, -8.1583, -8.1583, -8.1724, -8.1724, -8.2536, -7.9965, -8.0674,\n",
      "        -8.1724, -7.9780], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.014504295773804188\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-8.2636, -8.2170, -8.4267, -7.8807, -8.2037, -8.2636, -8.2642, -7.8767,\n",
      "        -8.2170, -8.1763], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.2542, -8.2660, -8.2163, -8.0890, -8.2404, -8.2542, -8.1325, -8.0603,\n",
      "        -8.2660, -8.4177], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.020329082384705544\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-7.9855, -8.0453, -8.1611, -8.2700, -8.2842, -8.0025, -8.2700, -8.2589,\n",
      "        -8.2041, -8.2700], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.1366, -8.1366, -8.1658, -8.3296, -8.2536, -8.2022, -8.3296, -8.3296,\n",
      "        -8.3186, -8.3296], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01007670909166336\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.2198, -8.1635, -7.9735, -8.0782], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.4231, -7.9735, -8.1620, -7.9883, -8.1748, -8.0169, -8.2965, -8.3787,\n",
      "        -8.2965, -8.0782], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.4072, -8.1424, -8.3820, -8.1424, -8.3208, -8.1793, -8.3978, -8.2603,\n",
      "        -8.3978, -8.1761], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01927672140300274\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-8.3105, -8.2225, -8.3821, -8.2225, -8.0558, -8.0323, -8.2800, -8.5163,\n",
      "        -7.9079, -8.2293], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.4064, -8.2290, -8.4064, -8.2290, -8.1171, -8.1984, -8.2184, -8.3511,\n",
      "        -8.2231, -8.2290], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.017172306776046753\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.9226, -8.3364, -8.3260, -8.1227], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-8.2183, -8.2356, -8.1227, -8.2183, -7.9226, -8.1227, -8.2428, -7.9811,\n",
      "        -8.4130, -8.1957], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.1932, -8.1409, -8.1304, -8.1932, -8.2459, -8.1304, -8.2299, -8.1093,\n",
      "        -8.2299, -8.1830], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01651613414287567\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.2718, -8.2689, -8.0245, -8.3037], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.3321, -8.3615, -7.9823, -8.5585, -8.3133, -8.2622, -7.9823, -8.3251,\n",
      "        -8.2718, -8.0245], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.1840, -8.2908, -8.2221, -8.3030, -8.4408, -8.2647, -8.2221, -8.2065,\n",
      "        -8.3030, -8.1840], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.026391398161649704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andyyang/Desktop/DS598 Reinforcement Learning/gym-examples/.env/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-8.4014, -8.1750, -8.2847, -8.2666, -8.4778, -8.3261, -8.3720, -8.3244,\n",
      "        -8.2847, -8.3720], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.3583, -8.2263, -8.3313, -8.4400, -8.3583, -8.2889, -8.4400, -8.3313,\n",
      "        -8.3313, -8.4400], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.006386554334312677\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-8.1851, -8.2265, -8.3928, -8.3268, -8.3652, -8.3101, -8.5030, -8.2052,\n",
      "        -8.3928, -8.3928], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.1432, -8.2897, -8.4184, -8.3666, -8.3666, -8.3847, -8.3416, -8.2897,\n",
      "        -8.4184, -8.4184], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.004801067523658276\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.2557, -8.5506, -8.3698, -8.3632], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-8.4238, -8.2415, -8.3438, -8.3632, -8.2936, -8.3222, -7.9128, -8.2291,\n",
      "        -8.3491, -8.2557], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.4062, -8.3084, -8.1215, -8.4301, -8.3405, -8.4062, -8.4301, -8.4062,\n",
      "        -8.4265, -8.1215], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03909062221646309\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-8.2426, -8.1984, -8.2757, -8.4089, -8.4470, -8.6213, -8.4094, -8.3770,\n",
      "        -8.2551, -8.4656], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.1902, -8.2566, -8.3011, -8.4183, -8.2864, -8.4283, -8.4275, -8.4459,\n",
      "        -8.4459, -8.4459], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.011178936809301376\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.2886, -8.4132, -8.2459, -8.3232], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.3263, -8.2938, -8.0650, -8.2984, -8.4334, -8.1505, -8.2220, -8.3905,\n",
      "        -8.4506, -8.4898], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.4363, -8.3998, -8.3998, -8.2669, -8.4213, -8.2357, -8.2585, -8.3901,\n",
      "        -8.4241, -8.4739], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.014610822312533855\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.1310, -8.2738, -8.1391, -8.2965], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-8.2209, -8.3395, -8.4836, -8.4364, -8.4836, -8.1595, -8.2627, -8.4992,\n",
      "        -8.3345, -8.1729], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.2319, -8.3998, -8.4684, -8.3556, -8.4684, -8.3556, -8.3783, -8.3988,\n",
      "        -8.2319, -8.3436], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.011225983500480652\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.2354, -8.4721, -8.3819, -8.3619], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-8.5302, -8.3537, -8.5293, -8.4565, -8.3090, -8.3537, -8.5274, -8.2166,\n",
      "        -8.4647, -8.5115], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.3074, -8.2152, -8.2152, -8.4119, -8.3074, -8.2152, -8.4763, -8.1757,\n",
      "        -8.2152, -8.4389], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02604507841169834\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-8.4178, -8.3405, -8.3970, -8.5443, -8.1941, -8.4583, -8.1400, -8.3389,\n",
      "        -8.5692, -8.1400], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.5065, -8.4343, -8.3984, -8.4939, -8.3201, -8.2717, -8.4939, -8.2717,\n",
      "        -8.4506, -8.4939], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0338895320892334\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-8.1651, -8.3224, -8.3009, -8.4823, -8.5856, -8.3609, -8.5172, -8.3224,\n",
      "        -8.3224, -8.4445], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.3486, -8.3746, -8.4226, -8.5184, -8.5517, -8.4226, -8.4226, -8.3746,\n",
      "        -8.3746, -8.5517], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00833815522491932\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-8.4503, -8.3270, -8.6736, -8.5672, -8.3959, -8.4262, -8.4231, -8.5714,\n",
      "        -8.4443, -8.5714], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.5808, -8.4592, -8.3283, -8.5344, -8.5369, -8.5344, -8.4592, -8.4580,\n",
      "        -8.3669, -8.4580], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.021939972415566444\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-8.3522, -8.4648, -8.3231, -8.5637, -8.3774, -8.4323, -8.3522, -8.2671,\n",
      "        -8.3800, -8.4557], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.5149, -8.5344, -8.5344, -8.3470, -8.4702, -8.5431, -8.5149, -8.5068,\n",
      "        -8.3470, -8.6101], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02526436746120453\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-8.4545, -8.1479, -8.3025, -8.3750], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-8.4582, -8.2902, -8.5548, -8.4810, -8.5007, -8.2196, -8.3958, -8.3164,\n",
      "        -8.5369, -8.4582], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.5605, -8.4917, -8.6209, -8.6209, -8.3976, -8.3331, -8.5562, -8.4917,\n",
      "        -8.4779, -8.5605], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.016899341717362404\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-8.3806, -8.5289, -8.5289, -8.2507, -8.6338, -8.5503, -8.3473, -8.3270,\n",
      "        -8.6043, -8.6193], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.5425, -8.5669, -8.5669, -8.4256, -8.5425, -8.4256, -8.4256, -8.5559,\n",
      "        -8.5669, -8.5224], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.015288740396499634\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.5250, -8.5374, -8.6373, -8.3889], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-8.5753, -8.4578, -8.3363, -8.3708, -8.3363, -8.2576, -8.6444, -8.5673,\n",
      "        -8.3546, -8.4503], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.5329, -8.5337, -8.5500, -8.3807, -8.5500, -8.2279, -8.3672, -8.5500,\n",
      "        -8.5192, -8.5022], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.020677991211414337\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.4597, -8.4694, -8.3777, -8.2448], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-8.5719, -8.2710, -8.3640, -8.3640, -8.4783, -8.3288, -8.6961, -8.6066,\n",
      "        -8.5961, -8.1517], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.5205, -8.5044, -8.5276, -8.5276, -8.5276, -8.4771, -8.4959, -8.4762,\n",
      "        -8.5205, -8.3366], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.023201486095786095\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-8.3351, -8.8644, -8.5937, -8.3644, -8.1528, -8.5247, -8.4624, -8.6299,\n",
      "        -8.4336, -8.3644], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.2406, -8.5060, -8.3375, -8.5279, -8.3375, -8.5279, -8.5103, -8.5124,\n",
      "        -8.3375, -8.5279], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.031600840389728546\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.8235, -8.6443, -8.4161, -8.4104], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-8.6140, -8.7289, -8.6055, -8.4161, -8.2221, -8.6383, -8.4519, -8.6383,\n",
      "        -8.6140, -8.6443], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.5405, -8.5405, -8.6067, -8.3999, -8.3999, -8.5871, -8.4796, -8.5871,\n",
      "        -8.5405, -8.4105], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.013886550441384315\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.7728, -8.5783, -8.3710, -8.4902], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.5841, -8.5841, -8.6057, -8.3710, -8.4902, -8.5766, -8.3710, -8.7450,\n",
      "        -8.5155, -8.3339], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7191, -8.7191, -8.6909, -8.5006, -8.5339, -8.7190, -8.5006, -8.6909,\n",
      "        -8.7191, -8.3713], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.014520701952278614\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.4499, -8.6167, -8.5849, -8.3739], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-8.6296, -8.5487, -8.5888, -8.3739, -8.6013, -8.6031, -8.5888, -8.5821,\n",
      "        -8.5849, -8.5770], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.5664, -8.6938, -8.6938, -8.5365, -8.5349, -8.7197, -8.6938, -8.5349,\n",
      "        -8.5365, -8.7197], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.011650770902633667\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.4283, -8.6230, -8.5922, -8.4081], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-8.5547, -8.4283, -8.5922, -8.4443, -8.4848, -8.6331, -8.5069, -8.6131,\n",
      "        -8.6607, -8.5988], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.5435, -8.5435, -8.5673, -8.6963, -8.5435, -8.7389, -8.6364, -8.7175,\n",
      "        -8.5435, -8.6963], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.014309050515294075\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.4073, -8.6626, -8.5916, -8.4562], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-8.6908, -8.5732, -8.6129, -8.7246, -8.7900, -8.5885, -8.6302, -8.3700,\n",
      "        -8.6129, -8.6626], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7318, -8.6882, -8.6540, -8.6041, -8.8077, -8.5330, -8.5932, -8.5665,\n",
      "        -8.6540, -8.8419], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.010833248496055603\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.6272, -8.6415, -8.3762, -8.6166], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.8895, -8.6591, -8.5872, -8.3762, -8.6591, -8.6786, -8.7339, -8.6947,\n",
      "        -8.6166, -8.7434], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.5333, -8.5993, -8.5993, -8.5333, -8.5993, -8.6752, -8.7382, -8.5385,\n",
      "        -8.5385, -8.5993], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02101432904601097\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.3661, -8.7801, -8.6119, -8.4879], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-8.6100, -8.3890, -8.6909, -8.5471, -8.6909, -8.6100, -8.3951, -8.3951,\n",
      "        -8.6172, -8.6119], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7026, -8.6924, -8.5700, -8.5373, -8.5700, -8.7026, -8.5295, -8.5295,\n",
      "        -8.8281, -8.5295], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.022588638588786125\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.6723, -8.7442, -8.4385, -8.6371], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.3894, -8.6940, -8.6371, -8.4385, -8.8449, -8.8449, -8.6371, -8.8862,\n",
      "        -8.8207, -8.6940], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.5946, -8.5818, -8.5946, -8.5505, -8.7263, -8.7263, -8.5946, -8.7611,\n",
      "        -8.7680, -8.5818], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.012998285703361034\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.4452, -8.7795, -8.6198, -8.4662], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-8.6454, -8.6636, -8.4739, -8.6691, -8.6636, -8.7228, -8.4973, -8.6454,\n",
      "        -8.8271, -8.8271], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.6265, -8.6189, -8.6006, -8.6916, -8.6189, -8.7751, -8.6384, -8.6265,\n",
      "        -8.7751, -8.7751], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.004934924189001322\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-8.6333, -8.8062, -8.4863, -8.5244, -8.8196, -8.6333, -8.6311, -8.5977,\n",
      "        -8.5244, -8.8196], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.6492, -8.8141, -8.6720, -8.6020, -8.8166, -8.6492, -8.6246, -8.6246,\n",
      "        -8.6020, -8.8166], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.004786573350429535\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-8.5398, -8.6094, -8.6725, -8.5398, -8.8559, -8.6094, -8.8229, -8.8229,\n",
      "        -8.6947, -8.6573], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7149, -8.6885, -8.7870, -8.7149, -8.8353, -8.6885, -8.8353, -8.8353,\n",
      "        -8.8048, -8.7149], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.010310156270861626\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-8.6083, -8.5087, -8.9386, -8.7960, -8.6639, -8.4383, -8.8461, -8.6083,\n",
      "        -8.6438, -8.6639], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.6828, -8.7082, -8.8262, -8.8262, -8.7082, -8.5945, -8.8409, -8.6828,\n",
      "        -8.5945, -8.7082], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0095206368714571\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-8.5525, -8.7250, -8.5939, -8.8715, -8.6203, -8.5500, -8.7219, -8.6939,\n",
      "        -8.4803, -8.9999], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7680, -8.5123, -8.8318, -8.8128, -8.6973, -8.6777, -8.7680, -8.7295,\n",
      "        -8.6323, -8.8525], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.022213760763406754\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.8840, -8.8310, -8.5842, -8.7897], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.5792, -8.8515, -8.9011, -8.6318, -8.5888, -8.6568, -8.6731, -8.5792,\n",
      "        -8.6242, -8.4753], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7722, -8.8704, -8.7618, -8.6278, -8.6870, -8.7404, -8.5631, -8.7722,\n",
      "        -8.7618, -8.6870], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.018677841871976852\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.7827, -8.4255, -8.6141, -8.4375], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-8.6076, -8.6934, -8.7420, -8.4517, -8.5493, -8.9454, -8.5140, -8.9979,\n",
      "        -8.7713, -8.4375], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7265, -8.7468, -8.6321, -8.7265, -8.6944, -8.7468, -8.7938, -8.6649,\n",
      "        -8.7468, -8.7411], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04470745474100113\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.9422, -8.9108, -8.7303, -8.8133], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.7571, -8.7445, -8.4924, -8.7133, -8.9642, -8.9642, -8.7689, -8.9422,\n",
      "        -8.9642, -8.7927], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7608, -8.7608, -8.7136, -8.6431, -8.7538, -8.7538, -8.6586, -8.7608,\n",
      "        -8.7538, -8.6380], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.025599082931876183\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-8.8092, -8.5552, -8.8092, -8.9488, -8.6942, -8.5552, -8.5552, -8.8038,\n",
      "        -8.9088, -8.7980], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.6997, -8.7432, -8.6997, -8.8105, -8.7507, -8.7432, -8.7432, -8.9594,\n",
      "        -8.8247, -8.6997], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.019342519342899323\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-8.9243, -8.9017, -8.7431, -8.7372, -8.8791, -8.8099, -8.8560, -8.7613,\n",
      "        -8.9462, -8.7613], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.9222, -8.9600, -8.8458, -8.6868, -8.9222, -8.8458, -8.8274, -8.8274,\n",
      "        -8.8037, -8.8274], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0049455007538199425\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-8.9080, -8.8115, -8.8073, -8.5294, -8.7953, -9.0242, -8.9992, -8.9080,\n",
      "        -8.9080, -8.8115], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.9005, -8.9303, -8.8639, -8.8110, -8.8270, -8.8600, -8.8270, -8.9005,\n",
      "        -8.9005, -8.9303], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.016854386776685715\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.7706, -8.7533, -8.9531, -8.6615], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-8.7822, -8.8450, -8.6204, -8.7698, -9.0222, -8.7822, -8.9884, -8.8450,\n",
      "        -8.7698, -8.9884], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.8609, -8.9605, -8.7954, -8.8609, -8.9064, -8.8609, -8.8385, -8.9605,\n",
      "        -8.8609, -8.8385], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.014461278915405273\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.7481, -8.6702, -8.9554, -8.6876], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-8.8712, -8.6007, -9.2463, -9.0407, -8.8130, -8.8130, -8.9982, -9.0882,\n",
      "        -8.6597, -9.0775], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.9841, -8.9433, -8.9944, -8.8719, -8.8805, -8.8805, -8.7834, -8.8805,\n",
      "        -8.7092, -8.8971], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03554265946149826\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.6752, -8.7134, -8.9675, -8.7266], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-8.7134, -8.6895, -8.7559, -8.7559, -8.8818, -8.7825, -8.8250, -8.9059,\n",
      "        -8.5668, -8.8250], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.9713, -8.8077, -8.8361, -8.8361, -8.8763, -8.8892, -8.8892, -8.8361,\n",
      "        -9.0915, -8.8892], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03930731862783432\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-8.9916, -8.7695, -8.8548, -8.7871, -8.7987, -8.6527, -8.7695, -8.7695,\n",
      "        -8.7695, -8.8695], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.9693, -8.8090, -8.9693, -8.9171, -8.9084, -8.7874, -8.8090, -8.8090,\n",
      "        -8.8090, -8.8890], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.006731752306222916\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.6355, -8.7651, -8.9645, -8.8287], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-8.5593, -8.8652, -9.0294, -9.0406, -9.0294, -9.0457, -8.6511, -9.0615,\n",
      "        -9.2578, -9.0708], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-9.1412, -8.9413, -8.9413, -8.8900, -8.9413, -9.1554, -8.8900, -8.9085,\n",
      "        -8.8756, -8.7034], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07560525834560394\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-8.9560, -8.6943, -8.7257, -9.0695, -8.8862, -8.8522, -8.9347, -8.9340,\n",
      "        -9.0707, -8.8711], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.8249, -9.0604, -8.8249, -8.9712, -8.9670, -8.9670, -8.9686, -8.9686,\n",
      "        -9.0604, -8.9489], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01989036798477173\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-8.8064, -8.9310, -8.8585, -8.8404, -8.7852, -8.8404, -9.2495, -8.7437,\n",
      "        -9.0265, -9.1895], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.9466, -9.0464, -8.6749, -8.9033, -8.9067, -8.9033, -8.8693, -8.9726,\n",
      "        -8.9519, -8.9726], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03388965129852295\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-8.8391, -8.7846, -8.8614, -8.9729, -8.8256, -8.9813, -8.8614, -8.9430,\n",
      "        -8.8185, -8.9938], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7057, -8.8856, -9.0756, -8.8745, -8.9430, -9.0756, -9.0756, -9.1036,\n",
      "        -8.8745, -8.8100], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.021483542397618294\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-9.2291, -9.0527, -8.8424, -9.2378], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-9.0682, -9.1781, -8.9935, -8.8762, -9.0536, -9.1088, -9.0101, -9.0996,\n",
      "        -9.0101, -9.0101], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.8017, -8.8393, -8.6999, -8.5221, -8.7525, -8.8445, -8.9609, -8.7144,\n",
      "        -8.9609, -8.9609], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07134576141834259\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-9.1996, -9.1061, -8.8498, -9.1981], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.9359, -8.5169, -8.9021, -9.0545, -8.9930, -8.9133, -8.9972, -8.9133,\n",
      "        -8.9478, -8.9678], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.9073, -8.9225, -8.6652, -8.8851, -8.8184, -8.6878, -8.9073, -8.6878,\n",
      "        -8.9225, -8.9394], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.039179541170597076\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-9.1521, -9.1147, -8.9208, -9.1157], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.9483, -8.8669, -9.0428, -8.8721, -8.8687, -8.8514, -8.7946, -8.7888,\n",
      "        -8.9483, -8.7093], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.9173, -8.7051, -8.7464, -8.9818, -8.9818, -8.8121, -8.7615, -8.7043,\n",
      "        -8.9173, -8.9759], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.022162478417158127\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-8.9338, -8.8921, -8.7798, -8.8572, -8.6558, -8.8921, -8.8117, -8.7088,\n",
      "        -8.7047, -8.9894], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.8399, -8.8450, -8.9018, -8.8450, -8.7624, -8.8450, -8.7535, -8.9381,\n",
      "        -8.7691, -8.9311], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.010317658074200153\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-9.0150, -9.1090, -9.1241, -8.9444], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-8.7244, -8.6262, -9.0238, -8.5799, -8.6666, -8.8606, -8.6628, -8.9999,\n",
      "        -8.8929, -8.8443], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.8333, -8.6856, -9.1015, -8.9169, -8.6448, -8.8218, -8.8730, -8.8730,\n",
      "        -8.8730, -8.7347], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0209705401211977\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-8.8234, -8.9581, -8.9128, -8.6245], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-8.9973, -9.0228, -9.1624, -8.7335, -8.9973, -8.7296, -8.9901, -9.0888,\n",
      "        -8.4613, -8.7296], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.8066, -8.6268, -8.7692, -8.7692, -8.8066, -8.5536, -9.0430, -8.5536,\n",
      "        -8.7692, -8.5536], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08313216269016266\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-9.0589, -8.8346, -9.1421, -8.5333, -8.9085, -8.5016, -8.4596, -8.4891,\n",
      "        -8.6991, -8.9085], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.6108, -8.6137, -8.7020, -8.8528, -8.7987, -9.0079, -8.6137, -8.6402,\n",
      "        -8.5790, -8.7987], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08868055790662766\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-8.7333, -8.5640, -8.2943, -8.6942, -8.5465, -8.5640, -8.5640, -8.4591,\n",
      "        -8.4591, -8.6942], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.9461, -8.4648, -8.5294, -8.6918, -8.8075, -8.4648, -8.4648, -8.6748,\n",
      "        -8.6748, -8.6918], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0291338749229908\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-8.6000, -8.7137, -8.8915, -9.0007], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-8.5461, -8.5949, -8.4418, -8.4399, -8.3481, -8.4399, -8.5949, -8.8745,\n",
      "        -8.4532, -8.6636], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.5976, -8.5589, -8.5589, -8.3831, -8.4558, -8.3831, -8.5589, -8.7244,\n",
      "        -8.8655, -8.8655], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.027030816301703453\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-8.5361, -8.6314, -8.8820, -9.1200], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-8.4538, -8.4107, -8.3756, -8.4850, -8.4387, -8.5494, -8.9509, -8.8718,\n",
      "        -8.8976, -8.5415], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.5696, -8.4795, -8.4795, -8.5810, -8.4560, -8.5949, -8.6365, -8.6874,\n",
      "        -8.5345, -8.6084], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.030970897525548935\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-8.5310, -8.5423, -8.8333, -9.1989], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-8.9380, -8.6040, -8.2332, -8.4763, -8.2332, -8.8523, -8.5310, -8.6040,\n",
      "        -8.4646, -8.8885], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.5223, -8.6079, -8.2936, -8.7365, -8.2936, -8.6079, -8.6779, -8.6079,\n",
      "        -8.6182, -8.7365], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03758227452635765\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-8.5236, -8.4991, -8.7725, -9.2246], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-8.5236, -8.9573, -8.1671, -9.0728, -8.9573, -8.7731, -9.2246, -8.1671,\n",
      "        -8.4059, -8.4290], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.6492, -8.4813, -8.2674, -8.4198, -8.4813, -8.2674, -8.6492, -8.2674,\n",
      "        -8.6964, -8.4023], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15872782468795776\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-8.3187, -8.2398, -8.6030, -8.8759], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-8.3741, -8.7914, -8.5914, -8.2282, -8.7070, -8.3741, -8.3766, -8.7914,\n",
      "        -8.5009, -8.7878], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.5367, -8.5389, -8.2463, -8.3164, -8.6373, -8.5367, -8.5064, -8.5389,\n",
      "        -8.6508, -8.5064], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.043058883398771286\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-8.5555, -8.4107, -8.6412, -9.0263], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-8.5249, -9.0263, -8.2089, -8.3676, -8.3447, -8.2984, -8.7215, -8.5378,\n",
      "        -8.3853, -8.2166], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.6724, -8.4143, -8.3098, -8.3098, -8.5853, -8.7450, -8.8328, -8.4697,\n",
      "        -8.4697, -8.2780], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0695120245218277\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-7.9676, -8.0388, -8.2169, -8.2568], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-8.3399, -8.2219, -8.3399, -8.6527, -8.4143, -8.4532, -8.3399, -8.4188,\n",
      "        -8.2556, -8.5611], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.5059, -8.5059, -8.5059, -8.7349, -8.5291, -8.3997, -8.5059, -8.8075,\n",
      "        -8.2930, -8.5492], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03387640044093132\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-7.9128, -8.0189, -8.1058, -7.9979], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-8.1943, -8.4382, -8.4023, -8.5491, -8.2992, -7.9979, -8.1685, -8.3040,\n",
      "        -8.3040, -7.9979], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.3082, -8.4458, -8.3517, -8.6644, -8.3517, -8.6470, -8.3517, -8.3061,\n",
      "        -8.3061, -8.6470], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09078238904476166\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-7.9534, -8.0305, -8.1021, -7.9287], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-8.5034, -7.9957, -8.5664, -8.3069, -8.1382, -8.4841, -8.3069, -8.4426,\n",
      "        -8.3742, -8.8366], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.2466, -8.1962, -8.5863, -8.2466, -8.2466, -8.5863, -8.2466, -8.3936,\n",
      "        -8.2727, -8.3936], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03449324518442154\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-8.4556, -8.3240, -8.3357, -8.3229], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-8.0455, -8.4266, -8.6914, -8.1002, -7.9055, -8.1002, -8.5708, -8.2194,\n",
      "        -8.3851, -8.0455], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.2410, -8.3823, -8.6069, -8.6069, -8.4906, -8.6069, -8.3611, -8.1753,\n",
      "        -8.3673, -8.2410], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09876628965139389\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-8.4179, -8.3398, -8.4452, -8.0895], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-8.2750, -8.4103, -8.2750, -8.2750, -8.3142, -8.2413, -8.2336, -8.4751,\n",
      "        -8.2413, -8.1676], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.1661, -8.3462, -8.1661, -8.1661, -8.2904, -8.0403, -8.1455, -8.0403,\n",
      "        -8.0403, -8.3509], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0351574569940567\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-8.5246, -8.2129, -8.4122, -8.5761, -8.4511, -8.3127, -7.8024, -8.2407,\n",
      "        -8.1520, -8.1691], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.4457, -8.1471, -8.3076, -8.4457, -8.3076, -8.2487, -8.0222, -8.0222,\n",
      "        -8.5643, -8.1450], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03298185020685196\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-8.3998, -8.3813, -8.3436, -8.1216], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-8.1758, -8.2623, -8.5480, -8.0553, -8.1981, -8.1758, -8.4334, -8.2815,\n",
      "        -8.5114, -7.9542], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.1638, -8.4107, -8.3735, 10.0000, -8.0510, -8.1638, -8.4107, -8.1188,\n",
      "        -8.1873, -8.1543], grad_fn=<AddBackward0>)\n",
      "LOSS: 32.624019622802734\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-7.9925, -8.0060, -7.9750, -7.4794], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-7.9216, -7.7643, -7.9241, -8.1185, -8.0006, -7.8976, -7.9216, -7.4794,\n",
      "        -7.2964, -7.8310], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.5426, -7.7828, -7.6494, -7.7828, -7.8935, -7.5840, -7.5426, -7.7314,\n",
      "        -7.6882, -7.5289], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08939661085605621\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-7.6009, -7.5378, -7.5485, -7.4423, -7.6368, -7.0673, -6.7589, -7.4508,\n",
      "        -7.6040, -7.4478], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.2184, -7.2637, -7.2184, -7.1130, -7.2628, -7.0830, -7.0830, -7.1130,\n",
      "        -7.2628, -7.1130], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10265103727579117\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-7.1580, -7.1795, -6.6165, -7.2884, -7.0638, -7.2679, -6.3688, -7.2150,\n",
      "        -7.2074, -7.4946], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.7084, -6.8371, -6.7319, -6.9548, -6.7498, -6.7084, -6.7319, -6.9031,\n",
      "        -6.8750, -6.9548], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14866697788238525\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-6.8154, -6.9313, -6.8635, -6.2226], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-6.8154, -7.1112, -6.7683, -6.7785, -6.7440, -6.7440, -6.3953, -6.7645,\n",
      "        -6.9038, -6.7785], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.6003, -6.4465, -6.4313, -6.5023, -6.4623, -6.4623, -6.4425, -6.4623,\n",
      "        -6.6158, -6.5023], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10894240438938141\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-6.4375, -6.5416, -6.4860, -5.9533], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.7823, -6.3053, -6.3830, -6.4042, -6.3053, -6.3794, -6.3053, -6.4042,\n",
      "        -6.4704, -5.8367], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.2041, -6.4105, -6.2380, -6.2041, -6.4105, -6.2380, -6.4105, -6.2041,\n",
      "        -6.4105, -6.3580], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06074143201112747\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-6.3362, -6.0078, -5.5354, -5.8232, -6.1190, -5.8883, -6.3671, -6.0188,\n",
      "        -5.8232, -6.0573], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.1784, -6.0133, -5.9118, -5.9118, -6.0482, -5.4857, -6.2995, -6.0133,\n",
      "        -5.9118, -6.0482], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03541944921016693\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.8615, -5.8798, -5.7838, -5.4011], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-6.0228, -5.8618, -5.3752, -6.0264, -5.7762, -5.4011, -5.3719, -5.5472,\n",
      "        -5.9667, -5.9303], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.8180, -5.8825, -5.8377, -5.8180, -5.8825, -5.8610, -6.0207, -5.8377,\n",
      "        -6.0682, -6.1062], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1069066971540451\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.6395, -5.6357, -5.5222, -5.2414], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.5261, -5.6389, -5.3476, -5.5972, -5.6982, -5.5222, -5.2058, -5.2188,\n",
      "        -5.5324, -5.4373], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([10.0000, -5.7398, -5.5405, -5.7455, -5.8382, -5.6852, -5.6852, -5.6962,\n",
      "        -5.7398, -5.6595], grad_fn=<AddBackward0>)\n",
      "LOSS: 21.167381286621094\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3383, -5.3321, -5.2066, -4.8894], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.8894, -4.6724, -5.4509, -5.2684, -5.0701, -5.2172, -5.2684, -4.8073,\n",
      "        -5.1330, -5.2514], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4004, -5.4124, -5.3775, -5.3647, -5.4991, -5.4114, -5.3647, -5.5818,\n",
      "        -5.5972, -5.5972], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.19894471764564514\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-4.8239, -4.6033, -4.5114, -4.9840, -4.8979, -4.7494, -4.9564, -4.9564,\n",
      "        -4.8715, -4.6666], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1282, -5.1429, -5.2573, -5.2981, -5.2981, -5.1999, -5.1282, -5.1282,\n",
      "        -5.0603, -5.0847], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16714736819267273\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8061, -4.8655, -4.7985, -4.3355], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.8968, -4.3975, -4.8968, -4.6954, -4.3213, -4.4534, -4.6092, -4.2588,\n",
      "        -4.7662, -4.7464], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0249, -4.0816, -5.0249, -4.8448, -4.7721, -4.6839, -4.8794, -4.8329,\n",
      "        -4.8870, -4.8794], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08461640775203705\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-4.4886, -4.5783, -4.6057, -3.9392, -4.5783, -4.5982, -4.0659, -4.5885,\n",
      "        -4.5830, -3.5851], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6680, -4.6521, -4.6042, -4.4723, -4.6521, -4.7038, -4.6680, -4.6586,\n",
      "        -4.6083, -3.8417], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07723675668239594\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-4.5548, -4.2914, -4.5548, -4.1503, -3.7285, -4.5548, -3.7900, -3.8431,\n",
      "        -4.4398, -4.4449], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5673, -4.4546, -4.5673, -4.2825, -4.2825, -4.5673, -4.4110, -4.4710,\n",
      "        -4.4633, -4.4758], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1132851392030716\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3255, -4.3228, -4.2436, -3.6895], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.3338, -3.9106, -3.6517, -3.6602, -4.3624, -4.4386, -4.3492, -4.3073,\n",
      "        -4.2729, -3.6602], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.2172, -4.2015, -4.2932, -4.2942, -4.3831, -4.2932, -4.2825, -4.3227,\n",
      "        -4.2825, -4.2942], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13399866223335266\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-4.0962, -3.4981, -4.1268, -4.3031, -4.2109, -3.4211, -4.2237, -4.2237,\n",
      "        -3.5027, -4.2286], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.1342, -4.1482, -4.1412, -4.2230, -4.1342, -3.3196, -4.1342, -4.1342,\n",
      "        -4.1448, -4.1673], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08790189027786255\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-4.0998, -4.1478, -3.1619, -4.1515, -4.0074, -4.1265, -4.2460, -3.3429,\n",
      "        -4.1821, -4.1181], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9733, -4.0777, -3.9344, -3.9733, -4.1776, -4.0086, -4.0357, -4.0086,\n",
      "        -4.0777, -4.0097], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12023158371448517\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-4.0274, -4.0887, -4.0750, -4.0274, -4.1045, -2.8963, -4.0573, -3.9461,\n",
      "        -4.0062, -3.9955], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9026, -3.9631, -3.9631, -3.9026, -3.9802, -3.0870, -3.8649, -3.8968,\n",
      "        -3.8649, -3.9026], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.017931003123521805\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-4.0284, -3.8834, -3.9354, -3.8293, -3.0805, -3.1364, -3.0764, -3.9584,\n",
      "        -3.9353, -3.9354], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.8610, -3.7992, -3.8110, -3.7688, -3.7724, -3.8227, -3.6657, -3.7083,\n",
      "        -3.8002, -3.8110], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14476455748081207\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.8460, -3.9524, -3.8762, -2.9939], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.6445, -3.8965, -3.8106, -3.0220, -3.8410, -3.7468, -3.9397, -3.8454,\n",
      "        -3.0220, -3.8854], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7062, -3.8107, -3.8905, -3.7198, -3.7563, -3.7062, -3.8233, -3.7346,\n",
      "        -3.7198, -3.7774], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1037740707397461\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.7672, -3.8624, -3.7985, -2.9203], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.9465, -3.8599, -3.7594, -3.7672, -2.9465, -3.6936, -3.8146, -2.8321,\n",
      "        -3.7594, -2.9203], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6519, -3.7614, -3.6688, -3.6519, -3.6519, -3.6519, -3.6688, -3.6737,\n",
      "        -3.6688, -3.6282], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.22668269276618958\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-3.6791, -3.6873, -2.6062, -3.6651, -3.7848, -2.8600, -3.7848, -2.8999,\n",
      "        -3.7848, -3.6818], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6161, -3.6360, -2.7984, -3.4993, -3.7118, -3.5740, -3.7118, -3.6099,\n",
      "        -3.7118, -3.5740], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11125357449054718\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.6122, -3.5866, -3.5731, -2.8782], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.8625, -3.6332, -3.6747, -3.6837, -2.8107, -3.7132, -3.6040, -3.6253,\n",
      "        -3.6747, -2.8263], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4314, -3.5527, -3.5436, -3.5716, -3.5296, -3.6700, -3.5716, -3.5982,\n",
      "        -3.5436, -3.5527], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14252376556396484\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.5744, -3.6268, -3.6111, -2.7694], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.6417, -3.5330, -3.5659, -2.8184, -3.5722, -3.5371, -3.5928, -3.5560,\n",
      "        -3.6247, -2.7882], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6340, -3.5366, -3.5529, -3.6512, -3.5579, -3.5112, -3.5098, -3.4925,\n",
      "        -3.6340, -3.5094], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12257909774780273\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-3.5769, -3.4801, -3.5769, -3.5052, -3.5221, -3.4495, -3.4764, -3.2957,\n",
      "        -2.9123, -2.7467], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6037, -3.5247, -3.6037, -3.6211, -3.4720, -3.5382, -3.5136, -3.3578,\n",
      "        -3.4813, -3.4720], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08822719752788544\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4612, -3.3983, -3.4317, -2.7723], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.5255, -3.3903, -2.5865, -3.4248, -2.7261, -2.7039, -3.4484, -3.2359,\n",
      "        -3.4761, -3.5255], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5782, -3.5234, -3.3769, -3.4956, -3.4399, -3.4335, -3.4335, -3.5987,\n",
      "        -3.4951, -3.5782], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1827104538679123\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-2.3269, -3.4425, -2.6745, -2.6767, -3.4852, -2.8631, -3.3320, -3.4048,\n",
      "        -3.4843, -3.4425], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.6260, -3.4070, -3.4070, -3.4091, -3.4091, -3.4348, -3.3485, -3.4717,\n",
      "        -3.5549, -3.4070], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15073643624782562\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4315, -3.3608, -3.3748, -2.6445], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.3949, -3.4513, -3.0104, -3.3782, -2.7821, -2.7383, -3.4147, -3.4658,\n",
      "        -3.3451, -3.3451], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3252, -3.5363, -3.5039, -3.4490, -3.3003, -3.5636, -3.3801, -3.3003,\n",
      "        -3.4490, -3.4490], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12604138255119324\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3988, -3.3327, -3.3486, -2.6180], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.4375, -2.7320, -3.3876, -3.5383, -3.3251, -2.3183, -2.6180, -2.5455,\n",
      "        -3.3876, -3.3678], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3699, -3.5442, -3.3562, -3.4305, -3.4305, -2.5843, -3.3562, -3.3051,\n",
      "        -3.3562, -3.4362], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1886274516582489\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3704, -3.3113, -3.3299, -2.5969], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.4402, -3.4199, -3.0705, -3.3697, -3.5175, -3.3465, -3.3697, -3.2780,\n",
      "        -3.4199, -3.5077], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5060, -3.5060, -3.2125, -3.4614, -3.4171, -3.5224, -3.4614, -3.3372,\n",
      "        -3.5060, -3.4171], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.010887643322348595\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3462, -3.2987, -3.3151, -2.5761], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.2372, -2.5913, -3.3097, -3.4991, -3.5094, -3.3806, -2.5761, -3.3151,\n",
      "        -2.5972, -2.5761], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.1978, -3.3322, -3.4059, -3.4059, -3.4907, -3.4619, -3.3185, -3.3375,\n",
      "        -3.3375, -3.3185], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.31472888588905334\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3268, -3.2990, -3.3056, -2.5574], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.2898, -3.3859, -3.4012, -3.3221, -3.3991, -3.4161, -2.5574, -3.4161,\n",
      "        -3.5015, -3.3056], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4607, -3.3993, -3.4607, -3.3017, -3.3220, -3.4740, -3.3017, -3.4740,\n",
      "        -3.4740, -3.3220], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06009403616189957\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3136, -3.3032, -3.2987, -2.5418], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.3416, -2.6063, -3.3307, -2.7584, -3.2987, -3.3082, -3.3239, -2.6063,\n",
      "        -2.5418, -3.4351], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4826, -3.2876, -3.3852, -3.2814, -3.3079, -3.2876, -3.4826, -3.2876,\n",
      "        -3.2876, -3.4593], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1807187795639038\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.2968, -3.3031, -3.2869, -2.5218], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.2989, -2.4470, -3.0736, -2.5472, -3.4304, -3.2856, -2.5512, -3.3216,\n",
      "        -3.3480, -2.5512], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2696, -3.2925, -3.4873, -3.2925, -3.4456, -3.4594, -3.2961, -3.3475,\n",
      "        -3.3794, -3.2961], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2584022283554077\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.2882, -3.3116, -3.2814, -2.5059], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.4473, -3.1994, -3.4013, -3.4473, -3.2637, -2.5059, -3.3116, -2.5059,\n",
      "        -2.5308, -3.4767], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4357, -3.2777, -3.3732, -3.4357, -3.4333, -3.2553, -3.3497, -3.2553,\n",
      "        -3.1990, -3.4605], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16073529422283173\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.2851, -3.3225, -3.2820, -2.4945], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.4945, -2.4945, -3.2851, -3.4645, -3.2761, -2.5200, -3.3089, -3.4713,\n",
      "        -3.2820, -2.6146], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2451, -3.2451, -3.2680, -3.4284, -3.1912, -3.2680, -3.4627, -3.4284,\n",
      "        -3.2822, -3.2451], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.21179616451263428\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-3.5305, -2.4842, -2.4842, -2.6934, -3.1799, -3.2534, -3.3295, -2.9762,\n",
      "        -2.6261, -3.2824], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4663, -3.2358, -3.2358, -3.0998, -3.5017, -3.2998, -3.3635, -3.0998,\n",
      "        -3.2358, -3.2608], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1793314814567566\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4015, -3.5156, -3.4891, -2.6426], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.4295, -2.6836, -2.4747, -2.4747, -2.2853, -3.4503, -3.4801, -3.6161,\n",
      "        -3.5588, -2.3238], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5049, -3.3784, -3.2273, -3.2273, -2.4998, -3.3709, -3.3868, -3.4152,\n",
      "        -3.4708, -2.4998], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1761065423488617\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-3.5844, -3.3185, -3.2629, -3.2544, -3.5844, -3.3621, -3.3185, -3.4224,\n",
      "        -2.2142, -3.3621], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4784, -3.2228, -3.5158, -3.2552, -3.4784, -3.4784, -3.2228, -3.1872,\n",
      "        -2.4987, -3.4784], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.026812192052602768\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-2.6109, -3.6049, -3.3209, -3.5102, -3.4480, -3.5546, -2.6988, -3.6598,\n",
      "        -3.2895, -3.4260], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.1910, -3.4829, -3.2198, -3.4032, -3.5100, -3.4306, -3.3993, -3.4032,\n",
      "        -3.2788, -3.2788], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0970686823129654\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-2.8089, -2.9802, -3.4851, -2.4662, -2.4662, -3.5581, -3.2855, -2.6728,\n",
      "        -3.5411, -3.6157], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3742, -3.0873, -3.3869, -3.2195, -3.2195, -3.4375, -3.2803, -3.2195,\n",
      "        -3.3869, -3.4874], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.18295447528362274\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3159, -3.4860, -3.4160, -2.5378], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.2562, -3.5461, -3.4817, -3.4012, -3.5461, -3.6229, -3.4160, -3.3159,\n",
      "        -3.3159, -2.2186], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2591, -3.4282, -3.4585, -3.3960, -3.4282, -3.4963, -3.2840, -3.2203,\n",
      "        -3.2203, -2.5010], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.015990231186151505\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-2.5152, -3.6230, -2.7837, -3.2142, -3.5531, -3.2687, -3.5780, -3.7221,\n",
      "        -2.8469, -3.5404], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2637, -3.5053, -3.4743, -3.4164, -3.4414, -3.2637, -3.4556, -3.5053,\n",
      "        -3.4164, -3.4414], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15005473792552948\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.2648, -3.3495, -3.2538, -2.4757], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.4757, -2.4757, -2.5457, -3.5567, -3.7268, -3.4119, -2.5457, -3.2474,\n",
      "        -3.5567, -3.2538], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2281, -3.2281, -3.2911, -3.4569, -3.5246, -3.4315, -3.2911, -3.2281,\n",
      "        -3.4569, -3.2911], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.23066166043281555\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.2641, -3.3441, -3.2420, -2.4861], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.5348, -2.5547, -3.5573, -3.2560, -2.2193, -2.4861, -3.5243, -3.6356,\n",
      "        -2.8298, -3.2988], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2813, -3.2992, -3.4739, -3.2813, -2.5151, -3.2375, -3.6064, -3.5468,\n",
      "        -3.5012, -3.2375], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2240498811006546\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-3.4015, -2.7520, -3.4919, -2.4995, -3.2352, -3.4536, -2.4995, -3.3006,\n",
      "        -2.5654, -3.5299], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6297, -3.2495, -3.5668, -3.2495, -3.3088, -3.4590, -3.2495, -3.2495,\n",
      "        -3.3088, -3.4928], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.19925543665885925\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-3.5451, -3.4681, -3.2719, -3.2317, -3.3033, -3.6650, -3.6258, -3.6769,\n",
      "        -3.2719, -2.7910], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4779, -3.4858, -3.3071, -3.3190, -3.2613, -3.6033, -3.5244, -3.5119,\n",
      "        -3.3071, -3.5796], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06797386705875397\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.1943, -3.2160, -3.3111, -2.5144], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.2329, -3.6764, -2.2687, -3.2160, -3.5784, -3.3049, -3.5784, -2.2329,\n",
      "        -3.3049, -2.5789], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.5376, -3.6288, -2.5376, -3.5047, -3.5290, -3.2740, -3.5290, -2.5376,\n",
      "        -3.2740, -3.3210], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09010038524866104\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.2960, -3.3368, -3.2313, -2.5932], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.6911, -3.3491, -3.5840, -3.3120, -2.8167, -3.6911, -2.8167, -3.5840,\n",
      "        -3.5840, -2.9458], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6512, -3.5350, -3.5415, -3.2854, -3.2854, -3.6512, -3.2854, -3.5415,\n",
      "        -3.5415, -3.5464], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08440002053976059\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.3117, -3.3442, -3.2365, -2.6091], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.0167, -2.6091, -3.5672, -3.5357, -3.5752, -3.7129, -3.5885, -3.4417,\n",
      "        -2.5486, -3.5885], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.1619, -3.3482, -3.2927, -3.5539, -3.5862, -3.5862, -3.5539, -3.7098,\n",
      "        -3.2938, -3.5539], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12887094914913177\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-2.5590, -3.3543, -3.3365, -3.3256, -3.3365, -3.3236, -3.5959, -3.3365,\n",
      "        -2.5590, -3.5959], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3031, -3.5804, -3.5804, -3.3631, -3.5804, -3.3031, -3.5670, -3.5804,\n",
      "        -3.3031, -3.5670], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13404841721057892\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3302, -3.3683, -3.2438, -2.5713], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.2438, -3.7553, -3.5270, -2.8775, -3.3320, -2.5713, -3.4765, -3.6576,\n",
      "        -3.7553, -3.3301], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3610, -3.7282, -3.5897, -3.3142, -3.3142, -3.3142, -3.7398, -3.6327,\n",
      "        -3.7282, -3.3761], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08340869098901749\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-3.6767, -3.7813, -2.5823, -2.5823, -2.5823, -3.7815, -3.3518, -3.2782,\n",
      "        -3.0558, -2.8620], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5911, -3.7489, -3.3241, -3.3241, -3.3241, -3.7489, -3.3671, -3.3241,\n",
      "        -3.5835, -3.6022], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.24888265132904053\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-3.5785, -2.5974, -3.5179, -3.5307, -3.5939, -2.6705, -3.0777, -2.6705,\n",
      "        -3.2805, -2.3488], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7734, -3.3376, -3.5901, -3.6747, -3.6066, -3.4034, -3.5989, -3.4034,\n",
      "        -3.3762, -3.2009], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2693454623222351\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.4023, -3.4371, -3.3228, -2.6851], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.6765, -3.3642, -3.4986, -3.6765, -3.4337, -3.8465, -2.6851, -3.6765,\n",
      "        -3.5506, -3.7321], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6183, -3.3493, -3.6030, -3.6183, -3.6193, -3.7924, -3.4166, -3.6183,\n",
      "        -3.7910, -3.6183], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06645574420690536\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.4237, -3.4705, -3.3462, -2.6995], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.6416, -3.7094, -2.6242, -3.7767, -3.4683, -3.1186, -3.3918, -3.6801,\n",
      "        -3.8843, -3.5295], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6135, -3.6299, -3.3618, -3.7027, -3.6240, -3.6135, -3.3873, -3.8086,\n",
      "        -3.8086, -3.6240], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08570303022861481\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.4456, -3.5008, -3.3696, -2.7134], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.9183, -3.9183, -3.3575, -3.4356, -3.9183, -3.1426, -3.3909, -3.5680,\n",
      "        -3.3575, -3.7395], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.8245, -3.8245, -3.3933, -3.4420, -3.8245, -3.6180, -3.3762, -3.6180,\n",
      "        -3.3933, -3.6418], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.026725884526968002\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.4656, -3.5232, -3.3910, -2.7273], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.6581, -3.7031, -3.9436, -3.3846, -2.7273, -3.5614, -3.3846, -2.4383,\n",
      "        -3.4607, -3.9421], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3923, -3.8414, -3.8414, -3.3992, -3.4546, -3.6363, -3.3992, -2.6023,\n",
      "        -3.4546, -3.8414], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11406298726797104\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.4862, -3.5472, -3.4121, -2.7432], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.9438, -2.6777, -3.6914, -3.7515, -2.6777, -3.8590, -3.4167, -2.3812,\n",
      "        -2.6743, -2.7432], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7478, -3.4099, -3.4847, -3.8580, -3.4099, -3.7478, -3.4099, -2.6087,\n",
      "        -3.4068, -3.4688], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.22922363877296448\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-3.9865, -3.6983, -3.7936, -3.8694, -3.5946, -3.4294, -3.9596, -2.9572,\n",
      "        -2.4494, -3.7012], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.8739, -3.5091, -3.6787, -3.9084, -3.6615, -3.4308, -3.7686, -3.4178,\n",
      "        -2.6164, -3.6615], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03457456827163696\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-2.9487, -2.7251, -2.7251, -3.7049, -2.6969, -3.8206, -2.8178, -3.4559,\n",
      "        -3.7734, -2.7821], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4525, -3.4525, -3.4525, -3.6538, -3.4272, -3.6995, -3.5039, -3.4272,\n",
      "        -3.8901, -3.5039], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.28692007064819336\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.6127, -3.6562, -3.7318, -2.8606], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.5247, -4.0130, -2.7973, -3.4717, -3.8896, -3.7486, -3.7871, -3.0037,\n",
      "        -2.7479, -3.8384], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5175, -3.9093, -3.5175, -3.4414, -3.7197, -3.7197, -3.8193, -3.4414,\n",
      "        -3.4731, -3.7197], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12928135693073273\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-3.5319, -3.2564, -3.4622, -3.6923, -4.0225, -3.4427, -3.8282, -3.7588,\n",
      "        -4.0225, -3.5563], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5339, -3.7483, -3.4537, -3.7483, -3.9308, -3.4955, -3.9308, -3.7435,\n",
      "        -3.9308, -3.5339], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02760108932852745\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.5712, -3.6628, -3.4938, -2.7960], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.9086, -4.0331, -3.6964, -3.6255, -3.9645, -2.7960, -4.0331, -3.0754,\n",
      "        -2.7399, -3.8278], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.0300, -3.9555, -3.7679, -3.7592, -3.8712, -3.5164, -3.9555, -3.7059,\n",
      "        -3.4659, -3.9555], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15183135867118835\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.5892, -3.6765, -3.5074, -2.8192], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.8782, -3.5074, -2.8192, -3.8782, -3.7192, -3.0193, -2.8192, -1.8529,\n",
      "        -2.8192, -3.4597], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7886, -3.4806, -3.5373, -3.7886, -3.7842, -3.5373, -3.5373, 10.0000,\n",
      "        -3.5373, -3.5373], grad_fn=<AddBackward0>)\n",
      "LOSS: 14.23328971862793\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-4.0003, -3.8947, -3.8402, -3.5560, -4.0003, -2.7655, -3.7996, -3.1967,\n",
      "        -2.7999, -4.0003], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9297, -3.7414, -3.9297, -3.5199, -3.9297, -3.4890, -3.6688, -3.7414,\n",
      "        -3.5199, -3.9297], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14033612608909607\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-3.9127, -3.8009, -3.3988, -2.9980, -3.7921, -3.7039, -3.8153, -2.7180,\n",
      "        -3.6624, -2.9091], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.8126, -3.6955, -3.4462, -3.3968, -3.8852, -3.6955, -3.8852, -3.4462,\n",
      "        -3.6182, -3.4809], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10552890598773956\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.5030, -3.5752, -3.4011, -2.6787], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.4674, -3.2735, -3.5854, -2.8903, -3.6621, -2.6237, -3.9279, -2.9891,\n",
      "        -2.4142, -2.6237], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4430, -3.5761, -3.9461, -3.4430, -3.5761, -3.3614, -3.8477, -3.5927,\n",
      "        -2.5142, -3.3614], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2004004269838333\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4835, -3.5558, -3.3766, -2.6470], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.3472, -3.8499, -2.3948, -3.8103, -3.9863, -3.7688, -2.6735, -3.7229,\n",
      "        -3.3766, -3.7461], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3346, -3.7575, -2.4765, -3.6296, -3.8181, -3.8181, -3.4062, -3.7575,\n",
      "        -3.3346, -3.6296], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06320525705814362\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-3.6109, -3.4099, -3.3424, -2.8121, -3.8795, -3.5561, -3.3455, -3.3455,\n",
      "        -3.7659, -3.4637], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6080, -3.3761, -3.3585, -3.3585, -3.7936, -3.9023, -3.3129, -3.3129,\n",
      "        -3.7936, -3.5980], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04480795934796333\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-2.6117, -3.7023, -3.7023, -3.7501, -3.3285, -2.5901, -3.0776, -2.5901,\n",
      "        -2.8744, -2.5901], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3505, -3.6531, -3.6531, -3.5869, -3.3311, -3.3311, -3.6531, -3.3311,\n",
      "        -3.1312, -3.3311], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.262176513671875\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.7055, -3.7256, -3.7417, -2.8865], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.1817, -3.3180, -3.8311, -2.5929, -3.6838, -3.8311, -2.5714, -3.5493,\n",
      "        -3.8544, -3.8311], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5075, -3.3143, -3.7034, -3.3336, -3.7034, -3.7034, -3.3143, -3.6049,\n",
      "        -3.7602, -3.7034], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1267942637205124\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.3624, -3.4169, -3.2074, -2.5804], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.8415, -3.4159, -3.4881, -2.5238, -3.8415, -3.3091, -2.5804, -3.3091,\n",
      "        -2.5590, -3.5414], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7559, -3.3223, -3.4851, -3.2715, -3.7559, -3.3031, -3.3223, -3.3031,\n",
      "        -3.3031, -3.8584], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17870917916297913\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-3.2431, -3.6932, -3.7317, -3.1913, -2.5498, -2.5498, -3.4642, -3.3545,\n",
      "        -2.9867, -2.8574], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2657, -3.5716, -3.4778, -3.2948, -3.2948, -3.2948, -3.5983, -3.3160,\n",
      "        -3.5465, -3.1116], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15980127453804016\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.3501, -3.4028, -3.1781, -2.5721], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.9087, -3.6600, -3.0445, -3.6589, -3.7465, -3.6381, -3.5348, -3.8239,\n",
      "        -3.4354, -3.2210], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3149, -3.6380, -3.1068, -3.5740, -3.7537, -3.4986, -3.5356, -3.7537,\n",
      "        -3.5974, -3.2643], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02291814610362053\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.3402, -3.3924, -3.1628, -2.5670], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.8156, -3.8156, -2.5441, -3.2135, -3.5387, -3.6888, -3.7101, -2.5670,\n",
      "        -3.3903, -2.5670], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7551, -3.7551, -3.2897, -3.2643, -3.8474, -3.5786, -3.7551, -3.3103,\n",
      "        -3.3103, -3.3103], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17866069078445435\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-3.7888, -3.8093, -3.7088, -3.7888, -2.5676, -2.8847, -3.8093, -2.7880,\n",
      "        -3.7888, -2.9372], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6990, -3.7571, -3.7571, -3.6990, -3.3108, -3.2655, -3.7571, -3.6435,\n",
      "        -3.6990, -3.3108], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16009005904197693\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-3.6863, -3.7437, -3.1956, -3.7988, -3.6504, -3.7988, -3.4443, -3.1451,\n",
      "        -2.5684, -2.5684], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5936, -3.5300, -3.2668, -3.7629, -3.6465, -3.7629, -3.6057, -3.2939,\n",
      "        -3.3115, -3.3115], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12147227674722672\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-3.7118, -3.6910, -3.1927, -3.3316, -3.6802, -3.7861, -3.6802, -3.6585,\n",
      "        -3.7861, -3.6802], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7691, -3.5286, -3.2707, -3.3172, -3.6029, -3.7691, -3.6029, -3.7169,\n",
      "        -3.7691, -3.6029], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.005783010274171829\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.5331, -3.6336, -3.4171, -2.7783], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.1926, -2.5574, -3.7143, -3.6336, -3.1905, -3.6676, -3.1905, -3.5794,\n",
      "        -3.7679, -2.9658], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2743, -3.3017, -3.7754, -3.6605, -3.2743, -3.6121, -3.2743, -3.8555,\n",
      "        -3.7754, -3.3228], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07858521491289139\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3842, -3.4233, -3.1983, -2.5652], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.5389, -2.9803, -3.4233, -2.5652, -3.5137, -3.4233, -3.1737, -3.1983,\n",
      "        -3.6387, -3.2876], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6823, -3.3241, -3.5071, -3.3087, -3.6212, -3.5071, -3.5963, -3.2768,\n",
      "        -3.6212, -3.3087], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09025716036558151\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-3.2152, -3.1486, -2.5806, -2.3641, -2.9717, -2.9717, -3.7027, -2.5772,\n",
      "        -2.5772, -2.5806], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2797, -3.3195, -3.3226, -2.3041, -3.5144, -3.5144, -3.7463, -3.3195,\n",
      "        -3.3195, -3.3226], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.28307998180389404\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.5545, -3.6271, -3.4457, -2.7939], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.5391, -2.5863, -3.6814, -2.9359, -3.3043, -3.3443, -3.2378, -2.5863,\n",
      "        -3.7559, -2.5863], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2852, -3.3277, -3.7582, -3.2852, -3.3327, -3.7193, -3.2852, -3.3277,\n",
      "        -3.7972, -3.3277], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.24788916110992432\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-2.6074, -2.6074, -3.2647, -3.3145, -3.1897, -3.3653, -3.3145, -3.7757,\n",
      "        -2.0410, -2.6074], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3467, -3.3467, -3.2890, -3.3467, -3.3467, -3.3418, -3.3467, -3.7695,\n",
      "        -2.3038, -3.3467], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17364531755447388\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3298, -3.4324, -3.2549, -2.5491], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.3909, -3.3298, -3.7063, -3.7757, -2.6238, -3.7317, -3.7317, -2.6238,\n",
      "        -3.7317, -3.4324], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.3051, -3.3614, -3.6750, -3.6753, -3.3614, -3.7813, -3.5208, -3.3614,\n",
      "        -3.7813, -3.6730], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12148658186197281\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3457, -3.4465, -3.2788, -2.5583], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.8030, -3.7433, -2.6410, -2.6410, -2.6410, -3.6333, -3.3263, -2.6410,\n",
      "        -3.8232, -3.7220], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7917, -3.6885, -3.3769, -3.3769, -3.3769, -3.6885, -3.3024, -3.3769,\n",
      "        -3.9134, -3.5903], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2198370397090912\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3625, -3.4609, -3.3024, -2.5705], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.4416, -2.6562, -3.7408, -3.7661, -2.5705, -2.6562, -3.5221, -2.6562,\n",
      "        -3.5221, -3.8080], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3906, -3.3906, -3.7054, -3.7054, -3.3134, -3.3906, -3.5358, -3.3906,\n",
      "        -3.5358, -3.7162], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.21862800419330597\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3796, -3.4750, -3.3253, -2.5863], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.7574, -3.3796, -3.6158, -3.7574, -3.1110, -3.3910, -3.8281, -3.8261,\n",
      "        -2.6803, -2.6889], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7250, -3.4200, -3.7948, -3.7250, -3.5471, -3.3277, -3.8674, -3.8181,\n",
      "        -3.4122, -3.4200], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13019254803657532\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-3.9440, -3.8564, -3.4198, -3.8649, -3.8580, -2.6040, -2.6040, -3.4198,\n",
      "        -3.7727, -3.4198], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.8850, -3.8328, -3.3436, -3.9674, -3.7651, -3.3436, -3.3436, -3.3436,\n",
      "        -3.7469, -3.3436], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11352884769439697\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-3.8057, -2.7353, -3.8558, -3.5203, -3.5203, -3.8747, -3.3702, -3.6854,\n",
      "        -3.7862, -3.8462], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9865, -3.4618, -3.9056, -3.4567, -3.4567, -3.9056, -3.4618, -3.8639,\n",
      "        -3.7702, -3.8526], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06125596910715103\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.6075, -3.6025, -3.4580, -2.7533], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.7521, -2.7521, -2.7533, -2.7521, -3.8756, -3.8384, -3.2435, -3.5436,\n",
      "        -3.9086, -3.8763], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4769, -3.4769, -3.4780, -3.4769, -3.8833, -3.9951, -3.8833, -3.4769,\n",
      "        -3.8168, -3.9191], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.25497621297836304\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.6260, -3.6293, -3.4803, -2.7759], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.5576, -3.4803, -2.7759, -2.7807, -3.1735, -3.8892, -3.8892, -2.6686,\n",
      "        -3.4253, -3.8892], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.8561, -3.4017, -3.4983, -3.5026, -3.5026, -3.8795, -3.8795, -3.4017,\n",
      "        -3.4983, -3.8795], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1789759397506714\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.6445, -3.6587, -3.5026, -2.7980], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.9189, -2.7980, -3.9459, -3.9169, -3.9169, -4.0002, -3.8619, -3.5026,\n",
      "        -3.9459, -3.9251], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9712, -3.5182, -3.9712, -3.8976, -3.8976, -3.9712, -3.8198, -3.4233,\n",
      "        -3.9712, -3.8976], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.053311217576265335\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.6616, -3.6847, -3.5209, -2.8193], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.8193, -2.0866, -2.7164, -3.9787, -3.6616, -3.8867, -3.6350, -3.5209,\n",
      "        -3.1037, -3.8900], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5373, -2.3514, -3.4448, -3.9957, -3.5565, -3.8379, -3.8919, -3.4448,\n",
      "        -3.4448, -3.8379], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1320827752351761\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.6751, -3.7111, -3.5377, -2.8386], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.8386, -3.8806, -4.0277, -3.2466, -2.8386, -2.8744, -3.9143, -4.0277,\n",
      "        -3.9973, -3.7122], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5547, -3.7377, -4.0201, -3.9054, -3.5547, -3.5869, -3.8563, -4.0201,\n",
      "        -4.0499, -3.9054], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.20315344631671906\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.6945, -3.7366, -3.5568, -2.8607], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.1214, -1.5192, -3.5568, -4.0428, -3.9948, -3.7214, -2.9092, -3.3833,\n",
      "        -2.8607, -3.2317], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.3673, 10.0000, -3.4825, -4.0450, -3.9524, -3.9086, -3.6183, -3.9506,\n",
      "        -3.5747, -3.6183], grad_fn=<AddBackward0>)\n",
      "LOSS: 13.4278564453125\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.6701, -3.7184, -3.5308, -2.8148], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.8152, -2.8772, -4.0375, -3.1708, -4.0426, -3.5229, -4.0045, -2.7151,\n",
      "        -3.5229, -2.8148], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.8991, -3.5895, -4.0105, -3.5895, -4.0187, -3.5333, -4.0187, -3.4436,\n",
      "        -3.5333, -3.5333], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17383496463298798\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.6527, -3.7060, -3.5114, -2.7782], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.4586, -3.8722, -2.7782, -3.1732, -3.6761, -3.8752, -3.9465, -2.6814,\n",
      "        -2.7782, -2.8471], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2679, -3.6795, -3.5004, -3.5887, -3.5624, -3.7952, -3.5887, -3.4132,\n",
      "        -3.5004, -3.5624], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.24838118255138397\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.6358, -3.6927, -3.4873, -2.7521], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.0249, -2.8479, -3.2776, -3.1072, -3.9785, -3.9077, -3.4937, -3.7114,\n",
      "        -3.7114, -3.4873], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9636, -3.4769, -3.8844, -3.1828, -3.9636, -3.8472, -3.4769, -3.7813,\n",
      "        -3.7813, -3.3914], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07964317500591278\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.6110, -3.6732, -3.4569, -2.7214], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.4232, -3.8717, -2.7214, -2.7214, -3.8717, -3.2777, -3.4804, -3.8865,\n",
      "        -2.7214, -3.8717], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3737, -3.7447, -3.4493, -3.4493, -3.7447, -3.8030, -3.4493, -3.8256,\n",
      "        -3.4493, -3.7447], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.19207869470119476\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-3.8495, -2.7032, -4.0199, -2.6244, -3.8642, -2.7032, -3.1259, -3.6521,\n",
      "        -4.0185, -3.4965], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7267, -3.4328, -3.9509, -3.3619, -3.8132, -3.4328, -3.1393, -3.5223,\n",
      "        -3.9301, -3.4328], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16601839661598206\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-1.9007, -3.4123, -3.9422, -3.8994, -3.4123, -2.6179, -3.2567, -3.8238,\n",
      "        -3.7783, -3.6955], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.1583, -3.3561, -3.7650, -3.8072, -3.3561, -3.3561, -3.8408, -3.7140,\n",
      "        -3.7140, -3.7112], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10151676833629608\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.5567, -3.6159, -3.3884, -2.6874], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.5951, -3.9062, -2.7990, -3.9705, -2.3508, -3.6159, -3.6543, -4.0080,\n",
      "        -2.6874, -3.8794], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6635, -3.9672, -3.5191, -3.9446, -2.1431, -3.5453, -3.7059, -3.9672,\n",
      "        -3.4187, -3.8045], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1120414137840271\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-3.0525, -2.6827, -2.8326, -2.8010, -2.8326, -3.6303, -3.7723, -3.9992,\n",
      "        -3.4374, -3.9992], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5494, -3.4144, -3.4144, -3.5209, -3.4144, -3.6991, -3.6991, -3.9765,\n",
      "        -3.4144, -3.9765], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1989104449748993\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-2.8086, -3.3429, -2.6697, -2.6232, -3.8559, -3.9926, -3.9138, -3.5167,\n",
      "        -2.9835, -3.5730], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5277, -3.3609, -3.4028, -3.3609, -3.7516, -3.9904, -3.9785, -3.5277,\n",
      "        -3.5277, -3.6558], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.19171565771102905\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-3.9915, -3.8317, -3.6687, -2.6631, -3.7218, -3.8075, -3.3489, -3.9657,\n",
      "        -2.6631, -2.6631], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.0064, -3.8112, -3.6937, -3.3968, -3.7606, -4.0064, -3.3700, -3.8053,\n",
      "        -3.3968, -3.3968], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1683368980884552\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.6139, -3.6482, -3.4358, -2.8164], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.6625, -2.3235, -3.6482, -1.8263, -3.9903, -2.6625, -2.6625, -2.9408,\n",
      "        -3.3564, -3.5381], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3963, -2.1135, -3.7035, -2.1135, -4.0207, -3.3963, -3.3963, -3.0911,\n",
      "        -3.7701, -3.8219], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.20200219750404358\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.6167, -3.6522, -3.4390, -2.8228], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.1168, -3.6167, -3.3082, -3.0195, -2.6658, -3.0753, -4.0061, -3.8315,\n",
      "        -3.3082, -3.4444], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7123, -3.5405, -3.3902, -3.0854, -3.3992, -3.7024, -4.0406, -3.8288,\n",
      "        -3.3902, -3.3992], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1312505155801773\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.6206, -3.6658, -3.4468, -2.8270], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.4495, -3.6206, -3.6658, -3.7182, -2.9341, -3.4923, -2.6695, -2.6695,\n",
      "        -3.6206, -4.0320], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4026, -3.5443, -3.7091, -3.8129, -3.4026, -3.5443, -3.4026, -3.4026,\n",
      "        -3.5443, -4.0569], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1322164088487625\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.6232, -3.6818, -3.4555, -2.8334], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.6730, -2.6667, -3.3737, -3.4164, -2.6730, -2.6730, -2.9684, -3.9170,\n",
      "        -3.4528, -3.7235], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4057, -3.4000, -3.4000, -3.7834, -3.4057, -3.4057, -3.1063, -4.0783,\n",
      "        -3.4057, -3.6897], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.23321203887462616\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.6295, -3.7027, -3.4701, -2.8439], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.0515, -3.7330, -3.7439, -2.6842, -3.6710, -2.6842, -3.4701, -3.6295,\n",
      "        -2.6762, -3.7465], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.0987, -3.6889, -3.8342, -3.4158, -3.6795, -3.4158, -3.4158, -3.5595,\n",
      "        -3.4086, -3.6889], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16303594410419464\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.6352, -3.7224, -3.4820, -2.8554], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.3708, -3.0190, -2.9910, -4.0344, -3.7446, -2.7004, -4.1375, -3.9886,\n",
      "        -4.0461, -3.7446], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.1197, -3.4304, -3.1337, -4.1282, -3.6919, -3.4304, -4.1282, -3.8478,\n",
      "        -4.1193, -3.6919], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0825158879160881\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-2.8668, -2.7121, -3.6396, -3.9277, -3.9277, -3.8196, -2.7011, -3.1808,\n",
      "        -3.8469, -4.1698], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5801, -3.4409, -3.5801, -3.8756, -3.8756, -3.7293, -3.4310, -3.7293,\n",
      "        -4.1531, -4.1531], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.19847196340560913\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.7453, -3.7847, -3.7383, -3.0367], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.8486, -3.7071, -3.7640, -3.4871, -3.0367, -2.7268, -3.5088, -3.9500,\n",
      "        -2.7268, -3.8192], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7330, -3.8940, -3.7041, -3.4542, -3.5933, -3.4542, -3.4542, -3.8940,\n",
      "        -3.4542, -3.8104], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1427016407251358\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.6468, -3.7617, -3.5119, -2.8877], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.8709, -3.9708, -4.2200, -2.8877, -4.2200, -3.7617, -3.5441, -3.7424,\n",
      "        -3.9708, -4.1126], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7531, -3.9119, -4.1973, -3.5990, -4.1973, -3.7531, -3.9159, -3.9119,\n",
      "        -3.9119, -4.1973], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07018347084522247\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-3.5182, -2.8963, -3.6725, -4.2407, -4.2407, -2.7512, -3.0575, -3.8078,\n",
      "        -3.5078, -3.5781], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4913, -3.6067, -3.8323, -4.2203, -4.2203, -3.4761, -3.1675, -3.9369,\n",
      "        -3.4913, -3.8364], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11529342085123062\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.7986, -3.8547, -3.8043, -3.1086], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.6956, -3.8078, -3.4410, -2.7918, -3.9039, -2.7918, -2.7918, -3.7770,\n",
      "        -1.8993, -3.4834], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.8560, -3.7475, -3.4929, -3.5126, -4.2485, -3.5126, -3.5126, -3.7977,\n",
      "        -2.1533, -3.4929], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17745614051818848\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.6692, -3.7920, -3.5459, -2.9169], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.8199, -3.5466, -4.1175, -3.9368, -2.8199, -2.9169, -3.1319, -4.1755,\n",
      "        -1.2887, -3.1945], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5379, -3.5379, -3.8580, -4.2680, -3.5379, -3.6252, -3.1878, -3.9651,\n",
      "        10.0000, -3.5379], grad_fn=<AddBackward0>)\n",
      "LOSS: 12.93089485168457\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.6538, -3.7635, -3.5236, -2.8721], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.8721, -2.7875, -2.8721, -3.5383, -3.5741, -2.8721, -2.7463, -3.6102,\n",
      "        -3.8066, -3.7136], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5849, -3.5087, -3.5849, -3.5087, -3.8159, -3.5849, -3.4717, -3.9277,\n",
      "        -3.7266, -3.8373], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.27523958683013916\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.6484, -3.7471, -3.5124, -2.8430], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.7471, -4.1331, -3.7930, -1.9095, -3.6191, -2.7688, -4.1803, -2.7172,\n",
      "        -2.8430, -4.2199], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7587, -3.9208, -3.9208, -3.1093, -3.6855, -3.4919, -4.2320, -3.4455,\n",
      "        -3.5587, -4.1849], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.30747637152671814\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-2.8130, -2.7483, -3.8828, -4.0219, -4.0219, -3.6452, -3.5004, -3.0859,\n",
      "        -3.9628, -2.8130], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5317, -3.4734, -3.6720, -3.9006, -3.9006, -3.5317, -3.4734, -3.4734,\n",
      "        -3.7437, -3.5317], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1844606250524521\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.6413, -3.7370, -3.4824, -2.7944], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.6659, -3.8664, -3.0687, -4.0620, -3.8664, -4.0166, -3.6413, -4.2098,\n",
      "        -3.5493, -2.9670], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3993, -3.6548, -3.4593, -3.7193, -3.6548, -3.8853, -3.5149, -4.1226,\n",
      "        -3.8152, -3.0055], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10104839503765106\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.6318, -3.7186, -3.4550, -2.7828], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.0613, -3.7807, -4.1808, -3.4299, -3.7705, -4.1782, -3.8235, -2.7194,\n",
      "        -3.5222, -3.9893], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4475, -3.8709, -4.1066, -3.3885, -3.6514, -4.1066, -3.7872, -3.4475,\n",
      "        -3.4475, -3.8709], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07347841560840607\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.6235, -3.6966, -3.4280, -2.7764], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.7555, -2.8754, -2.6461, -4.1533, -2.7077, -2.8754, -3.7910, -3.7555,\n",
      "        -2.7077, -2.7077], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6526, -3.3815, -3.3815, -4.0976, -3.4370, -3.3815, -3.6526, -3.6526,\n",
      "        -3.4370, -3.4370], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.26918041706085205\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-4.0139, -3.9213, -2.6321, -2.7775, -4.1195, -3.0006, -3.4959, -3.6441,\n",
      "        -4.1195, -3.0774], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7005, -3.8551, -3.3689, -3.4997, -4.0996, -3.7697, -3.4365, -3.5991,\n",
      "        -4.0996, -3.4365], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.18939292430877686\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.8000, -3.7481, -3.6775, -3.0049], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.4777, -3.8744, -3.6499, -2.7828, -3.0049, -3.4777, -3.6129, -4.0842,\n",
      "        -3.0818, -3.0049], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4351, -4.2218, -3.7044, -3.5045, -3.5045, -3.4351, -3.5045, -4.1054,\n",
      "        -3.4351, -3.5045], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12844735383987427\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-3.3174, -3.0349, -3.9078, -3.5925, -3.7166, -3.7166, -3.8561, -3.7513,\n",
      "        -4.0625, -3.7019], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3563, -3.4971, -3.7407, -3.7839, -3.6939, -3.6939, -3.8563, -3.7521,\n",
      "        -4.1125, -3.7343], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02842102386057377\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.8359, -3.7670, -3.6827, -3.0727], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.7593, -2.7593, -2.7593, -3.0698, -3.5994, -3.5643, -2.9408, -3.0727,\n",
      "        -3.0698, -2.7046], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4834, -3.4834, -3.4834, -3.7905, -3.7554, -3.4834, -3.3521, -3.4834,\n",
      "        -3.7905, -3.4342], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.351237952709198\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-3.4674, -3.2773, -3.5425, -3.2859, -2.6107, -2.7125, -3.7191, -3.4231,\n",
      "        -3.5727, -3.7191], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6746, -3.4413, -3.6746, -3.3496, -3.3496, -3.4413, -3.7338, -3.4413,\n",
      "        -3.7836, -3.7338], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1213720291852951\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.9017, -3.8290, -3.7323, -3.1715], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.7331, -2.7377, -3.8290, -3.5783, -4.2167, -2.7207, -3.1715, -3.1927,\n",
      "        -2.7207, -3.4923], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7542, -3.4639, -3.7712, -3.7728, -4.2703, -3.4486, -3.4639, -3.7785,\n",
      "        -3.4486, -3.8519], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.21897199749946594\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.5068, -3.5437, -3.2500, -2.7317], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.6125, -2.6125, -2.7395, -3.7682, -2.7395, -4.0584, -2.7317, -2.7395,\n",
      "        -3.4103, -3.5380], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3513, -3.3513, -3.4656, -3.7657, -3.4656, -4.1656, -3.4585, -3.4656,\n",
      "        -3.4656, -3.7208], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3249173164367676\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-2.7643, -3.7145, -2.7318, -3.7784, -3.8662, -3.8662, -3.9059, -2.8289,\n",
      "        -2.7643, -3.7784], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4878, -3.9263, -3.4586, -3.8016, -4.3107, -4.3107, -3.9187, -2.8696,\n",
      "        -3.4878, -3.8016], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.20184020698070526\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.5058, -3.5695, -3.2523, -2.7299], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.7299, -4.1267, -2.7854, -2.7299, -2.6345, -4.1267, -2.7043, -4.1267,\n",
      "        -2.6345, -3.2819], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4569, -4.2041, -3.5069, -3.4569, -3.3710, -4.2041, -2.8701, -4.2041,\n",
      "        -3.3710, -3.7553], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.29320988059043884\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.5160, -3.6006, -3.2674, -2.7339], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.6006, -2.7339, -4.0317, -3.3632, -4.1768, -3.6265, -3.0501, -4.1768,\n",
      "        -4.0847, -3.7205], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.0269, -3.4605, -3.8559, -3.4605, -4.2201, -3.8420, -3.5285, -4.2201,\n",
      "        -4.2201, -3.7450], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10480304062366486\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.5345, -3.6491, -3.2991, -2.7437], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.6325, -4.2342, -3.9333, -2.8216, -3.7442, -3.4175, -4.0058, -4.0058,\n",
      "        -2.7437, -3.5894], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7952, -4.2304, -3.8576, -3.5394, -3.8576, -3.3963, -3.9661, -3.9661,\n",
      "        -3.4693, -4.0204], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12763063609600067\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.5561, -3.6953, -3.3365, -2.7580], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.7580, -3.8200, -1.6372, -2.6764, -4.2945, -3.7192, -4.2945, -3.0492,\n",
      "        -3.4939, -3.9827], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4822, -3.7443, -1.8611, -3.4088, -4.2517, -3.8098, -4.2517, -3.5535,\n",
      "        -3.5535, -3.8720], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13985861837863922\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.5815, -3.7376, -3.3736, -2.7760], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.8446, -2.7760, -3.8528, -2.8446, -3.9820, -2.7760, -3.5203, -3.4849,\n",
      "        -3.5815, -2.8446], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5602, -3.4984, -3.7590, -3.5602, -4.0041, -3.4984, -3.5602, -3.4263,\n",
      "        -3.4984, -3.5602], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.26009243726730347\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-3.8838, -2.8583, -3.6093, -3.6093, -4.3963, -4.0550, -3.5121, -2.8583,\n",
      "        -3.8838, -2.8010], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7794, -3.5725, -3.5209, -3.5209, -4.3021, -4.3021, -3.4489, -3.5725,\n",
      "        -3.7794, -3.5209], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16497254371643066\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.6373, -3.8146, -3.4448, -2.8291], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.8146, -2.8732, -2.8291, -3.5779, -4.4315, -4.1861, -3.4679, -3.8146,\n",
      "        -4.0928, -2.8291], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.1746, -3.5859, -3.5462, -3.5859, -4.3274, -4.0572, -3.4740, -4.1746,\n",
      "        -3.9269, -3.5462], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.18506905436515808\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.6836, -3.8719, -3.4959, -2.8719], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.8878, -3.8719, -3.6836, -3.6058, -3.6836, -3.5036, -3.7580, -4.4692,\n",
      "        -2.7738, -3.5005], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5990, -4.1896, -3.5847, -3.5990, -3.5847, -3.8287, -3.5847, -4.3504,\n",
      "        -3.4964, -3.4964], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12983883917331696\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.7277, -3.9360, -3.5505, -2.9217], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.5741, -3.8633, -4.1548, -3.5741, -3.7642, -2.9217, -3.5505, -2.9016,\n",
      "        -4.5056, -3.7487], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5201, -3.9632, -3.9632, -3.5201, -3.6295, -3.6295, -3.6114, -3.6114,\n",
      "        -4.3738, -4.1841], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1286221295595169\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.7715, -3.9949, -3.6066, -2.9780], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.1756, -3.7639, -4.0737, -3.2528, -3.9674, -2.9780, -4.0082, -3.6440,\n",
      "        -3.5875, -2.9161], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9844, -3.6802, -4.1390, -3.5473, -3.8716, -3.6802, -3.9844, -3.6245,\n",
      "        -3.5473, -3.6245], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11412099748849869\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.8139, -4.0471, -3.6593, -3.0374], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.2887, -3.0374, -4.4248, -4.5671, -3.0374, -3.6540, -3.9755, -3.8139,\n",
      "        -4.1963, -4.4411], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5703, -3.7337, -4.1118, -4.4492, -3.7337, -3.6388, -4.2331, -3.7337,\n",
      "        -4.2382, -4.4366], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12355241924524307\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.8589, -4.0892, -3.7109, -3.0984], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.6602, -4.5815, -3.7109, -4.5815, -4.1933, -4.2519, -4.5469, -3.8775,\n",
      "        -3.9248, -4.5815], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6553, -4.4898, -3.6553, -4.4898, -4.0352, -4.2670, -4.4898, -4.2670,\n",
      "        -4.0352, -4.4898], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.022077830508351326\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.8993, -4.1170, -3.7580, -3.1570], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.5900, -3.1570, -3.2961, -4.3818, -4.5900, -4.3706, -4.3208, -2.8967,\n",
      "        -4.1851, -4.5900], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5406, -3.8413, -3.6751, -4.1925, -4.5406, -4.2331, -4.2331, -3.6070,\n",
      "        -4.0619, -4.5406], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12013968080282211\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.9365, -4.1316, -3.7977, -3.2187], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.9927, -4.2539, -4.3073, -2.9451, -3.9377, -3.9309, -3.6111, -4.3932,\n",
      "        -2.9927, -2.9228], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6934, -4.3160, -4.2729, -3.0211, -4.0822, -4.2402, -3.6305, -4.2729,\n",
      "        -3.6934, -3.6305], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16251854598522186\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.9732, -4.1498, -3.8353, -3.2786], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.9541, -3.6463, -3.2786, -4.4966, -3.6137, -4.0468, -3.0178, -3.2786,\n",
      "        -3.3959, -4.5739], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6587, -3.9507, -3.9507, -4.6421, -3.6587, -4.3421, -3.7160, -3.9507,\n",
      "        -3.7160, -4.6421], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2197762429714203\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.0095, -4.1731, -3.8733, -3.3367], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.6769, -4.0058, -4.1731, -4.4750, -3.3367, -3.4546, -3.9493, -3.3367,\n",
      "        -4.0377, -4.1467], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.0030, -4.1636, -4.3092, -4.6339, -4.0030, -3.7390, -4.3612, -4.0030,\n",
      "        -4.3225, -4.1551], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13948646187782288\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-4.3309, -4.2863, -4.4815, -3.0607, -4.4571, -4.1382, -4.3309, -3.5571,\n",
      "        -4.1649, -4.3127], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3925, -4.7469, -4.3925, -3.7546, -4.3574, -4.3387, -4.3925, -3.7179,\n",
      "        -4.1849, -4.3574], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07876352965831757\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.0970, -4.2545, -3.9574, -3.4317], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.4317, -4.3322, -3.0751, -3.7442, -4.0204, -3.0751, -3.0103, -4.6433,\n",
      "        -4.5090, -3.4317], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.0885, -4.3861, -3.7676, -3.7676, -4.4304, -3.7676, -3.1083, -4.7781,\n",
      "        -4.4304, -4.0885], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.20274074375629425\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.1497, -4.3113, -4.0015, -3.4744], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.6859, -4.6859, -4.0015, -3.4744, -4.3113, -4.6859, -3.6300, -4.3113,\n",
      "        -4.0015, -3.4744], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8041, -4.8041, -3.7818, -4.1269, -4.3802, -4.8041, -3.7584, -4.3802,\n",
      "        -3.7818, -4.1269], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1016053706407547\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-3.1145, -1.8574, -4.3708, -3.7285, -4.2542, -3.1145, -4.7081, -3.8043,\n",
      "        -3.5109, -3.1145], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.8030, -3.1436, -4.3972, -3.7753, -4.4511, -3.8030, -4.7944, -3.8030,\n",
      "        -4.1598, -3.8030], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.35469427704811096\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-4.6692, -3.5415, -3.1359, -3.7856, -3.6973, -4.5790, -3.5415, -3.5415,\n",
      "        -3.6921, -3.5415], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8498, -4.1873, -3.8223, -4.1873, -3.8223, -4.5009, -4.1873, -4.1873,\n",
      "        -3.7863, -4.1873], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.23642882704734802\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.2827, -4.5122, -4.0881, -3.5737], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.8146, -3.1611, -4.1937, -4.6092, -3.1119, -3.0862, -4.3956, -3.1611,\n",
      "        -3.5737, -3.8146], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.2163, -3.8450, -4.3524, -4.5256, -3.8007, -3.1139, -4.2907, -3.8450,\n",
      "        -4.2163, -4.2163], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2189549207687378\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.3116, -4.5616, -4.1068, -3.5943], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.6261, -3.8714, -3.5943, -3.8847, -3.5943, -3.5943, -2.3438, -4.3179,\n",
      "        -4.6236, -4.5616], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5563, -4.2349, -4.2349, -4.4843, -4.2349, -4.2349, -2.0214, -4.4962,\n",
      "        -4.4962, -4.4843], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.18853852152824402\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-4.2587, -3.2300, -3.2300, -4.3849, -3.9813, -4.3541, -4.6858, -3.8345,\n",
      "        -4.9923, -3.7694], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4223, -3.9070, -3.9070, -4.5922, -4.2521, -4.4223, -4.3191, -3.9070,\n",
      "        -4.9282, -3.8463], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12141820043325424\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0364, -4.2645, -3.8067, -3.2717], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.6251, -3.6251, -4.4061, -3.9731, -4.7479, -3.1900, -5.0314, -4.6197,\n",
      "        -4.3935, -4.6447], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.2626, -4.2626, -4.3839, -4.2626, -4.5827, -3.8710, -4.9541, -4.5758,\n",
      "        -4.5519, -4.9541], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1516885608434677\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0973, -4.3073, -3.8429, -3.3152], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.8183, -3.6367, -3.8429, -4.4442, -3.8502, -3.8429, -4.0170, -3.8429,\n",
      "        -4.6433, -3.6367], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6607, -4.2731, -3.8997, -4.4222, -3.8997, -3.8997, -3.9837, -3.8997,\n",
      "        -4.5791, -4.2731], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08525173366069794\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1576, -4.3480, -3.8821, -3.3621], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.6542, -4.5604, -3.0150, -3.3621, -4.0846, -3.3621, -4.1464, -4.7742,\n",
      "        -4.4546, -3.8784], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.2887, -4.6999, -3.0991, -4.0259, -4.2887, -4.0259, -4.0259, -5.0092,\n",
      "        -4.6111, -3.9236], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14483432471752167\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2322, -4.3944, -3.9263, -3.4132], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.5951, -4.5951, -3.6674, -4.7402, -2.1059, -3.4132, -3.6674, -3.6674,\n",
      "        -4.0226, -4.8945], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6203, -4.6203, -4.3007, -4.7415, -3.1024, -4.0719, -4.3007, -4.3007,\n",
      "        -4.0719, -4.7414], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2657165825366974\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3039, -4.4451, -3.9715, -3.4606], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.6843, -3.6843, -4.1568, -3.9715, -4.7125, -3.9715, -5.1376, -3.6843,\n",
      "        -4.9518, -5.1373], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3159, -4.3159, -4.1146, -3.9687, -4.5415, -3.9687, -5.0627, -4.3159,\n",
      "        -4.7574, -5.1506], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1271289438009262\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-4.6877, -4.9653, -4.6877, -4.7806, -3.5114, -3.7115, -3.5114, -4.7410,\n",
      "        -5.1530, -4.3697], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8280, -4.8191, -4.8280, -4.7001, -4.1602, -4.3404, -4.1602, -4.5822,\n",
      "        -5.0935, -4.3404], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13343651592731476\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-4.1244, -4.4145, -5.1645, -3.5648, -4.7095, -4.1607, -4.2990, -4.4433,\n",
      "        -4.5932, -3.7508], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7446, -4.5959, -5.1267, -4.2083, -4.8691, -4.2083, -4.3757, -4.5959,\n",
      "        -4.3757, -4.3757], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1327902376651764\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9630, -5.0617, -4.5913, -4.1717], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.6145, -4.6624, -5.1826, -1.3731, -4.8390, -4.0195, -4.0195, -5.1826,\n",
      "        -3.3854, -4.0041], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.2531, -4.6617, -5.1567, 10.0000, -4.8995, -4.0468, -4.0468, -5.1567,\n",
      "        -4.0468, -4.0468], grad_fn=<AddBackward0>)\n",
      "LOSS: 13.020187377929688\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9860, -5.0776, -4.6228, -4.1156], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.7089, -4.4236, -3.5638, -3.3226, -4.6333, -4.9404, -4.6105, -4.1156,\n",
      "        -4.7617, -3.5638], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8452, -4.6111, -4.2074, -3.9903, -4.6111, -4.8384, -4.3572, -4.2074,\n",
      "        -4.6111, -4.2074], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14343011379241943\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.4465, -4.5116, -4.0728, -3.5229], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.5229, -3.6844, -3.5229, -4.9388, -5.0226, -4.9388, -3.6844, -4.0541,\n",
      "        -4.6850, -5.0827], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.1706, -4.3160, -4.1706, -4.7877, -5.0202, -4.7877, -4.3160, -4.4846,\n",
      "        -4.7914, -5.0202], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1882990151643753\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-2.8392, -5.0341, -3.4970, -4.5679, -4.1818, -4.1947, -4.4082, -5.0341,\n",
      "        -3.4970, -3.6575], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.8529, -4.9725, -4.1473, -4.2917, -4.7505, -4.1473, -4.5428, -4.9725,\n",
      "        -4.1473, -4.2917], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16758808493614197\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1672, -4.3570, -3.9475, -3.2070], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.9869, -3.2070, -3.6405, -4.8638, -4.8573, -4.6830, -4.0023, -3.4838,\n",
      "        -3.2070, -3.2070], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9356, -3.8863, -4.2764, -4.7265, -4.7580, -4.5241, -4.5701, -4.1354,\n",
      "        -3.8863, -3.8863], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2592371106147766\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1514, -4.3310, -3.9477, -3.1971], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.4117, -2.8768, -4.8247, -4.6509, -3.7242, -4.6509, -4.6216, -4.0831,\n",
      "        -4.6216, -4.4148], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5185, -2.7725, -4.6748, -4.5185, -3.8774, -4.5185, -4.6748, -4.2724,\n",
      "        -4.6748, -4.2724], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.016502745449543\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-3.6373, -4.1235, -4.6155, -4.4147, -4.5129, -4.6155, -4.7752, -4.7425,\n",
      "        -4.2218, -3.6373], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.2735, -4.1352, -4.5177, -4.5177, -4.2735, -4.5177, -4.7056, -4.7056,\n",
      "        -4.1352, -4.2735], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09105166047811508\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-3.6451, -4.8245, -4.2644, -3.6451, -4.7278, -3.4910, -5.0823, -4.4946,\n",
      "        -4.3170, -3.6451], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.2805, -4.8853, -4.3171, -4.2805, -4.7054, -4.1419, -5.0447, -4.3171,\n",
      "        -4.6198, -4.2805], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17667844891548157\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0692, -4.2380, -3.9385, -3.1856], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.6867, -5.0598, -3.6653, -4.4700, -4.5410, -4.1015, -4.7112, -3.5065,\n",
      "        -3.1856, -4.6671], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7124, -5.0498, -4.2988, -4.3143, -4.5321, -3.8670, -4.7124, -4.1558,\n",
      "        -3.8670, -4.6436], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13678807020187378\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0559, -4.2207, -3.9342, -3.2027], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.7511, -3.5233, -4.6174, -4.0927, -3.2027, -4.6535, -4.5119, -4.8030,\n",
      "        -3.5233, -4.7511], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9050, -4.1709, -4.6479, -3.8825, -3.8825, -4.7240, -4.8448, -4.9050,\n",
      "        -4.1709, -4.9050], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15197421610355377\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0544, -4.2243, -3.9356, -3.2272], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.6513, -3.6974, -4.0544, -3.5386, -4.8407, -4.3484, -3.5386, -4.7435,\n",
      "        -4.6513, -4.6394], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7353, -3.9045, -4.1848, -4.1848, -5.0830, -4.6572, -4.1848, -4.8599,\n",
      "        -4.7353, -4.7353], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10857989639043808\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0725, -4.2315, -3.9439, -3.2458], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.7384, -4.0725, -4.6251, -5.0207, -3.9439, -5.0207, -4.9094, -3.5519,\n",
      "        -4.5399, -4.0604], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3646, -4.1967, -4.7373, -5.0939, -3.9212, -5.0939, -4.6648, -4.1967,\n",
      "        -4.6543, -4.3646], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10126221179962158\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0975, -4.2514, -3.9510, -3.2680], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.0707, -4.5543, -4.5291, -3.2680, -3.9510, -3.2680, -3.7560, -3.5640,\n",
      "        -3.2680, -3.8229], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3804, -4.6636, -4.4406, -3.9412, -3.9412, -3.9412, -4.3804, -4.2076,\n",
      "        -3.9412, -4.2076], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.24275419116020203\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1298, -4.2801, -3.9637, -3.3016], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.5881, -3.8360, -3.7444, -4.8013, -4.2404, -3.7760, -3.3016, -4.5738,\n",
      "        -4.5130, -4.6644], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8879, -4.2182, -3.9715, -4.9865, -4.2182, -4.3984, -3.9715, -4.8879,\n",
      "        -4.3984, -4.7498], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1277417242527008\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1608, -4.3126, -3.9772, -3.3279], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.4112, -4.8237, -3.5789, -4.3168, -3.3279, -5.0447, -4.8267, -4.3707,\n",
      "        -4.8267, -3.5789], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6418, -5.0133, -4.2210, -4.7107, -3.9951, -5.1433, -5.0133, -4.4742,\n",
      "        -5.0133, -4.2210], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16041865944862366\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-4.7064, -3.8134, -4.8819, -1.6361, -4.8819, -5.0937, -3.5836, -4.7064,\n",
      "        -4.6467, -4.0041], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7763, -4.4321, -5.0262, -1.7920, -5.0262, -4.7348, -4.2252, -4.7763,\n",
      "        -4.7205, -4.0159], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10045753419399261\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2329, -4.4219, -4.0299, -3.3720], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.8320, -3.8895, -3.8320, -4.2060, -4.2329, -4.9500, -4.0837, -3.8320,\n",
      "        -3.5890, -4.3973], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4488, -4.2301, -4.4488, -4.8170, -4.2301, -5.0365, -4.0348, -4.4488,\n",
      "        -4.2301, -4.4271], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2052375078201294\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2647, -4.4810, -4.0570, -3.3966], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.4159, -4.4289, -4.6358, -4.6968, -4.8428, -4.7190, -4.7813, -5.0206,\n",
      "        -5.0206, -1.8334], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4771, -4.4462, -4.5278, -4.7368, -4.8173, -4.6803, -4.9115, -5.0536,\n",
      "        -5.0536, -1.7882], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.004065416753292084\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-4.2222, -5.0893, -4.1043, -4.4289, -4.1632, -3.8884, -4.9166, -5.0893,\n",
      "        -4.5408, -3.8884], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5544, -5.0670, -4.0756, -4.4996, -4.4996, -4.4996, -4.8445, -5.0670,\n",
      "        -4.5544, -4.4996], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09827728569507599\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.4462, -4.5813, -4.1169, -3.6061], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.9071, -3.4429, -3.6061, -3.6061, -4.8728, -4.9826, -4.1169, -3.6061,\n",
      "        -5.1501, -4.6959], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5164, -4.0986, -4.2455, -4.2455, -4.8752, -4.8752, -4.0986, -4.2455,\n",
      "        -5.0846, -4.5164], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.20760512351989746\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-5.2306, -4.7173, -3.9350, -5.0388, -4.7324, -4.7173, -4.1332, -4.8933,\n",
      "        -4.8658, -4.9362], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1103, -4.5415, -4.5415, -4.9127, -4.9471, -4.5415, -4.1296, -4.9127,\n",
      "        -4.7292, -4.8047], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05424647405743599\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-4.6388, -3.6581, -3.6581, -4.4694, -1.6710, -3.5161, -3.6581, -4.9555,\n",
      "        -5.2327, -5.3607], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6383, -4.2923, -4.2923, -4.5697, -1.7936, -4.1645, -4.2923, -4.8467,\n",
      "        -5.1470, -5.3030], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16746163368225098\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.4871, -4.6597, -4.1647, -3.6990], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.6990, -4.5458, -4.4359, -4.9768, -3.5604, -3.6990, -5.2559, -5.2559,\n",
      "        -3.6990, -4.9624], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3291, -4.5574, -4.8950, -4.9566, -4.2043, -4.3291, -5.1876, -5.1876,\n",
      "        -4.3291, -4.8950], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1830945909023285\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5108, -4.6802, -4.1891, -3.7498], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.6569, -4.4170, -4.1338, -4.1338, -4.3737, -5.2717, -3.6113, -4.8970,\n",
      "        -3.7498, -4.0266], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.6938, -4.3748, -4.3748, -4.3748, -4.6239, -5.2328, -4.2502, -5.0378,\n",
      "        -4.3748, -4.6239], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13588765263557434\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5265, -4.6933, -4.2075, -3.7944], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.7944, -4.7726, -5.1378, -5.1444, -4.0576, -1.6767, -3.6657, -4.9462,\n",
      "        -4.6023, -5.0555], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4150, -4.7814, -5.1092, -5.2832, -4.6518, -1.8106, -4.2991, -5.1092,\n",
      "        -4.6351, -5.0833], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12059788405895233\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-4.7655, -4.2645, -3.8380, -4.8963, -4.9512, -4.9512, -3.8380, -3.8380,\n",
      "        -4.9141, -3.8380], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6716, -4.4542, -4.4542, -4.8760, -4.9277, -4.9277, -4.4542, -4.4542,\n",
      "        -5.0410, -4.4542], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15812498331069946\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.7627, -4.9686, -4.4505, -4.1240], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.1240, -4.7627, -3.8864, -4.4180, -4.3435, -5.3205, -4.1240, -4.5184,\n",
      "        -4.4505, -4.7285], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7116, -4.7116, -4.4978, -2.7489, -4.7131, -5.3713, -4.7116, -4.4978,\n",
      "        -4.4978, -4.9050], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4025706648826599\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.7798, -4.9778, -4.4655, -4.1462], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.3889, -4.1462, -1.6737, -4.8611, -5.2010, -4.5420, -3.7980, -3.7980,\n",
      "        -4.7798, -4.7798], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.8002, -4.7316, -1.8294, -4.9475, -5.2430, -4.5228, -4.4182, -4.4182,\n",
      "        -4.7316, -4.7316], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.36743563413619995\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.7890, -4.9839, -4.4777, -4.1550], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.8105, -4.1550, -5.0432, -3.9208, -4.7297, -4.1550, -4.2974, -4.5566,\n",
      "        -4.1359, -4.8785], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4294, -4.7395, -5.1232, -4.5287, -4.9475, -4.7395, -4.4294, -4.5287,\n",
      "        -4.4294, -4.9716], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16028359532356262\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.8099, -4.9996, -4.5054, -4.1736], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.1736, -4.1736, -4.1736, -4.1736, -3.9372, -3.9372, -4.7749, -4.8400,\n",
      "        -4.5939, -4.5646], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7563, -4.7563, -4.7563, -4.7563, -4.5435, -4.5435, -4.9983, -4.9983,\n",
      "        -4.7563, -4.5435], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2194812297821045\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.8428, -5.0284, -4.5400, -4.1996], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.3718, -3.9697, -1.7232, -4.5001, -4.1996, -3.8197, -4.9851, -5.3880,\n",
      "        -4.4635, -4.9477], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4377, -4.5728, -1.8452, -4.7515, -4.7796, -4.4377, -4.7515, -5.4303,\n",
      "        -4.5728, -5.0171], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12376173585653305\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.8782, -5.0612, -4.5814, -4.2277], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.7417, -5.2163, -4.2277, -5.3078, -5.4856, -4.6007, -4.0025, -4.0025,\n",
      "        -4.2277, -4.0025], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7118, -5.2412, -4.8050, -5.3305, -5.4477, -4.6023, -4.6023, -4.6023,\n",
      "        -4.8050, -4.6023], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17489294707775116\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.9193, -5.0985, -4.6271, -4.2653], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.4505, -5.4505, -4.7229, -5.4520, -3.8435, -5.3393, -4.2653, -4.7229,\n",
      "        -5.3745, -4.9193], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4721, -5.4721, -4.6831, -5.7401, -4.4592, -5.3570, -4.8387, -4.6831,\n",
      "        -5.2610, -4.8387], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08146838843822479\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.9648, -5.1361, -4.6729, -4.3034], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.5279, -4.0867, -4.0867, -3.8606, -5.4901, -5.1079, -4.0867, -5.0916,\n",
      "        -4.3034, -5.4901], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4746, -4.6781, -4.6781, -4.4746, -5.5029, -4.6587, -4.6781, -5.0875,\n",
      "        -4.8731, -5.5029], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.19553899765014648\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-3.8904, -5.5384, -5.0088, -5.0997, -4.5632, -3.8904, -4.0658, -3.8904,\n",
      "        -4.3496, -4.9948], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5014, -5.5408, -4.9147, -5.4027, -4.5014, -4.5014, -3.2247, -4.5014,\n",
      "        -4.9147, -5.1188], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.22664251923561096\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.0465, -5.2195, -4.7231, -4.3877], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.3877, -4.3877, -4.3877, -3.2543, -4.6963, -4.3877, -5.0763, -5.5728,\n",
      "        -5.5728, -5.4060], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9489, -4.9489, -4.9489, -3.2889, -4.6322, -4.9489, -5.3536, -5.5686,\n",
      "        -5.5686, -5.3060], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13521644473075867\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.0896, -5.2711, -4.7561, -4.4398], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.7357, -3.9536, -3.9536, -4.2290, -4.7357, -4.2290, -4.4398, -4.5277,\n",
      "        -4.2290, -3.3095], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8061, -4.5582, -4.5582, -4.8061, -4.8061, -4.8061, -4.9958, -4.9107,\n",
      "        -4.8061, -3.3519], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.21978798508644104\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.1413, -5.3256, -4.7945, -4.4974], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.0049, -5.5147, -5.3387, -4.0016, -4.6644, -5.1413, -4.6886, -4.4974,\n",
      "        -4.4974, -4.5320], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([10.0000, -5.3988, -5.3988, -4.6015, -4.8542, -5.0477, -4.6098, -5.0477,\n",
      "        -5.0477, -4.6015], grad_fn=<AddBackward0>)\n",
      "LOSS: 12.214588165283203\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.1416, -5.3251, -4.7931, -4.4856], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.9443, -4.6528, -5.1421, -4.6528, -4.6360, -4.4856, -5.4881, -4.3236,\n",
      "        -4.6360, -5.4146], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([10.0000, -4.5862, -5.1655, -4.5862, -4.5426, -5.0370, -5.6309, -4.5862,\n",
      "        -4.5426, -5.3925], grad_fn=<AddBackward0>)\n",
      "LOSS: 12.01988697052002\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.1163, -5.2845, -4.7611, -4.4214], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.7602, -4.9903, -5.4185, -5.6024, -4.7496, -4.7442, -5.4296, -5.6024,\n",
      "        -4.1670, -5.6024], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2698, -4.8093, -5.2698, -5.5473, -5.0846, -4.9792, -5.2842, -5.5473,\n",
      "        -4.7503, -5.5473], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08524896204471588\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.0732, -5.2284, -4.7222, -4.3648], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.1025, -3.3300, -4.1025, -4.9493, -4.3648, -5.2429, -5.5315, -4.1679,\n",
      "        -4.7089, -4.4640], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6922, -3.3265, -4.6922, -4.3583, -4.9283, -5.2155, -5.4836, -4.4716,\n",
      "        -4.6922, -4.3583], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14692017436027527\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.0343, -5.1736, -4.6653, -4.3286], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.0617, -3.8066, -4.9317, -4.0617, -4.9380, -3.8066, -4.6653, -3.4419,\n",
      "        -1.5507, -4.0617], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6555, -4.4259, -5.1739, -4.6555, -4.9581, -4.4259, -4.6555, -3.3012,\n",
      "        -1.6302, -4.6555], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.19103392958641052\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.0018, -5.1284, -4.6185, -4.3056], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.5978, -4.6206, -4.3704, -5.0018, -4.8610, -4.5717, -4.6206, -4.3056,\n",
      "        -5.0830, -5.2112], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9168, -4.6367, -4.6999, -4.8750, -4.8750, -4.8750, -4.6367, -4.8750,\n",
      "        -5.1086, -5.4154], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06857429444789886\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.9684, -5.0811, -4.5695, -4.2762], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.5949, -4.3042, -5.1554, -4.2762, -3.7492, -4.2862, -5.0213, -4.0255,\n",
      "        -4.0255, -5.1554], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6229, -4.3743, -5.1206, -4.8486, -4.3743, -4.2420, -5.0722, -4.6229,\n",
      "        -4.6229, -5.1206], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14448878169059753\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.9452, -5.0425, -4.5326, -4.2583], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.4027, -4.2583, -4.2583, -5.0425, -5.3133, -4.2583, -5.3133, -4.8659,\n",
      "        -3.7356, -3.7356], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3621, -4.8324, -4.8324, -5.0908, -5.3749, -4.8324, -5.3749, -4.8380,\n",
      "        -4.3621, -4.3621], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17862346768379211\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.9384, -5.0250, -4.5108, -4.2581], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.4075, -4.2362, -4.9308, -4.2581, -4.9543, -4.2581, -5.0250, -4.9384,\n",
      "        -4.8181, -4.6075], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4986, -4.6194, -5.0281, -4.8323, -5.0281, -4.8323, -5.0879, -4.8323,\n",
      "        -4.8126, -4.2063], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10055781900882721\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-5.2696, -4.5680, -4.0148, -4.0148, -4.0148, -4.5680, -4.5410, -4.7273,\n",
      "        -4.8915, -4.5680], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3639, -4.6133, -4.6133, -4.6133, -4.6133, -4.6133, -4.8431, -4.6415,\n",
      "        -5.0033, -4.6133], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12008919566869736\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-5.1825, -5.0683, -4.7590, -4.5569], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.5789, -4.0142, -4.0470, -4.8657, -4.1991, -5.2656, -4.2718, -5.2656,\n",
      "        -4.0142, -4.9300], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8031, -4.6127, -4.3731, -4.9835, -4.2086, -5.3646, -4.8446, -5.3646,\n",
      "        -4.6127, -4.8446], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12421778589487076\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.9289, -5.0327, -4.4402, -4.2798], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.0214, -4.2087, -4.2798, -4.3016, -4.2087, -4.5972, -4.5754, -4.8579,\n",
      "        -4.5681, -4.2798], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6192, -4.2138, -4.8518, -4.3735, -4.2138, -4.7971, -4.8518, -4.9683,\n",
      "        -4.6192, -4.8518], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1148233413696289\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.9284, -5.0474, -4.4252, -4.2847], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.0347, -4.0347, -4.0347, -4.6095, -3.5803, -4.2248, -4.2847, -5.0474,\n",
      "        -4.2847, -4.2847], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6312, -4.6312, -4.6312, -4.8562, -3.2801, -4.2223, -4.8562, -5.1485,\n",
      "        -4.8562, -4.8562], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.22087781131267548\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-3.5868, -4.3211, -4.5762, -4.8415, -3.1698, -4.2990, -4.0608, -4.1083,\n",
      "        -3.7621, -4.2990], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2998, -4.9559, -4.6547, -4.9559, -3.2998, -4.8691, -4.6547, -4.3859,\n",
      "        -4.3859, -4.8691], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.199030801653862\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.9749, -5.1041, -4.4314, -4.3125], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.5982, -4.2679, -4.0792, -4.8775, -5.3672, -4.1825, -4.3164, -1.2768,\n",
      "        -4.3125, -4.0792], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3033, -4.2372, -4.6713, -4.8812, -5.3699, -4.3859, -4.3859, -1.3756,\n",
      "        -4.8812, -4.6713], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11684270203113556\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.0044, -5.1379, -4.4553, -4.3330], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.1379, -4.3475, -4.2955, -4.4500, -1.2740, -4.6194, -4.1078, -4.9158,\n",
      "        -3.7628, -5.0482], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2555, -4.3865, -4.2497, -4.7533, -1.3660, -4.6970, -4.6970, -4.8997,\n",
      "        -4.3865, -5.2442], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08988148719072342\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.0444, -5.1824, -4.5070, -4.3518], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.7515, -4.3518, -4.3518, -4.2338, -5.4419, -5.4419, -4.2338, -3.9475,\n",
      "        -4.9583, -4.1287], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9166, -4.9166, -4.9166, -4.3863, -5.4260, -5.4260, -4.3863, -4.2571,\n",
      "        -4.9166, -4.7158], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11546182632446289\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.0849, -5.2272, -4.5772, -4.3690], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.5679, -3.7600, -5.4823, -4.2162, -4.3690, -4.5772, -5.1840, -3.7600,\n",
      "        -4.0062, -4.1485], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3795, -4.3840, -5.4327, -4.7337, -4.9321, -4.7337, -5.2682, -4.3840,\n",
      "        -4.2619, -4.7337], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.18409235775470734\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.1323, -5.2804, -4.6704, -4.3979], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.8884, -4.8884, -5.5230, -4.1495, -4.1495, -5.2804, -4.3979, -4.1495,\n",
      "        -5.0245, -5.0861], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8430, -4.8430, -5.4432, -4.7345, -4.7345, -5.3288, -4.9582, -4.7345,\n",
      "        -5.3822, -4.8430], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15406212210655212\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.1819, -5.3306, -4.7509, -4.4416], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.8489, -4.1687, -4.2746, -4.4416, -4.7175, -4.7509, -4.4416, -4.2322,\n",
      "        -3.7811, -4.4416], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9974, -4.7518, -5.2153, -4.9974, -4.8089, -4.7518, -4.9974, -4.4030,\n",
      "        -4.4030, -4.9974], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2598304748535156\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.2354, -5.3838, -4.8349, -4.4930], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.4853, -4.7356, -4.9452, -4.4930, -4.1957, -4.4930, -4.7561, -4.8349,\n",
      "        -5.5798, -5.5798], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4170, -4.8387, -4.8867, -5.0437, -4.7761, -5.0437, -4.7761, -4.7761,\n",
      "        -5.4850, -5.4850], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09839823096990585\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.2886, -5.4268, -4.9135, -4.5514], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.7296, -4.5514, -4.9560, -5.4268, -4.5514, -4.5514, -5.5976, -4.7296,\n",
      "        -4.2319, -5.1807], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4371, -5.0963, -5.0963, -5.4604, -5.0963, -5.0963, -5.5166, -4.4371,\n",
      "        -4.8087, -4.9423], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1478656679391861\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.3375, -5.4561, -4.9633, -4.6246], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.5996, -3.8758, -4.4696, -2.7871, -4.5821, -4.4696, -5.3418, -5.5996,\n",
      "        -4.6246, -5.5996], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5621, -4.4882, -4.3469, -1.3309, -4.4882, -4.3469, -4.9847, -5.5621,\n",
      "        -5.1621, -5.5621], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2955109179019928\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.3663, -5.4452, -4.9903, -4.6794], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.8467, -3.9187, -5.5580, -5.6309, -5.3663, -4.2698, -3.9187, -4.2698,\n",
      "        -4.2710, -4.2698], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8428, -4.5268, -5.5907, -5.5907, -5.2115, -4.8428, -4.5268, -4.8428,\n",
      "        -4.3642, -4.8428], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17600591480731964\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-4.2956, -5.5251, -4.7389, -4.6337, -4.4114, -5.0836, -5.3892, -4.7290,\n",
      "        -5.1065, -2.7498], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8661, -5.6270, -5.2650, -4.5735, -4.3871, -5.1019, -5.2650, -4.5735,\n",
      "        -5.2650, -1.3335], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2687694728374481\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-5.7566, -5.5537, -5.4253, -5.1853], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.7653, -4.3023, -5.1853, -5.0479, -4.7653, -4.7150, -4.7653, -4.0056,\n",
      "        -4.6479, -4.3023], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2888, -4.8721, -5.2888, -5.1273, -5.2888, -4.6050, -5.2888, -4.6050,\n",
      "        -4.6050, -4.8721], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.18616357445716858\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-5.2900, -4.6199, -4.6199, -4.9106, -4.8042, -4.9106, -4.3231, -5.4102,\n",
      "        -4.3231, -5.4621], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1397, -4.8908, -4.8908, -4.8908, -5.3238, -4.8908, -4.8908, -5.7118,\n",
      "        -4.8908, -5.6582], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1214052066206932\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0735, -5.0154, -4.6835, -4.3306], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.4594, -4.3306, -5.4304, -5.4409, -4.6835, -4.8574, -4.6754, -5.0436,\n",
      "        -4.8574, -4.8382], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6733, -4.8975, -5.3716, -5.7363, -4.6885, -5.3716, -4.8975, -4.8975,\n",
      "        -5.3716, -5.0333], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10956175625324249\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0642, -5.0283, -4.6718, -4.3348], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.0642, -4.9911, -4.5364, -3.8048, -5.3627, -4.3348, -5.2136, -5.4160,\n",
      "        -5.5732, -4.9152], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4237, -5.2049, -5.0482, -3.2838, -5.7412, -4.9013, -5.2594, -5.2594,\n",
      "        -5.5949, -5.4237], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1458117812871933\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-4.5455, -5.0649, -4.9419, -4.3421, -4.1610, -4.6736, -4.3421, -5.5195,\n",
      "        -4.3421, -4.9419], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0351, -5.3130, -5.4477, -4.9079, -4.7449, -4.7449, -4.9079, -5.6800,\n",
      "        -4.9079, -5.4477], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2145083248615265\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.1618, -5.1345, -4.7006, -4.3706], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.0635, -4.9765, -5.0635, -5.3058, -5.1345, -5.6384, -4.9765, -4.3706,\n",
      "        -5.1269, -4.7952], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9335, -5.4788, -4.9335, -5.6802, -5.3587, -5.5997, -5.4788, -4.9335,\n",
      "        -4.9335, -4.4022], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12391921132802963\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.2410, -5.2248, -4.7223, -4.4266], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.6205, -5.0110, -5.2410, -5.1069, -5.0110, -5.7200, -5.4697, -5.0110,\n",
      "        -4.4266, -5.2209], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6722, -5.5099, -5.5099, -4.9839, -5.5099, -5.6913, -5.5272, -5.5099,\n",
      "        -4.9839, -5.2730], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11542898416519165\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3384, -5.3281, -4.7614, -4.4947], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.1512, -4.4947, -5.7921, -1.3475, -5.2718, -4.4947, -5.0391, -4.7205,\n",
      "        -5.0391, -4.9277], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0452, -5.0452, -5.6894, -1.3608, -4.9794, -5.0452, -5.5352, -4.8084,\n",
      "        -5.5352, -5.0452], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12273355573415756\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-5.1735, -5.8088, -4.5729, -5.9525, -5.1396, -5.7730, -5.4733, -4.5729,\n",
      "        -1.3521, -5.2943], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1156, -5.6786, -5.1156, -5.9800, -5.1156, -5.5722, -5.6786, -5.1156,\n",
      "        -1.3674, -5.3196], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06940373033285141\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.5159, -5.4961, -4.8419, -4.6569], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.5890, -4.6569, -4.5890, -5.3452, -4.6569, -5.1234, -5.8912, -5.8190,\n",
      "        -4.6569, -4.8090], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4734, -5.1912, -4.4734, -5.3281, -5.1912, -5.6110, -5.7284, -5.6110,\n",
      "        -5.1912, -4.4734], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13036377727985382\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.5899, -5.5597, -4.8648, -4.7583], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.2919, -4.9052, -4.8648, -5.1760, -5.1189, -4.7583, -4.7583, -4.7583,\n",
      "        -5.8471, -5.7865], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8628, -5.0322, -4.8628, -5.6070, -5.2825, -5.2825, -5.2825, -5.2825,\n",
      "        -5.6365, -5.7409], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14251771569252014\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.6619, -5.6115, -4.8864, -4.8662], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.2546, -5.2546, -5.2546, -5.2062, -5.1090, -5.2546, -4.8662, -5.6115,\n",
      "        -5.7358, -4.8662], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5981, -5.5981, -5.5981, -5.3796, -5.3796, -5.5981, -5.3796, -5.6581,\n",
      "        -5.6157, -5.3796], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11187447607517242\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.7275, -5.6484, -4.8601, -4.9813], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.2319, -5.0658, -5.3842, -4.5796, -4.8601, -4.6360, -5.3842, -5.3842,\n",
      "        -4.7212, -5.3842], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3741, -5.2491, -5.5728, -4.9645, -4.9645, -4.6408, -5.5728, -5.5728,\n",
      "        -4.6408, -5.5728], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03615473955869675\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.2665, -5.3131, -4.5249, -4.4751], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.2665, -4.4751, -5.7449, -5.7954, -5.2665, -4.6434, -5.4598, -5.7449,\n",
      "        -5.5248, -4.9969], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3367, -5.0276, -5.4972, -5.5390, -5.3367, -5.0276, -5.4593, -5.4972,\n",
      "        -5.5390, -5.1848], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06867276877164841\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-4.5318, -5.1907, -4.5264, -6.0205, -5.7348, -4.4923, -5.6652, -5.6652,\n",
      "        -5.6195, -5.6195], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0431, -5.3147, -5.0737, -5.7348, -5.4692, -5.0431, -5.5388, -5.5388,\n",
      "        -5.5388, -5.5388], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10768647491931915\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.4722, -5.2739, -4.5296, -4.8026], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.2749, -4.5296, -5.2749, -4.1013, -4.6374, -6.0544, -4.8026, -4.5922,\n",
      "        -5.2749, -5.5814], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3154, -5.0767, -5.3154, -4.6911, -4.6911, -5.5419, -5.0367, -5.0367,\n",
      "        -5.3154, -5.3154], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12407892942428589\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.5216, -5.2594, -4.5479, -4.8957], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.9611, -6.0227, -1.3504, -5.8875, -5.9279, -5.7128, -4.8078, -5.3705,\n",
      "        -4.6587, -5.8875], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1211, -5.5477, -1.4569, -5.5704, -5.7397, -5.4650, -5.0300, -5.3270,\n",
      "        -5.0300, -5.5704], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07495570927858353\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.5477, -5.2270, -4.6167, -4.9700], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.5486, -4.8807, -5.2118, -5.6226, -5.9276, -5.9276, -5.6226, -4.7102,\n",
      "        -5.6226, -5.4506], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5880, -5.0624, -5.1531, -5.5306, -5.6107, -5.6107, -5.5306, -5.0624,\n",
      "        -5.5306, -5.3926], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03917495533823967\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-5.6803, -5.8773, -4.9819, -5.9407, -5.9407, -5.1316, -4.7504, -6.3916,\n",
      "        -5.9407, -4.5712], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2368, -5.8886, -5.1141, -5.6716, -5.6716, -5.2084, -5.1141, -5.8921,\n",
      "        -5.6716, -5.1141], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11139073222875595\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-5.9101, -4.7681, -5.1585, -5.9101, -5.5401, -5.9101, -5.9101, -5.9101,\n",
      "        -5.5125, -5.3008], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.7768, -5.2084, -5.8241, -5.7768, -5.7813, -5.7768, -5.7768, -5.7768,\n",
      "        -5.7585, -5.6233], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09482957422733307\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.6258, -5.1789, -5.0017, -5.1351], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.9426, -5.8641, -5.7900, -5.4058, -5.9426, -6.0574, -5.5578, -5.4922,\n",
      "        -4.8013, -5.8465], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.8482, -5.8482, -6.0892, -5.5016, -5.8482, -5.9668, -5.8652, -5.8687,\n",
      "        -5.2909, -5.6831], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06275570392608643\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.6585, -5.2094, -5.1034, -5.1940], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.8359, -6.0555, -5.8359, -5.8359, -5.4848, -5.8359, -5.5450, -5.4359,\n",
      "        -4.8470, -5.7804], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.8923, -5.8923, -5.8923, -5.8923, -5.9360, -5.8923, -6.2024, -5.8084,\n",
      "        -5.3480, -5.7461], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10661053657531738\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-5.5329, -4.8870, -5.6462, -4.9939, -5.8459, -4.9103, -5.3117, -5.5329,\n",
      "        -4.9103, -5.4310], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.9371, -5.0475, -5.8109, -5.3983, -5.8879, -5.3496, -5.8109, -5.9371,\n",
      "        -5.3496, -5.8109], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13244052231311798\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3637, -5.2009, -4.7755, -5.0149], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.0149, -5.9000, -5.9000, -5.6338, -5.2009, -5.6762, -5.6762, -4.4056,\n",
      "        -5.0682, -5.3637], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2980, -5.8113, -5.8113, -5.8485, -5.5611, -5.7068, -5.7068, -4.9650,\n",
      "        -5.3067, -5.7068], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07612435519695282\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.4543, -5.3565, -4.7075, -5.1436], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.3891, -5.6998, -4.6539, -5.4543, -5.6998, -5.9493, -4.2901, -2.8110,\n",
      "        -5.6918, -5.1436], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.7270, -5.5681, -4.8610, -5.5681, -5.5681, -5.7000, -4.8610, -1.5463,\n",
      "        -5.7270, -5.2367], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2202220857143402\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-5.5402, -6.0120, -5.1166, -5.9570, -5.6853, -5.7603, -5.2676, -5.2581,\n",
      "        -5.3581, -5.7886], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4490, -5.4551, -5.4490, -5.6049, -5.4490, -5.4490, -5.6183, -5.0956,\n",
      "        -5.4551, -5.5523], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09202947467565536\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-5.3864, -6.1037, -5.1074, -4.6786, -5.6334, -5.6334, -6.2063, -2.9470,\n",
      "        -5.3297, -5.6334], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4181, -5.5966, -5.4113, -5.2107, -5.4113, -5.4113, -5.5966, -3.4413,\n",
      "        -5.5235, -5.4113], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14351773262023926\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.7520, -5.5173, -4.9442, -5.4438], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.8421, -5.6877, -5.3094, -5.5322, -6.1435, -6.1037, -5.3948, -5.3920,\n",
      "        -5.8944, -5.8944], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7867, -5.5638, -5.2947, -5.4369, -5.8573, -5.6824, -5.5638, -5.2947,\n",
      "        -5.6824, -5.6824], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04150523617863655\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.6761, -5.5548, -5.0257, -5.3385], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.8387, -6.0472, -5.6283, -5.4147, -6.0428, -5.1593, -6.2746, -1.5419,\n",
      "        -5.8195, -5.4147], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.9649, -5.6728, -5.5041, -5.5041, -5.8001, -5.4937, -5.9649, -3.2993,\n",
      "        -5.8001, -5.5041], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.35429346561431885\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.5986, -5.5638, -5.1172, -5.2115], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.2604, -5.9332, -5.9076, -5.7327, -6.3657, -5.7327, -5.6206, -5.4889,\n",
      "        -5.7327, -5.2905], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5463, -5.7432, -5.9330, -5.9330, -6.0617, -5.9330, -5.5866, -5.3125,\n",
      "        -5.9330, -5.5866], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0451195165514946\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-5.2020, -6.3803, -4.9114, -5.6754, -5.6754, -5.1344, -4.9114, -5.1617,\n",
      "        -5.5449, -5.2020], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6383, -6.1401, -4.9208, -6.0261, -6.0261, -5.6455, -4.9208, -5.6209,\n",
      "        -5.3817, -5.6383], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11833502352237701\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-5.6606, -2.2410, -5.6606, -5.6606, -5.3889, -6.0830, -5.8738, -5.6606,\n",
      "        -5.4747, -5.3272], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.0682, -1.5908, -6.0682, -6.0682, -5.6131, -5.8352, -5.7945, -6.0682,\n",
      "        -5.4138, -5.6496], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13131287693977356\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.2738, -5.4432, -4.8950, -5.0697], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.5018, -5.2901, -5.6147, -5.8049, -5.6942, -5.6942, -2.8650, -5.4432,\n",
      "        -5.1300, -5.1456], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.0739, -5.6847, -6.1038, -6.0739, -6.0739, -6.0739, -2.9503, -5.4055,\n",
      "        -5.6847, -5.6170], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16216932237148285\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-5.0485, -5.2066, -5.2066, -5.7493, -5.5765, -5.6926, -4.8031, -6.0092,\n",
      "        -6.0388, -4.8322], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5437, -5.5390, -5.5390, -5.9756, -5.5390, -5.7486, -4.9291, -5.7453,\n",
      "        -5.7486, -4.9291], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07009652256965637\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3017, -5.3737, -4.7165, -5.1727], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.5600, -5.8627, -4.7879, -5.5526, -5.0079, -5.8218, -5.5600, -5.2879,\n",
      "        -5.8218, -5.2879], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6625, -6.0346, -4.8502, -5.8798, -5.5071, -5.8798, -5.6811, -5.4635,\n",
      "        -5.8798, -5.4635], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04832485690712929\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7642, -4.7516, -4.1705, -4.4772], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.8318, -5.9671, -5.2864, -6.1434, -5.3987, -5.9317, -5.9671, -2.0660,\n",
      "        -4.7516, -4.7642], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5678, -5.7516, -5.1442, -5.9152, -5.3745, -5.9152, -5.7516, -1.6785,\n",
      "        -4.7534, -5.1442], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.053022246807813644\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7745, -4.7134, -4.1000, -4.5370], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.5144, -5.4921, -5.6186, -5.7857, -5.6364, -5.6186, -5.7857, -5.9561,\n",
      "        -4.9921, -5.8791], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6900, -5.3117, -5.3117, -5.4928, -5.6513, -5.3117, -5.4928, -5.6513,\n",
      "        -5.0630, -5.4724], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06868292391300201\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7524, -4.6396, -4.0836, -4.5734], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.5733, -2.8328, -5.5733, -6.0068, -5.9840, -4.4949, -1.4837, -4.4949,\n",
      "        -4.0836, -5.6980], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3185, -4.6752, -5.3185, -5.8048, -5.6167, -4.6752, -1.7203, -4.6752,\n",
      "        -4.6752, -5.4816], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.42178091406822205\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6790, -4.5358, -4.0213, -4.5378], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.6093, -5.3202, -4.7931, -5.6305, -5.9768, -5.6093, -5.9100, -5.8459,\n",
      "        -5.8459, -5.3147], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3138, -5.4543, -5.2282, -5.5793, -5.5793, -5.3138, -5.7832, -5.5793,\n",
      "        -5.5793, -5.4120], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07102279365062714\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5892, -4.4396, -4.0099, -4.4852], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.5588, -5.5317, -5.6171, -4.7991, -5.5317, -5.5274, -5.9286, -5.9286,\n",
      "        -5.4123, -5.6171], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5067, -5.2279, -5.3608, -5.3192, -5.2279, -5.8221, -5.5873, -5.5873,\n",
      "        -5.3608, -5.3608], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09115475416183472\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5158, -4.3531, -4.0369, -4.4044], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.8325, -4.9935, -5.7354, -5.5766, -5.2020, -5.3317, -5.8325, -5.5766,\n",
      "        -4.4044, -5.4962], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6325, -5.3880, -5.5662, -5.4429, -5.5845, -5.3880, -5.6325, -5.4429,\n",
      "        -2.7537, -5.5845], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3181977868080139\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.4150, -4.2392, -4.0260, -4.2480], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.5120, -5.0169, -5.4257, -5.7019, -5.4928, -5.6076, -5.4432, -5.7019,\n",
      "        -5.7019, -5.2598], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5678, -5.2941, -5.6432, -5.6722, -5.5152, -5.9608, -5.5152, -5.6722,\n",
      "        -5.6722, -5.5152], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03255458548665047\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3352, -4.1572, -3.9972, -4.1113], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.9972, -4.9810, -5.5993, -5.7561, -5.9022, -5.2121, -5.5879, -5.4206,\n",
      "        -5.5993, -5.8035], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5974, -5.2898, -5.6968, -6.0401, -5.6968, -5.5610, -5.6161, -5.5610,\n",
      "        -5.6968, -5.8114], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07398547232151031\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2778, -4.0950, -3.9492, -4.0003], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.0950, -5.3615, -5.1098, -5.3615, -5.3615, -5.5278, -5.1433, -5.5278,\n",
      "        -5.5278, -5.5278], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5543, -5.5706, -5.2960, -5.5706, -5.5706, -5.6984, -5.2960, -5.6984,\n",
      "        -5.6984, -5.6984], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05166052654385567\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2278, -4.0499, -3.8648, -3.9286], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.3634, -5.5075, -5.3488, -5.5075, -4.2278, -5.3291, -5.0029, -5.3634,\n",
      "        -5.5075, -4.0499], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6328, -5.6546, -5.5332, -5.6546, -5.2899, -5.6328, -5.2899, -5.6328,\n",
      "        -5.6546, -4.4783], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17300395667552948\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-2.7597, -5.0921, -5.7334, -5.3551, -5.3947, -5.0323, -5.5084, -5.0921,\n",
      "        -5.3947, -5.7110], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.7979, -5.1335, -5.8906, -5.4395, -5.5189, -5.1809, -5.5524, -5.1335,\n",
      "        -5.5189, -5.7074], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.009156020358204842\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.7995, -2.9740, -2.5810, -2.0242], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.6585, -5.0737, -5.6303, -5.0737, -4.0972, -5.5175, -5.3694, -5.6642,\n",
      "        -5.5175, -4.2550], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.2927, -5.0669, -5.4431, -5.0669, -4.2927, -5.4431, -5.3382, -5.7702,\n",
      "        -5.4431, -5.0669], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11579886823892593\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.6130, -1.7565, -1.5125, -0.8318], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.8032, -4.8176, -5.6781, -1.7565, -5.5261, -5.6136, -5.8032, -5.6917,\n",
      "        -5.7506, -4.3465], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6474, -4.9363, -5.3346, -2.8433, -5.3346, -5.3750, -5.6474, -5.5374,\n",
      "        -5.5374, -4.9118], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.18442612886428833\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.6943, -5.7318, -4.7352, -5.4920], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.4920, -4.7352, -5.5607, -5.3526, -5.3526, -5.4345, -4.5005, -5.4920,\n",
      "        -4.2025, -4.8661], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2617, -5.1676, -5.2330, -5.1676, -5.1676, -4.8608, -5.0504, -5.2617,\n",
      "        -4.1765, -4.8493], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11013554036617279\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.7257, -5.6457, -4.6040, -5.2936], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.5407, -5.5407, -4.4674, -5.2936, -5.7540, -5.2936, -5.4426, -5.4426,\n",
      "        -5.4426, -5.4426], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1795, -5.1795, -5.0206, -5.1436, -5.4543, -5.1436, -5.2462, -5.2462,\n",
      "        -5.2462, -5.2462], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0856173187494278\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-4.6301, -5.2359, -5.0961, -5.5069, -4.9690, -4.2810, -5.7167, -5.7167,\n",
      "        -5.3512, -4.6301], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8529, -5.3431, -5.3431, -5.2015, -5.2015, -4.8529, -5.4994, -5.4994,\n",
      "        -5.2676, -4.8529], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07475898414850235\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3213, -5.2234, -4.3222, -4.8703], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.6108, -3.6108, -1.5733, -4.4179, -1.5733, -4.5316, -5.4448, -5.9497,\n",
      "        -5.1504, -5.1504], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.2497, -4.2497, -1.7484, -4.8561, -1.7484, -5.0785, -5.2382, -5.5411,\n",
      "        -5.2188, -5.2188], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15879395604133606\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-4.7528, -5.3427, -5.3427, -4.4709, -5.1008, -5.1008, -5.1726, -4.7510,\n",
      "        -4.7510, -4.9369], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0541, -5.2759, -5.2759, -4.8847, -5.2759, -5.2759, -5.3344, -4.9326,\n",
      "        -4.9326, -4.8847], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04270442947745323\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3573, -5.0884, -4.3800, -4.7664], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.4823, -4.7986, -5.2839, -1.8834, -5.0884, -5.3573, -5.1182, -5.5090,\n",
      "        -5.2991, -4.7542], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4024, -4.9420, -5.4024, -1.7450, -5.3188, -5.3188, -5.3485, -5.3485,\n",
      "        -5.2987, -4.9420], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.022880394011735916\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3591, -5.0129, -4.3701, -4.7239], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.0868, -5.0918, -4.7834, -5.9473, -5.0918, -5.0868, -5.9473, -3.7712,\n",
      "        -5.4183, -5.0868], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3558, -5.3498, -4.8842, -5.7431, -5.3498, -5.3558, -5.7431, -4.3726,\n",
      "        -5.7368, -5.3558], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09069975465536118\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3450, -4.9553, -4.3461, -4.7002], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.1716, -4.1716, -5.1895, -5.3853, -5.7236, -4.7002, -4.3461, -4.1716,\n",
      "        -5.1170, -5.0952], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3887, -4.3887, -5.2994, -5.7374, -5.7379, -4.9115, -4.9115, -4.3887,\n",
      "        -5.3654, -5.3595], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07735107839107513\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3385, -4.9400, -4.3044, -4.6888], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.8426, -6.2615, -5.1873, -5.1232, -5.1232, -5.1232, -5.3385, -5.4994,\n",
      "        -5.1232, -5.1489], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8740, -5.7301, -5.2739, -5.3505, -5.3505, -5.3505, -5.3583, -5.3505,\n",
      "        -5.3505, -5.3583], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05639712139964104\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3098, -4.9262, -4.2543, -4.6921], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.2543, -3.7959, -4.2543, -5.1854, -5.1786, -4.6921, -5.1306, -5.2568,\n",
      "        -5.1960, -4.6921], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8288, -4.4163, -4.8288, -5.2382, -5.3366, -4.8288, -5.3398, -5.2382,\n",
      "        -5.3398, -4.8288], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11751075834035873\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.2928, -4.9217, -4.2017, -4.7173], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.2532, -5.4536, -5.2481, -6.2605, -5.1882, -4.6978, -5.1881, -3.7812,\n",
      "        -5.5306, -5.2928], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3081, -5.3487, -5.3132, -5.6694, -5.1954, -4.7291, -5.1954, -4.4031,\n",
      "        -5.3132, -5.3081], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08030177652835846\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.2513, -4.9143, -4.1718, -4.7358], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.2157, -5.5695, -5.3122, -4.7738, -5.3002, -5.3122, -5.6165, -5.1704,\n",
      "        -5.1861, -5.5362], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2964, -5.6518, -5.3133, -4.7547, -5.2964, -5.3133, -5.6517, -5.0442,\n",
      "        -5.1734, -5.3133], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.008066978305578232\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.2139, -4.9077, -4.1481, -4.7499], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.3728, -5.0329, -5.7892, -5.1841, -5.3728, -5.1841, -5.3728, -5.5511,\n",
      "        -5.3728, -5.5272], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3208, -5.3098, -5.5296, -5.1568, -5.3208, -5.1568, -5.3208, -5.6390,\n",
      "        -5.3208, -5.3208], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.020669467747211456\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-5.3608, -4.3263, -5.5183, -5.5421, -5.4203, -4.7540, -4.7540, -5.1618,\n",
      "        -5.7590, -5.2516], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2938, -4.4259, -5.3459, -5.5286, -5.3459, -4.7276, -4.7276, -5.1578,\n",
      "        -5.6428, -5.2938], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.006655402481555939\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.1226, -4.9001, -4.1436, -4.7502], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.5185, -4.7502, -5.3731, -4.8461, -4.8665, -5.1226, -5.1870, -4.9001,\n",
      "        -4.7502, -3.9357], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0667, -4.7293, -5.3075, -4.7293, -5.3075, -5.3075, -5.1670, -5.0667,\n",
      "        -4.7293, -2.7047], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.20916616916656494\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0938, -4.9092, -4.1507, -4.7413], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.4536, -5.4519, -5.5419, -4.3439, -4.7413, -4.1507, -4.7413, -5.4892,\n",
      "        -3.8263, -5.1895], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4174, -5.1733, -5.6677, -4.4437, -4.7356, -4.7356, -4.7356, -5.4174,\n",
      "        -4.4437, -5.1733], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08334919065237045\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0703, -4.9176, -4.1656, -4.7335], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.5158, -5.3295, -5.3295, -5.3295, -5.5273, -5.8013, -1.9220, -4.7335,\n",
      "        -5.5158, -5.2027], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4604, -5.3087, -5.3087, -5.3087, -5.3192, -5.4604, -1.7983, -4.7491,\n",
      "        -5.4604, -5.1933], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0182623453438282\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-5.5285, -5.5082, -4.6425, -5.2980, -5.7828, -5.2980, -5.5439, -4.8028,\n",
      "        -5.1629, -0.8846], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5117, -5.3464, -4.6826, -5.3225, -5.5117, -5.3225, -5.7084, -4.7736,\n",
      "        -5.1263, 10.0000], grad_fn=<AddBackward0>)\n",
      "LOSS: 11.86073112487793\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9295, -4.8516, -4.1673, -4.6164], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.4238, -5.4473, -5.4238, -5.4473, -5.1674, -5.4473, -4.5248, -4.8516,\n",
      "        -5.1674, -5.7126], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3327, -5.5172, -5.3327, -5.5172, -5.2878, -5.5172, -5.0723, -5.0723,\n",
      "        -5.2878, -5.7072], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04087735712528229\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8445, -4.8006, -4.1405, -4.5375], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.5375, -3.7745, -5.0554, -5.3867, -5.2080, -4.1405, -4.7198, -4.7198,\n",
      "        -4.8006, -5.0554], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7264, -4.2273, -5.2478, -5.5136, -5.5164, -4.7264, -4.7264, -4.7264,\n",
      "        -5.0387, -5.2478], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08261527866125107\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7804, -4.7714, -4.1094, -4.4897], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.3964, -4.7667, -4.4897, -5.3489, -4.9760, -5.0538, -4.6260, -5.3769,\n",
      "        -4.9076, -4.4897], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4946, -4.9891, -4.6985, -5.4946, -5.1979, -5.1634, -4.6029, -5.6104,\n",
      "        -5.2901, -4.6985], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.043003350496292114\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7310, -4.7494, -4.0517, -4.4754], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.3414, -4.9375, -5.0187, -5.0187, -4.6135, -4.7210, -5.3414, -4.9733,\n",
      "        -4.4754, -3.9350], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4419, -5.1225, -5.1004, -5.1004, -4.8436, -5.1522, -5.4419, -5.4419,\n",
      "        -4.6465, -4.0992], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.058246683329343796\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6900, -4.7353, -3.9771, -4.4846], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.9771, -5.4609, -5.3500, -5.2045, -5.2530, -4.3193, -5.3500, -5.3500,\n",
      "        -3.5740, -4.8457], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5794, -5.3612, -5.3612, -5.1504, -5.3612, -4.8153, -5.3612, -5.3612,\n",
      "        -4.0654, -5.0316], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09097722917795181\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6578, -4.7267, -3.9355, -4.4851], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.9507, -4.9980, -5.3498, -5.3498, -4.4851, -1.4272, -5.1467, -4.4851,\n",
      "        -4.4078, -4.6578], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3161, -4.9953, -5.3161, -5.3161, -4.5419, -1.4075, -4.9953, -4.5419,\n",
      "        -4.4277, -4.9740], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.026598211377859116\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6500, -4.7239, -3.8942, -4.5042], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.1542, -3.8942, -4.5042, -5.4562, -4.8914, -5.3480, -3.8942, -1.6238,\n",
      "        -4.8914, -5.3480], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6925, -4.5047, -4.5047, -5.4100, -4.9050, -5.2551, -4.5047, -1.3814,\n",
      "        -4.9050, -5.2551], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1113806813955307\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6474, -4.7236, -3.8575, -4.5301], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.8456, -5.3689, -4.3691, -5.1925, -3.4146, -4.6474, -4.8834, -5.1108,\n",
      "        -3.4146, -5.2238], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9115, -5.3610, -4.3347, -4.9093, -3.9679, -4.8403, -4.8403, -5.1968,\n",
      "        -3.9679, -5.2648], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07463470101356506\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6621, -4.7238, -3.8508, -4.5478], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.3324, -5.3348, -4.8374, -5.0577, -5.3348, -5.3932, -3.8508, -3.9607,\n",
      "        -4.2265, -4.9635], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1645, -5.1645, -4.9742, -4.9742, -5.1645, -5.3446, -4.4657, -3.9294,\n",
      "        -4.4657, -4.8957], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05551224946975708\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6662, -4.7149, -3.8567, -4.5559], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.4022, -5.3110, -5.3110, -4.8303, -5.1254, -2.5885, -4.2085, -3.3823,\n",
      "        -4.8363, -4.8363], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5571, -5.1484, -5.1484, -4.8960, -4.8879, -2.4006, -4.4710, -3.8924,\n",
      "        -4.7877, -4.7877], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.050672758370637894\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-1.5285, -5.2716, -3.8831, -5.2716, -4.8079, -3.8831, -4.8079, -4.9728,\n",
      "        -5.2716, -5.0415], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3032, -5.1579, -4.4948, -5.1579, -4.8033, -4.4948, -4.8033, -5.1579,\n",
      "        -5.1579, -5.1579], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08857909590005875\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6683, -4.6857, -3.9142, -4.5340], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.7815, -4.7815, -4.7815, -5.2187, -1.4960, -4.2488, -4.1989, -3.9142,\n",
      "        -5.3038, -4.6857], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8239, -4.8239, -4.8239, -5.1624, -1.2876, -4.5228, -4.7053, -4.5228,\n",
      "        -5.2349, -4.5570], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07751922309398651\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6650, -4.6618, -3.9397, -4.5168], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.9397, -4.7924, -4.6325, -4.7670, -3.4325, -4.7670, -3.4325, -5.0136,\n",
      "        -4.2613, -4.2665], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5457, -5.0072, -4.8527, -4.8527, -3.7623, -4.8527, -3.7623, -4.9772,\n",
      "        -4.3771, -4.6610], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08645801246166229\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6676, -4.6484, -3.9800, -4.4979], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.4979, -4.7467, -4.8114, -4.6676, -4.7467, -4.7843, -4.3238, -4.4979,\n",
      "        -5.1264, -4.9481], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5820, -4.8914, -5.0342, -4.8914, -4.8914, -5.0342, -4.5820, -4.5820,\n",
      "        -5.1954, -5.1954], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03507755696773529\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6814, -4.6375, -3.9943, -4.5062], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.5062, -4.9836, -5.1041, -4.7310, -4.6814, -4.5062, -3.8237, -5.1041,\n",
      "        -4.3402, -4.7575], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5949, -5.0330, -5.1914, -5.0558, -4.9062, -4.5949, -3.6917, -5.1914,\n",
      "        -4.5949, -4.9062], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02938113547861576\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7159, -4.6309, -3.9852, -4.5314], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.5314, -4.9074, -5.0994, -4.3322, -3.8151, -4.7852, -5.0994, -3.9852,\n",
      "        -3.9852, -3.9852], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5867, -5.0536, -5.1662, -4.5867, -3.6819, -4.8990, -5.1662, -4.5867,\n",
      "        -4.5867, -4.5867], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12141413986682892\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7498, -4.6279, -3.9743, -4.5641], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.0156, -4.8803, -1.3898, -5.1781, -4.3297, -4.8276, -3.9743, -4.5641,\n",
      "        -3.8092, -4.2467], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0139, -4.9706, -1.2380, -5.5141, -4.5768, -5.0139, -4.5768, -4.5768,\n",
      "        -3.6809, -4.4327], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06542131304740906\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7783, -4.6411, -3.9502, -4.5921], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.8419, -4.8779, -4.8419, -4.5921, -3.9502, -4.8779, -4.4361, -3.8187,\n",
      "        -2.3968, -4.3211], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9925, -4.8890, -4.9925, -4.5551, -4.5551, -4.8890, -4.4188, -3.6823,\n",
      "        -2.2451, -4.5551], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05096706002950668\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8013, -4.6564, -3.9206, -4.6172], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.8640, -4.4186, -3.8316, -4.9647, -4.8013, -5.1296, -3.4456, -4.9897,\n",
      "        -5.1485, -4.8640], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9696, -4.6291, -3.6874, -4.8982, -4.8823, -5.0794, -3.6874, -5.3864,\n",
      "        -5.3864, -4.9696], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03734022006392479\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-4.2718, -4.9028, -5.1834, -3.8841, -4.9028, -4.6385, -4.8426, -4.9740,\n",
      "        -4.2908, -4.6385], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6263, -4.9383, -5.3519, -4.4957, -4.9383, -4.4957, -4.8617, -4.8617,\n",
      "        -4.4957, -4.4957], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06263262033462524\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3368, -5.0735, -4.2911, -5.0014], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.2791, -2.3562, -4.2911, -5.0014, -4.6580, -5.0014, -5.1498, -5.1498,\n",
      "        -5.1498, -4.7746], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4548, -2.2319, -4.4777, -4.8620, -4.9262, -4.8620, -5.0302, -5.0302,\n",
      "        -5.0302, -4.8620], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.024253977462649345\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9166, -4.7537, -3.8501, -4.6240], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.9785, -4.7794, -4.9908, -5.3690, -5.0105, -4.7794, -4.7167, -4.6240,\n",
      "        -3.8501, -4.6240], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8518, -4.8713, -4.9177, -5.0236, -4.9612, -4.8713, -4.9177, -4.4651,\n",
      "        -4.4651, -4.4651], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06291736662387848\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9426, -4.7762, -3.8500, -4.6011], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.9338, -4.6011, -4.2990, -5.1239, -5.3060, -4.2627, -4.6011, -3.8500,\n",
      "        -5.1239, -5.1239], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6472, -4.4650, -4.4484, -5.0270, -5.3342, -4.5489, -4.4650, -4.4650,\n",
      "        -5.0270, -5.0270], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06305204331874847\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9658, -4.7926, -3.8709, -4.5653], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.3290, -4.3745, -3.8709, -4.5505, -5.0936, -5.0936, -3.8140, -5.3205,\n",
      "        -5.0389, -4.3597], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3620, -4.4326, -4.4838, -4.5066, -5.0485, -5.0485, -3.6255, -5.2810,\n",
      "        -4.9371, -4.4838], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.044900570064783096\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9855, -4.8031, -3.8904, -4.5330], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.9976, -4.9855, -5.0547, -4.5351, -3.8904, -3.8672, -5.0664, -0.2309,\n",
      "        -4.5330, -4.5351], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9606, -4.9606, -4.9576, -4.4629, -4.5014, -4.4704, -5.0706, 10.0000,\n",
      "        -4.5014, -4.4629], grad_fn=<AddBackward0>)\n",
      "LOSS: 10.543055534362793\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9826, -4.7835, -3.9147, -4.4545], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.0372, -5.0372, -5.0372, -5.0690, -5.3322, -4.9406, -4.4545, -4.4545,\n",
      "        -5.3322, -4.9986], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9827, -4.9827, -4.9827, -5.0938, -5.4201, -5.0023, -4.5232, -4.5232,\n",
      "        -5.4201, -5.0938], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00472856592386961\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9807, -4.7665, -3.9289, -4.3927], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.8042, -4.5274, -4.3927, -4.9439, -4.5274, -4.5129, -0.1352, -5.2134,\n",
      "        -5.3274, -4.4421], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0192, -4.5212, -4.5360, -5.1168, -4.5212, -4.3077, 10.0000, -4.9979,\n",
      "        -5.4363, -4.5212], grad_fn=<AddBackward0>)\n",
      "LOSS: 10.29251480102539\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-3.9188, -4.8806, -3.9188, -5.0089, -1.2962, -4.8806, -4.8388, -4.4883,\n",
      "        -4.3203, -5.4597], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5269, -5.1187, -4.5269, -4.9949, -1.0537, -5.1187, -5.0395, -4.5269,\n",
      "        -4.5269, -5.1187], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11127251386642456\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6151, -4.3953, -3.8521, -3.5284], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.8107, -4.9287, -5.1790, -4.8493, -3.9009, -4.4461, -4.4512, -5.2582,\n",
      "        -4.8107, -4.6757], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0339, -4.9799, -5.0155, -5.1087, -4.5108, -5.0528, -4.5095, -5.3709,\n",
      "        -5.0339, -4.1756], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12025884538888931\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9081, -4.6078, -3.8443, -4.2601], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.4255, -4.8675, -4.6078, -4.4406, -5.2162, -4.8911, -4.8268, -3.8443,\n",
      "        -4.2601, -4.4025], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.1670, -5.0750, -4.1670, -4.4599, -5.3445, -4.9531, -4.9966, -4.4599,\n",
      "        -4.4599, -4.4732], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07774941623210907\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8644, -4.5290, -3.7600, -4.2578], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.9169, -4.8430, -4.5202, -4.5202, -1.1828, -3.7600, -4.3515, -4.9169,\n",
      "        -3.3993, -4.8393], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0234, -5.0234, -4.9214, -4.9214, -0.9093, -4.3840, -4.4177, -5.0234,\n",
      "        -3.3594, -4.9044], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08516242355108261\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8297, -4.4597, -3.6578, -4.2849], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.8297, -4.9256, -4.9956, -5.0305, -4.3837, -4.3092, -4.8343, -4.2831,\n",
      "        -4.2831, -4.8335], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8548, -4.8087, -4.9453, -4.8388, -4.8548, -4.3440, -4.8475, -4.2920,\n",
      "        -4.2920, -4.8475], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.027728896588087082\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7996, -4.3895, -3.5720, -4.3086], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.0715, -5.1042, -5.1058, -4.3291, -4.7508, -4.9929, -4.7175, -3.6455,\n",
      "        -4.7996, -5.0715], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8962, -5.1942, -5.0831, -4.7750, -4.7750, -4.7750, -4.7723, -3.3927,\n",
      "        -4.7750, -4.8962], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03845691680908203\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7693, -4.3222, -3.5029, -4.3230], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.1902, -5.6149, -5.0168, -3.5977, -3.5029, -5.0168, -3.5029, -5.0476,\n",
      "        -3.5964, -3.2112], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.2380, -5.0207, -4.7046, -3.4047, -4.1526, -4.7046, -4.1526, -5.1291,\n",
      "        -3.4047, -3.4047], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15125644207000732\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7216, -4.2632, -3.4685, -4.3249], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.1808, -4.2632, -5.1808, -4.0722, -5.4444, -5.0070, -4.3249, -4.3249,\n",
      "        -4.3011, -5.4677], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8710, -4.2448, -4.8710, -4.1216, -4.8710, -4.6650, -4.1216, -4.1216,\n",
      "        -4.6650, -4.8710], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12114045768976212\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6220, -4.2013, -3.4795, -4.2801], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.7361, -5.1896, -3.9222, -5.1225, -4.2801, -4.9397, -5.4113, -4.9498,\n",
      "        -5.1896, -3.2047], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6199, -4.9320, -4.2358, -5.0253, -4.1316, -4.9320, -4.9320, -5.1190,\n",
      "        -4.9320, -3.3899], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05688167363405228\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5205, -4.1469, -3.5087, -4.2189], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.0852, -4.7208, -5.1605, -4.7208, -4.8368, -0.9279, -4.8368, -3.4842,\n",
      "        -3.5087, -5.4019], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.1578, -4.8819, -5.0009, -4.8819, -4.6767, -1.8369, -4.6767, -3.3633,\n",
      "        -4.1578, -5.1529], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14582720398902893\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.4197, -4.1091, -3.5439, -4.1463], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.2821, -5.1107, -3.7486, -4.7276, -4.7276, -3.4707, -4.1091, -4.5115,\n",
      "        -4.7276, -4.0483], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3245, -5.0696, -4.3481, -4.6993, -4.6993, -3.3245, -4.3481, -4.9504,\n",
      "        -4.6993, -4.3017], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07005833089351654\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-4.0757, -4.1371, -5.1833, -4.0489, -3.4691, -4.6149, -4.5003, -3.4691,\n",
      "        -5.0253, -4.0069], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.2273, -4.2273, -5.1339, -4.3499, -3.2809, -4.7234, -5.0090, -3.2809,\n",
      "        -5.1381, -4.3499], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0596037395298481\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8395, -4.5611, -4.1313, -4.5300], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.0113, -5.0113, -1.0053, -5.0113, -4.1066, -3.7303, -3.8117, -2.5047,\n",
      "        -4.0246, -3.4860], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1662, -5.1662, -0.6917, -5.1662, -4.2402, -4.3602, -4.2402, -1.6836,\n",
      "        -4.2357, -3.2542], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1540994495153427\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2531, -4.1174, -3.5888, -3.9864], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.3371, -3.9864, -4.9837, -3.5888, -4.2531, -4.1045, -4.0691, -3.8033,\n",
      "        -3.8033, -4.5258], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2230, -4.2299, -5.1781, -4.2299, -4.6940, -4.2299, -4.3391, -4.1878,\n",
      "        -4.1878, -4.6940], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11281950771808624\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2495, -4.1490, -3.5881, -3.9734], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.5103, -4.7199,  0.3728, -4.6475, -3.5881, -3.5881, -4.8121, -4.7816,\n",
      "        -3.5881, -4.5742], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.1981, -5.1828, 10.0000, -4.6586, -4.2293, -4.2293, -5.0234, -4.9999,\n",
      "        -4.2293, -4.8743], grad_fn=<AddBackward0>)\n",
      "LOSS: 9.441142082214355\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2598, -4.1805, -3.5807, -3.9457], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.9249, -4.7381, -0.9740, -3.7693, -4.9249, -3.5185, -4.6247, -4.2784,\n",
      "        -4.3066, -4.3066], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1622, -4.7187, -0.6206, -4.0609, -5.1622, -3.1537, -4.6250, -4.0609,\n",
      "        -4.6250, -4.6250], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07061117142438889\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2663, -4.1968, -3.5499, -3.9448], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.9202, -4.9202, -3.5499, -4.8077, -4.0849, -4.2793, -4.8547, -3.9448,\n",
      "        -4.9202, -4.3722], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1210, -5.1210, -4.1949, -4.9268, -4.2335, -4.5720, -4.9350, -4.1949,\n",
      "        -5.1210, -5.0553], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11945698410272598\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2771, -4.2238, -3.5008, -3.9725], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.6718, -4.9481, -4.2847, -3.5008, -4.2862, -4.0905, -4.2847,  0.4987,\n",
      "        -4.5540, -3.6718], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.0047, -5.0615, -4.5021, -4.1508, -4.1666, -4.1666, -4.5021, 10.0000,\n",
      "        -4.6192, -4.0047], grad_fn=<AddBackward0>)\n",
      "LOSS: 9.105024337768555\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2802, -4.2420, -3.4541, -3.9780], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.8796, -4.8243, -4.9617, -4.9617, -4.8243, -4.2790, -4.2790, -3.4511,\n",
      "        -3.5060, -4.8243], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4932, -4.7899, -5.0024, -5.0024, -4.7899, -4.4380, -4.4380, -3.1084,\n",
      "        -3.1084, -4.7899], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04822816327214241\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2788, -4.2425, -3.3939, -4.0000], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.9902, -4.2983, -4.0000, -4.2983, -2.2190, -4.2983, -4.8037, -4.6582,\n",
      "        -4.7151, -0.5476], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9320, -4.3628, -4.0545, -4.3628, -3.1123, -4.3628, -4.7054, -4.8299,\n",
      "        -4.4912, -0.4463], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09163513779640198\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2813, -4.2413, -3.3364, -4.0105], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.0562, -3.3364, -3.3364, -5.0057, -3.3364, -5.0057, -5.0057, -4.0403,\n",
      "        -4.6379, -4.3088], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8636, -4.0028, -4.0028, -4.8636, -4.0028, -4.8636, -4.8636, -3.9509,\n",
      "        -4.7487, -4.2941], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1450316160917282\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-5.0010, -4.3100, -3.4380, -5.0010, -4.5765, -4.7690, -3.3087, -5.1622,\n",
      "        -4.1505, -4.4432], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8227, -4.2489, -3.8459, -4.8227, -4.8976, -4.5828, -3.9778, -4.8976,\n",
      "        -4.5828, -4.6909], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11374646425247192\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2963, -4.2479, -3.2997, -4.0165], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.2997, -3.4130, -4.0165, -4.3002, -4.2210, -4.3002, -4.2963, -3.4130,\n",
      "        -3.2997, -3.7626], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9697, -3.0165, -3.9697, -4.2239, -4.2239, -4.2239, -4.2239, -3.0165,\n",
      "        -3.9697, -3.6977], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12355989217758179\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3015, -4.2333, -3.2963, -4.0225], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.9599, -3.2963, -2.9575, -4.4433, -3.7215, -3.3737, -3.2963, -4.9599,\n",
      "        -1.8862, -4.0225], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7823, -3.9666, -2.9990, -4.6183, -3.6794, -2.9990, -3.9666, -4.7823,\n",
      "        -1.4139, -3.9666], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13626420497894287\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3078, -4.2145, -3.2972, -4.0336], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.2942, -3.3266, -3.7956, -4.0096, -3.2972, -4.9966, -4.2145, -4.2984,\n",
      "        -3.2972, -3.2972], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.1904, -2.9907, -3.7212, -3.6702, -3.9675, -4.7675, -3.7212, -4.1904,\n",
      "        -3.9675, -3.9675], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.18996116518974304\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2942, -4.1582, -3.2980, -4.0481], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.7855, -4.3140, -4.9318, -4.1582, -4.3140, -4.9782, -3.7046, -4.0481,\n",
      "        -0.6848, -2.0524], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7197, -4.1841, -4.7602, -3.7197, -4.1841, -4.7081, -3.7018, -3.9682,\n",
      "        -0.2527, -1.4124], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09353803098201752\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2546, -4.0703, -3.2960, -4.0456], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.2960, -4.3204, -4.9210, -4.9210, -3.1351, -4.0456, -3.0458, -3.1732,\n",
      "        -4.3204, -4.1583], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9664, -4.1850, -4.7610, -4.7610, -3.0109, -3.9664, -3.9664, -3.0109,\n",
      "        -4.1850, -4.7610], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17961111664772034\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-3.2613, -4.3849, -4.8482, -3.2613, -3.6991, -3.0929, -4.3849, -4.2007,\n",
      "        -3.1340, -4.3409], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9352, -4.4723, -4.5382, -3.9352, -3.7908, -3.0443, -4.4723, -4.1672,\n",
      "        -3.9352, -4.1672], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17034587264060974\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1078, -3.8366, -3.4589, -3.2639], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.4589, -4.6424, -3.0162, -4.4482, -4.2997, -3.2152, -4.1090, -4.4482,\n",
      "        -3.7446, -3.9735], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9375, -4.5330, -3.0857, -4.5330, -4.4538, -3.8937, -4.1482, -4.5330,\n",
      "        -3.9375, -4.4538], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10139014571905136\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1101, -3.7836, -3.1825, -4.0295], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.8922, -0.4783, -3.1825, -3.3512, -4.6580, -3.1825, -4.7755, -4.3791,\n",
      "        -4.1274, -2.3468], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6741, -1.5003, -3.8642, -3.8642, -4.7147, -3.8642, -4.7147, -4.1361,\n",
      "        -4.4369, -1.5003], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3163141906261444\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0692, -3.7063, -3.1472, -3.9889], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.1472, -3.4065, -4.8474, -3.1472, -3.1472, -1.9689, -4.1829, -3.9889,\n",
      "        -4.0585, -4.3584], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.8325, -3.7835, -4.6527, -3.8325, -3.8325, -1.5016, -4.8508, -3.8325,\n",
      "        -4.1245, -4.1245], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.23369145393371582\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-2.8874, -3.5781, -4.3662, -4.3247, -3.7077, -3.1170, -3.1170, -3.4564,\n",
      "        -4.7952, -4.6639], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.0748, -4.1479, -4.6318, -4.1108, -3.9274, -3.8053, -3.8053, -3.8053,\n",
      "        -4.6318, -4.8290], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16474483907222748\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0174, -3.6262, -3.1084, -3.8918], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.1192, -3.1084, -4.4650, -4.1192, -4.7227, -3.3858, -3.9452, -4.4777,\n",
      "        -4.8328, -3.8918], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4113, -3.7975, -4.3900, -4.4113, -4.6302, -3.7870, -4.3900, -4.5973,\n",
      "        -4.7685, -3.7975], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10459347069263458\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0093, -3.6256, -3.0854, -3.8389], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.9080, -4.6503, -4.6938, -3.4041, -4.2521, -0.5510, -3.4781, -4.4339,\n",
      "        -4.6503, -4.6503], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.0308, -4.6103, -4.6103, -3.7767, -4.1303, -0.1753, -3.7768, -4.6103,\n",
      "        -4.6103, -4.6103], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04420972615480423\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0073, -3.6234, -3.0596, -3.7810], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.6592, -3.4914, -3.5443, -3.0596, -4.5702, -2.9450, -3.9867, -3.7810,\n",
      "        -4.5702, -4.0193], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8036, -3.7536, -3.7605, -3.7536, -4.5880, -3.0185, -4.1423, -3.7536,\n",
      "        -4.5880, -4.3848], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07826307415962219\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0038, -3.6358, -3.0259, -3.7292], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.9936, -3.0259, -4.2109, -4.5097, -4.2215, -4.2215, -4.4352, -4.5097,\n",
      "        -4.0038, -3.5043], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.0104, -3.7233, -4.3784, -4.5639, -4.1539, -4.1539, -4.5639, -4.5639,\n",
      "        -4.1539, -3.7233], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06168090179562569\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0088, -3.6547, -2.9939, -3.6886], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.2591, -3.6886, -3.0375, -4.2192, -4.4608, -4.2192, -2.9939, -4.2591,\n",
      "        -4.2373, -4.4608], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3611, -3.6945, -3.0070, -4.1617, -4.5369, -4.1617, -3.6945, -4.3611,\n",
      "        -4.1617, -4.5369], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05365123599767685\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-4.2163, -2.9667, -4.3109, -2.9667, -2.9667, -4.4222, -2.9667, -3.6545,\n",
      "        -4.2163, -4.2163], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.1686, -3.6701, -4.3448, -3.6701, -3.6701, -4.5125, -3.6701, -3.6701,\n",
      "        -4.1686, -4.1686], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1995035707950592\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-2.9559, -4.2116, -3.7448, -3.6319, -4.7385, -4.2116, -4.3922, -4.0846,\n",
      "        -4.3628, -4.0331], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6603, -4.1823, -3.8791, -3.6603, -4.4988, -4.1823, -4.4988, -4.3386,\n",
      "        -4.3386, -4.1823], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06729069352149963\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-4.2031, -4.2031, -4.4032, -2.9519, -4.0950, -2.8503, -4.6714, -4.4042,\n",
      "        -4.2031, -4.7276], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.1930, -4.1930, -3.8752, -3.6567, -4.3090, -3.0032, -4.6855, -4.3289,\n",
      "        -4.1930, -4.8168], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08588254451751709\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-4.3577, -4.3500, -3.6167, -4.2031, -2.9537, -2.9537, -4.3249, -4.6829,\n",
      "        -4.0901, -4.8003], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3004, -4.4781, -3.6584, -4.2063, -3.6584, -3.6584, -4.3553, -4.7081,\n",
      "        -4.2063, -4.8261], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1030183807015419\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1302, -3.7922, -2.9595, -3.6273], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.8140, -4.2066, -4.3470, -4.2066, -4.3470, -4.2066, -4.3470, -3.5725,\n",
      "        -4.3470, -2.9595], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.5051, -4.2152, -4.4706, -4.2152, -4.4706, -4.2152, -4.4706, -3.6635,\n",
      "        -4.4706, -3.6635], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06607263535261154\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1673, -3.8156, -2.9545, -3.6509], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.3810, -4.3776, -4.3776, -4.3776, -4.3776, -2.9545, -3.8400, -3.6719,\n",
      "        -3.8400, -3.6509], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.2736, -4.4560, -4.4560, -4.4560, -4.4560, -3.6590, -4.2141, -3.6806,\n",
      "        -4.2141, -3.6590], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08125313371419907\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2038, -3.8392, -2.9500, -3.6794], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.2582, -4.2419, -4.4422, -4.4883, -4.7181, -4.4883, -2.9500, -3.8448,\n",
      "        -5.2827, -4.2443], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.0575, -4.2913, -4.4603, -4.2913, -4.4603, -4.2913, -3.6550, -4.2021,\n",
      "        -4.6579, -4.2021], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12038829177618027\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2129, -3.8447, -2.9691, -3.6984], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.8820, -4.7917, -4.5061, -4.2129, -2.8816, -4.7917, -3.4595, -2.9691,\n",
      "        -4.5061, -3.2610], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6642, -4.7552, -4.4997, -4.2097, -3.0724, -4.7552, -4.4303, -3.6722,\n",
      "        -4.4997, -3.0724], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15589231252670288\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2251, -3.8463, -2.9807, -3.7349], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.9807, -3.2588, -4.4477, -3.5650, -4.5815, -4.5815, -3.9178, -4.8121,\n",
      "        -4.2758, -4.2469], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6826, -3.1023, -4.2782, -3.6826, -4.5261, -4.5261, -4.2085, -4.7719,\n",
      "        -4.3028, -4.2085], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06542034447193146\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2356, -3.8389, -2.9991, -3.7666], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.7575, -3.5689, -2.9991, -3.7666, -4.4506, -3.8009, -4.2770, -4.6519,\n",
      "        -4.4506, -4.2216], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7050, -3.6992, -3.6992, -3.6992, -4.3129, -3.7050, -4.3129, -4.5647,\n",
      "        -4.3129, -4.2949], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05757785961031914\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2435, -3.8216, -3.0265, -3.7890], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.7092, -4.2674, -4.8657, -0.7036, -3.7890, -4.7597, -4.2668, -3.0265,\n",
      "        -3.0265, -3.8345], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6117, -4.3209, -4.6117, -0.1419, -3.7238, -4.8983, -4.2297, -3.7238,\n",
      "        -3.7238, -4.4403], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17567622661590576\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2425, -3.8100, -3.0486, -3.8142], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.0486, -3.0486, -4.7667, -3.5944, -4.7667, -4.7667, -2.9411, -3.8142,\n",
      "        -3.8142, -4.3784], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7437, -3.7437, -4.6527, -3.7437, -4.6527, -4.6527, -3.1759, -3.7437,\n",
      "        -3.7437, -3.7478], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14903314411640167\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2393, -3.7818, -3.0932, -3.8183], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.0932, -4.7899, -4.7899, -3.0932, -3.0932, -3.0932, -4.3073, -4.3718,\n",
      "        -4.7899, -4.6990], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7839, -4.7139, -4.7139, -3.7839, -3.7839, -3.7839, -4.3779, -3.7969,\n",
      "        -4.7139, -4.9610], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2329699993133545\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-4.9991, -4.2335, -4.2636, -3.1520, -4.2335, -1.8626, -4.2636, -4.4519,\n",
      "        -4.2335, -4.7976], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8298, -4.3133, -4.4239, -3.8368, -4.3133, -1.6184, -4.4239, -4.4281,\n",
      "        -4.3133, -4.7805], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0628650113940239\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8031, -4.2457, -3.7280, -4.2413], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.7280, -4.2663, -3.8376, -3.2002, -3.7280, -4.8080, -3.8352, -3.7455,\n",
      "        -3.2002, -4.2413], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.8802, -4.4621, -3.8802, -3.8802, -3.8802, -4.8332, -3.8802, -4.4538,\n",
      "        -3.8802, -4.3552], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15285637974739075\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2775, -3.7760, -3.2558, -3.8559], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.8559, -4.8223, -4.2532, -3.7760, -3.8809, -3.8809, -4.8106, -3.2558,\n",
      "        -3.7779, -4.6655], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9302, -4.9267, -4.4002, -4.4118, -3.9657, -3.9657, -4.8810, -3.9302,\n",
      "        -3.9302, -5.0542], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10907167196273804\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3193, -3.8517, -3.3093, -3.8887], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.8105, -4.7309, -4.8105, -4.6718, -4.2684, -4.8105, -4.3835, -3.8193,\n",
      "        -3.8193, -4.3110], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9146, -5.0515, -4.9146, -5.1304, -4.4355, -4.9146, -4.5134, -3.9996,\n",
      "        -3.9996, -4.5134], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.049638718366622925\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-4.8848, -4.8369, -4.3757, -3.3229, -3.2099, -3.3358, -4.3787, -3.8805,\n",
      "        -4.9364, -3.8150], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9093, -4.9093, -4.4925, -3.9906, -3.1886, -3.1886, -4.4335, -4.0022,\n",
      "        -5.1064, -3.9906], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05649659037590027\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.4364, -4.0396, -3.3266, -3.9769], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.3266, -4.3460, -3.3266, -4.8746, -3.9769, -3.3266, -3.3245, -3.9791,\n",
      "        -4.5002, -4.3460], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9939, -4.4269, -3.9939, -4.9004, -3.9939, -3.9939, -3.2037, -3.9921,\n",
      "        -4.4562, -4.4269], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1366785317659378\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-4.3973, -4.4964, -4.0489, -5.0273, -4.9177, -4.5157, -3.3337, -4.3243,\n",
      "        -2.0624, -4.0297], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4217, -4.4217, -3.9801, -4.8926, -4.8919, -4.4467, -4.0003, -4.4217,\n",
      "        -1.6169, -4.0003], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06877203285694122\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5399, -4.2056, -3.3464, -4.0773], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.2817, -4.2884, -4.9614, -4.2959, -4.3640, -3.0304, -4.4461, -3.5838,\n",
      "        -5.1853, -4.9614], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8939, -4.4345, -4.8939, -4.8664, -4.4243, -3.2490, -4.4243, -4.0118,\n",
      "        -5.0191, -4.8939], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07689423859119415\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5552, -4.2593, -3.3605, -4.1134], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.2593, -4.9194, -3.3820, -5.2333, -4.7139, -4.1457, -4.3346, -2.0557,\n",
      "        -5.2845, -4.9889], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.2281, -4.9579, -3.2685, -5.0192, -4.4418, -3.9810, -4.4369, -1.6388,\n",
      "        -4.9012, -4.9012], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.050122521817684174\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5445, -4.2804, -3.3887, -4.1440], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.3887, -4.1440, -5.0003, -3.3887, -3.3330, -4.5168, -5.0003, -3.3875,\n",
      "        -5.0003, -5.0003], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.0498, -4.0498, -4.9202, -4.0498, -3.2868, -4.4569, -4.9202, -3.2868,\n",
      "        -4.9202, -4.9202], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09246194362640381\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5377, -4.3009, -3.4288, -4.1642], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.5293, -4.5377, -4.9909, -5.1234, -4.6141, -3.8976, -3.4288, -5.1234,\n",
      "        -4.5293, -4.5293], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4865, -4.4865, -4.9481, -4.9481, -4.4757, -4.2261, -4.0859, -4.9481,\n",
      "        -4.4865, -4.4865], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06302111595869064\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5295, -4.3171, -3.4883, -4.1627], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.9678, -4.9715, -4.1627, -4.9678, -3.4883, -0.7265, -4.5162, -3.4883,\n",
      "        -4.6145, -4.1627], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0058, -5.0058, -4.1395, -5.0058, -4.1395, -0.1693, -4.5367, -4.1395,\n",
      "        -4.5234, -4.1395], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11723126471042633\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-0.4956, -4.9412, -5.0468, -3.5458, -4.7255, -4.5002, -3.5458, -4.9412,\n",
      "        -3.5458, -4.5002], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.1715, -5.0574, -5.0574, -4.1912, -4.5659, -4.5833, -4.1912, -5.0574,\n",
      "        -4.1912, -4.5833], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1421164721250534\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-4.7371, -4.7371, -4.9383, -4.5046, -4.5241, -4.9383, -4.9383, -4.9383,\n",
      "        -4.9383, -4.7371], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6859, -4.6859, -5.1048, -4.6266, -4.6266, -5.1048, -5.1048, -5.1048,\n",
      "        -5.1048, -4.6859], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.017187634482979774\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-4.1816, -4.5534, -4.0910, -3.6364, -4.4343, -3.6364, -4.5315, -3.6364,\n",
      "        -4.5315, -4.9877], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0779, -5.0779, -5.0779, -4.2727, -4.6237, -4.2727, -4.6421, -4.2727,\n",
      "        -4.6421, -5.1322], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.33484089374542236\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-5.2437, -4.2252, -4.5558, -5.0073, -3.7208, -4.5453, -5.1611, -4.5823,\n",
      "        -4.5383, -5.0073], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2726, -4.3341, -4.6897, -5.1904, -4.2351, -4.6897, -5.1904, -4.6681,\n",
      "        -4.6897, -5.1904], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04142271727323532\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6047, -4.3920, -3.7410, -4.2672], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.4595, -3.3829, -5.2024, -5.0597, -5.2576, -4.1981, -5.2576, -2.5773,\n",
      "        -5.0597, -5.0597], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3669, -3.3196, -5.2148, -5.2148, -5.2809, -4.2592, -5.2809, -1.5962,\n",
      "        -5.2148, -5.2148], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.18670538067817688\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6229, -4.3900, -3.7197, -4.3015], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.3653, -4.6107, -4.6324, -5.1495, -4.6107, -4.3638, -4.7150, -4.0842,\n",
      "        -5.1495, -4.0842], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3223, -4.6758, -4.8039, -5.2090, -4.6758, -4.8039, -4.8039, -4.3478,\n",
      "        -5.2090, -4.3478], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.038741566240787506\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6448, -4.3808, -3.6752, -4.3338], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.2495, -4.6448, -5.3091, -3.3533, -5.2495, -3.6752, -5.0734, -4.3338,\n",
      "        -5.2495, -5.2495], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1876, -4.6524, -5.1876, -3.3358, -5.1876, -4.3077, -5.1876, -4.3077,\n",
      "        -5.1876, -5.1876], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04441981762647629\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6619, -4.3744, -3.6441, -4.3610], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.3099, -3.1728, -4.4797, -4.5748, -4.5748, -3.6441, -5.3532, -4.3610,\n",
      "        -3.3429, -4.3610], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1799, -3.3460, -4.7469, -4.6006, -4.6006, -4.2797, -5.1727, -4.2797,\n",
      "        -3.3460, -4.2797], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05694937705993652\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-5.1121, -3.5769, -4.8066, -4.4112, -5.3902, -3.6261, -4.3773, -3.5769,\n",
      "        -5.3639, -5.3639], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2089, -3.3494, -4.6337, -4.5892, -5.1652, -4.2635, -4.2635, -3.3494,\n",
      "        -5.1652, -5.1652], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07232867181301117\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6855, -4.3638, -3.6185, -4.3814], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.5727, -3.6185, -4.5727, -4.8378, -3.7963, -5.4016, -3.3365, -3.6185,\n",
      "        -4.8038, -5.3815], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5827, -4.2566, -4.5827, -4.6356, -4.2139, -5.1632, -3.3598, -4.2566,\n",
      "        -4.6356, -5.1632], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11633148044347763\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6966, -4.3665, -3.6284, -4.3757], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.7730, -4.5732, -4.8406, -5.1632, -4.7869, -3.6284, -5.3751, -4.4276,\n",
      "        -5.1258, -3.6284], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7172, -4.5893, -4.5893, -5.1715, -4.6536, -4.2656, -5.1715, -4.5893,\n",
      "        -5.2230, -4.2656], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09733410179615021\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-2.6417, -0.8556, -3.6557, -3.6557, -4.8533, -3.6557, -4.3680, -2.0245,\n",
      "        -3.6557, -4.8533], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7700, -0.2138, -4.2902, -4.2902, -4.6874, -4.2902, -4.4690, -1.7700,\n",
      "        -4.2902, -4.6874], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2911751866340637\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-4.1379, -4.3445, -4.2353, -5.3182, -4.8188, -3.8466, -3.3558, -4.3445,\n",
      "        -3.6967, -4.8188], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3270, -4.3270, -4.2482, -5.2218, -4.7241, -4.3270, -3.3357, -4.3270,\n",
      "        -4.3270, -4.7241], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0692378506064415\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7232, -4.3736, -3.7218, -4.3133], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.2302, -3.7218, -5.2857, -5.3955, -4.5725, -4.5725, -4.7930, -4.2302,\n",
      "        -4.7901, -4.7930], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.2683, -4.3496, -5.2488, -5.2488, -4.6716, -4.6716, -4.7609, -4.2683,\n",
      "        -4.7661, -4.7609], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04421910271048546\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7186, -4.3805, -3.7484, -4.2851], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.7484, -5.0813, -4.7650, -4.0713, -2.5246, -4.1120, -3.7484, -5.2522,\n",
      "        -3.7484, -5.2362], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3735, -5.2746, -4.7959, -4.4939, -1.7781, -4.2887, -4.3735, -5.2746,\n",
      "        -4.3735, -5.1356], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1988489031791687\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7207, -4.3893, -3.7891, -4.2566], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.1618, -5.2129, -3.7891, -4.5877, -4.2566, -4.8438, -0.8848, -5.1942,\n",
      "        -4.2566, -3.7891], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3167, -5.3062, -4.4102, -4.8399, -4.4102, -5.3908, -0.2276, -5.3062,\n",
      "        -4.4102, -4.4102], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16586828231811523\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7312, -4.3993, -3.8020, -4.2622], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.3993, -4.7312, -4.7312, -4.7251, -5.3551, -3.9051, -4.7251, -4.2622,\n",
      "        -5.2933, -4.2278], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5146, -4.8485, -4.8485, -4.8485, -5.3047, -4.4218, -4.8485, -4.4218,\n",
      "        -5.4103, -4.3125], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03871441259980202\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7484, -4.4178, -3.7851, -4.2903], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.7413, -4.2518, -4.7413, -4.7413, -3.7851, -4.7413, -3.7851, -5.3021,\n",
      "        -5.2170, -4.6396], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8266, -4.4066, -4.8266, -4.8266, -4.4066, -4.8266, -4.4066, -5.3931,\n",
      "        -5.3384, -4.7527], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0861431434750557\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7665, -4.4472, -3.7612, -4.3261], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.7813, -4.2471, -4.8544, -5.2271, -5.3668, -5.2271, -3.7612, -4.0273,\n",
      "        -5.2271, -4.4472], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8032, -4.2505, -4.7897, -5.2546, -5.2546, -5.2546, -4.3851, -4.3851,\n",
      "        -5.2546, -4.6246], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05682230740785599\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-5.2551, -4.4788, -4.8264, -3.7328, -3.5728, -4.1968, -5.3605, -0.6818,\n",
      "        -4.8967, -4.7608], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2260, -4.6917, -4.7771, -4.3595, -3.1563, -4.3595, -5.3395, -0.2614,\n",
      "        -4.7659, -4.9035], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08559757471084595\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-4.8710, -4.4085, -4.5116, -4.7580, -4.8710, -3.1594, -3.3256, -4.4941,\n",
      "        -4.1512, -5.2822], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7461, -4.3323, -4.7361, -4.6904, -4.7461, -3.1754, -3.1754, -4.3890,\n",
      "        -4.7361, -5.1926], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04759598150849342\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.2516, -4.9555, -4.1384, -4.8955], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.7958, -4.4458, -4.4458, -4.9999, -4.8955, -2.4328, -2.4328, -4.4458,\n",
      "        -4.6329, -3.6881], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6760, -4.3193, -4.3193, -4.7226, -4.7246, -1.8932, -1.8932, -4.3193,\n",
      "        -4.7246, -4.3193], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11575444787740707\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8057, -4.5726, -3.6908, -4.4435], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.4435, -5.2906, -4.4435, -5.4318, -4.4435, -0.6522, -3.4595, -3.6908,\n",
      "        -4.4435, -5.2906], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3217, -5.1695, -4.3217, -5.2801, -4.3217, -0.2904, -3.1449, -4.3217,\n",
      "        -4.3217, -5.1695], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0739680603146553\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7961, -4.5835, -3.7030, -4.4193], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.1409, -5.2625, -4.1409, -3.7030, -4.9244, -5.2625, -4.1409, -4.5835,\n",
      "        -4.4193, -4.1548], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3327, -5.1750, -4.3327, -4.3327, -4.7517, -5.1750, -4.3327, -4.7394,\n",
      "        -4.3327, -4.7394], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09254594892263412\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7833, -4.5952, -3.7217, -4.3821], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.8414, -4.3821, -3.7217, -5.2284, -3.7217, -4.8414, -4.5536, -5.2284,\n",
      "        -4.6615, -4.3821], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7601, -4.3495, -4.3495, -5.1954, -4.3495, -4.7601, -4.7256, -5.1954,\n",
      "        -4.7601, -4.3495], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08451403677463531\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7828, -4.6104, -3.7487, -4.3458], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.8827, -5.1950, -5.1950, -4.8162, -3.7487, -5.0285, -5.0631, -5.4451,\n",
      "        -4.6104, -5.1950], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7564, -5.2232, -5.2232, -4.7953, -4.3738, -5.3616, -4.7564, -5.1590,\n",
      "        -4.7658, -5.2232], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07206409424543381\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8022, -4.6110, -3.7858, -4.3181], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.8176, -4.2613, -3.4833, -5.1652, -4.7305, -4.3144, -3.7858, -4.8155,\n",
      "        -3.4833, -4.5475], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9152, -4.4073, -2.9988, -5.2575, -4.8352, -4.7456, -4.4073, -4.7982,\n",
      "        -2.9988, -4.7982], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11550639569759369\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-0.5775, -3.2727, -4.8263, -4.7823, -4.2806, -4.8132, -4.8821, -5.1523,\n",
      "        -3.7986, -5.1523], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.0072, -3.0033, -4.8526, -4.8526, -4.4187, -4.9127, -4.8120, -5.2728,\n",
      "        -4.4187, -5.2728], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.25699055194854736\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8421, -4.5968, -3.7996, -4.2799], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.2719, -4.8421, -4.1714, -5.2120, -3.2719, -3.4369, -4.8712, -5.1851,\n",
      "        -4.2799, -3.7996], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.0010, -4.8599, -4.0932, -5.3603, -3.0010, -3.0010, -4.8158, -5.3688,\n",
      "        -4.4197, -4.4197], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08060108870267868\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8662, -4.5765, -3.7842, -4.2821], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.8444, -5.1356, -4.2821, -4.8444, -4.7779, -4.8444, -4.7779, -4.0913,\n",
      "        -4.2821, -3.2658], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7980, -5.2580, -4.4058, -4.7980, -4.8463, -4.7980, -4.8463, -4.4058,\n",
      "        -4.4058, -3.0270], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.021731160581111908\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8828, -4.5422, -3.7496, -4.3034], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.2438, -5.1586, -4.9579, -5.1586, -4.8071, -4.3034, -5.4944, -3.2502,\n",
      "        -4.8828, -1.9811], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3746, -5.2290, -4.9489, -5.2290, -4.7669, -4.3746, -5.1388, -3.0676,\n",
      "        -4.8194, -1.9606], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.019804496318101883\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8744, -4.5002, -3.7182, -4.3241], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.3241, -5.1872, -4.8529, -3.7182, -4.3241, -3.7182, -5.1872, -4.9944,\n",
      "        -4.8529, -3.7182], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3464, -5.2084, -4.8029, -4.3464, -4.3464, -4.3464, -5.2084, -4.8029,\n",
      "        -4.8029, -4.3464], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12273912131786346\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8756, -4.4702, -3.7059, -4.3455], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.7308, -4.2249, -3.7059, -4.8678, -3.0754, -3.0754, -3.2132, -5.0321,\n",
      "        -4.7308, -4.8885], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7328, -4.3353, -4.3353, -5.2127, -3.1451, -3.1451, -3.1451, -4.8024,\n",
      "        -4.7328, -4.8024], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.060180384665727615\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8769, -4.4420, -3.6980, -4.3641], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.1984, -3.6980, -4.1883, -5.6613, -4.3641, -3.1984, -3.6980, -4.4135,\n",
      "        -4.5643, -3.6980], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.1777, -4.3282, -4.3282, -5.2557, -4.3282, -3.1777, -4.3282, -4.7271,\n",
      "        -4.3147, -4.3282], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15382707118988037\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8740, -4.4101, -3.6973, -4.3906], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.2574, -3.1977, -5.2574, -4.2503, -4.9715, -4.2503, -5.2574, -4.5582,\n",
      "        -3.1977, -3.6973], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1987, -3.2149, -5.1987, -4.3275, -4.8252, -4.3275, -5.1987, -4.7336,\n",
      "        -3.2149, -4.3275], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.047226496040821075\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8712, -4.3817, -3.7074, -4.4061], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.4061, -5.2721, -5.0048, -4.6148, -4.4061, -5.2721, -5.2721, -5.0048,\n",
      "        -4.1476, -5.2721], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3367, -5.2131, -4.8533, -4.7268, -4.3367, -5.2131, -5.2131, -4.8533,\n",
      "        -4.3753, -5.2131], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01338447816669941\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8667, -4.3601, -3.7340, -4.3966], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.0039, -3.1523, -3.7340, -4.1500, -5.0039, -3.7846, -4.3287, -5.4157,\n",
      "        -2.1183, -5.0039], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8959, -3.2536, -4.3606, -4.7350, -4.8959, -3.8954, -4.3606, -5.2438,\n",
      "        -1.9525, -4.8959], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08505040407180786\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8583, -4.3454, -3.7727, -4.3817], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.7727, -4.9870, -4.9870, -5.0847, -4.5665, -5.4006, -3.7727, -5.2404,\n",
      "        -3.7727, -3.7727], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3954, -4.9470, -4.9470, -5.2840, -4.7887, -5.2840, -4.3954, -5.2840,\n",
      "        -4.3954, -4.3954], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16589787602424622\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8571, -4.3488, -3.8173, -4.3817], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.9773, -4.9773, -4.6901, -4.2492, -3.1535, -5.1200, -5.2236, -4.9773,\n",
      "        -4.5611, -4.5611], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9957, -4.9957, -5.0920, -3.9398, -3.2695, -5.2452, -5.3175, -4.9957,\n",
      "        -4.8243, -4.8243], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.043478500097990036\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8544, -4.3746, -3.8277, -4.3916], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.8277, -4.4540, -3.8277, -3.8277, -3.2700, -4.9013, -3.8277, -4.9809,\n",
      "        -3.1749, -5.3607], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4449, -4.4449, -4.4449, -4.4449, -3.2843, -5.2925, -4.4449, -5.0086,\n",
      "        -3.2843, -5.3183], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16917604207992554\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8619, -4.4243, -3.8452, -4.4099], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.4099, -4.8619, -4.9873, -5.2255, -4.4099, -4.2380, -4.4687, -3.8452,\n",
      "        -4.9035, -5.1003], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4606, -5.0218, -5.0218, -5.3201, -4.4606, -3.9476, -4.4606, -4.4606,\n",
      "        -5.0883, -5.3201], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.058651626110076904\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8844, -4.4829, -3.8497, -4.4479], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.2444, -5.0079, -4.2047, -5.2337, -4.4479, -5.2337, -5.2337, -3.2465,\n",
      "        -3.2711, -5.0580], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4647, -5.0154, -3.9440, -5.2965, -4.4647, -5.2965, -5.2965, -3.3187,\n",
      "        -3.3187, -5.0154], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.013797560706734657\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8990, -4.5323, -3.8389, -4.4835], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.2540, -3.2797, -3.8389, -5.2511, -3.8389, -4.8125, -5.1158, -3.8389,\n",
      "        -3.2797, -5.4482], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2348, -3.3401, -4.4551, -5.2671, -4.4551, -4.7413, -5.2348, -4.4551,\n",
      "        -3.3401, -5.2671], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11986857652664185\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9111, -4.5853, -3.8404, -4.5232], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.4418, -4.5232, -4.5232, -4.6910, -5.0562, -3.8404, -4.5232, -3.8404,\n",
      "        -5.0562, -4.5232], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2523, -4.4564, -4.4564, -4.7134, -4.9891, -4.4564, -4.4564, -4.4564,\n",
      "        -4.9891, -4.4564], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08220575749874115\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9147, -4.6362, -3.8596, -4.5470], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.8073, -5.0553, -5.3080, -4.1160, -4.4337, -5.2925, -5.2925, -5.0451,\n",
      "        -4.5470, -3.8596], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9539, -4.9903, -5.2591, -3.9539, -4.4736, -5.2591, -5.2591, -4.8257,\n",
      "        -4.4736, -4.4736], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.048877764493227005\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-5.0482, -5.3015, -5.0482, -5.2691, -4.7921, -4.0114, -4.5628, -5.3015,\n",
      "        -5.3015, -4.5628], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0001, -5.2719, -5.0001, -5.3129, -4.6999, -3.9723, -4.4970, -5.2719,\n",
      "        -5.2719, -4.4970], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.002791126724332571\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9224, -4.7095, -3.9141, -4.5663], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.4437, -3.9141, -3.3260, -5.3951, -4.7095, -4.7319, -5.3002, -4.9070,\n",
      "        -4.3448, -3.2367], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8035, -4.5227, -3.3876, -5.2755, -4.8035, -4.4477, -5.2886, -4.8700,\n",
      "        -4.5227, -3.3876], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0663415864109993\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-4.7376, -4.4641, -4.7376, -5.3027, -3.9456, -5.0225, -4.7376, -5.0764,\n",
      "        -3.9456, -4.7376], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7100, -4.8129, -4.7100, -5.3077, -4.5510, -5.0329, -4.7100, -4.8992,\n",
      "        -4.5510, -4.7100], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08893280476331711\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3612, -5.1405, -4.5126, -5.0173], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.3613, -4.9553, -3.9883, -5.0494, -3.9883, -5.3075, -3.8140, -5.4412,\n",
      "        -5.3514, -4.9355], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3369, -5.0504, -4.5894, -5.0614, -4.5894, -5.3369, -4.0514, -5.3618,\n",
      "        -5.3369, -5.0614], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08122336119413376\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9611, -4.7697, -4.0307, -4.6161], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.8668, -4.0307, -3.4150, -5.0848, -0.6442, -5.3177, -4.0307, -2.0611,\n",
      "        -4.7180, -4.0307], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0748, -4.6276, -3.4150, -4.9794, -0.3839, -5.3610, -4.6276, -1.9704,\n",
      "        -4.7373, -4.6276], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12015960365533829\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9905, -4.7938, -4.0813, -4.6486], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.9905, -5.3319, -5.3460, -4.3619, -5.2747, -5.3319, -4.0813, -1.9711,\n",
      "        -4.0813, -5.3319], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1060, -5.3898, -5.4062, -4.5431, -5.3898, -5.3898, -4.6731, -1.9775,\n",
      "        -4.6731, -5.3898], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0773785263299942\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0378, -4.8266, -4.1282, -4.6918], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.1282, -4.2811, -4.1282, -4.6918, -5.5064, -4.1282, -5.0203, -5.3552,\n",
      "        -5.2955, -4.6918], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7154, -4.8530, -4.7154, -4.7154, -5.4299, -4.7154, -5.1207, -5.4108,\n",
      "        -5.4108, -4.7154], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13948430120944977\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0961, -4.8651, -4.1798, -4.7435], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.6394, -0.7089, -5.5187, -5.3849, -4.1798, -5.0346, -5.3849, -3.4985,\n",
      "        -5.3849, -4.1798], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3971, -1.9978, -5.4591, -5.4342, -4.7618, -5.1387, -5.4342, -3.4745,\n",
      "        -5.4342, -4.7618], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.24197249114513397\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.1476, -4.9036, -4.2245, -4.7816], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.3999, -5.1938, -5.0357, -4.7816, -5.3999, -5.3576, -5.1476, -5.3953,\n",
      "        -5.1986, -4.3602], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4485, -5.4354, -5.1492, -4.8020, -5.4485, -5.5200, -5.1492, -5.4354,\n",
      "        -5.5200, -4.8020], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04028499871492386\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.1975, -4.9380, -4.2188, -4.8257], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.2188, -5.0634, -4.9219, -4.8257, -4.8257, -5.1248, -4.5840, -4.3382,\n",
      "        -5.1975, -4.2188], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7969, -5.1256, -5.1256, -4.7969, -4.7969, -5.1256, -4.7969, -4.1536,\n",
      "        -5.1256, -4.7969], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08000743389129639\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.2357, -4.9600, -4.2205, -4.8613], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.2205, -3.5063, -5.4631, -4.2205, -4.2205, -4.1565, -5.0954, -5.4326,\n",
      "        -5.4815, -4.8613], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7984, -3.5303, -5.4777, -4.7984, -4.7984, -4.1557, -5.1179, -5.4299,\n",
      "        -5.4299, -4.7984], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10099951922893524\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.2775, -4.9867, -4.2355, -4.8970], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.4977, -5.1251, -5.5204, -4.9792, -4.2355, -4.5779, -4.7341, -5.1251,\n",
      "        -5.5204, -4.8970], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4385, -5.1201, -5.4385, -5.1144, -4.8119, -4.8119, -4.7377, -5.1201,\n",
      "        -5.4385, -4.8119], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.042961180210113525\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3088, -5.0082, -4.2567, -4.9181], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.1626, -1.0141, -5.2159, -4.2567, -4.2103, -4.5981, -4.7879, -5.5101,\n",
      "        -0.6356, -5.2895], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.1841, -0.4310, -5.1366, -4.8311, -4.1841, -4.8311, -4.6715, -5.4594,\n",
      "        -0.4310, -5.1366], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08129797130823135\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-4.6267, -5.6344, -4.9241, -4.5188, -5.5660, -4.2780, -5.1694, -4.6080,\n",
      "        -4.2780, -5.3749], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8502, -5.4843, -4.8502, -4.8502, -5.4843, -4.8502, -5.1641, -4.6781,\n",
      "        -4.8502, -5.5011], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08700709044933319\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3386, -5.0085, -4.2914, -4.9175], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.1699, -5.1302, -5.5857, -5.1940, -2.1848, -5.5857, -5.5857, -5.0085,\n",
      "        -4.9175, -4.9175], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7776, -5.1994, -5.5166, -5.1980, -3.5793, -5.5166, -5.5166, -4.9126,\n",
      "        -4.8623, -4.8623], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2132870852947235\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3423, -4.9983, -4.3139, -4.8702], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.5685, -4.7175, -5.5685, -5.1784, -5.5685, -5.1784, -5.3423, -5.1784,\n",
      "        -4.7604, -5.1784], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5607, -4.9575, -5.5607, -5.2429, -5.5607, -5.2429, -5.2429, -5.2429,\n",
      "        -4.8191, -5.2429], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.008777020499110222\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3377, -4.9883, -4.3279, -4.8307], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.5599, -5.5599, -5.1809,  0.5897, -5.1091, -5.5599, -5.5599, -4.3279,\n",
      "        -5.5599, -5.1572], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5982, -5.5982, -5.2840, 10.0000, -5.2840, -5.5982, -5.5982, -4.8951,\n",
      "        -5.5982, -4.8537], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.901655197143555\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3238, -4.9599, -4.3332, -4.7765], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.2788, -4.8033, -4.3332, -4.8033, -4.5472, -4.7745, -5.1618, -5.1672,\n",
      "        -4.7745, -5.5479], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3230, -5.0112, -4.8999, -5.0112, -4.5722, -4.8839, -5.3117, -5.3230,\n",
      "        -4.8839, -5.6274], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0487116277217865\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3151, -4.9468, -4.3289, -4.7402], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.2504, -5.3151, -5.2504, -4.3289, -5.2896, -5.5986, -3.4003, -5.5435,\n",
      "        -5.3151, -5.1622], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3285, -5.3285, -5.3285, -4.8960, -5.3485, -5.4939, -3.3508, -5.5602,\n",
      "        -5.3285, -5.3285], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03790280222892761\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3128, -4.9354, -4.3202, -4.7200], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.4973, -4.3202, -4.9354, -4.7200, -4.8172, -4.3202, -3.4973, -4.3202,\n",
      "        -4.3202, -4.3202], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3090, -4.8881, -5.0059, -4.8881, -4.8881, -4.8881, -3.3090, -4.8881,\n",
      "        -4.8881, -4.8881], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17221909761428833\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3262, -4.9307, -4.3266, -4.7281], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.6989, -4.9307, -5.3262, -4.2358, -4.8322, -5.2044, -5.4307, -5.2044,\n",
      "        -4.6683, -4.3266], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2850, -5.0039, -5.3490, -4.3290, -4.8940, -5.3490, -5.7148, -5.3490,\n",
      "        -4.4999, -4.8940], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06624209880828857\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3425, -4.9324, -4.3037, -4.7643], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.7677, -4.7643, -5.6391, -4.3037, -4.2268, -4.3037, -5.6391, -4.8043,\n",
      "        -5.2690, -3.3293], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9088, -4.8733, -5.6501, -4.8733, -4.3014, -4.8733, -5.6501, -4.9088,\n",
      "        -5.3327, -3.2949], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07026951014995575\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3707, -4.9458, -4.2792, -4.8107], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.6916, -4.2792, -4.2792, -4.8107, -3.6385, -4.2510, -5.6916, -5.3707,\n",
      "        -5.1400, -4.2792], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6260, -4.8513, -4.8513, -4.8513, -3.3098, -4.5601, -5.6260, -5.3110,\n",
      "        -5.3110, -4.8513], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12285002321004868\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3995, -4.9636, -4.2717, -4.8553], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.2717, -4.5733, -5.1869, -5.3995, -5.5003, -2.0686, -5.7337, -5.3434,\n",
      "        -5.3995, -4.8269], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8445, -4.8445, -5.3659, -5.3005, -5.3005, -1.7543, -5.4435, -5.3659,\n",
      "        -5.3005, -4.8701], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06786493957042694\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3821, -4.9694, -4.2586, -4.8893], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.3690, -4.9694, -4.2586, -4.7827, -4.2586, -4.2586, -5.3690, -5.4672,\n",
      "        -5.3690, -4.9391], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3465, -4.9078, -4.8328, -4.8328, -4.8328, -4.8328, -5.3465, -5.3044,\n",
      "        -5.3465, -4.8637], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10289038717746735\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3711, -4.9740, -4.2629, -4.9195], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.3711, -4.2629, -5.1985, -4.9488, -5.5241, -4.9740, -4.8006, -5.5241,\n",
      "        -4.8006, -5.2785], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3206, -4.8366, -5.0850, -4.8727, -5.3206, -4.9153, -4.8366, -5.3206,\n",
      "        -4.8366, -5.4540], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04700619727373123\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3556, -4.9819, -4.2779, -4.9284], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.8585, -4.9607, -0.8579, -5.8585, -5.2729, -5.5510, -5.5486, -5.5510,\n",
      "        -4.2779, -4.2779], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6589, -5.3566, -0.1961, -5.6589, -5.3566, -5.3490, -5.3490, -5.3490,\n",
      "        -4.8501, -4.8501], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14576756954193115\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3413, -4.9919, -4.3211, -4.9067], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.4803, -4.3211, -5.5295, -5.8335, -4.0151, -5.8335, -5.8335, -5.8335,\n",
      "        -5.5295, -4.9067], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4020, -4.8890, -5.4020, -5.7041, -4.9768, -5.7041, -5.7041, -5.7041,\n",
      "        -5.4020, -4.8890], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13533984124660492\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3315, -5.0035, -4.3641, -4.8928], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.5989, -5.8353, -4.3641, -5.3315, -4.3641, -5.5080, -4.8928, -4.8928,\n",
      "        -3.5204, -5.8030], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.7457, -5.7457, -4.9277, -5.4520, -4.9277, -5.4520, -4.9277, -4.9277,\n",
      "        -3.3705, -5.7457], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07106982916593552\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3344, -5.0184, -4.4110, -4.8973], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.7793, -4.4110, -3.9346, -5.0184, -4.6106, -5.7793, -4.4110, -3.5282,\n",
      "        -5.7793, -4.4573], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.7820, -4.9699, -4.3113, -5.0116, -4.9699, -5.7820, -4.9699, -3.3726,\n",
      "        -5.7820, -5.0116], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12271963059902191\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3467, -5.0238, -4.4388, -4.9101], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.7102, -5.4876, -4.1794, -5.4876, -5.3467, -4.6256, -4.2224, -5.4876,\n",
      "        -5.7740, -4.9101], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6904, -5.5203, -5.0311, -5.5203, -5.5203, -4.9949, -4.3179, -5.5203,\n",
      "        -5.8067, -4.9949], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09127464145421982\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-4.9070, -4.4203, -4.6760, -5.5287, -4.4203, -4.9594, -4.4203, -0.5540,\n",
      "        -4.5711, -4.4203], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0131, -4.9783, -4.9783, -5.5020, -4.9783, -4.9783, -4.9783, -0.1623,\n",
      "        -4.2968, -4.9783], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1577679067850113\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.8660, -5.4602, -4.9882, -5.5778], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.8844, -4.4060, -4.4060, -5.3130, -4.7483, -4.4060, -5.8533, -5.0138,\n",
      "        -4.9069, -5.1947], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7068, -4.9654, -4.9654, -5.4894, -4.9654, -4.9654, -5.7817, -4.9654,\n",
      "        -5.0038, -5.3476], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10888560116291046\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.4059, -5.0245, -4.3900, -5.0684], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.9308, -4.9695, -5.2321, -5.0684, -5.0684, -4.3900, -5.4059, -0.5400,\n",
      "        -5.4059, -5.9058], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.7102, -4.9510, -5.3199, -4.9510, -4.9510, -4.9510, -5.4725, -0.1675,\n",
      "        -5.4725, -5.7764], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05633504316210747\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-4.7546, -5.9345, -5.4298, -5.9345, -4.3942, -5.6468, -3.4298, -5.9345,\n",
      "        -5.4377, -5.6468], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9993, -5.7841, -5.4679, -5.7841, -4.9548, -5.4679, -3.6110, -5.7841,\n",
      "        -5.4679, -5.4679], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05413011461496353\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.9352, -5.4528, -4.9780, -5.6277], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-6.0308, -5.1185, -4.4213, -4.4213, -4.4213, -5.9264, -5.1185, -4.4213,\n",
      "        -0.8082, -5.9264], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.8078, -4.9792, -4.9792, -4.9792, -4.9792, -5.8078, -4.9792, -4.9792,\n",
      "        -1.7426, -5.8078], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.22344744205474854\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.5249, -5.0629, -4.4754, -5.0984], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.4754, -5.9173, -4.4754, -5.8864, -5.3962, -5.8864, -4.9348, -4.9391,\n",
      "        -5.5502, -5.5734], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0279, -5.8566, -5.0279, -5.8566, -5.5157, -5.8566, -5.0532, -5.0279,\n",
      "        -5.8275, -5.5157], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07323391735553741\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.5766, -5.0879, -4.5228, -5.0829], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.8554, -4.1895, -5.8554, -4.5228, -3.5318, -4.9433, -4.5228, -1.8693,\n",
      "        -4.5228, -4.5228], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.9019, -4.3760, -5.9019, -5.0705, -3.6208, -5.0705, -5.0705, -1.6685,\n",
      "        -5.0705, -5.0705], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1303454041481018\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.6386, -5.1169, -4.5694, -5.0822], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.5694, -4.5694, -5.8013, -4.5694, -5.4862, -4.9641, -4.6222, -4.5694,\n",
      "        -3.5697, -4.5694], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1124, -5.1124, -5.8986, -5.1124, -5.5670, -5.1124, -5.1599, -5.1124,\n",
      "        -3.6282, -5.1124], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1805230975151062\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.7060, -5.1564, -4.6231, -5.0964], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.8999, -5.8408, -4.6231, -4.6231, -4.9325, -4.6231, -3.8162, -4.6231,\n",
      "        -4.6231, -5.9393], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.9331, -5.9911, -5.1608, -5.1608, -4.4346, -5.1608, -3.6421, -5.1608,\n",
      "        -5.1608, -5.9911], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17500841617584229\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.7795, -5.1803, -4.6899, -5.1305], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.4659, -4.9371, -5.8545, -4.9965, -4.6899, -5.8545, -3.8647, -4.6899,\n",
      "        -4.6899, -2.9650], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6342, -4.4782, -6.0418, -5.1868, -5.2209, -6.0418, -3.6685, -5.2209,\n",
      "        -5.2209, -1.6178], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3044494390487671\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.8432, -5.1869, -4.7375, -5.1627], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.4662, -5.1869, -1.8961, -4.7375, -4.9828, -4.7375, -5.1627, -5.8430,\n",
      "        -5.3300, -5.4662], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6522, -5.3038, -1.6447, -5.2638, -5.2078, -5.2638, -5.2638, -5.9897,\n",
      "        -5.6522, -5.6522], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08860567957162857\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.9115, -5.2193, -4.7541, -5.2270], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.0443, -4.7541, -5.2270, -4.7541, -5.0231, -4.7541, -4.7541, -5.6400,\n",
      "        -5.1457, -5.1547], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.6819, -5.2787, -5.2787, -5.2787, -5.4000, -5.2787, -5.2787, -5.6312,\n",
      "        -5.2787, -5.2787], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14099036157131195\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.9728, -5.2369, -4.7862, -5.2983], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.2369, -6.3085, -3.6559, -5.6611, -5.5389, -5.5389, -5.2306, -4.7862,\n",
      "        -5.9864, -3.6215], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3219, -6.0950, -3.6620, -5.6332, -5.6332, -5.6332, -5.3076, -5.3076,\n",
      "        -6.0950, -3.6620], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.036261700093746185\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.0192, -5.2583, -4.8129, -5.3745], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.8129, -6.0481, -4.9886, -5.8861, -6.0192, -6.0192, -4.8475, -4.8129,\n",
      "        -4.8129, -4.3618], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3316, -6.1063, -5.2059, -5.9576, -5.6331, -5.6331, -4.5452, -5.3316,\n",
      "        -5.3316, -4.5452], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12862160801887512\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.9890, -5.2619, -4.8341, -5.4144], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.6711, -5.6711, -5.9076, -5.6711, -6.1096, -4.8341, -6.1096, -6.1096,\n",
      "        -4.8341, -5.7126], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6709, -5.6709, -5.9689, -5.6709, -6.1396, -5.3507, -6.1396, -6.1396,\n",
      "        -5.3507, -5.5588], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05638350173830986\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.9676, -5.2730, -4.8617, -5.4538], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.6663, -4.8617, -4.4168, -5.9676, -5.4538, -4.2910, -6.1387, -4.2910,\n",
      "        -5.9676, -5.9676], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.7103, -5.3755, -4.6184, -5.7103, -5.3755, -4.6184, -5.8266, -4.6184,\n",
      "        -5.7103, -5.7103], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08233408629894257\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-5.9698, -4.8736, -5.1471, -5.3392, -5.8786, -4.8736, -4.8736, -5.8221,\n",
      "        -6.2220,  0.8911], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.0070, -5.3863, -5.3241, -5.5319, -5.7757, -5.3863, -5.3863, -5.7757,\n",
      "        -6.2240, 10.0000], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.384260177612305\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.7512, -5.1814, -4.9701, -5.5310], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.8786, -5.8694, -5.0569, -6.2470, -6.2470, -6.2470, -5.4227, -5.0569,\n",
      "        -4.8786, -4.8786], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3908, -5.8327, -5.3007, -6.2589, -6.2589, -6.2589, -5.3908, -5.3007,\n",
      "        -5.3908, -5.3908], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0908561497926712\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.7100, -5.2382, -5.0004, -5.5538], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-6.2718, -5.7802, -5.9123, -5.7788, -4.8837, -4.8837, -6.1121, -5.8385,\n",
      "        -5.1066, -3.6838], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.2858, -5.8804, -5.8804, -6.0245, -5.3954, -5.3954, -6.1453, -5.8886,\n",
      "        -5.3154, -3.7153], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06433393806219101\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.6900, -5.3055, -5.0154, -5.5799], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-6.3007, -5.3370, -5.9528, -5.3370, -5.6569, -4.8794, -6.3007, -6.1358,\n",
      "        -5.9528, -5.6569], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.2979, -5.5139, -5.9089, -5.5139, -5.9089, -5.3914, -6.2979, -6.0080,\n",
      "        -5.9089, -5.9089], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04719867557287216\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-5.7473, -3.7643, -5.3510, -4.7829, -5.9679, -5.4123, -6.0450, -5.4123,\n",
      "        -5.9679, -5.8327], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.1385, -3.7082, -5.3046, -4.6438, -5.9061, -5.4002, -5.9820, -5.4002,\n",
      "        -5.9061, -5.7498], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.019646454602479935\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.8271, -5.6994, -5.2873, -5.9093], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.8946, -6.3305, -4.8946, -4.9929, -5.1703, -5.4413, -4.8946, -6.1372,\n",
      "        -5.9707, -5.7252], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4051, -6.2751, -5.4051, -5.4306, -5.4878, -5.4051, -5.4051, -6.2751,\n",
      "        -5.8972, -5.7586], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11042870581150055\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.6812, -5.4701, -4.9739, -5.5992], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.3425, -6.3529, -3.8384, -4.9160, -5.9817, -6.3529, -5.5992, -4.9160,\n",
      "        -4.9160, -4.1882], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2809, -6.2753, -3.7052, -5.4244, -5.8982, -6.2753, -5.4244, -5.4244,\n",
      "        -5.4244, -4.6292], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10410115867853165\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.6922, -5.4915, -4.9669, -5.5858], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-6.3611, -5.4930, -3.8728, -4.9669, -5.9811, -5.6562, -6.3611, -4.9669,\n",
      "        -5.3763, -5.9811], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.2835, -5.4604, -3.7179, -5.4702, -5.9088, -5.5118, -6.2835, -5.4702,\n",
      "        -5.2821, -5.9088], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05839567258954048\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.7061, -5.5055, -4.9795, -5.5711], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.9696, -5.0076, -5.3984, -3.8973, -5.0076, -5.9696, -6.3583, -5.4805,\n",
      "        -6.3583, -5.5055], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.9324, -5.5068, -5.2973, -3.7273, -5.5068, -5.9324, -6.3054, -5.5068,\n",
      "        -6.3054, -5.4693], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05479755252599716\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.7190, -5.5076, -5.0030, -5.5559], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-6.3523, -5.5183, -5.0697, -6.3523, -6.2274, -5.9573, -6.3523, -5.4281,\n",
      "        -5.9573, -5.0697], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.3372, -5.5628, -5.5628, -6.3372, -6.0391, -5.9665, -6.3372, -5.5027,\n",
      "        -5.9665, -5.5628], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05299835279583931\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.7309, -5.5101, -5.0350, -5.5345], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-6.1282, -5.9429, -5.0399, -5.5345, -4.9624, -4.3324, -5.7572, -5.4834,\n",
      "        -5.1413, -5.1413], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.3793, -6.0117, -5.3559, -5.6272, -4.6677, -4.6677, -5.5315, -5.9288,\n",
      "        -5.6272, -5.6272], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10970108211040497\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.8113, -5.4989, -5.0491, -5.5473], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-6.3435, -5.9879,  1.1439, -0.8292, -4.7045, -5.1756, -5.1756, -6.3198,\n",
      "        -3.8464, -2.1333], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.3850, -6.1298, 10.0000, -1.8580, -4.6789, -5.6581, -5.6581, -6.3850,\n",
      "        -3.7719, -1.8580], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.006291389465332\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.8706, -5.4899, -5.0443, -5.5171], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.8706, -5.1945, -6.0991, -5.6054, -5.6054, -5.5297, -6.0371, -6.3085,\n",
      "        -5.9335, -5.1945], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.9768, -5.6751, -6.0321, -5.6500, -5.6500, -5.5399, -5.9768, -6.3703,\n",
      "        -5.6500, -5.6751], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05695149302482605\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.9326, -5.4689, -5.0522, -5.5000], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-6.2867, -6.2867, -5.7682, -5.9061, -5.5738, -6.3368, -5.6035, -4.8672,\n",
      "        -5.4011, -6.2748], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.3657, -6.3657, -5.5470, -6.0431, -5.6994, -6.1685, -5.6994, -4.6775,\n",
      "        -5.3804, -6.1685], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.018117595463991165\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-5.3607, -5.9571, -6.4520, -5.9107, -5.9536, -6.2761, -5.2381, -5.9107,\n",
      "        -5.2381, -5.9107], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3848, -6.0691, -6.3582, -6.0579, -6.0579, -6.3582, -5.7143, -6.0579,\n",
      "        -5.7143, -6.0579], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.055810779333114624\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.2855, -5.7036, -5.2433, -5.6028], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.5998, -6.2855, -5.2433, -5.2433, -6.2912, -5.9656, -6.2855, -6.2668,\n",
      "        -5.3265, -5.9841], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6889, -6.0594, -5.7190, -5.7190, -6.3432, -6.0398, -6.0594, -6.3432,\n",
      "        -5.3783, -6.2227], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0636376440525055\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-6.4956, -5.3403, -6.2920, -6.0060, -5.6395, -6.3162, -5.6395, -5.6395,\n",
      "        -6.3162, -3.8981], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.3307, -5.3752, -6.0756, -6.0756, -5.7132, -6.3307, -5.7132, -5.7132,\n",
      "        -6.3307, -3.7902], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0108501510694623\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.2588, -5.6233, -5.2249, -5.6230], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.9211, -5.5991, -6.3425, -2.0842, -6.3425, -3.8859, -5.2249, -3.9211,\n",
      "        -5.6792, -5.6230], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.8151, -5.7024, -6.3281, -1.6358, -6.3281, -3.8151, -5.7024, -3.8151,\n",
      "        -5.7024, -5.7024], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04744568094611168\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.2179, -5.5742, -5.2022, -5.6406], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-6.4702, -3.9236, -5.2022, -5.2022, -6.1028, -5.7138, -6.4702, -5.2022,\n",
      "        -5.7051, -0.5122], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.3170, -3.8536, -5.6820, -5.6820, -6.2781, -5.7099, -6.3170, -5.6820,\n",
      "        -5.6820,  0.2520], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13576549291610718\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.1879, -5.5447, -5.1921, -5.6566], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.7018, -5.2326, -6.4263, -6.2485, -2.0433, -5.2326, -5.1921, -5.6566,\n",
      "        -3.8614, -5.1921], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6729, -5.3757, -6.3294, -6.1605, -1.6298, -5.3757, -5.6729, -5.6729,\n",
      "        -3.8862, -5.6729], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06931183487176895\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.1584, -5.5375, -5.1896, -5.6738], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-6.7110, -5.6738, -1.8807, -6.7110, -6.4630, -5.1896, -5.1896, -6.1584,\n",
      "        -5.1896, -0.7011], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.3418, -5.6706, -1.6310, -6.3418, -6.3418, -5.6706, -5.6706, -6.1856,\n",
      "        -5.6706,  0.2784], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.20040766894817352\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.0790, -5.5263, -5.2094, -5.6725], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.6725, -4.5462, -6.3024, -5.2279, -5.2094, -5.6824, -5.2279, -5.5263,\n",
      "        -5.2094, -5.2279], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6885, -4.7263, -6.2013, -5.3938, -5.6885, -5.7504, -5.3938, -5.6155,\n",
      "        -5.6885, -5.3938], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.059699177742004395\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.0164, -5.5642, -5.2184, -5.6760], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.9757, -4.1424, -5.2184, -6.5228, -6.5346, -5.2662, -6.4855, -6.4855,\n",
      "        -6.5228, -5.7773], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.9930, -3.9232, -5.6966, -6.4256, -6.4256, -5.3939, -6.4256, -6.4256,\n",
      "        -6.4256, -5.6966], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03378145396709442\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.9565, -5.6084, -5.2341, -5.6774], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.4493, -4.1370, -0.9065, -5.6774, -6.2621, -5.7315, -5.2341, -6.5438,\n",
      "        -5.8175, -5.2341], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.3040, -3.9304, -1.4927, -5.7107, -6.1961, -5.7818, -5.7107, -6.4643,\n",
      "        -6.0179, -5.7107], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1462630331516266\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.9140, -5.6523, -5.2386, -5.6760], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.4433, -5.2386, -5.3428, -6.0217, -6.2675, -6.5496, -5.2386, -5.6760,\n",
      "        -5.2386, -5.7017], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1686, -5.7148, -5.3836, -5.9939, -6.4851, -6.4851, -5.7148, -5.7148,\n",
      "        -5.7148, -5.7595], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08144260942935944\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.9023, -5.7000, -5.2484, -5.6835], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-6.5575, -1.6509, -5.6835, -5.9023, -5.2484, -5.2484, -5.3776, -0.4096,\n",
      "        -5.6835, -4.6669], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.5043, -1.3733, -5.7235, -6.1627, -5.7235, -5.7235, -5.3718,  0.3149,\n",
      "        -5.7235, -4.6788], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11275939643383026\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-4.6562, -5.2603, -6.1299, -5.8003, -6.1651, -6.1651, -5.7165, -6.1651,\n",
      "        -5.8003, -5.7165], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6529, -5.7342, -6.1311, -5.7107, -6.1311, -6.1311, -5.7342, -6.1311,\n",
      "        -5.7107, -5.7342], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02448219619691372\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.9479, -5.8004, -5.2839, -5.7482], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.7907, -4.0397, -6.1424, -5.2839, -5.0607, -6.5789, -6.5789, -5.6800,\n",
      "        -5.2839, -5.2839], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.9723, -3.9773, -6.1120, -5.7555, -5.5546, -6.5282, -6.5282, -5.7555,\n",
      "        -5.7555, -5.7555], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09598396718502045\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.9771, -5.8451, -5.3245, -5.7828], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-6.5862, -5.7828, -5.8518, -6.5862, -5.0646, -6.1094, -6.5862, -4.0413,\n",
      "        -5.3245, -4.0624], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.5576, -5.7921, -5.9215, -6.5576, -5.5582, -6.1137, -6.5576, -3.9936,\n",
      "        -5.7921, -3.9936], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.047658082097768784\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.0074, -5.8860, -5.3674, -5.8211], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.3674, -6.5951, -6.5345, -5.8211, -6.5951, -4.8464, -6.1005, -5.3674,\n",
      "        -5.3674, -6.5345], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.8307, -6.5877, -6.2421, -5.8307, -6.5877, -4.6378, -6.1190, -5.8307,\n",
      "        -5.8307, -6.2421], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08587954938411713\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.0440, -5.8979, -5.4379, -5.8656], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.0803, -5.4379, -6.0918, -5.4379, -0.9526, -5.4379, -6.0440, -5.2902,\n",
      "        -5.7195, -0.2499], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.0375, -5.8941, -6.1475, -5.8941, -1.2249, -5.8941, -6.1475, -5.3848,\n",
      "        -5.8941,  0.3151], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10727627575397491\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.0834, -5.9078, -5.5032, -5.9018], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.1926, -3.3893, -4.0960, -4.8872, -5.1326, -5.9078, -4.6418, -6.5541,\n",
      "        -6.5541, -5.7487], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.3151, -1.1734, -4.0504, -4.6864, -5.6194, -5.6194, -4.6864, -6.6417,\n",
      "        -6.6417, -5.9529], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5589739084243774\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.0743, -5.8669, -5.5213, -5.8566], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-6.4889, -5.5213, -6.0193, -4.1058, -5.5213, -1.4036, -5.5998, -5.4412,\n",
      "        -6.4889, -5.5998], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.6469, -5.9691, -6.1834, -3.9336, -5.9691, -1.1587, -5.9691, -5.2158,\n",
      "        -6.6469, -5.9691], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08913106471300125\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.0261, -5.8069, -5.4807, -5.8377], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-6.3495, -5.8069, -6.0261, -6.4833, -5.4807, -6.0201, -5.4807, -5.1586,\n",
      "        -5.7350, -6.0201], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.3874, -5.6428, -6.1615, -6.6118, -5.9326, -6.1615, -5.9326, -5.6428,\n",
      "        -5.9326, -6.1615], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07851653546094894\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.9851, -5.7452, -5.4403, -5.8348], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.7280, -5.4402, -6.0472, -5.8416, -5.1741, -5.4402, -5.7280, -5.7186,\n",
      "        -6.5008, -6.0516], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.8962, -5.8962, -6.1467, -6.3813, -5.6567, -5.8962, -5.8962, -5.8962,\n",
      "        -6.5878, -6.1467], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10546629130840302\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.9320, -5.6771, -5.3618, -5.8565], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-6.5598, -6.1316, -5.8565, -0.2113, -5.9320, -6.5598, -5.3618, -3.7164,\n",
      "        -5.3618, -5.6792], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.5608, -6.1113, -5.8256,  0.3001, -6.1113, -6.5608, -5.8256, -3.8103,\n",
      "        -5.8256, -5.8256], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07555653154850006\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.9042, -5.6227, -5.2999, -5.8821], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.2999, -5.2999, -5.2320, -6.2031, -5.2999, -3.7425, -5.9042, -6.2031,\n",
      "        -6.2031, -5.2999], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.7699, -5.7699, -5.2255, -6.0821, -5.7699, -3.7986, -6.0821, -6.0821,\n",
      "        -6.0821, -5.7699], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09624052792787552\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-5.2751, -6.6508, -6.0785, -5.4845, -5.4253, -6.2383, -3.9733, -6.6508,\n",
      "        -5.9112, -6.1912], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.7476, -6.5085, -5.7476, -5.6819, -5.0872, -6.0633, -3.7803, -6.5085,\n",
      "        -5.7476, -5.9360], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06862679868936539\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.1555, -5.7818, -5.6431, -6.2248], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.6713, -5.8544, -3.5821, -4.4182, -6.2248, -5.2997, -5.6431, -6.6404,\n",
      "        -5.5180, -5.2997], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7470, -5.9748, -3.7470, -4.5817, -6.0788, -5.7698, -5.7698, -6.5275,\n",
      "        -5.7174, -5.7698], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06058571860194206\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.9445, -5.5811, -5.3264, -5.8975], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.8975, -5.3264, -5.3264,  1.4234, -6.6165, -5.5447, -5.3264, -5.3264,\n",
      "        -5.3264, -4.4163], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.7938, -5.7938, -5.7938, 10.0000, -6.5486, -5.7316, -5.7938, -5.7938,\n",
      "        -5.7938, -4.5888], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.472995758056641\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.9767, -5.6215, -5.3652, -5.8709], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-6.1555, -5.9866, -6.5828, -5.6059, -5.3652, -3.6763, -4.6102, -5.2676,\n",
      "        -5.3652, -3.6763], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.1266, -5.9835, -6.5727, -5.7598, -5.8287, -3.6434, -4.5978, -5.2286,\n",
      "        -5.8287, -3.6434], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04580948129296303\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.0116, -5.6604, -5.4109, -5.8528], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.4109, -5.7138, -5.4109, -5.8528, -6.1170, -6.5544, -5.4109, -5.4109,\n",
      "        -5.1375, -5.4109], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.8698, -5.8306, -5.8698, -5.8698, -6.1570, -6.5904, -5.8698, -5.8698,\n",
      "        -5.2464, -5.8698], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10816758871078491\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.0577, -5.7174, -5.4660, -5.8541], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.2220, -5.8541, -6.0913, -4.0305, -5.4660, -5.3092, -6.2298, -5.4660,\n",
      "        -6.2624, -5.4660], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.3719, -5.9194, -6.1893, -3.5457, -5.9194, -5.7680, -6.1893, -5.9194,\n",
      "        -6.1893, -5.9194], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14358840882778168\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.1279, -5.7697, -5.4895, -5.8691], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.2129, -6.0001, -4.5003, -5.4895, -6.5209, -6.5209, -3.5718, -5.4895,\n",
      "        -5.4895, -6.5209], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2688, -5.9405, -4.5956, -5.9405, -6.6279, -6.6279, -3.5267, -5.9405,\n",
      "        -5.9405, -6.6279], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06624852120876312\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.2023, -5.8327, -5.5115, -5.9022], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.2591, -5.5115, -3.7602, -6.3198, -5.7612, -5.5115, -5.7209, -5.2591,\n",
      "        -6.5250, -6.3198], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2650, -5.9603, -3.5174, -6.6148, -5.9603, -5.9603, -5.8036, -5.2650,\n",
      "        -6.6148, -6.6148], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06906719505786896\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.2710, -5.8797, -5.5125, -5.9630], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.5125, -6.4077, -0.4212, -3.5420, -6.5398, -6.2710, -4.8855, -4.2367,\n",
      "        -6.1300, -5.5125], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.9613, -6.5662,  0.4317, -3.5360, -6.5662, -6.1720, -5.3421, -4.5244,\n",
      "        -6.1720, -5.9613], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14587634801864624\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.3398, -5.9026, -5.5263, -6.0344], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.7144, -6.3470, -6.5297, -3.7879, -6.5615, -3.5455, -5.5263, -5.9026,\n",
      "        -3.5456, -3.8775], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4897, -6.5283, -6.3524, -3.5753, -6.5283, -3.5753, -5.9736, -5.7614,\n",
      "        -3.5753, -3.5753], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04742081090807915\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.3984, -5.9022, -5.5338, -6.1057], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.9022, -5.5338, -6.3984, -6.5890, -5.5338, -6.2664, -5.5338, -6.6362,\n",
      "        -5.7050, -5.5338], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.7504, -5.9804, -6.1758, -6.4917, -5.9804, -6.1758, -5.9804, -6.4917,\n",
      "        -5.7504, -5.9804], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09110825508832932\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.4311, -5.8889, -5.5693, -6.1526], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-6.6091, -2.9782, -6.5428, -6.6091, -5.8889, -6.3268, -5.8723, -6.4311,\n",
      "        -6.0652, -6.6586], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.4976, -1.1344, -6.4976, -6.4976, -5.7763, -6.2160, -5.5469, -6.2160,\n",
      "        -6.1675, -6.4148], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.36735832691192627\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.3898, -5.8265, -5.6108, -6.1246], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.7801, -5.8031, -6.5793, -5.2311, -5.6108, -5.8665, -6.5793, -5.2311,\n",
      "        -6.1405, -4.3764], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.1603, -5.8273, -6.5265, -5.2816, -6.0498, -6.0498, -6.5265, -5.2816,\n",
      "        -6.4680, -4.4512], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04949180409312248\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.3492, -5.7816, -5.6287, -6.1053], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-6.3568, -6.1129, -5.5018, -5.6287, -6.1053, -5.7816, -5.6287, -6.3568,\n",
      "        -5.6287, -5.5341], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.3316, -6.4361, -5.2644, -6.0658, -6.0658, -5.8599, -6.0658, -6.3316,\n",
      "        -6.0658, -5.6339], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07529316842556\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-5.1956, -5.6577, -5.4283, -5.6577, -6.5377, -6.5878, -0.0833, -6.7284,\n",
      "        -5.9814, -6.5878], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3377, -6.0919, -5.8854, -6.0919, -6.4966, -6.4966,  0.4707, -6.4966,\n",
      "        -6.0919, -6.4966], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09976162016391754\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-6.3871, -6.5897, -5.6836, -5.4562, -5.6836, -6.1451, -5.7226, -0.3259,\n",
      "        -6.5448, -6.3871], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.3984, -6.5306, -6.1153, -5.9106, -6.1153, -6.3973, -5.6983,  0.4727,\n",
      "        -6.5306, -6.3984], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12849922478199005\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.2861, -5.7682, -5.7111, -6.0438], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-6.3798, -3.5374, -5.7111, -6.5854, -5.9747, -5.7111, -5.7111, -6.3798,\n",
      "        -6.0438, -3.5899], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.4240, -3.4070, -6.1400, -6.5682, -5.9341, -6.1400, -6.1400, -6.4240,\n",
      "        -6.1400, -3.4070], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06174200773239136\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.2763, -5.7708, -5.7433, -6.0429], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.7433, -5.7433, -6.5956, -6.1295, -6.1295, -2.6370, -5.7433, -5.7433,\n",
      "        -6.5472, -6.3946], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.1690, -6.1690, -6.5941, -6.1690, -6.1690, -1.2913, -6.1690, -6.1690,\n",
      "        -6.5941, -6.4387], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2543005645275116\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-6.2672, -5.5351, -5.7734, -6.0414, -6.5894, -5.3086, -4.7065, -5.7734,\n",
      "        -5.7677, -6.0256], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.4469, -5.9442, -6.1909, -5.9442, -6.6107, -5.9442, -5.4322, -6.1909,\n",
      "        -5.9442, -6.1909], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15473683178424835\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.3485, -5.6805, -5.8633, -6.0895], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.8633, -5.8633, -6.4332, -6.6256, -6.4332, -5.6805, -5.7877, -6.6256,\n",
      "        -6.1288, -6.3243], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.1125, -6.1125, -6.3538, -6.5331, -6.3538, -5.8326, -5.8199, -6.5331,\n",
      "        -6.1125, -6.2103], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01913592591881752\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.1606, -5.2562, -5.6297, -6.1205], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-3.3102, -5.9803, -6.4476, -6.4355, -6.4355, -6.0591, -5.6113, -3.9316,\n",
      "        -6.6376, -4.1413], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.1851, -6.0502, -6.2718, -6.2718, -6.2718, -6.1274, -5.7306, -4.5691,\n",
      "        -6.4646, -4.5691], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07433632761240005\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.9611, -4.5948, -5.4558, -5.5239], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.0485, -6.5036, -6.0485, -6.0485, -6.0485, -6.3864, -6.4500, -6.4500,\n",
      "        -6.6308, -6.0485], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.0262, -6.2457, -6.0262, -6.0262, -6.0262, -6.0262, -6.2457, -6.2457,\n",
      "        -6.4412, -6.0262], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.031809061765670776\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8692, -4.1206, -3.9565, -4.8204], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-6.0776, -6.3885, -6.3885, -6.0776, -6.0393, -6.0776, -6.3885, -5.9876,\n",
      "        -6.0776, -5.6267], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.0644, -6.2765, -6.2765, -6.0644, -6.0644, -6.0644, -6.2765, -5.8796,\n",
      "        -6.0644, -5.6806], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.005357490852475166\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.9290, -3.2909, -3.4373, -2.2962], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-6.5335, -5.6106, -6.0995, -6.3711, -6.4821, -6.3036, -6.0995, -4.9988,\n",
      "        -5.7373, -6.5335], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.5301, -5.7059, -6.1173, -6.1173, -6.1177, -6.3219, -6.1173, -5.4390,\n",
      "        -5.7731, -6.5302], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.040238115936517715\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.7291, -2.1563, -1.5672, -0.5387], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-6.1065, -5.5708, -5.5708, -3.3304, -5.0633, -6.2084, -5.5708, -6.1016,\n",
      "        -3.9026, -3.9026], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.2111, -5.7668, -5.7668, -3.0176, -5.3925, -6.3955, -5.7668, -6.2111,\n",
      "        -3.0176, -3.0176], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.19462451338768005\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3262, -0.9836, -0.0187,  1.6186], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-6.0718, -6.0718, -6.4729, -2.3064, -6.2776, -6.4641, -6.4729, -6.0718,\n",
      "        -5.0714, -6.4729], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.2519, -6.2519, -6.6498, -1.5097, -6.2310, -6.6498, -6.6498, -6.2519,\n",
      "        -5.3118, -6.6498], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09204081445932388\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-6.4599, -6.2601, -6.5019, -6.4949], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.0674, -5.3990, -6.5484, -6.2601, -5.4632, -6.1918, -5.8254, -6.0674,\n",
      "        -6.0674, -6.1785], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.2429, -5.6948, -6.3840, -6.2086, -5.7453, -6.2429, -5.7453, -6.2429,\n",
      "        -6.2429, -6.3840], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03403762727975845\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-6.4224, -5.7097, -6.1227, -6.3891], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.2036, -6.1917, -4.9654, -6.1875, -6.1046, -6.5297, -6.1046, -2.3743,\n",
      "        -6.5297, -6.5297], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.3107, -6.1787, -5.2411, -6.1387, -6.1787, -6.5688, -6.1787, -1.6109,\n",
      "        -6.5688, -6.5688], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06883098185062408\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-6.1154, -5.3579, -5.6972, -6.1532], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.1330, -6.3058, -6.3058, -6.1330, -6.1330, -6.1330, -6.1330, -6.1330,\n",
      "        -5.4733, -6.1330], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.1133, -6.1133, -6.1133, -6.1133, -6.1133, -6.1133, -6.1133, -6.1133,\n",
      "        -5.5978, -6.1133], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.009239166975021362\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-5.7641, -4.9798, -5.3332, -5.9212], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.4737, -3.8361, -6.1467, -4.6447, -3.4853, -6.5740, -5.6338, -6.5740,\n",
      "        -6.1467, -6.0064], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5498, -4.1368, -6.0704, -4.1368, -3.1247, -6.4599, -5.5498, -6.4599,\n",
      "        -6.0704, -5.9892], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.052925385534763336\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-5.7459, -4.9198, -5.3000, -5.9134], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.7459, -3.3554, -6.0262, -4.8130, -6.1406, -6.0181, -6.1406, -5.5350,\n",
      "        -6.1406, -6.2216], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4278, -3.1243, -6.0245, -5.1839, -6.0245, -5.9815, -6.0245, -5.7214,\n",
      "        -6.0245, -6.1446], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03746415302157402\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-5.6903, -4.9142, -5.2215, -5.8883], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.0875, -6.2070, -3.8812, -3.8812, -6.0875, -6.3122, -4.5680, -3.3111,\n",
      "        -6.2070, -6.0440], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.0294, -6.1507, -4.0129, -4.0129, -6.0294, -6.0294, -4.0129, -3.1201,\n",
      "        -6.1507, -5.7167], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05795425921678543\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-5.6202, -4.9130, -5.1390, -5.8482], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.0125, -6.5399, -3.7411, -6.1720, -5.4913, -6.2778, -6.5399, -6.0215,\n",
      "        -3.2667, -6.0263], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.0384, -6.4363, -3.9523, -6.1622, -5.6157, -6.1622, -6.4363, -6.0384,\n",
      "        -3.1136, -6.0384], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.011958264745771885\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-5.5556, -4.9335, -5.0559, -5.8019], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.9528, -5.2597, -5.9528, -4.3058, -6.5027, -5.9579, -6.5027, -5.9528,\n",
      "        -3.8255, -5.9579], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.0658, -5.4402, -6.0658, -5.0141, -6.4671, -6.0658, -6.4671, -6.0658,\n",
      "        -3.8919, -6.0658], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06027306243777275\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-3.1829, -5.8954, -3.1829, -6.0834, -6.4694, -5.8954, -6.1128, -4.3727,\n",
      "        -6.1128, -6.4694], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.1009, -6.0671, -3.1009, -6.0661, -6.4750, -6.0671, -6.2137, -3.8586,\n",
      "        -6.2137, -6.4750], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.035739801824092865\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-5.8533, -3.7641, -5.8533,  0.0633, -5.7813, -6.4486, -4.7967, -5.8533,\n",
      "        -5.8533, -5.2445], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.0425, -3.8435, -6.0425,  0.4160, -5.9463, -6.4543, -4.8447, -6.0425,\n",
      "        -6.0425, -5.4010], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03280653804540634\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-5.4840, -4.8271, -4.8648, -5.6972], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.1915, -3.1410, -5.8634, -5.1915, -5.4411, -4.7263, -5.8634, -6.4505,\n",
      "        -5.8634, -6.1273], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4830, -3.1195, -5.9982, -5.4830, -5.5686, -4.7871, -5.9982, -6.4118,\n",
      "        -5.9982, -6.1736], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02485920488834381\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-5.9265, -6.1992, -6.4339, -2.3757, -5.9265, -5.9265, -6.4339, -6.1516,\n",
      "        -5.9265, -3.7082], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.9231, -5.9231, -6.3399, -1.9763, -5.9231, -5.9231, -6.3399, -6.1103,\n",
      "        -5.9231, -3.8701], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02814057469367981\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-5.9014, -5.0938, -5.3574, -5.9979], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.5732, -4.1632, -6.1427, -5.6679, -6.4038, -4.4075, -6.2222, -5.4228,\n",
      "        -5.6448, -4.5732], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7468, -3.8867, -6.0801, -5.5844, -6.3012, -4.7468, -6.0801, -5.3733,\n",
      "        -5.5844, -4.7468], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02995908260345459\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-5.4362, -4.6576, -4.8333, -5.5879], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.3544, -2.9988, -3.1486, -4.2180, -6.1214, -4.5324, -6.1477, -5.9681,\n",
      "        -2.9988, -5.9737], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.2910, -3.1018, -3.1018, -3.8986, -6.0868, -4.6791, -5.8717, -5.8717,\n",
      "        -3.1018, -5.8717], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.024805430322885513\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-5.4152, -4.6582, -4.7846, -5.5153], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.6472, -5.6793, -6.3079, -6.0074, -5.9606, -6.0733, -4.9613, -6.2886,\n",
      "        -5.9221, -5.8896], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6142, -5.7876, -6.3006, -6.1113, -5.8848, -6.1113, -4.7859, -6.3006,\n",
      "        -5.8848, -5.9264], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.006452202796936035\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-5.3931, -4.6666, -4.7294, -5.4531], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.0371, -3.0047, -3.0047, -5.6675, -5.9361, -5.9086, -5.9361, -6.2628,\n",
      "        -5.4495, -5.9361], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.9045, -3.0327, -3.0327, -5.8068, -5.9045, -5.9498, -5.9045, -6.3178,\n",
      "        -5.4020, -5.9045], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.004849078133702278\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-5.3731, -4.6766, -4.6759, -5.3984], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.5667, -5.9037, -5.9037, -5.2014, -5.9996, -5.9037, -5.8762, -5.9834,\n",
      "        -5.9037, -0.3590], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4765, -5.9229, -5.9229, -5.4229, -6.1711, -5.9229, -5.9745, -5.9229,\n",
      "        -5.9229,  0.4003], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06778918206691742\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8737, -4.4722, -3.8149, -4.6527], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.8906, -5.6134, -5.9882, -5.8935, -5.8906, -0.8909, -5.1994, -5.8935,\n",
      "        -3.8792, -6.4322], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.9173, -5.8157, -6.1763, -5.9173, -5.9173, -2.0562, -5.4212, -5.9173,\n",
      "        -3.9280, -6.1763], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1553703397512436\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-5.3195, -5.8924, -5.8768, -5.9802, -6.1106, -5.4251, -5.8768, -4.4322,\n",
      "        -5.8768, -5.8768], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6687, -5.8826, -5.8826, -6.1670, -6.3008, -5.3975, -5.8826, -4.3929,\n",
      "        -5.8826, -5.8826], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.019556868821382523\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5841, -3.8537, -3.2704, -4.3848], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.8666, -6.1034, -6.0103, -3.2704, -4.8275, -2.9538, -3.8537, -6.1034,\n",
      "        -5.8666, -6.1738], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.8180, -6.2435, -6.1254, -2.9521, -4.6587, -2.9521, -3.9433, -6.2435,\n",
      "        -5.8180, -6.2435], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.019996119663119316\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.7321, -2.9184, -2.9945, -2.1879], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.2719, -5.8424, -5.8424, -2.9945, -6.1220, -5.8691, -5.8424, -5.1654,\n",
      "        -5.2023, -5.8424], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2778, -5.7447, -5.7447, -2.9691, -6.1751, -5.7447, -5.7447, -5.3415,\n",
      "        -5.2778, -5.7447], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.009386749938130379\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.0047, -2.2210, -1.6313, -1.1036], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-6.1368, -4.7043, -6.1368, -5.3092, -5.7930, -6.2555, -3.6387, -6.2555,\n",
      "        -4.2446, -3.2408], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.1305, -4.5461, -6.1305, -5.5017, -5.6949, -6.0409, -3.9167, -6.0409,\n",
      "        -4.2749, -2.9811], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03095916472375393\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.2989, -0.9375,  0.1713,  1.5366], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.7170, -6.1517, -5.7170, -5.1806, -5.1415, -4.4825, -5.7170, -4.2280,\n",
      "        -5.7170, -6.4595], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6625, -6.1155, -5.6625, -5.2137, -5.2137, -5.0342, -5.6625, -4.2368,\n",
      "        -5.6625, -6.1155], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0442349798977375\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-6.0540, -5.6343, -6.1710, -5.6343, -6.2790, -1.5767, -5.7767, -6.1710,\n",
      "        -6.1859, -5.6333], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.0700, -5.6565, -6.1361, -5.6565, -6.1361, -1.9797, -5.6565, -6.1361,\n",
      "        -6.0700, -5.6006], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.021544545888900757\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-5.5660, -6.1698, -5.5660, -2.9394, -5.5660, -5.8293, -5.5660, -6.1791,\n",
      "        -5.5660, -5.5660], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6675, -6.1748, -5.6675, -3.0120, -5.6675, -5.6675, -5.6675, -6.1061,\n",
      "        -5.6675, -5.6675], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.009863331913948059\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.8964, -5.7748, -5.8757, -6.1667], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.8964, -6.1623, -5.5415, -5.8784, -6.1667, -6.1623, -5.2511, -4.3677,\n",
      "        -5.5415, -5.8964], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.1973, -6.1213, -5.6704, -6.1213, -6.1973, -6.1213, -5.2446, -4.1520,\n",
      "        -5.6704, -6.1973], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03242127224802971\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-5.9852, -5.3446, -5.6195, -6.2313], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.1781, -5.5571, -5.5571, -5.6582, -5.5571, -5.5571, -6.1443, -3.7625,\n",
      "        -5.5571, -5.5571], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.1778, -5.6406, -5.6406, -5.6406, -5.6406, -5.6406, -6.0863, -3.7898,\n",
      "        -5.6406, -5.6406], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.004628215916454792\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-5.5564, -4.9229, -5.1038, -5.8298], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.6044, -6.1275, -6.1884, -4.2099, -1.0213, -5.8533, -5.8533, -5.6044,\n",
      "        -4.2099, -2.9238], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6004, -6.0393, -6.1457, -4.1750,  0.3792, -5.6004, -5.6004, -5.6004,\n",
      "        -4.1750, -3.0128], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.21093256771564484\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-5.0762, -4.4744, -4.5613, -5.4616], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.6174, -6.0384,  1.5305, -5.9580, -5.6174, -3.5702, -6.2041, -6.0384,\n",
      "        -6.1650, -5.6174], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5898, -5.9974, 10.0000, -5.9974, -5.5898, -3.7670, -6.1252, -5.9974,\n",
      "        -6.1252, -5.5898], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.178590297698975\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-5.0601, -4.4709, -4.5411, -5.3920], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.1813, -5.1054, -5.2613, -5.6173, -5.9237, -5.9237, -6.1548, -5.6173,\n",
      "        -5.9237, -5.7138], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.1346, -5.1289, -5.3884, -5.5949, -5.9727, -5.9727, -6.1210, -5.5949,\n",
      "        -5.9727, -5.5949], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.004236062988638878\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-5.0429, -4.4682, -4.5219, -5.3303], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.6191, -6.0037, -5.7053, -6.0037, -5.6191, -5.6191, -5.6191, -6.0672,\n",
      "        -6.0672, -4.1763], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6040, -6.1195, -5.6040, -6.1195, -5.6040, -5.6040, -5.6040, -6.1195,\n",
      "        -6.1195, -4.1122], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.004758227616548538\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-5.0280, -4.4548, -4.5003, -5.2867], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.6169, -5.4833, -5.4833, -5.0265, -5.6169, -6.0404, -5.0265, -5.6169,\n",
      "        -5.6169, -4.2009], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6048, -5.6048, -5.6048, -5.0944, -5.6048, -6.1014, -5.0944, -5.6048,\n",
      "        -5.6048, -4.0897], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.005545228254050016\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-5.0119, -4.4269, -4.4897, -5.2594], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.9129, -5.9024, -5.6892, -4.4897, -2.0266, -4.5119, -5.6271, -5.6271,\n",
      "        -4.1394, -5.6125], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.8240, -6.0705, -5.8765, -4.0787, -1.5409, -4.4372, -5.5909, -5.5909,\n",
      "        -4.0787, -5.5909], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04885470122098923\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.9810, -4.3669, -4.4420, -5.2090], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.5333, -4.1133, -5.6226, -5.6431, -4.1133, -5.6431, -5.1907, -5.5333,\n",
      "        -5.0674, -4.7420], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.8187, -4.0763, -5.5607, -5.8187, -4.0763, -5.8187, -5.2678, -5.8187,\n",
      "        -5.0121, -4.9302], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.027561625465750694\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-6.0542, -5.6194, -2.8626, -5.6403, -5.6194, -6.0542, -5.5004, -5.6194,\n",
      "        -5.6194, -3.5043], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.9503, -5.4946, -2.7753, -5.7187, -5.4946, -5.9503, -5.6022, -5.4946,\n",
      "        -5.4946, -3.5903], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.011545125395059586\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3879, -4.0238, -3.4032, -4.2444], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.4539, -4.0238, -5.5761, -6.0738, -3.4933, -4.1078, -5.5761, -5.5243,\n",
      "        -5.1706, -5.5761], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5613, -4.0629, -5.4572, -5.9085, -3.5545, -4.0629, -5.4572, -5.4572,\n",
      "        -5.3515, -5.4572], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01258312352001667\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0868, -3.4144, -2.7900, -3.8940], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.8991, -6.0799, -5.6184, -5.6184, -5.8327, -4.4756, -5.4431, -6.0036,\n",
      "        -5.4429, -5.5120], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7556, -5.8988, -5.6253, -5.6253, -5.6253, -4.0392, -5.5485, -5.8988,\n",
      "        -5.3459, -5.4498], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03222274035215378\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-5.3112, -3.3681, -4.9501, -5.1049, -5.5951, -5.4485, -2.7842, -5.6833,\n",
      "        -5.4485, -5.3112], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4551, -3.4725, -4.8632, -5.1214, -5.6173, -5.4551, -2.7223, -5.6173,\n",
      "        -5.4551, -5.4551], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.006889957934617996\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0329, -3.3776, -2.7153, -3.8266], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.3995, -1.8945, -5.3995, -6.0499, -5.5928, -5.3995, -5.4414, -5.3995,\n",
      "        -5.3995, -5.3995], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4401, -1.3864, -5.4401, -5.8947, -5.6064, -5.4401, -5.4401, -5.4401,\n",
      "        -5.4401, -5.4401], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02922889217734337\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4249, -2.7559, -2.6787, -1.8495], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.5793, -5.8521, -2.7559, -5.3680, -5.4262, -5.7976, -5.8646, -5.3680,\n",
      "        -5.6899, -5.2625], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5919, -5.5068, -2.6645, -5.4223, -5.5068, -5.5919, -5.8835, -5.4223,\n",
      "        -5.5919, -5.4223], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.021795064210891724\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.5310, -1.9535, -1.1826, -0.4049], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.3393, -1.8082, -5.2679, -5.8737, -4.0595, -4.0595, -5.2679, -4.5379,\n",
      "        -5.3393, -1.9535], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4094, -1.3644, -5.4094, -5.8883, -4.0659, -4.0659, -5.4094, -4.6625,\n",
      "        -5.4094, -2.6274], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.071680948138237\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.2763, -1.1272,  0.2007,  1.8164], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.6643, -2.7208, -5.6643, -5.9620, -5.7864, -5.3184, -5.3184, -5.3184,\n",
      "        -3.3102, -1.1272], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6127, -2.5434, -5.6127, -5.8913, -5.8913, -5.3894, -5.3894, -5.3894,\n",
      "        -3.3975, -1.3653], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.013220992870628834\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.7508, -5.4266, -5.7294, -5.9256], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.3072, -4.5288, -5.3072, -4.0811, -5.3253, -4.2366, -5.9256, -5.3072,\n",
      "        -4.6717, -5.4446], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3600, -4.6306, -5.3600, -4.1098, -5.3600, -4.2154, -5.8839, -5.3600,\n",
      "        -4.9217, -5.4748], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.008634472265839577\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-4.0784, -5.1989, -0.3940, -4.9097, -5.3161, -4.8071, -4.5662, -5.3161,\n",
      "        -5.3735, -5.8910], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.1390, -5.3243,  0.6565, -5.0697, -5.3243, -5.0697, -4.6206, -5.3243,\n",
      "        -5.3243, -5.8664], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12237407267093658\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-5.1287, -4.4737, -4.7228, -5.2478], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.8882, -5.3184, -5.3184, -4.7443, -5.2460, -5.3184, -4.7806, -5.5417,\n",
      "        -5.8501, -5.7624], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0263, -5.2698, -5.2698, -4.7475, -5.2698, -5.2698, -5.0263, -5.5765,\n",
      "        -5.8229, -5.5765], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.012365497648715973\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.5391, -3.9485, -4.0456, -4.7922], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.1270, -3.5129, -5.2153, -4.4216, -4.7922, -5.4922, -5.5468, -5.3034,\n",
      "        -5.5468, -5.8130], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2232, -3.3672, -5.3610, -4.5537, -4.9794, -5.5531, -5.5531, -5.2232,\n",
      "        -5.5531, -5.7820], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.011538684368133545\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.5478, -3.9096, -4.0587, -4.8345], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.8356, -3.9096, -4.6312, -5.5674, -5.4698, -5.1124, -5.2894, -5.5674,\n",
      "        -5.0163, -5.2894], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9260, -4.5186, -4.6621, -5.5147, -5.5147, -4.6621, -5.1681, -5.5147,\n",
      "        -5.2057, -5.1681], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06556590646505356\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.5591, -3.9036, -4.0418, -4.8645], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-3.9784, -5.0978, -5.2439, -5.7648, -5.7648, -5.2439, -2.4393, -5.4126,\n",
      "        -5.2439, -5.7648], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.1411, -5.1471, -5.1471, -5.7097, -5.7097, -5.1471, -2.2277, -5.1471,\n",
      "        -5.1471, -5.7097], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.018144182860851288\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.5621, -3.9255, -3.9906, -4.8831], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.3423, -5.1669, -5.3154, -1.9290, -5.5611, -5.5611, -4.1569, -5.2413,\n",
      "        -5.5611, -5.5909], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1594, -5.1594, -5.2957, -2.1908, -5.5305, -5.5305, -4.0989, -5.2957,\n",
      "        -5.5305, -5.7172], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.012753461487591267\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.5675, -3.9529, -3.9358, -4.8887], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.0924, -5.5263, -4.7902, -5.0924, -5.0924, -5.5263, -5.0463, -3.1754,\n",
      "        -4.8657, -4.1085], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1814, -5.5516, -4.9198, -5.1814, -5.1814, -5.5516, -5.1814, -3.2591,\n",
      "        -4.6976, -4.0671], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.009700237773358822\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3369, -4.0282, -3.3585, -4.2191], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.6632, -5.0383, -5.6632, -5.5058, -5.0383, -5.0383, -4.1765, -5.0162,\n",
      "        -5.0383, -3.8893], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.7409, -5.1948, -5.7409, -5.5631, -5.1948, -5.1948, -4.0227, -4.9207,\n",
      "        -5.1948, -4.0227], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01638835482299328\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.7152, -3.1632, -2.4590, -3.4506], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.0358, -5.4253, -5.0573, -5.6555, -5.6555, -4.7768, -5.5052, -5.0358,\n",
      "        -4.0140, -2.4879], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1775, -5.5411, -5.1775, -5.7179, -5.7179, -4.8913, -5.5411, -5.1775,\n",
      "        -4.0130, -2.0773], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.025882374495267868\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.9370, -2.4432, -2.1932, -1.1963], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.0613, -5.0613, -5.5289, -5.5289, -5.0613, -5.0613, -4.5919, -5.2178,\n",
      "        -5.5289, -5.0613], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1327, -5.1327, -5.4845, -5.4845, -5.1327, -5.1327, -4.6573, -5.2477,\n",
      "        -5.4845, -5.1327], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0036596450954675674\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4166, -1.9049, -1.0504, -0.2023], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.4632, -5.5429, -4.1864,  0.2118, -5.4900, -5.0341, -5.1085, -5.1255,\n",
      "        -1.0504, -5.1085], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6208, -5.4253, -4.4788,  0.6834, -5.4253, -4.7678, -5.0866, -5.1930,\n",
      "        -1.1821, -5.0866], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.044446300715208054\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.2849, -1.1787,  0.2188,  1.8689], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.2043, -3.9036, -5.1443, -4.6267, -5.1443, -3.9827, -3.0587, -4.8832,\n",
      "        -4.7709, -5.5337], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1640, -4.0264, -5.0638, -4.7460, -5.0638, -3.9246, -3.2072, -4.9840,\n",
      "        -4.7460, -5.3949], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00993812270462513\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-0.1754, -1.0387, -5.5122, -4.4148, -4.6773, -5.2107, -5.5122, -5.5122,\n",
      "        -5.6754, -5.1595], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.6819, -1.1579, -5.3930, -4.5754, -4.5838, -5.0654, -5.3930, -5.3930,\n",
      "        -5.5881, -5.0654], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08639129251241684\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.7085, -4.9060, -5.1263, -5.4310], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.6225, -5.1563, -5.1563, -5.1563, -3.9268, -5.1563, -5.1503, -5.1563,\n",
      "        -5.1263, -5.1563], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6092, -5.0946, -5.0946, -5.0946, -3.9989, -5.0946, -5.0946, -5.0946,\n",
      "        -5.0946, -5.0946], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0032337703742086887\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.1235, -4.4639, -4.5795, -5.1923], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.1267, -5.1235, -5.1267, -2.3764, -4.4947, -5.1489, -5.1267, -5.5214,\n",
      "        -3.9620, -5.5690], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1401, -5.2368, -5.1401, -1.9657, -4.6394, -5.1401, -5.1401, -5.4534,\n",
      "        -3.9667, -5.6455], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.021351326256990433\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6762, -4.0114, -4.0881, -4.8057], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-3.3100, -5.5293, -4.6223, -4.1184, -5.1272, -2.8439, -5.5293, -2.3700,\n",
      "        -5.3681, -5.1080], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.1293, -5.6623, -4.6508, -4.6102, -5.1601, -3.1293, -5.6623, -1.9444,\n",
      "        -5.6623, -5.1601], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06636250764131546\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3403, -3.9488, -3.2746, -4.1775], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.2989, -5.5550, -5.0037, -5.0037, -5.1028,  1.8630, -3.8024, -5.2989,\n",
      "        -5.5550, -5.6551], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4223, -5.6161, -5.1232, -5.1232, -5.1232, 10.0000, -4.3678, -5.4223,\n",
      "        -5.6161, -5.6161], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.659820556640625\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.6577, -3.0423, -2.3226, -3.3219], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.0850, -5.0850, -5.0850, -4.4212, -5.3127, -5.3127, -5.3127, -3.7653,\n",
      "        -5.3127, -5.1664], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0907, -5.0907, -5.0907, -4.5706, -5.3937, -5.3937, -5.3937, -4.3287,\n",
      "        -5.3937, -5.3937], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.041775695979595184\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.8778, -2.2761, -2.0275, -1.0977], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.3543, -4.6167, -4.8434, -3.9176, -5.0474, -5.6231, -4.7875, -5.6231,\n",
      "        -5.0605, -5.2089], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3590, -4.7120, -4.9242, -3.9306, -5.1550, -5.5427, -4.9242, -5.5427,\n",
      "        -5.0547, -5.3590], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.008158540353178978\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2828, -1.7932, -0.8390,  0.1019], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.2495, -5.3937, -5.5508, -5.2607, -5.6666, -5.0391, -5.6666, -5.2582,\n",
      "        -4.0868, -5.0391], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.0035, -5.3231, -5.5102, -5.3231, -5.5102, -5.0196, -5.5102, -5.0196,\n",
      "        -4.2291, -5.0196], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01979057863354683\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3154, -1.1978,  0.2953,  2.0000], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.4038, -5.4038, -5.1519, -4.4787, -5.2643, -5.0041, -2.9880, -5.5733,\n",
      "        -2.2350, -3.9035], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3127, -5.3127, -5.1210, -4.1686, -5.0096, -5.0096, -3.0224, -5.5038,\n",
      "        -2.0123, -3.9072], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.023424264043569565\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.5371, -5.0160, -5.1702, -5.6758], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.7788, -4.9716, -3.9213, -4.7966, -2.9921, -4.9716, -4.9716, -4.8605,\n",
      "        -5.1900, -4.5900], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.0148, -5.0145, -3.8986, -4.9097, -3.0034, -5.0145, -5.0145, -5.0145,\n",
      "        -4.9097, -4.6822], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.018544143065810204\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-5.4556, -4.6020, -4.7556, -5.5662], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.6714, -5.6714, -4.8040, -2.2129, -4.2852, -3.9367, -4.3927, -1.9658,\n",
      "        -5.1075, -4.9523], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5240, -5.5240, -4.9239, -2.0070, -4.4917, -3.8979, -4.4917, -2.0070,\n",
      "        -5.0140, -5.0140], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01684301719069481\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-4.8926, -4.1035, -4.1492, -5.0725], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.3141, -5.0351, -5.0351, -5.6350, -4.9505, -5.3597, -4.9505, -4.9505,\n",
      "         0.2095,  0.2095], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3337, -5.1530, -5.1530, -5.5316, -5.0170, -5.3337, -5.0170, -5.0170,\n",
      "         0.8681,  0.8681], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09200556576251984\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.2730, -3.6624, -3.3773, -4.5955], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.0131, -4.9498, -3.2207, -5.3135, -4.9498, -2.9824, -4.9498, -4.0960,\n",
      "        -4.8543, -4.9498], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0099, -5.0099, -2.9617, -5.3309, -5.0099, -2.9617, -5.0099, -4.0396,\n",
      "        -5.0099, -5.0099], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.010971532203257084\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3303, -3.9285, -3.2176, -4.2938], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.8796, -4.2626, -4.9621, -4.9688, -2.1690, -2.9632, -4.2938, -1.0642,\n",
      "        -3.3679, -4.3642], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9839, -4.4356, -4.9839, -4.9839, -1.9578, -2.9580, -4.3825, -0.6681,\n",
      "        -3.8958, -4.3825], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05299283191561699\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4926, -2.9334, -2.1806, -3.1680], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.9334, -5.0006, -5.0006,  2.1239, -3.6159, -4.2876, -5.0006, -5.2730,\n",
      "        -5.0006, -3.1680], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.9625, -4.9396, -4.9396, 10.0000, -4.0667, -4.3890, -4.9396, -5.2830,\n",
      "        -4.9396, -3.6969], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.254277229309082\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.7780, -2.0949, -1.9725, -0.9918], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.9989, -2.1876, -5.4328, -4.3406, -4.4763, -3.4105, -5.4328, -3.8552,\n",
      "        -4.9434, -5.2500], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9065, -1.8926, -5.4638, -4.3529, -4.3529, -3.8713, -5.4638, -3.8713,\n",
      "        -4.9065, -5.2646], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03270522505044937\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.0673, -1.6621, -0.6362,  0.5520], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.9846, -4.9846, -5.0055, -3.8235, -5.0055, -5.3993, -5.3993, -5.3993,\n",
      "        -3.8235, -3.8235], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8832, -4.8832, -4.8832, -3.8288, -4.8832, -5.4510, -5.4510, -5.4510,\n",
      "        -3.8288, -3.8288], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.005862629506736994\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-4.9454, -4.9498, -2.9209, -3.8037, -5.2662, -5.3746, -1.9552, -4.1572,\n",
      "        -5.2151, -4.9361], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0176, -4.8714, -2.9465, -3.7843, -5.2526, -5.4509, -1.8417, -4.0982,\n",
      "        -5.2526, -4.8714], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.004034988582134247\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.2601, -4.9486, -5.1302, -5.3652], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.2288, -4.8009, -4.9098, -4.4514, -2.7438, -5.1302, -5.2017, -4.9098,\n",
      "        -4.9185, -4.9098], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2523, -4.8622, -4.8622, -4.5760, -2.9316, -5.2523, -5.2523, -4.8622,\n",
      "        -4.8622, -4.8622], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.008254112675786018\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-5.1818, -4.4441, -4.6856, -5.2715], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.3699, -4.2122, -4.8657, -5.2018, -4.8677, -4.8677, -5.1911, -4.9535,\n",
      "        -4.8657, -4.8934], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4581, -4.2937, -4.8525, -5.2468, -4.8525, -4.8525, -5.2468, -4.9997,\n",
      "        -4.8525, -4.8525], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0024160812608897686\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-4.6556, -3.9485, -4.0683, -4.7985], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.8377, -2.0315, -4.8285, -5.1877, -3.6842, -4.8713, -2.1024, -5.3879,\n",
      "        -5.1877, -5.3879], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8392, -1.8061, -4.8392, -5.2339, -4.1066, -4.8392, -1.8061, -5.4574,\n",
      "        -5.2339, -5.4574], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.033215202391147614\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.2516, -3.6932, -3.4218, -4.6064], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.8292, -4.2516, -5.4307, -5.1670, -5.4307, -4.9542, -2.0225, -4.7777,\n",
      "        -3.4375, -4.7777], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8284, -4.0796, -5.4588, -5.2230, -5.4588, -4.9733, -1.8266, -4.8284,\n",
      "        -3.6046, -4.8284], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.010608971118927002\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0021, -3.6889, -2.8423, -3.8888], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.4821, -5.2323, -4.8363, -4.8324, -5.1601, -3.1430, -4.7450, -3.2704,\n",
      "        -5.1601, -4.3873], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4481, -5.1988, -4.8081, -4.8354, -5.1988, -3.4662, -4.8081, -3.4662,\n",
      "        -5.1988, -4.5200], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.017041120678186417\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4506, -2.8526, -2.0267, -3.1893], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.0895, -4.7320, -5.3052, -2.8049, -3.6436, -3.8653, -4.7320, -3.8653,\n",
      "        -2.7961, -5.2623], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1576, -4.7742, -5.4199, -2.8240, -3.5244, -4.0589, -4.7742, -4.0589,\n",
      "        -2.8240, -5.1576], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01226592343300581\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.8179, -1.9464, -1.9068, -1.0204], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.7184, -4.8727, -4.1172, -5.0922, -4.7184, -5.2770, -4.8704, -0.3755,\n",
      "        -5.2770, -4.7184], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7496, -4.7496, -4.1709, -5.1248, -4.7496, -5.1248, -4.7496,  1.2684,\n",
      "        -5.1248, -4.7496], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2785433828830719\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9626, -1.5491, -0.4517,  0.8244], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.6090, -4.7124, -2.0073, -3.5838, -5.1004, -4.7412,  2.5267, -5.6090,\n",
      "        -3.5838, -5.2547], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4001, -4.7417, -1.9385, -3.4659, -5.1058, -4.7417, 10.0000, -5.4001,\n",
      "        -3.4659, -5.1058], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.599279403686523\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3135, -1.2927,  0.4347,  2.5939], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.5542, -5.5542, -4.1956, -5.2097, -5.5542, -4.1702, -4.3116, -3.5795,\n",
      "        -4.7031, -5.1724], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4158, -5.4158, -4.1710, -5.1079, -5.4158, -4.1521, -4.4636, -3.4379,\n",
      "        -4.7532, -5.1079], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.011857724748551846\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.3298, -4.9404, -5.0672, -5.4502], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.4502, -5.0674, -4.6964, -5.0674, -4.6964, -5.0674, -4.6964, -5.4502,\n",
      "        -3.9024, -5.0674], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4463, -5.1289, -4.7798, -5.1289, -4.7798, -5.1289, -4.7798, -5.4463,\n",
      "        -3.9641, -5.1289], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.003984423819929361\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-5.1616, -4.3571, -4.5335, -5.1700], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.7091, -3.6575, -4.6987, -5.0503, -0.9442, -4.6987, -4.3571, -3.6575,\n",
      "        -4.7091, -1.9454], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7870, -3.9503, -4.7870, -5.1315, -0.1099, -4.7870, -4.4939, -3.9503,\n",
      "        -4.7870, -1.8498], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09297265112400055\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-4.6185, -3.8919, -3.9120, -4.6443], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-3.5882, -4.9392, -4.0597, -0.8801, -1.8901, -4.5971, -4.6539, -5.2931,\n",
      "        -4.7118, -4.6682], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3788, -5.1374, -4.1692, -0.0931, -1.7921, -4.7765, -4.7982, -5.4691,\n",
      "        -4.7982, -4.7982], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08325609564781189\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.1499, -3.6485, -3.2079, -4.3223], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.7264, -3.5638, -5.2580, -2.7126, -5.2580, -4.7264, -4.6587, -4.1953,\n",
      "        -5.2580, -5.1806], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7757, -3.3663, -5.4538, -2.7617, -5.4538, -4.7757, -4.7757, -4.1422,\n",
      "        -5.4538, -5.1184], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.018164869397878647\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.8414, -3.5108, -2.6271, -3.5533], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.8102, -5.2999, -4.7548, -4.1753, -3.5954, -4.5186, -5.0507, -4.6852,\n",
      "        -4.7548, -4.1336], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.8747, -5.4072, -4.7202, -4.0869, -3.8747, -4.7327, -5.0667, -4.7202,\n",
      "        -4.7202, -4.0869], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.015346484258770943\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4400, -2.7797, -1.9488, -3.1090], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.7621, -4.9475, -4.9591, -2.7797, -4.9591, -4.7099, -2.6144, -4.8653,\n",
      "        -4.0575, -4.7621], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6820, -5.0378, -5.0378, -2.7539, -5.0378, -4.6820, -2.7539, -4.8474,\n",
      "        -4.0530, -4.6820], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.005464869551360607\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6079, -1.7712, -1.7643, -0.6675], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.7841, -4.7587, -2.7517, -4.0484, -5.3425, -4.7587, -4.7587, -4.7587,\n",
      "        -4.7587,  2.9050], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6512, -4.6436, -2.7427, -4.0195, -5.3480, -4.6436, -4.6436, -4.6436,\n",
      "        -4.6436, 10.0000], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.042352676391602\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9084, -1.5660, -0.4328,  0.9460], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.7411, -4.6363, -4.7028, -4.6363, -5.4081, -4.7028, -3.5267, -4.0424,\n",
      "        -3.4446, -4.7028], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7547, -4.6382, -4.6382, -4.6382, -5.3519, -4.6382, -3.7547, -4.0157,\n",
      "        -3.3182, -4.6382], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.008451960049569607\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.1662, -1.3829,  0.5126,  3.0793], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.5116, -4.6316, -4.0257, -4.6316, -4.2672, -1.6496, -2.5304, -4.6316,\n",
      "        -4.6316, -2.5304], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.1311, -4.6452, -3.6913, -4.6452, -4.3816, -1.4604, -2.6834, -4.6452,\n",
      "        -4.6452, -2.6834], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.035299692302942276\n",
      "---\n",
      "end game\n"
     ]
    }
   ],
   "source": [
    "# Deep Q Network\n",
    "import gym \n",
    "import gym_examples\n",
    "from dqn import Agent\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('gym_examples/GridWorld-v0', size=4)\n",
    "agent = Agent(gamma=0.9, epsilon=0.5, batch_size=10, n_actions=4, eps_end=0.01, input_dims=[16], lr=0.003)\n",
    "scores, eps_hist,avg_scores = [], [], []\n",
    "n_games = 10\n",
    "\n",
    "for i in range(n_games):\n",
    "    score = 0\n",
    "    done=False\n",
    "    observation = env.reset()[0]['agent']\n",
    "    counter = 0\n",
    "\n",
    "    while not done:\n",
    "        rewards = []\n",
    "        agent_location = env.get_agent_location()\n",
    "        print(f\"Agent's Location: {agent_location}\")\n",
    "        # choose an action based on current state of env\n",
    "        action = agent.choose_action(observation)\n",
    "        observation_, reward, done, truncated, info = env.step(action) #observation_ is just next_state\n",
    "        observation_ = observation_['agent']\n",
    "        score += reward\n",
    "        rewards\n",
    "        agent.store_transitions(observation, action, reward, observation_, done)\n",
    "        agent.learn()\n",
    "        observation = observation_\n",
    "        # end of an episode\n",
    "        counter += 1\n",
    "        print('---')\n",
    "\n",
    "    scores.append(score)\n",
    "    eps_hist.append(agent.epsilon)\n",
    "    print('end game')\n",
    "\n",
    "    avg_score = np.mean(scores)\n",
    "    \n",
    "    avg_scores.append(avg_score)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the average score per episode\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "episodes = list(range(1, n_games + 1))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(episodes, avg_scores, marker='o', linestyle='-', color='b', label='Average Score per Episode')\n",
    "plt.title('Average Scores Over Episodes')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Score')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
