{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent's INITIAL Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 3], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [1 3], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [1 3], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 3], Target Location: [0 0]\n",
      "Agent's Location: [1 3], Target Location: [0 0]\n",
      "Agent's Location: [0 3], Target Location: [0 0]\n",
      "Agent's Location: [0 3], Target Location: [0 0]\n",
      "Agent's Location: [0 3], Target Location: [0 0]\n",
      "Agent's Location: [0 3], Target Location: [0 0]\n",
      "Agent's Location: [1 3], Target Location: [0 0]\n",
      "Agent's Location: [1 3], Target Location: [0 0]\n",
      "Agent's Location: [1 3], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [1 3], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 3], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [1 3], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 3], Target Location: [0 0]\n",
      "Agent's Location: [0 3], Target Location: [0 0]\n",
      "Agent's Location: [0 3], Target Location: [0 0]\n",
      "Agent's Location: [0 3], Target Location: [0 0]\n",
      "Agent's Location: [1 3], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 3], Target Location: [0 0]\n",
      "Agent's Location: [0 3], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 1], Target Location: [0 0]\n",
      "Agent's Location: [0 1], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 3], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 3], Target Location: [0 0]\n",
      "Agent's Location: [0 3], Target Location: [0 0]\n",
      "Agent's Location: [1 3], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 3], Target Location: [0 0]\n",
      "Agent's Location: [1 3], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [1 3], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [1 3], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 3], Target Location: [0 0]\n",
      "Agent's Location: [0 3], Target Location: [0 0]\n",
      "Agent's Location: [1 3], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [1 3], Target Location: [0 0]\n",
      "Agent's Location: [1 3], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 3], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [3 1], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [3 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [1 0], Target Location: [0 0]\n",
      "Agent's Location: [1 1], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [2 0], Target Location: [0 0]\n",
      "Agent's Location: [2 1], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 3], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 3], Target Location: [0 0]\n",
      "Agent's Location: [1 3], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [1 3], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [1 3], Target Location: [0 0]\n",
      "Agent's Location: [1 3], Target Location: [0 0]\n",
      "Agent's Location: [2 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 3], Target Location: [0 0]\n",
      "Agent's Location: [3 2], Target Location: [0 0]\n",
      "Agent's Location: [2 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 2], Target Location: [0 0]\n",
      "Agent's Location: [1 3], Target Location: [0 0]\n",
      "Agent's Location: [0 3], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 1], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 3], Target Location: [0 0]\n",
      "Agent's Location: [0 3], Target Location: [0 0]\n",
      "Agent's Location: [0 2], Target Location: [0 0]\n",
      "Agent's Location: [0 1], Target Location: [0 0]\n",
      "Agent's Location: [0 0], Target Location: [0 0]\n",
      "Episode finished\n"
     ]
    }
   ],
   "source": [
    "# Complete random movements on environment\n",
    "\n",
    "import gym\n",
    "import gym_examples\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('gym_examples/GridWorld-v0', size=4)\n",
    "\n",
    "state = env.reset()\n",
    "agent_location = env.get_agent_location()\n",
    "target_location = env.get_target_location()\n",
    "print(f\"Agent's INITIAL Location: {agent_location}, Target Location: {target_location}\")\n",
    "\n",
    "done = False\n",
    "\n",
    "\n",
    "while not done:\n",
    "    # Select action randomly\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # take step, returns transition\n",
    "    next_state, reward, done, truncated, info = env.step(action)\n",
    "    \n",
    "    agent_location = env.get_agent_location()\n",
    "    target_location = env.get_target_location()\n",
    "    print(f\"Agent's Location: {agent_location}, Target Location: {target_location}\")\n",
    "\n",
    "    #update state\n",
    "    state = next_state\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(\"Episode finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0327, 0.0611, 0.0447, 0.0547], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([0.0283, 0.0731, 0.0763, 0.0709], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([0.0766, 0.0389, 0.0202, 0.0806], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([0.0327, 0.0611, 0.0447, 0.0547], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([0.0283, 0.0731, 0.0763, 0.0709], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 0\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 3\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([0.0692, 0.0806, 0.0611, 0.0763, 0.0763, 0.0766, 0.0709, 0.0327, 0.0547,\n",
      "        0.0611], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9450, -0.9377, -0.9314, -0.9274, -0.9274, -0.9314, -0.9450, -0.9450,\n",
      "        -0.9450, -0.9314], grad_fn=<AddBackward0>)\n",
      "LOSS: 1.0053720474243164\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([ 0.1371,  0.0005,  0.0013,  0.0013, -0.0707, -0.0324,  0.1371,  0.1580,\n",
      "         0.1580, -0.0370], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.8578, -0.8767, -0.8767, -0.8767, -0.8767, -0.8578, -0.8578, -0.9165,\n",
      "        -0.9165, -0.8767], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.8635309338569641\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-0.0690, -0.0577,  0.0245,  0.2089,  0.2086, -0.0577,  0.2089, -0.1332,\n",
      "        -0.1535, -0.1134], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.8120, -0.8120, -0.8383, -0.8123, -0.8570, -0.8120, -0.8123, -0.8120,\n",
      "        -0.8120, -0.8123], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.703802227973938\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.2305,  0.2217,  0.2487, -0.1080], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.2305,  0.1944, -0.1879, -0.0355, -0.1292,  0.2742,  0.2846,  0.2742,\n",
      "         0.2487,  0.2846], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.7439, -0.7762, -0.7532, -0.7762, -0.7439, -0.7975, -0.7532, -0.7975,\n",
      "        -0.8032, -0.7532], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.8009533882141113\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([ 0.2185, -0.2102, -0.1028,  0.3314,  0.3517,  0.2067, -0.3183, -0.2258,\n",
      "         0.3314, -0.2658], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.7242, -0.6835, -0.7242, -0.7316, -0.7017, -0.7284, -0.6835, -0.6835,\n",
      "        -0.7316, -0.7017], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6275495290756226\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3629,  0.3917,  0.2249, -0.2014], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.4302, -0.4324,  0.3917,  0.4302, -0.1766, -0.4011, -0.3158,  0.2215,\n",
      "         0.3552,  0.2249], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6803, -0.6129, -0.6859, -0.6803, -0.6623, -0.6129, -0.6129, -0.6623,\n",
      "        -0.6562, -0.6475], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6594301462173462\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4594,  0.4346,  0.2476, -0.3311], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.2100,  0.3723, -0.4813,  0.5017, -0.3904, -0.2546, -0.4116,  0.3772,\n",
      "         0.2167,  0.4721], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5788, -0.5751, -0.5484, -0.6290, -0.5484, -0.5788, -0.5484, -0.5807,\n",
      "        -0.5751, -0.6089], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5664698481559753\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([ 0.3587, -0.4765, -0.5544,  0.3522,  0.1782, -0.4676, -0.6269, -0.4765,\n",
      "         0.5829, -0.3337], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4978, -0.4754, -0.4754, -0.4965, -0.4965, -0.5316, -0.4754, -0.4754,\n",
      "        -0.5491, -0.4856], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3246987760066986\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.5181,  0.6155,  0.1774, -0.4491], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.6156,  0.1190,  0.6572,  0.6598,  0.6323,  0.3064, -0.5530,  0.1259,\n",
      "         0.3159, -0.5730], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4062, -0.3903, -0.4310, -0.4612, -0.4237, -0.4086, -0.4062, -0.4086,\n",
      "        -0.4111, -0.4612], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5218505263328552\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.5904,  0.6929,  0.1166, -0.5129], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.2414,  0.7299, -0.6604, -0.6119, -0.6314,  0.0600, -0.7725,  0.7307,\n",
      "         0.6929,  0.7299], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3217, -0.3674, -0.3431, -0.3431, -0.3674, -0.3217, -0.3431, -0.3257,\n",
      "        -0.3763, -0.3674], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5556877851486206\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.6093,  0.7462,  0.0601, -0.5482], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.1696,  0.7462, -0.7108, -0.6608, -0.6411, -0.0166,  0.7923,  0.7604,\n",
      "        -0.5321,  0.1891], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.2486, -0.3284, -0.3156, -0.3156, -0.3156, -0.2224, -0.2424, -0.2855,\n",
      "        -0.2224, -0.2586], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4214347302913666\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.6116,  0.7794,  0.0018, -0.5567], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.1169,  0.7660, -0.6464, -0.6390,  0.7794,  0.8411, -0.8124, -0.0753,\n",
      "        -0.6766,  0.7794], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.2083, -0.2153, -0.3106, -0.3106, -0.2985, -0.2985, -0.3106, -0.1867,\n",
      "        -0.2430, -0.2985], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.536433219909668\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.5982,  0.8051, -0.0571, -0.5605], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.9473, -0.1438,  0.8412,  0.7421,  0.0192, -0.5634, -0.6642,  0.8585,\n",
      "         0.8051,  0.0437], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.2429, -0.1474, -0.1249, -0.1656, -0.1474, -0.1409, -0.2273, -0.2754,\n",
      "        -0.2754, -0.1786], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6073626279830933\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.5774,  0.8236, -0.1152, -0.5552], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.5617, -0.2102,  0.8559,  0.8419, -0.7441, -0.5921, -0.7501, -0.0288,\n",
      "         0.7100,  0.8236], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3610, -0.1331, -0.2588, -0.0905, -0.3610, -0.3610, -0.3610, -0.1621,\n",
      "        -0.1258, -0.2588], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4397614896297455\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-0.0937,  0.8309,  0.8309, -0.1666, -0.5531,  0.8378, -0.2684, -0.2379,\n",
      "         0.6674, -0.5531], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.1587, -0.2522, -0.2522, -0.1164, -0.3994, -0.2522, -0.1310, -0.1339,\n",
      "        -0.0977, -0.3994], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.420341432094574\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-0.1500,  0.8117, -0.3152, -0.2096, -0.5789, -0.1500, -0.5124, -0.4672,\n",
      "        -0.5124, -0.5897], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.1621, -0.2483, -0.1360, -0.1253, -0.2483, -0.1621, -0.4417, -0.4417,\n",
      "        -0.4417, -0.0744], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15485996007919312\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-0.3501,  0.5708, -0.4720, -0.6892, -0.1969, -0.2038, -0.4996, -0.5427,\n",
      "         0.8288,  0.8288], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.1556, -0.0722, -0.4863, -0.4863, -0.1792, -0.1556, -0.1665, -0.3065,\n",
      "        -0.2541, -0.2541], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.30073824524879456\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-0.5476, -0.2348, -0.2987, -0.4450, -0.2741, -0.4321, -0.3759,  0.9068,\n",
      "         0.5211, -0.4951], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5310, -0.2045, -0.1977, -0.2045, -0.1784, -0.5310, -0.5310, -0.3484,\n",
      "        -0.0765, -0.3470], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.20668940246105194\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([ 0.7920, -0.2658, -0.4503,  0.7920, -0.3970,  0.4767, -0.4090, -0.4321,\n",
      "        -0.4974, -0.3381], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.2872, -0.2319, -0.3863, -0.2872, -0.5709, -0.0804, -0.2319, -0.2872,\n",
      "        -0.5709, -0.5709], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2787213623523712\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4543,  0.8216, -0.2744, -0.4246], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.6044,  0.7711,  0.7711, -0.3907,  0.7711,  0.7711, -0.4543,  0.8267,\n",
      "        -0.2930, -0.4116], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6068, -0.3060, -0.3060, -0.3060, -0.3060, -0.3060, -0.6068, -0.4042,\n",
      "        -0.2588, -0.4228], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6187528371810913\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-0.3112,  0.4006,  0.4006,  0.6013,  0.7440, -0.2895,  0.7782, -0.4189,\n",
      "         0.7440,  0.7920], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.2925, -0.0876, -0.0876, -0.3304, -0.3304, -0.2996, -0.4266, -0.6395,\n",
      "        -0.3304, -0.4588], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6718708276748657\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-0.3567, -0.3947, -0.5645,  0.7260,  0.7260,  0.7311, -0.4226, -0.3360,\n",
      "        -0.3221,  0.5677], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3202, -0.6693, -0.6693, -0.3466, -0.3466, -0.4394, -0.3420, -0.3197,\n",
      "        -0.6693, -0.3466], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.47220277786254883\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-0.3064,  0.5354,  0.7299,  0.7250,  0.6831, -0.3402, -0.3800, -0.3100,\n",
      "        -0.3127, -0.3486], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3554, -0.3554, -0.3475, -0.5182, -0.4435, -0.3431, -0.6914, -0.6914,\n",
      "        -0.3852, -0.3375], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5019186735153198\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.2196,  0.5104, -0.2726, -0.2900], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.5104,  0.7152,  0.6973, -0.3053, -0.3512,  0.6973,  0.7013,  0.6228,\n",
      "        -0.3253,  0.3254], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3563, -0.3563, -0.5406, -0.7072, -0.0779, -0.5406, -0.3725, -0.1497,\n",
      "        -0.5406, -0.0779], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.7158783674240112\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3391,  0.7153, -0.3714, -0.3694], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.3126, -0.3220,  0.7153, -0.3391,  0.3074,  0.6624,  0.4940, -0.3064,\n",
      "        -0.3714, -0.2833], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4012, -0.4012, -0.3563, -0.0741, -0.0741, -0.5554, -0.3563, -0.7233,\n",
      "        -0.3874, -0.3563], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3763420581817627\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3380,  0.7214, -0.3835, -0.3774], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.2851,  0.7214,  0.7214, -0.3190, -0.3246, -0.3504,  0.7214, -0.5602,\n",
      "        -0.2336,  0.5583], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4360, -0.3507, -0.3507, -0.4282, -0.0675, -0.7372, -0.3507, -0.7372,\n",
      "        -0.7372, -0.4420], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.49843257665634155\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3433,  0.7280, -0.3953, -0.3913], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.7280, -0.4175,  0.7280, -0.3424, -0.3171, -0.3253, -0.3230,  0.4722,\n",
      "        -0.3175, -0.5760], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3448, -0.4610, -0.3448, -0.5285, -0.5750, -0.7447, -0.4490, -0.3448,\n",
      "        -0.0608, -0.7447], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3358518183231354\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-0.4385, -0.3444,  0.7408, -0.6020, -0.3348,  0.5971, -0.3259,  0.6400,\n",
      "         0.7408, -0.2724], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5521, -0.7476, -0.3332, -0.7476, -0.4626, -0.4802, -0.4626, -0.1485,\n",
      "        -0.3332, -0.3332], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.43248850107192993\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([ 0.7540,  0.5674,  0.7540,  0.2811, -0.3340, -0.3098, -0.6381, -0.4466,\n",
      "        -0.3700,  0.5674], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3214, -0.5860, -0.3214, -0.0385, -0.5860, -0.0385, -0.7470, -0.5704,\n",
      "        -0.7470, -0.5860], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5382756590843201\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-0.2782, -0.4800, -0.3403,  0.7638, -0.3675, -0.3993, -0.4773, -0.3755,\n",
      "         0.4656,  0.7638], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3126, -0.5065, -0.5065, -0.3126, -0.3126, -0.7479, -0.5810, -0.4909,\n",
      "        -0.3126, -0.3126], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.310081422328949\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([ 0.7808, -0.5016,  0.4711,  0.4711, -0.3051,  0.7808, -0.2500,  0.6691,\n",
      "        -0.3673, -0.4655], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.2972, -0.5174, -0.2972, -0.2972, -0.0116, -0.2972, -0.7454, -0.0996,\n",
      "        -0.5174, -0.5961], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.44675788283348083\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3885,  0.8060, -0.4781, -0.5617], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.4663, -0.3043, -0.5238, -0.3680, -0.5238,  0.8060, -0.4265, -0.3832,\n",
      "         0.8060,  0.4415], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.7400,  0.0046, -0.5247, -0.5041, -0.5247, -0.2746, -0.5041, -0.2746,\n",
      "        -0.2746, -0.3889], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3231562376022339\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3957,  0.8243, -0.4950, -0.6040], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.8243, -0.4950,  0.8243, -0.2933,  0.4703, -0.3875,  0.8243, -0.4149,\n",
      "         0.5194, -0.2632], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.2582, -0.4382, -0.2582, -0.2582, -0.2582, -0.5768, -0.2582, -0.5325,\n",
      "        -0.5768, -0.7353], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5524190664291382\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4115,  0.8448, -0.5108, -0.6460], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.6460, -0.4396,  0.8448, -0.4115, -0.4281,  0.7025, -0.5629,  0.8448,\n",
      "         0.8448, -0.4778], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5753, -0.5422, -0.2397, -0.5832, -0.6249, -0.0315, -0.5422, -0.2397,\n",
      "        -0.2397, -0.5142], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4152641296386719\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4296,  0.8612, -0.5278, -0.6845], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.3146, -0.4420,  0.7096, -0.6845, -0.5020,  0.8612, -0.5678, -0.5678,\n",
      "         0.8612, -0.3840], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.2249, -0.6322, -0.0040, -0.5734, -0.5171, -0.2249, -0.7240, -0.7240,\n",
      "        -0.2249, -0.7240], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3089748024940491\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4494,  0.8753, -0.5461, -0.7206], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.4042, -0.6047, -0.5172, -0.4155, -0.4592,  0.8753, -0.4001,  0.7133,\n",
      "        -0.9533,  0.8753], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3580, -0.5550, -0.6362, -0.2122, -0.6362, -0.2122, -0.7159,  0.0303,\n",
      "        -0.7159, -0.2122], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.36583518981933594\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.4712,  0.8852, -0.5623, -0.7465], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.8852, -0.6237, -0.4770, -0.3244,  0.3966, -0.5063, -0.9818,  0.8852,\n",
      "         0.4884,  0.8852], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.2033, -0.5604, -0.6431, -0.7086, -0.3548, -0.5604, -0.7086, -0.2033,\n",
      "        -0.5691, -0.2033], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5494408011436462\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-0.5772,  0.8909, -0.6562, -0.6397,  0.8909,  0.4848,  0.8909,  0.8909,\n",
      "        -0.4305,  0.3299], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4283, -0.1982, -0.7031, -0.5698, -0.1982, -0.1982, -0.1982, -0.1982,\n",
      "        -0.1982,  0.0682], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5362647771835327\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([ 0.8884,  0.4643, -0.5822, -0.5689, -0.6461, -0.3445, -0.3780,  0.4812,\n",
      "        -0.6461,  0.8884], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.2005, -0.5669, -0.4362, -0.5259, -0.5821,  0.0718, -0.2005, -0.2005,\n",
      "        -0.5821, -0.2005], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.41354164481163025\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-0.5457,  0.8747, -0.4200,  0.8747,  0.8747,  0.4705,  0.8747, -0.5417,\n",
      "         0.7119, -0.4592], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6868, -0.2127, -0.2127, -0.2127, -0.2127, -0.2127, -0.2127, -0.6047,\n",
      "         0.1338, -0.5426], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5605151653289795\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([ 0.8589, -0.5484,  0.8589, -0.7129, -0.5766, -0.5293,  0.8589, -0.5766,\n",
      "        -0.3867,  0.4148], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.2269, -0.6267, -0.2269, -0.7170, -0.4772, -0.6117, -0.2269, -0.4772,\n",
      "        -0.7170, -0.5907], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.46902018785476685\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([ 0.3891,  0.8397,  0.3054,  0.8397,  0.3054, -0.5402, -0.3456,  0.8397,\n",
      "        -0.6338,  0.8397], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6000, -0.2443,  0.0270, -0.2443,  0.0270, -0.6257,  0.0270, -0.2443,\n",
      "        -0.6498, -0.2443], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5979710221290588\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([ 0.2898,  0.8135,  0.4360, -0.7450,  0.3578, -0.4627,  0.8135,  0.4493,\n",
      "         0.8135,  0.8135], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.0093, -0.2679, -0.2679, -0.7392, -0.6076, -0.5956, -0.2679, -0.6780,\n",
      "        -0.2679, -0.2679], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.7471832036972046\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([ 0.7878,  0.3265, -0.5901, -0.7587,  0.7878,  0.7878, -0.5579, -0.7587,\n",
      "        -0.6578, -0.8129], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.2910, -0.6161, -0.6257, -0.7560, -0.2910, -0.2910, -0.5532, -0.7560,\n",
      "        -0.6161, -0.6161], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4421519637107849\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-0.6599,  1.0799, -0.6202, -1.0442], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.7559,  0.7559,  0.3805,  0.7559, -0.4283,  0.7559,  0.6048, -0.3759,\n",
      "        -0.5134,  0.7559], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3197, -0.3197, -0.7375, -0.3197, -0.7753, -0.3197,  0.1432, -0.3197,\n",
      "        -0.7753, -0.3197], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.7439773678779602\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-0.4662,  0.4070, -0.5376, -0.7261], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.7145,  0.5695, -0.5470,  0.2067, -0.7786,  0.7145,  0.7145, -0.3191,\n",
      "        -0.5925,  0.7145], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3570,  0.1344, -0.6137, -0.4875, -0.7932, -0.3570, -0.3570, -0.6337,\n",
      "        -0.7652, -0.3570], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5396683216094971\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([ 0.6754, -0.4703,  0.3943,  0.6754,  0.2346,  0.3943, -0.7151, -0.5963,\n",
      "        -0.5471, -0.6155], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3921, -0.7338, -0.3921, -0.3921, -0.6452, -0.3921, -0.6452, -0.7338,\n",
      "        -0.6383, -0.7888], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4421781897544861\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.5851,  0.6587, -0.5554, -0.8191], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.6587, -0.5497,  0.2116,  0.3854, -0.7388, -0.6047, -0.3707,  0.6587,\n",
      "        -0.5851, -0.6260], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4071, -0.8192, -0.6531, -0.4071, -0.6531, -0.7572, -0.4071, -0.4071,\n",
      "        -0.7236, -0.8096], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3805457055568695\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([ 0.2521, -0.8326, -0.5747,  0.6550, -0.3770, -0.4242, -0.8262,  0.6550,\n",
      "         0.6550,  0.6550], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.8281, -0.6567, -0.8744, -0.4105, -0.4105, -0.4105, -0.8261, -0.4105,\n",
      "        -0.4105, -0.4105], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5829983949661255\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.6226,  0.6372, -0.5838, -0.8383], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.8383, -0.8404,  0.6372,  0.1816,  0.6372, -0.6443,  0.4983, -0.3876,\n",
      "         0.6372,  0.3759], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.6673, -0.8365, -0.4265, -0.0920, -0.4265, -0.8458,  0.1728, -0.4265,\n",
      "        -0.4265, -0.7344], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4879376292228699\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.6365,  0.6134, -0.5994, -0.8408], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.6107, -0.6838,  0.1653, -0.6838, -0.3891, -0.6558,  0.3542, -0.8408,\n",
      "         0.6134,  0.6134], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9124, -0.8642, -0.1048, -0.8642, -0.1048, -0.8642, -0.4479, -0.6812,\n",
      "        -0.4479, -0.4479], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.32750681042671204\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.6472,  0.5958, -0.6248, -0.8430], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.0789, -0.4558, -0.5538,  0.5958,  0.5958, -0.5459, -0.6681,  0.5958,\n",
      "         0.5958, -0.8651], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5958, -0.4638, -0.8646, -0.4638, -0.4638, -0.8590, -0.8787, -0.4638,\n",
      "        -0.4638, -0.8646], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5185117721557617\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.6631,  0.5798, -0.6542, -0.8507], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.1409,  0.9711,  0.5798, -0.8829, -0.4381,  0.5798, -0.5749,  0.5798,\n",
      "         0.3179,  0.5798], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.1260, -0.7385, -0.4782, -0.8731, -0.4782, -0.4782, -0.8731, -0.4782,\n",
      "        -0.4782, -0.4782], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.8195810317993164\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.6822,  0.5532, -0.6839, -0.8571], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.4589,  0.5532, -0.6822,  0.1283, -0.6566, -0.4203, -0.6839, -0.9011,\n",
      "         0.0393, -0.8051], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.5021, -0.5021, -0.7471, -0.1700, -0.8845, -0.1700, -0.7443, -0.8845,\n",
      "        -0.6255, -0.9024], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17787839472293854\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-0.7160,  0.1165,  0.5325,  0.5325,  0.0961,  0.5325, -0.6265,  0.5325,\n",
      "        -0.9157, -0.7038], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.7588, -0.2054, -0.5208, -0.5208, -0.7624, -0.5208, -0.9253, -0.5208,\n",
      "        -0.7624, -0.7489], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5394522547721863\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-0.7517, -0.9344,  0.5098,  0.5098,  0.5098,  0.5098,  0.5098, -0.0035,\n",
      "        -0.7213, -0.8724], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.7710, -0.9087, -0.5412, -0.5412, -0.5412, -0.5412, -0.5412, -0.6433,\n",
      "        -0.7511, -0.7790], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5942814350128174\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.7374,  0.4775, -0.7865, -0.8752], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.6911,  0.0584, -0.8669, -0.7924,  0.0825, -0.5320,  0.4775, -0.5320,\n",
      "         0.8147, -0.9547], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9683, -0.8000, -0.2668, -1.0287, -0.2668, -0.5702, -0.5702, -0.5702,\n",
      "        -0.7748, -0.8000], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5002539157867432\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.7473,  0.4325, -0.8256, -0.8621], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.9603, -0.7473,  0.4325,  0.4325, -0.8519,  0.4325,  0.4325,  0.4325,\n",
      "         0.4325, -0.6831], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.8250, -0.7694, -0.6108, -0.6108, -0.3341, -0.6108, -0.6108, -0.6108,\n",
      "        -0.6108, -0.7852], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6827419400215149\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-0.8366,  0.3253,  0.1546, -0.0056, -0.5134, -0.8366, -0.5861, -0.8640,\n",
      "        -0.8640,  0.3669], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.8609,  0.0542, -0.6698, -0.8609, -0.8082, -0.8609, -0.6698, -0.8440,\n",
      "        -0.8440, -0.6698], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2655196785926819\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.5290,  0.1244, -0.6872, -0.7171], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.9613,  0.3221, -0.8167,  0.3221, -0.6130, -0.9261,  0.3221,  0.3221,\n",
      "         0.3221, -0.8981], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.8881, -0.7101, -0.8881, -0.7101, -0.7101, -1.0101, -0.7101, -0.7101,\n",
      "        -0.7101, -0.8701], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.535498857498169\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.7759,  0.2702, -0.9336, -0.8037], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.2623,  0.2702,  0.2702, -0.9616, -0.4962,  0.2702,  0.1791, -1.1293,\n",
      "        -0.7759, -0.1087], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.0208, -0.7568, -0.7568, -0.9179, -0.5589, -0.7568, -0.8300, -1.0667,\n",
      "        -0.8300, -1.0667], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.519318699836731\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-0.5556, -0.1589,  0.2107, -0.9672, -0.7930, -0.2034, -0.9204,  0.2107,\n",
      "         0.0539,  0.1427], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.8716, -1.0980, -0.8104, -0.9345, -0.9515, -0.8023, -1.0761, -0.8104,\n",
      "        -0.8104, -0.8468], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5202175974845886\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([ 0.1633, -0.1221, -0.7474, -0.5230,  0.1633,  0.0162, -0.8061,  0.1633,\n",
      "        -0.6857,  0.1633], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.8530, -0.6986, -0.6986, -0.6986, -0.8530, -0.8530, -0.8537, -0.8530,\n",
      "        -0.8530, -0.8530], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5283147096633911\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([ 0.1286,  0.1286,  0.1286, -0.1606, -0.6198, -1.0484,  0.1286,  0.1286,\n",
      "         0.0683,  0.1286], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.8843, -0.8843, -0.8843, -0.7469, -0.8843, -1.2574, -0.8843, -0.8843,\n",
      "        -0.8591, -0.8843], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.747275173664093\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([ 0.0899, -0.7299, -0.6523, -0.6523,  0.0899, -1.0645, -0.8940, -0.6850,\n",
      "        -0.9456,  0.0899], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9191, -1.1783, -0.9191, -0.9191, -0.9191, -1.0294, -1.1853, -0.1473,\n",
      "        -1.1853, -0.9191], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.38302868604660034\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-0.9625, -1.0588, -0.8074,  0.0573,  0.0573, -1.0611,  0.0573, -0.6780,\n",
      "        -0.9078, -0.2209], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.2169, -1.0868, -1.0145, -0.9484, -0.9484, -1.2169, -0.9484, -1.3183,\n",
      "        -0.8625, -1.0868], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4329046607017517\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9187,  0.0914, -1.0637, -0.8623], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.1217, -0.7900,  0.0357, -1.1127, -0.1151, -0.7791, -1.3630,  0.0357,\n",
      "         0.0357,  0.0357], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3438, -0.9678, -0.9678, -1.1036, -0.9678, -1.2155, -1.2155, -0.9678,\n",
      "        -0.9678, -0.9678], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5049108862876892\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.1653,  0.8605, -1.3746, -1.2231], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.0230,  0.0230, -0.8031,  0.0230,  0.0230,  0.0805, -1.0078,  0.0230,\n",
      "         0.0230, -1.2231], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9793, -0.9793, -0.8741, -0.9793, -0.9793, -0.2255, -0.8524, -0.9793,\n",
      "        -0.9793, -0.9276], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6237843632698059\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.8479, -0.1319, -1.0845, -0.7788], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.0109, -0.0109, -0.0109,  0.0486, -0.0109, -1.2095, -0.0109, -1.0489,\n",
      "        -0.9121, -1.0845], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.0098, -1.0098, -1.0098, -0.2539, -1.0098, -1.1187, -1.0098, -0.8616,\n",
      "        -1.1527, -0.9685], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5195297002792358\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.8811, -0.1664, -1.1117, -0.8059], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.3080, -0.8867, -0.0571, -0.0571, -0.0571, -1.2314, -1.2031, -0.1322,\n",
      "        -0.0571, -0.8059], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.1863, -1.0514, -1.0514, -1.0514, -1.0514, -1.1863, -1.3117, -0.8803,\n",
      "        -1.0514, -0.2855], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.559728741645813\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9034, -0.2094, -1.1343, -0.8132], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.1128, -0.1161, -1.5090, -0.3921, -0.5064, -0.9254, -0.1161, -0.1161,\n",
      "        -0.9753, -0.2488], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.9038, -1.1045, -1.3140, -0.9997, -1.0445, -1.3140, -1.1045, -1.1045,\n",
      "        -1.2239, -1.1045], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4616278111934662\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9195, -0.2474, -1.1488, -0.8338], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.0170, -1.1321, -0.2927, -0.1625, -0.1625, -0.1625, -0.1625, -0.5699,\n",
      "        -1.2189, -0.1625], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.2634, -0.9262, -1.1463, -1.1463, -1.1463, -1.1463, -1.1463, -1.3491,\n",
      "        -1.5129, -1.1463], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6364129185676575\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-0.9305, -0.2905, -0.9778, -1.2612, -1.5536, -1.3142, -0.2138, -0.2138,\n",
      "        -0.2138, -0.9406], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.1924, -1.2614, -1.1924, -1.4440, -1.3843, -1.4440, -1.1924, -1.1924,\n",
      "        -1.1924, -1.2684], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.411686509847641\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.1645, -0.2355, -1.3055, -1.1143], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([ 0.5860, -1.1252, -0.2517, -1.3687, -0.2517, -0.2517, -0.5341, -1.1740,\n",
      "        -0.2517, -1.3018], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.2954, -1.3517, -1.2266, -1.2954, -1.2266, -1.2266, -1.1141, -0.9826,\n",
      "        -1.2266, -1.6072], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.7863821983337402\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.1967, -0.2996, -1.3532, -1.1747], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.3920, -0.3099, -0.3099, -0.4440, -1.2305, -0.5869, -0.9452, -0.1807,\n",
      "        -0.4440, -1.0218], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.5282, -1.2789, -1.2789, -1.2789, -1.2696, -1.1626, -0.5692, -1.3559,\n",
      "        -1.2789, -1.6511], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5542082786560059\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-0.3550, -0.3550, -1.2234, -0.3550,  0.3673, -0.3550, -0.3550, -1.3515,\n",
      "        -0.3889, -0.3550], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3195, -1.3195, -1.4857, -1.3195, -1.3500, -1.3195, -1.3195, -1.5776,\n",
      "        -1.3500, -1.3195], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.9574346542358398\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2646, -0.4303, -1.4424, -1.3058], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.4945, -0.4256, -0.4256, -1.3079, -0.4256, -1.4945, -0.5870, -1.1745,\n",
      "        -1.1745, -0.4256], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3808, -1.3830, -1.3830, -1.5283, -1.3830, -1.3808, -1.5274, -1.3830,\n",
      "        -1.3830, -1.3830], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.47126397490501404\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3041, -0.4993, -1.4930, -1.3798], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.4887, -1.1810, -1.2319, -1.3955, -0.4887, -0.4887, -0.6382, -0.4887,\n",
      "        -0.4887, -0.4581], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4398, -1.4927, -1.7822, -1.5743, -1.4398, -1.4398, -1.5871, -1.4398,\n",
      "        -1.4398, -1.4123], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6766082644462585\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-1.5910, -0.7059, -0.4981, -0.5472, -0.4879, -0.5472, -1.3092, -0.5472,\n",
      "        -1.1406, -1.3325], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.4390, -1.4925, -1.0559, -1.4925, -1.5308, -1.4925, -1.4925, -1.4925,\n",
      "        -1.0559, -1.8175], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4997236728668213\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.1291, -0.5203, -1.3681, -1.2097], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-0.7464, -0.6022, -0.7664, -0.6022, -0.5203, -1.2097, -0.6022, -0.6022,\n",
      "        -0.6155, -0.6022], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.6897, -1.5420, -1.5420, -1.5420, -1.4683, -1.1598, -1.5420, -1.5420,\n",
      "        -1.5539, -1.5420], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.7689195871353149\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-1.6758, -1.7871, -1.4503, -1.2218, -0.6929, -1.4024, -0.6644, -1.3813,\n",
      "        -0.6647, -0.6644], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7534, -1.8698, -1.5980, -1.5983, -1.6236, -1.6236, -1.5980, -1.6038,\n",
      "        -1.6038, -1.5980], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.37659531831741333\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-0.6017, -1.5397, -0.7523, -0.7188, -0.7479, -2.0901, -1.7863, -0.7188,\n",
      "        -0.7188, -0.8988], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.5415, -1.6469, -1.6297, -1.6469, -1.6731, -1.8983, -1.8089, -1.6469,\n",
      "        -1.6469, -1.6469], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5701689720153809\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-0.7839, -1.5765, -1.8678, -0.8646, -2.1868, -0.7839, -1.8678, -0.7788,\n",
      "        -1.8192, -1.0969], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7055, -1.5058, -1.8819, -1.6621, -1.9634, -1.7055, -1.8819, -1.4766,\n",
      "        -1.9629, -1.8685], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3492656350135803\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3404, -0.7112, -1.5775, -1.4676], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.1534, -2.2477, -0.7112, -1.5775, -1.9407, -1.0701, -1.9523, -0.8630,\n",
      "        -1.8958, -1.7332], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.8955, -1.9420, -1.6401, -1.8200, -1.9631, -1.7767, -2.0570, -1.7767,\n",
      "        -2.0266, -1.7767], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2930402159690857\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.3981, -0.7692, -1.6483, -1.5276], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.1617, -1.9167, -1.2410, -1.7301, -0.9292, -0.9292, -0.7692, -1.8857,\n",
      "        -0.9292, -1.7301], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.8362, -2.1169, -1.9899, -1.6923, -1.8362, -1.8362, -1.6923, -2.1169,\n",
      "        -1.8362, -1.6923], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.44326916337013245\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-2.4244, -1.7909, -0.9893, -2.0098, -0.9893, -1.2579, -1.6640, -1.5851,\n",
      "        -0.8278, -0.9893], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.0950, -1.7450, -1.8903, -1.7450, -1.8903, -1.8903, -2.0706, -1.7671,\n",
      "        -1.7450, -1.8903], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4056207537651062\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-1.0434, -1.0434, -1.0434, -1.0434, -1.3621, -1.9910, -1.4265, -2.2135,\n",
      "        -2.3611, -2.0246], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.9391, -1.9391, -1.9391, -1.9391, -1.9391, -1.9391, -2.1424, -2.2170,\n",
      "        -2.1805, -1.8031], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4138595461845398\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9249, -1.1583, -2.0158, -2.1050], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.1036, -1.1036, -1.7999, -1.1036, -1.1036, -2.2426, -0.9687, -2.6050,\n",
      "        -1.1036, -2.4507], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.9933, -1.9933, -2.0425, -1.9933, -1.9933, -2.3867, -1.8718, -2.3867,\n",
      "        -1.9933, -2.2791], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.49296075105667114\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.9853, -1.2233, -2.0117, -2.1135], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.0281, -1.1790, -2.5269, -2.6210, -1.1790, -2.1700, -1.6651, -1.1790,\n",
      "        -1.9677, -1.0587], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.9528, -2.0611, -2.3862, -2.4986, -2.0611, -2.4477, -2.2942, -2.0611,\n",
      "        -1.9814, -1.9528], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.36472970247268677\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-1.1543, -1.8918, -1.8359, -1.2654, -1.2929, -1.2654, -1.1543, -1.4375,\n",
      "        -1.2654, -1.2929], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.0389, -2.1636, -2.1388, -2.1388, -2.1636, -2.1388, -2.0389, -2.1492,\n",
      "        -2.1388, -2.1636], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.6042226552963257\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-1.3529, -1.3529, -1.3529, -2.4579, -1.8594, -2.0728, -1.9590, -1.9336,\n",
      "        -1.2604, -1.3529], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2176, -2.2176, -2.2176, -2.0324, -2.2176, -2.1343, -2.2176, -2.0324,\n",
      "        -2.1343, -2.2176], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.41444700956344604\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.1690, -1.4323, -2.0332, -2.1491], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.4459, -1.4459, -2.0457, -2.5560, -1.4323, -2.6564, -2.0882, -1.4459,\n",
      "        -2.1690, -2.0809], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.3014, -2.3014, -2.2890, -2.7347, -2.2890, -2.8793, -2.5186, -2.3014,\n",
      "        -2.2413, -2.3014], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.33091622591018677\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.2409, -1.4900, -2.0625, -2.1887], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.5272, -1.4928, -1.9878, -1.5272, -1.8080, -3.0148, -1.5272, -1.4900,\n",
      "        -2.1560, -2.8181], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.3745, -2.3436, -2.5825, -2.3745, -2.4143, -3.0001, -2.3745, -2.3410,\n",
      "        -2.3436, -2.8984], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4364621639251709\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-2.8175, -2.9427, -2.0985, -2.3875, -2.3737, -2.0017, -2.2551, -2.3737,\n",
      "        -1.5929, -1.6073], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2487, -2.7334, -2.6247, -2.4466, -2.4466, -2.7334, -2.3924, -2.4466,\n",
      "        -2.4336, -2.4466], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2623790502548218\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.3733, -1.5907, -2.1595, -2.3361], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.6961, -1.5907, -1.6961, -1.6771, -1.6771, -1.6771, -2.2995, -2.7491,\n",
      "        -1.5907, -1.5907], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.5265, -2.4316, -2.5265, -2.5094, -2.5094, -2.5094, -2.5094, -2.6541,\n",
      "        -2.4316, -2.4316], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5631803274154663\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.4399, -1.6789, -2.2331, -2.4352], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.5618, -2.9008, -2.3752, -1.7719, -1.7719, -1.8246, -3.1306, -2.3937,\n",
      "        -1.7719, -2.6134], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.6147, -3.0765, -2.6422, -2.5947, -2.5947, -2.6422, -2.9330, -3.1732,\n",
      "        -2.5947, -2.5947], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.34513574838638306\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.5082, -1.7687, -2.3302, -2.5176], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.5368, -2.7473, -2.4908, -1.7687, -2.5082, -1.7687, -2.6454, -1.9478,\n",
      "        -1.8806, -2.3302], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.5370, -3.1044, -2.7530, -2.5919, -2.7530, -2.5919, -2.7135, -2.7530,\n",
      "        -2.6925, -2.5919], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.29919886589050293\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-3.0954, -3.2094, -1.9749, -1.8575, -1.8575, -2.6278, -2.9138, -1.9749,\n",
      "        -1.9749, -2.5931], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.6197, -3.3338, -2.7775, -2.6717, -2.6717, -2.8319, -3.1891, -2.7775,\n",
      "        -2.7775, -2.7775], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3651271164417267\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-2.0782, -2.7707, -2.9805, -2.0782, -2.9476, -3.0251, -2.9260, -2.6797,\n",
      "        -1.9679, -2.0782], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.8704, -3.4025, -2.8620, -2.8704, -2.8704, -2.8704, -3.4936, -2.7711,\n",
      "        -2.7711, -2.8704], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3301493525505066\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.8745, -2.0794, -2.8930, -2.8836], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.1890, -3.1898, -2.0794, -2.0794, -3.0952, -3.3761, -2.7312, -2.2121,\n",
      "        -2.1890, -2.1890], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.9701, -2.8135, -2.8715, -2.8715, -2.9302, -3.4581, -2.9701, -2.9909,\n",
      "        -2.9701, -2.9701], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.39241090416908264\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.9791, -2.2196, -3.1096, -3.0179], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.3221, -3.4088, -2.1515, -2.2512, -3.6831, -2.3251, -2.3221, -2.2196,\n",
      "        -3.5308, -3.2728], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.0899, -3.9071, -3.0408, -2.9364, -3.5082, -3.0926, -3.0899, -2.9976,\n",
      "        -3.4713, -3.7210], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.41169309616088867\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-2.4449, -2.4449, -2.3516, -3.5500, -3.1116, -2.3516, -2.4449, -2.8430,\n",
      "        -3.1794, -3.1794], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2004, -3.2004, -3.1164, -3.8105, -3.0526, -3.1164, -3.2004, -3.0326,\n",
      "        -3.1003, -3.1003], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3002191185951233\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.2604, -2.5011, -3.5937, -3.2928], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.5011, -3.7119, -4.2544, -4.3013, -3.2283, -3.5937, -3.6588, -2.5815,\n",
      "        -2.3661, -2.5815], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2510, -3.3234, -4.1089, -3.7402, -3.6802, -3.2510, -4.1089, -3.3234,\n",
      "        -3.2942, -3.3234], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.35355955362319946\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-3.7493, -3.4081, -2.7652, -3.5828, -3.3863, -2.6870, -3.4081, -2.6870,\n",
      "        -2.6771, -2.7652], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4887, -3.4094, -3.4887, -3.7884, -3.3340, -3.4183, -3.4094, -3.4183,\n",
      "        -3.4094, -3.4887], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.27655482292175293\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-2.9734, -2.8908, -2.8908, -2.9734, -3.9865, -2.8201, -2.9734, -3.9054,\n",
      "        -2.8908, -3.7581], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6761, -3.6017, -3.6017, -3.6761, -4.3638, -3.5381, -3.6761, -3.6017,\n",
      "        -3.6017, -4.1962], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.39394235610961914\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-4.1681, -3.1848, -4.3741, -3.5270, -3.7675, -3.9520, -3.1848, -3.1083,\n",
      "        -3.7775, -3.8727], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.1099, -3.8663, -4.1099, -3.6463, -3.8663, -3.7975, -3.8663, -3.7975,\n",
      "        -3.6135, -3.7537], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1565961092710495\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-3.1453, -3.3299, -4.0905, -3.5170, -3.4122, -3.9435, -3.4122, -3.3299,\n",
      "        -3.4122, -4.3775], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7890, -3.9969, -4.4778, -4.0710, -4.0710, -3.9969, -4.0710, -3.9969,\n",
      "        -4.0710, -4.6206], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.31248411536216736\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.9878, -3.4909, -4.0778, -4.0939], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.0091, -3.2267, -4.2172, -3.6222, -3.5494, -4.0091, -4.4191, -3.5494,\n",
      "        -4.4213, -3.5494], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3158, -3.9040, -3.9040, -4.2600, -4.1945, -4.3158, -4.6909, -4.1945,\n",
      "        -4.6909, -4.1945], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.25465232133865356\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.8845, -3.4991, -3.9256, -3.9484], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.1932, -4.3293, -3.8230, -3.7791, -4.2913, -4.6162, -3.8349, -3.8230,\n",
      "        -4.2550, -3.8230], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4545, -4.1492, -4.4407, -4.4012, -4.3127, -4.4545, -4.4407, -4.4407,\n",
      "        -4.4012, -4.4407], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.20473714172840118\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.4984, -3.9872, -4.4151, -4.4997], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.3944, -3.9872, -3.9815, -4.1267, -4.4151, -3.9979, -3.9979, -3.9979,\n",
      "        -3.9872, -3.9872], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9410, -4.5885, -4.5981, -4.5885, -4.5885, -4.5981, -4.5981, -4.5981,\n",
      "        -4.5885, -4.5885], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3087608516216278\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-3.6435, -4.1413, -4.1248, -4.1413, -4.1413, -4.7930, -4.1413, -4.1413,\n",
      "        -4.2961, -4.1312], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.2792, -4.7271, -4.7123, -4.7271, -4.7271, -4.7181, -4.7271, -4.7271,\n",
      "        -4.7271, -4.7123], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2994577884674072\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-4.4943, -4.2940, -4.2422, -4.7542, -4.2422, -4.3074, -4.6937, -4.2422,\n",
      "        -4.1061, -4.2422], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1735, -4.8646, -4.8180, -4.7854, -4.8180, -4.8180, -4.7854, -4.8180,\n",
      "        -4.6673, -4.8180], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2697974741458893\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-4.9331, -4.8248, -4.3694, -4.7178, -5.1103, -3.9726, -5.4304, -5.0008,\n",
      "        -4.7381, -5.1799], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5753, -4.9473, -4.9324, -4.7792, -5.0624, -4.5753, -5.2323, -4.5753,\n",
      "        -5.0189, -5.2323], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1131191998720169\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0472, -4.6119, -5.1536, -5.2266], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.6171, -5.6036, -5.0479, -4.6119, -4.5050, -5.2354, -4.5050, -5.0479,\n",
      "        -4.7529, -4.7152], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4913, -5.4547, -4.7827, -5.1507, -5.0545, -5.2641, -5.0545, -4.7827,\n",
      "        -4.9493, -5.1265], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12815479934215546\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-5.7212, -5.2282, -4.7659, -5.2282, -5.7212, -4.6308, -4.6308, -5.0906,\n",
      "        -4.9261, -5.3667], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.8167, -5.2893, -5.2893, -5.2893, -5.8167, -5.1677, -5.1677, -5.0172,\n",
      "        -5.1677, -5.3978], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09409352391958237\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-5.1341, -5.9589, -5.1278, -5.2895, -5.1693, -5.3683, -4.7287, -4.9009,\n",
      "        -4.7287, -5.2331], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2381, -5.9540, -5.2738, -5.4523, -5.2559, -5.5368, -5.2559, -5.4108,\n",
      "        -5.2559, -5.4565], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09601572155952454\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0459, -4.8296, -5.1995, -5.6660], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.0328, -6.2115, -5.6510, -5.3059, -5.5886, -4.8296, -4.8296, -4.9352,\n",
      "        -5.5382, -5.6660], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5295, -6.0859, -5.8726, -5.4417, -5.5859, -5.3466, -5.3466, -5.4417,\n",
      "        -5.9216, -5.6969], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12692196667194366\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.1825, -4.9434, -5.2773, -5.8107], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.1571, -5.6088, -5.3816, -4.9434, -5.1724, -5.1847, -6.0585, -5.9276,\n",
      "        -5.1571, -5.5383], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6414, -5.4490, -5.4030, -5.4490, -5.4490, -5.6663, -6.0814, -5.4030,\n",
      "        -5.6414, -5.6663], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13512063026428223\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.2868, -5.0768, -5.3888, -5.8743], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.0768, -5.8743, -5.3947, -5.0947, -5.5879, -6.0335, -5.3216, -6.5086,\n",
      "        -6.4609, -5.8175], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5691, -6.0425, -5.8552, -5.7894, -5.8552, -5.7894, -5.7894, -6.2144,\n",
      "        -6.1931, -5.5691], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15352973341941833\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.4215, -5.2656, -5.4974, -5.9146], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.6976, -5.4827, -5.2656, -6.5289, -5.4827, -5.9502, -5.9855, -5.4975,\n",
      "        -5.2656, -6.1582], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.0601, -5.9344, -5.7390, -6.4187, -5.9344, -6.0650, -5.7390, -6.0601,\n",
      "        -5.7390, -6.1855], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1391330510377884\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.6235, -5.4615, -5.6999, -5.9866], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.7548, -5.4615, -5.4615, -5.6235, -5.6417, -5.4615, -6.1247, -5.6417,\n",
      "        -5.7548, -5.5931], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.1793, -5.9154, -5.9154, -5.6944, -6.0775, -5.9154, -5.9154, -6.0775,\n",
      "        -6.1793, -6.0775], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16417047381401062\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.8217, -5.7066, -5.9199, -6.0854], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.7066, -5.7066, -6.6250, -6.6040, -6.4641, -6.1151, -5.7066, -6.0854,\n",
      "        -5.7066, -5.9199], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.1360, -6.1360, -6.7208, -6.6580, -6.2285, -6.2599, -6.1360, -6.5036,\n",
      "        -6.1360, -6.3269], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11664165556430817\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-5.9794, -5.9794, -6.0859, -6.3071, -6.1866, -6.0426, -5.9794, -6.2177,\n",
      "        -6.2706, -6.2706], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.3815, -6.3815, -6.3815, -6.6009, -6.4383, -6.4383, -6.3815, -6.4202,\n",
      "        -6.3815, -6.3815], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09442038834095001\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.3499, -6.1178, -6.7524, -6.2787], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.1045, -6.3499, -6.2700, -6.1045, -6.2034, -7.1056, -6.7888, -6.1045,\n",
      "        -6.7895, -6.2377], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.4940, -6.5830, -6.5332, -6.4940, -6.5830, -7.0520, -6.6140, -6.4940,\n",
      "        -7.0520, -6.5060], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08973373472690582\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.4456, -6.1972, -6.9281, -6.4383], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.3754, -6.6341, -6.6787, -6.1972, -6.2373, -6.2373, -6.4383, -7.1452,\n",
      "        -6.3754, -6.6837], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.5775, -6.7665, -6.7764, -6.5775, -6.6136, -6.6136, -6.7687, -7.1204,\n",
      "        -6.5775, -6.6136], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06512375175952911\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.6093, -6.2239, -7.1055, -6.6494], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.3463, -6.6093, -6.7033, -6.3463, -6.4978, -7.2241, -6.5408, -7.1126,\n",
      "        -7.1392, -6.3463], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.7117, -6.8867, -6.4839, -6.7117, -6.6804, -7.0035, -6.8867, -6.6015,\n",
      "        -7.2690, -6.7117], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1005263477563858\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.8406, -6.3045, -7.2552, -6.9086], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.3045, -6.8406, -7.1295, -6.3045, -6.6294, -7.2975, -7.4825, -6.5921,\n",
      "        -6.3707, -6.8987], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.6740, -6.9665, -6.9665, -6.6740, -6.9665, -7.5565, -7.3425, -6.6740,\n",
      "        -6.6098, -6.6740], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06301499903202057\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.0609, -6.4212, -7.4280, -7.1506], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.7398, -6.7398, -7.5502, -6.7398, -7.1170, -7.9333, -6.7398, -6.5904,\n",
      "        -6.7398, -6.4212], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.0659, -7.0659, -7.4053, -7.0659, -7.0466, -7.3802, -7.0659, -6.9314,\n",
      "        -7.0659, -6.7791], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11076198518276215\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.1694, -6.5930, -7.5925, -7.3826], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.9163, -6.9163, -7.5698, -7.4916, -7.5925, -6.7681, -6.7681, -7.2767,\n",
      "        -6.5930, -7.1774], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.2247, -7.2247, -7.4596, -7.0913, -7.0913, -7.0913, -7.0913, -7.2190,\n",
      "        -6.9337, -6.9708], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09847934544086456\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-7.1559, -7.0395, -7.8508, -7.1559, -6.7967, -7.0395, -8.5122, -7.1559,\n",
      "        -7.6024, -7.6399], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.4403, -7.3355, -7.8255, -7.4403, -7.1170, -7.3355, -7.9043, -7.4403,\n",
      "        -7.3355, -7.1170], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12354516983032227\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.4421, -7.3999, -7.5785, -7.9419], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-7.7966, -7.2572, -7.7741, -7.4357, -7.9800, -7.4357, -7.3999, -7.5359,\n",
      "        -7.4357, -7.0793], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.8140, -7.6005, -7.6005, -7.6005, -7.9078, -7.6005, -7.6599, -7.6599,\n",
      "        -7.6005, -7.3714], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.040322866290807724\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.5049, -7.7532, -7.5904, -8.0807], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-7.7625, -7.3933, -8.2353, -8.2720, -7.3000, -8.0677, -7.3933, -7.3159,\n",
      "        -8.3919, -7.3933], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.7746, -7.5700, -7.8094, -7.9516, -7.5843, -8.0538, -7.5700, -7.4428,\n",
      "        -7.7746, -7.5700], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08558806031942368\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-7.9263, -8.5356, -7.6736, -7.4094, -7.4229, -7.4094, -8.4713, -8.0429,\n",
      "        -8.0429, -8.2907], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.6685, -7.9135, -7.7249, -7.5458, -7.7249, -7.5458, -7.9607, -7.8317,\n",
      "        -7.8317, -8.2242], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09388784319162369\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-8.1830, -7.6656, -8.0290, -8.0892, -8.5172, -7.4265, -8.0290, -8.1830,\n",
      "        -8.1830, -8.0290], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.9427, -7.7586, -7.7586, -7.6787, -8.1154, -7.7586, -7.7586, -7.9427,\n",
      "        -7.9427, -7.7586], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08415436744689941\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-8.1484, -7.9248, -7.9248, -7.9424, -8.1484, -7.9248, -7.8690, -8.1484,\n",
      "        -7.6890, -7.5350], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.0557, -7.8968, -7.8968, -8.0557, -8.0557, -7.8968, -8.0981, -8.0557,\n",
      "        -8.3343, -8.4014], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12605223059654236\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.1536, -8.3245, -8.0309, -8.0458], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-7.9834, -7.7644, -7.7644, -7.8867, -7.7644, -8.6122, -8.0444, -7.7356,\n",
      "        -7.8867, -7.7256], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.1766, -7.9879, -7.9879, -7.7234, -7.9879, -8.5270, -8.0719, -7.8557,\n",
      "        -7.7234, -7.7234], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02630452811717987\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-8.3462, -7.9858, -7.6436, -7.9373, -7.6436, -7.7300, -7.6436, -8.2747,\n",
      "        -8.1293, -7.6436], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.8792, -8.1079, -7.8792, -7.9412, -7.8792, -7.9412, -7.8792, -8.1079,\n",
      "        -8.5865, -7.8792], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07365070283412933\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.5738, -8.6104, -8.8613, -8.0317], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-8.0356, -7.6477, -8.2187, -8.5703, -8.4056, -7.7729, -7.7729, -8.5703,\n",
      "        -8.0356, -7.6477], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.0989, -7.8829, -8.0183, -7.9252, -7.8829, -7.9252, -7.9252, -7.9252,\n",
      "        -8.0989, -7.8829], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13106317818164825\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.4085, -8.6605, -8.9461, -8.0826], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-8.0203, -8.6444, -8.0203, -7.8758, -8.1586, -8.0299, -8.0203, -8.0203,\n",
      "        -7.8758, -8.0203], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.0813, -8.2743, -8.0813, -8.0221, -8.2640, -8.3869, -8.0813, -8.0813,\n",
      "        -8.0221, -8.0813], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.033695753663778305\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.3025, -8.7871, -9.0447, -8.1449], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-8.1397, -8.0083, -8.0469, -7.7758, -8.0469, -8.0469, -7.7978, -7.7978,\n",
      "        -8.0469, -8.2851], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.0828, -8.0828, -8.0395, -7.8735, -8.0395, -8.0395, -7.9223, -7.9223,\n",
      "        -8.0395, -7.8735], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.021896103397011757\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.2127, -8.8940, -9.1291, -8.2122], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-8.4506, -8.1239, -8.2122, -8.4366, -8.2927, -7.9030, -7.9016, -8.3756,\n",
      "        -8.2386, -7.8737], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.9878, -8.1493, -8.3909, -8.4210, -7.7887, -8.1493, -7.9878, -7.8183,\n",
      "        -8.2225, -7.8183], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0882948487997055\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.2777, -8.9176, -9.1433, -8.2482], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-8.0183, -8.0183, -8.0992, -7.8839, -7.8941, -8.3155, -7.7505, -8.4240,\n",
      "        -7.8941, -8.3407], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.9755, -7.9755, -7.8699, -8.1703, -7.8699, -7.8699, -8.5561, -8.4684,\n",
      "        -7.8699, -8.2929], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0991213321685791\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.3182, -8.9220, -9.1089, -8.3475], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-8.0272, -8.0272, -8.2348, -8.8487, -7.9246, -8.0272, -7.8806, -7.9246,\n",
      "        -8.2196, -7.9246], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.0646, -8.0646, -8.0590, -7.9264, -7.9301, -8.0646, -8.0879, -7.9301,\n",
      "        -7.9301, -7.9301], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10124742984771729\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-7.9386, -7.9386, -8.0097, -8.4170, -7.9386, -7.9386, -8.9837, -8.1918,\n",
      "        -7.9386, -8.4170], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.0564, -8.0564, -8.1247, -8.5753, -8.0564, -8.0564, -8.5753, -8.0564,\n",
      "        -8.0564, -8.5753], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.031780000776052475\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-8.0434, -8.0287, -8.0434, -7.8342, -8.0665, -7.9954, -8.0287, -8.0434,\n",
      "        -7.9954, -8.1720], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.2390, -8.2085, -8.2390, -8.2390, -8.2390, -8.1477, -8.2085, -8.2390,\n",
      "        -8.1477, -8.0497], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.043459203094244\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.0238, -8.1660, -7.9757, -8.2002], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.0526, -8.1490, -8.1156, -8.1037, -8.0955, -8.2327, -8.1037, -8.1156,\n",
      "        -8.2009, -8.0526], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.2473, -8.1536, -8.2279, -8.2279, -8.1536, -7.9255, -8.2279, -8.2279,\n",
      "        -8.0506, -8.2473], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.025229236111044884\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-8.1386, -8.4333, -8.2073, -8.2073, -7.9480, -8.1818, -8.1820, -8.1230,\n",
      "        -8.1386, -8.0905], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.1090, -8.1532, -8.1090, -8.1090, -8.1532, -7.9662, -8.1532, -8.3107,\n",
      "        -8.1090, -7.6921], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03828844055533409\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.5479, -9.0641, -8.6166, -8.6285], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-8.2431, -7.9690, -8.2538, -8.3445, -8.1340, -8.1858, -8.2218, -8.2218,\n",
      "        -7.9175, -8.6365], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.1257, -8.1721, -7.6720, -7.9682, -7.9682, -7.9682, -8.1721, -8.1721,\n",
      "        -8.1689, -8.2453], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08310969173908234\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.9345, -7.9490, -7.6803, -7.7718], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.1663, -8.1798, -8.3408, -8.1741, -8.1663, -7.7103, -8.2227, -7.7095,\n",
      "        -8.1798, -8.1663], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.2842, -8.2360, -8.1449, -8.6527, -8.2842, -7.9123, -7.9386, -8.1738,\n",
      "        -8.2360, -8.2842], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06524769961833954\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.3405, -8.9065, -8.8007, -8.4991], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-8.4991, -7.8339, -7.9109, -8.1585, -8.2710, -8.1647, -8.1647, -7.8420,\n",
      "        -8.1647, -7.6644], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.5065, -7.9128, -8.0578, -8.2187, -8.0124, -8.1526, -8.1526, -8.1526,\n",
      "        -8.1526, -8.1526], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.043361056596040726\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.6132, -7.7995, -7.8156, -7.5691], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-7.8872, -8.1661, -7.8584, -8.1217, -7.9137, -7.8512, -8.1217, -8.1217,\n",
      "        -8.2588, -8.1217], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.9357, -8.1223, -8.0112, -8.0112, -7.9357, -7.9605, -8.0112, -8.0112,\n",
      "        -8.0112, -8.0112], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.015019851736724377\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.5000, -7.6996, -7.8933, -7.4765], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-8.0584, -8.3608, -7.8545, -7.8278, -8.0385, -8.3608, -8.0385, -8.0066,\n",
      "        -8.3046, -8.0905], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.7288, -8.0691, -7.8622, -7.8622, -7.9207, -8.0691, -7.9207, -7.9123,\n",
      "        -8.2526, -7.8654], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03701837360858917\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.4581, -7.5971, -7.9324, -7.4157], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-8.0836, -8.1313, -7.7476, -7.9259, -7.5642, -8.3292, -8.6574, -7.7178,\n",
      "        -7.9259, -8.4606], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.0852, -7.8781, -7.9044, -7.8781, -7.8781, -7.8807, -8.0852, -7.7991,\n",
      "        -7.8781, -7.8731], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10720570385456085\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.5094, -7.4659, -7.8474, -7.3678], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-7.9910, -7.9910, -7.9910, -7.6246, -7.7782, -7.7707, -7.9910, -7.9478,\n",
      "        -8.2987, -7.9128], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.1836, -8.1836, -8.1836, -7.9232, -7.9232, -7.7259, -8.1836, -7.9332,\n",
      "        -8.1836, -7.9332], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.027447020635008812\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-8.2942, -8.1459, -7.6897, -8.5879, -7.9416, -7.9749, -7.3293, -7.3293,\n",
      "        -8.2569, -7.5239], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.9156, -7.9110, -7.9110, -8.1774, -7.9110, -8.1774, -7.5964, -7.5964,\n",
      "        -7.9156, -7.7715], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.077840156853199\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.5544, -7.3476, -7.6452, -7.2977], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-7.8880, -7.9643, -7.9643, -7.6259, -7.6259, -7.6259, -8.4920, -7.8905,\n",
      "        -7.6259, -8.1636], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.8633, -8.1678, -8.1678, -7.8633, -7.8633, -7.8633, -8.1678, -7.5679,\n",
      "        -7.8633, -7.9672], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05567099526524544\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.6093, -7.3374, -7.5327, -7.2856], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-7.7994, -7.9775, -7.9919, -7.5948, -7.9919, -7.9775, -7.3374, -7.5327,\n",
      "        -7.7100, -7.4589], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.9314, -8.1797, -7.7130, -7.8353, -7.7130, -8.1797, -8.0502, -8.0150,\n",
      "        -7.9892, -7.7130], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11957564204931259\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.7299, -7.4163, -7.5032, -7.3297], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-7.8282, -7.8282, -7.3297, -7.9267, -8.0251, -7.8470, -7.8002, -7.8470,\n",
      "        -8.3022, -7.5687], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.8118, -7.8118, -7.5968, -7.8949, -8.2226, -7.7436, -8.0670, -7.7436,\n",
      "        -7.7436, -7.8118], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05756257846951485\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.7961, -7.5228, -7.4628, -7.3693], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-7.8832, -7.5732, -7.5732, -7.6537, -8.0759, -7.5228, -7.4628, -7.4074,\n",
      "        -7.7186, -7.8694], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.8159, -7.8159, -7.8159, -8.0314, -8.2025, -7.9630, -7.7743, -7.8703,\n",
      "        -7.6323, -7.8159], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0796385332942009\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.8963, -7.6987, -7.4758, -7.4266], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-7.4266, -8.0867, -7.8573, -7.6393, -8.3070, -7.9085, -7.7157, -7.9045,\n",
      "        -7.5664, -8.1686], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.6839, -8.1694, -7.6346, -7.8331, -7.8699, -7.7897, -7.8699, -7.8331,\n",
      "        -7.7897, -8.1694], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.044415779411792755\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-7.2832, -8.2448, -7.5849, -7.8615, -7.4935, -7.9536, -7.5823, -7.3806,\n",
      "        -7.4865, -7.7125], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.5549, -7.9206, -7.7983, -8.1470, -7.9206, -7.7442, -7.7442, -7.7643,\n",
      "        -7.7304, -7.7442], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07661378383636475\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-8.2967, -7.5437, -8.0011, -8.1028, -7.6414, -7.5057, -7.8299, -7.5658,\n",
      "        -7.5658, -7.3576], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.0707, -7.7093, -7.9432, -7.8153, -7.9432, -7.6214, -7.7093, -7.7359,\n",
      "        -7.7359, -7.6695], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04386374354362488\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-7.7486, -7.9134, -7.5871, -7.7570, -8.3291, -7.6822, -8.0196, -8.1013,\n",
      "        -8.1013, -7.6600], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.9140, -7.6733, -7.5965, -7.9813, -7.9813, -7.9813, -7.6733, -7.9140,\n",
      "        -7.9140, -7.7404], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05424480512738228\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-7.6140, -7.9208, -7.9402, -7.5008, -7.7444, -7.2504, -7.9588, -7.9674,\n",
      "        -7.5423, -7.7444], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.5253, -7.6963, -7.6963, -7.7881, -7.7881, -7.8408, -7.9413, -7.6963,\n",
      "        -7.4687, -7.7880], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06318958103656769\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-7.6196, -7.8499, -7.9551, -7.2268, -7.9551, -7.8794, -7.7837, -7.4990,\n",
      "        -8.1317, -7.9551], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.8576, -7.9723, -7.7491, -7.4340, -7.7491, -7.7491, -7.5460, -7.9723,\n",
      "        -7.9723, -7.7491], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05647805333137512\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-7.9416, -7.3479, -7.9416, -7.1899, -7.8670, -7.9416, -8.1109, -8.0561,\n",
      "        -7.0594, -8.0561], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.8288, -7.8535, -7.8288, -7.3535, -7.3535, -7.8288, -7.9379, -8.0141,\n",
      "        -7.9221, -8.0141], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13619521260261536\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-7.4026, -7.7559, -7.9545, -7.8912, -7.8912, -7.3482, -7.5430, -7.8054,\n",
      "        -7.8912, -7.8160], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.7979, -7.4315, -8.0249, -7.8880, -7.8880, -7.4315, -7.6169, -7.8880,\n",
      "        -7.8880, -7.9236], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.029729735106229782\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-7.5275, -7.8767, -7.8359, -7.5907, -7.8453, -7.8453, -7.5481, -7.8479,\n",
      "        -7.8767, -7.6423], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.6847, -8.0523, -7.9468, -7.9509, -7.9468, -7.9468, -7.9468, -7.9136,\n",
      "        -8.0523, -7.9072], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04825201630592346\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-7.7251, -7.8381, -7.8668, -7.8381, -7.8569, -7.8381, -7.8668, -7.8247,\n",
      "        -7.8668, -7.8097], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.9246, -7.9526, -8.0540, -7.9526, -7.8775, -7.9526, -8.0540, -7.9381,\n",
      "        -8.0540, -7.8775], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.020215366035699844\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-7.8016, -7.3511, -7.8016, -7.8849, -7.7124, -7.8016, -7.8383, -7.9066,\n",
      "        -7.8074, -7.8258], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.8416, -7.9276, -7.8416, -7.9381, -7.6855, -7.8416, -7.9276, -8.0284,\n",
      "        -7.5168, -8.0284], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04890051856637001\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.4186, -7.9336, -7.6605, -7.5540], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-7.5155, -7.9511, -7.8063, -7.9511, -7.4135, -7.9511, -7.9511, -7.9511,\n",
      "        -7.9487, -8.0407], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.8863, -7.9373, -7.7872, -7.9373, -7.8800, -7.9373, -7.9373, -7.9373,\n",
      "        -8.0003, -7.7872], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04233473911881447\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-8.0826, -7.5154, -7.8548, -7.8088, -7.8088, -7.9997, -7.9170, -7.9997,\n",
      "        -7.6346, -7.6037], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.5291, -7.9902, -7.9902, -7.7630, -7.7630, -7.9536, -7.8062, -7.9536,\n",
      "        -7.9536, -7.8913], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07553084194660187\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.7943, -7.9679, -7.4965, -7.8337], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-7.7412, -8.0204, -8.0204, -7.8337, -7.4965, -7.3210, -7.8337, -7.5247,\n",
      "        -7.9679, -7.4407], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.5985, -7.9508, -7.9508, -7.7468, -7.8170, -7.8170, -7.7468, -7.9633,\n",
      "        -7.8812, -7.7468], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06873705983161926\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-7.8826, -7.9546, -8.1410, -7.8565, -8.0540, -7.8826, -7.9546, -8.0949,\n",
      "        -7.8826, -7.8558], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.7753, -7.8938, -7.6415, -7.8114, -7.6415, -7.7753, -7.8938, -7.7753,\n",
      "        -7.7753, -7.6376], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.061350513249635696\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.6332, -8.1205, -7.8241, -7.8563], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-7.6603, -7.8202, -7.8559, -7.8991, -7.7908, -7.9809, -7.9515, -7.7243,\n",
      "        -7.8549, -7.9515], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.9472, -7.9439, -7.8228, -7.9124, -7.6994, -7.6994, -7.9472, -7.8047,\n",
      "        -7.8943, -7.9472], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01945403218269348\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.7716, -7.7762, -7.4512, -7.7247], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-7.8602, -7.7247, -7.8646, -7.8326, -7.8646, -7.2368, -7.8646, -7.6413,\n",
      "        -7.8602, -7.8646], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.8661, -7.7061, -7.9551, -7.8771, -7.9551, -7.9214, -7.9551, -7.8771,\n",
      "        -7.8661, -7.9551], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.055951911956071854\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-7.8402, -7.9488, -7.8402, -7.8175, -8.0362, -7.6375, -7.8402, -7.7060,\n",
      "        -7.8669, -7.8175], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.9354, -7.9234, -7.9354, -7.9234, -7.9357, -7.9234, -7.9354, -7.9354,\n",
      "        -7.8737, -7.9234], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01948315091431141\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-7.7378, -7.6465, -7.8905, -8.0697, -7.7401, -7.8763, -7.7782, -7.7937,\n",
      "        -7.7937, -7.8763], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.8544, -7.9173, -7.8669, -7.3545, -7.8544, -7.9660, -7.9660, -7.8818,\n",
      "        -7.8818, -7.9660], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06789882481098175\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.6041, -8.0326, -7.9074, -7.7500], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-7.6280, -7.9265, -7.8038, -7.9050, -7.4873, -7.7570, -7.9265, -7.8689,\n",
      "        -7.6041, -7.3148], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.0439, -7.9813, -7.8554, -8.0439, -7.8882, -7.8380, -7.9813, -8.0423,\n",
      "        -7.7438, -7.8437], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0697505921125412\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-7.9798, -7.5315, -8.0153, -7.9333, -8.0153, -7.8532, -7.9077, -8.1677,\n",
      "        -7.8532, -7.9403], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.0753, -7.7509, -7.9755, -7.4295, -7.9755, -7.8085, -7.7965, -7.9180,\n",
      "        -7.8085, -8.0753], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04111526161432266\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.6987, -7.9517, -7.7671, -7.8147], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-7.7977, -8.0794, -7.8786, -8.0747, -7.8786, -7.6034, -7.8786, -8.0794,\n",
      "        -7.8147, -7.8582], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.7724, -8.0092, -7.8026, -7.5182, -7.8026, -7.7313, -7.8026, -8.0092,\n",
      "        -7.7313, -8.0598], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04014357179403305\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-7.5953, -7.6174, -8.0918, -7.8635, -7.8868, -7.5746, -7.9166, -7.8635,\n",
      "        -7.5582, -7.8466], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.8343, -7.7636, -8.0705, -7.8343, -7.8358, -7.7095, -8.0705, -7.8343,\n",
      "        -7.7636, -7.8358], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01674250140786171\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.7617, -7.9269, -7.7548, -7.8211], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-7.8508, -7.8405, -7.4889, -7.7548, -8.1052, -8.1052, -7.8396, -7.8405,\n",
      "        -7.8405, -7.4889], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.8684, -7.8435, -7.6935, -7.7671, -8.1163, -8.1163, -8.1163, -7.8435,\n",
      "        -7.8435, -7.6935], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01610497757792473\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.6462, -7.7985, -7.6645, -7.8169], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-8.1098, -7.6462, -8.1514, -7.8257, -7.8257, -7.8584, -7.7924, -7.4969,\n",
      "        -8.1098, -7.5174], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.1474, -7.9771, -8.0057, -7.8594, -7.8594, -7.9172, -8.0057, -7.6628,\n",
      "        -8.1474, -7.9172], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03721678629517555\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-8.1365, -7.9982, -7.5307, -7.8992, -7.5900, -7.7980, -7.8529, -7.7357,\n",
      "        -7.5307, -8.1035], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.1356, -8.0192, -7.6112, -7.9622, -7.6112, -7.9527, -8.1356, -7.8938,\n",
      "        -7.6112, -8.1423], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.014813384041190147\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-7.9080, -7.8244, -8.1543, -7.9495, -7.9439, -7.9675, -7.6859, -7.6548,\n",
      "        -8.1998, -7.5929], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.5474, -7.9398, -8.1005, -8.0420, -8.2251, -7.9138, -7.8336, -7.8336,\n",
      "        -8.0255, -7.5474], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03230247274041176\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.7328, -7.6806, -7.2354, -7.6212], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.1767, -8.1820, -8.3406, -8.0068, -7.2354, -8.1820, -7.6642, -7.6546,\n",
      "        -7.6642, -8.0068], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.0751, -8.0716, -7.8963, -8.0759, -7.8610, -8.0716, -7.8610, -8.1999,\n",
      "        -7.8610, -8.0759], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10079684108495712\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.5911, -7.9718, -7.6783, -7.6795], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-7.7204, -7.6414, -7.9653, -7.8727, -7.8727, -7.8727, -7.8727, -7.9351,\n",
      "        -7.7741, -7.8727], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.9520, -7.5584, -7.8784, -7.9388, -7.9388, -7.9388, -7.9388, -8.1137,\n",
      "        -7.9388, -7.9388], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.014902813360095024\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-7.8126, -7.9226, -7.7425, -7.9498, -7.6588, -8.2556, -7.9468, -8.1353,\n",
      "        -7.6588, -7.5422], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.1303, -7.9009, -8.1134, -8.1358, -7.6184, -8.1521, -8.1521, -8.1134,\n",
      "        -7.6184, -7.6184], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03360286355018616\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-8.2381, -7.6270, -7.8358, -7.8999, -7.5139, -7.5139, -7.6847, -8.3098,\n",
      "        -8.2381, -7.9682], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.1356, -7.7625, -8.3973, -8.1755, -7.6855, -7.6855, -7.6855, -8.1898,\n",
      "        -8.1356, -7.8897], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05100112035870552\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.9403, -8.3182, -8.0044, -8.1926], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-7.4769, -8.0148, -7.6704, -8.0148, -7.9343, -8.1926, -8.0148, -8.3402,\n",
      "        -7.5230, -7.5230], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.7707, -7.9033, -7.8398, -7.9033, -8.1157, -8.1157, -7.9033, -8.2158,\n",
      "        -7.7292, -7.7292], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.029160764068365097\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-8.0751, -8.1577, -8.0512, -8.2440], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-7.7752, -7.7180, -8.0430, -8.1993, -7.9475, -8.1577, -8.0006, -8.6292,\n",
      "        -8.0492, -8.3297], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.7725, -7.8624, -8.2387, -8.0354, -8.2006, -7.8624, -7.9462, -8.0354,\n",
      "        -7.9462, -8.2006], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06200601905584335\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.9796, -8.2217, -7.9908, -8.1371], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-7.9731, -8.3114, -7.8532, -8.3681, -8.0370, -8.2811, -8.5587, -7.9670,\n",
      "        -8.3114, -8.0370], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.0432, -8.2683, -7.8840, -8.5313, -8.0432, -8.2662, -7.8840, -8.0978,\n",
      "        -8.2683, -8.0432], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05088391900062561\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-8.2029, -8.0225, -8.0774, -8.1448], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-8.2533, -8.2594, -7.7561, -8.0117, -8.2533, -8.2280, -7.9304, -8.0117,\n",
      "        -8.2280, -8.1075], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.2606, -8.3158, -7.8590, -8.1400, -8.2606, -8.3158, -7.9213, -8.1400,\n",
      "        -8.3158, -8.2606], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.008573567494750023\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-7.8801, -7.7709, -8.0595, -7.7688], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-8.0142, -8.0142, -8.2126, -8.0745, -8.2409, -8.0142, -8.2409, -8.0142,\n",
      "        -7.7205, -7.9051], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.1476, -8.1476, -8.1146, -8.2480, -8.2209, -8.1476, -8.2209, -8.1476,\n",
      "        -7.8986, -8.1146], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.018730217590928078\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-8.3093, -7.9792, -8.1002, -7.9956], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-8.2526, -8.2526, -8.0478, -8.1765, -7.9480, -8.2330, -8.2104, -7.8817,\n",
      "        -8.3579, -8.1731], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.1691, -8.1691, -8.1129, -8.1982, -8.2146, -8.2146, -8.1982, -8.2146,\n",
      "        -8.4502, -8.1691], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.020955612882971764\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-8.0167, -7.8121, -8.1329, -7.6513], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-8.3285, -8.1911, -7.6454, -8.1982, -8.1982, -8.2319, -8.4673, -8.2073,\n",
      "        -7.6190, -8.4805], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.0036, -7.8862, -7.8809, -8.3644, -8.3644, -8.1310, -8.1310, -8.1114,\n",
      "        -7.8571, -8.1422], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.061288021504879\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-8.2643, -8.3207, -7.7972, -7.9387, -7.8194, -8.1038, -7.8194, -8.3833,\n",
      "        -7.8037, -8.1038], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.1162, -8.0907, -8.0175, -7.9991, -8.1577, -8.0907, -8.1577, -8.1162,\n",
      "        -7.9024, -8.0907], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04373861104249954\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-7.9780, -8.1211, -8.1211, -8.3839, -8.2624, -8.3659, -7.7738, -8.0595,\n",
      "        -8.2624, -7.8408], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.9965, -8.1583, -8.1583, -8.1724, -8.1724, -8.2536, -7.9965, -8.0674,\n",
      "        -8.1724, -7.9780], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.014504295773804188\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-8.2636, -8.2170, -8.4267, -7.8807, -8.2037, -8.2636, -8.2642, -7.8767,\n",
      "        -8.2170, -8.1763], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.2542, -8.2660, -8.2163, -8.0890, -8.2404, -8.2542, -8.1325, -8.0603,\n",
      "        -8.2660, -8.4177], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.020329082384705544\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-7.9855, -8.0453, -8.1611, -8.2700, -8.2842, -8.0025, -8.2700, -8.2589,\n",
      "        -8.2041, -8.2700], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.1366, -8.1366, -8.1658, -8.3296, -8.2536, -8.2022, -8.3296, -8.3296,\n",
      "        -8.3186, -8.3296], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01007670909166336\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.2198, -8.1635, -7.9735, -8.0782], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.4231, -7.9735, -8.1620, -7.9883, -8.1748, -8.0169, -8.2965, -8.3787,\n",
      "        -8.2965, -8.0782], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.4072, -8.1424, -8.3820, -8.1424, -8.3208, -8.1793, -8.3978, -8.2603,\n",
      "        -8.3978, -8.1761], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01927672140300274\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-8.3105, -8.2225, -8.3821, -8.2225, -8.0558, -8.0323, -8.2800, -8.5163,\n",
      "        -7.9079, -8.2293], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.4064, -8.2290, -8.4064, -8.2290, -8.1171, -8.1984, -8.2184, -8.3511,\n",
      "        -8.2231, -8.2290], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.017172306776046753\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-7.9226, -8.3364, -8.3260, -8.1227], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-8.2183, -8.2356, -8.1227, -8.2183, -7.9226, -8.1227, -8.2428, -7.9811,\n",
      "        -8.4130, -8.1957], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.1932, -8.1409, -8.1304, -8.1932, -8.2459, -8.1304, -8.2299, -8.1093,\n",
      "        -8.2299, -8.1830], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01651613414287567\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.2718, -8.2689, -8.0245, -8.3037], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.3321, -8.3615, -7.9823, -8.5585, -8.3133, -8.2622, -7.9823, -8.3251,\n",
      "        -8.2718, -8.0245], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.1840, -8.2908, -8.2221, -8.3030, -8.4408, -8.2647, -8.2221, -8.2065,\n",
      "        -8.3030, -8.1840], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.026391398161649704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andyyang/Desktop/DS598 Reinforcement Learning/gym-examples/.env/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-8.4014, -8.1750, -8.2847, -8.2666, -8.4778, -8.3261, -8.3720, -8.3244,\n",
      "        -8.2847, -8.3720], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.3583, -8.2263, -8.3313, -8.4400, -8.3583, -8.2889, -8.4400, -8.3313,\n",
      "        -8.3313, -8.4400], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.006386554334312677\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-8.1851, -8.2265, -8.3928, -8.3268, -8.3652, -8.3101, -8.5030, -8.2052,\n",
      "        -8.3928, -8.3928], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.1432, -8.2897, -8.4184, -8.3666, -8.3666, -8.3847, -8.3416, -8.2897,\n",
      "        -8.4184, -8.4184], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.004801067523658276\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.2557, -8.5506, -8.3698, -8.3632], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-8.4238, -8.2415, -8.3438, -8.3632, -8.2936, -8.3222, -7.9128, -8.2291,\n",
      "        -8.3491, -8.2557], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.4062, -8.3084, -8.1215, -8.4301, -8.3405, -8.4062, -8.4301, -8.4062,\n",
      "        -8.4265, -8.1215], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03909062221646309\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-8.2426, -8.1984, -8.2757, -8.4089, -8.4470, -8.6213, -8.4094, -8.3770,\n",
      "        -8.2551, -8.4656], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.1902, -8.2566, -8.3011, -8.4183, -8.2864, -8.4283, -8.4275, -8.4459,\n",
      "        -8.4459, -8.4459], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.011178936809301376\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.2886, -8.4132, -8.2459, -8.3232], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.3263, -8.2938, -8.0650, -8.2984, -8.4334, -8.1505, -8.2220, -8.3905,\n",
      "        -8.4506, -8.4898], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.4363, -8.3998, -8.3998, -8.2669, -8.4213, -8.2357, -8.2585, -8.3901,\n",
      "        -8.4241, -8.4739], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.014610822312533855\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.1310, -8.2738, -8.1391, -8.2965], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-8.2209, -8.3395, -8.4836, -8.4364, -8.4836, -8.1595, -8.2627, -8.4992,\n",
      "        -8.3345, -8.1729], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.2319, -8.3998, -8.4684, -8.3556, -8.4684, -8.3556, -8.3783, -8.3988,\n",
      "        -8.2319, -8.3436], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.011225983500480652\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.2354, -8.4721, -8.3819, -8.3619], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-8.5302, -8.3537, -8.5293, -8.4565, -8.3090, -8.3537, -8.5274, -8.2166,\n",
      "        -8.4647, -8.5115], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.3074, -8.2152, -8.2152, -8.4119, -8.3074, -8.2152, -8.4763, -8.1757,\n",
      "        -8.2152, -8.4389], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02604507841169834\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-8.4178, -8.3405, -8.3970, -8.5443, -8.1941, -8.4583, -8.1400, -8.3389,\n",
      "        -8.5692, -8.1400], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.5065, -8.4343, -8.3984, -8.4939, -8.3201, -8.2717, -8.4939, -8.2717,\n",
      "        -8.4506, -8.4939], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0338895320892334\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-8.1651, -8.3224, -8.3009, -8.4823, -8.5856, -8.3609, -8.5172, -8.3224,\n",
      "        -8.3224, -8.4445], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.3486, -8.3746, -8.4226, -8.5184, -8.5517, -8.4226, -8.4226, -8.3746,\n",
      "        -8.3746, -8.5517], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00833815522491932\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-8.4503, -8.3270, -8.6736, -8.5672, -8.3959, -8.4262, -8.4231, -8.5714,\n",
      "        -8.4443, -8.5714], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.5808, -8.4592, -8.3283, -8.5344, -8.5369, -8.5344, -8.4592, -8.4580,\n",
      "        -8.3669, -8.4580], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.021939972415566444\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-8.3522, -8.4648, -8.3231, -8.5637, -8.3774, -8.4323, -8.3522, -8.2671,\n",
      "        -8.3800, -8.4557], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.5149, -8.5344, -8.5344, -8.3470, -8.4702, -8.5431, -8.5149, -8.5068,\n",
      "        -8.3470, -8.6101], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02526436746120453\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-8.4545, -8.1479, -8.3025, -8.3750], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-8.4582, -8.2902, -8.5548, -8.4810, -8.5007, -8.2196, -8.3958, -8.3164,\n",
      "        -8.5369, -8.4582], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.5605, -8.4917, -8.6209, -8.6209, -8.3976, -8.3331, -8.5562, -8.4917,\n",
      "        -8.4779, -8.5605], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.016899341717362404\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-8.3806, -8.5289, -8.5289, -8.2507, -8.6338, -8.5503, -8.3473, -8.3270,\n",
      "        -8.6043, -8.6193], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.5425, -8.5669, -8.5669, -8.4256, -8.5425, -8.4256, -8.4256, -8.5559,\n",
      "        -8.5669, -8.5224], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.015288740396499634\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.5250, -8.5374, -8.6373, -8.3889], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-8.5753, -8.4578, -8.3363, -8.3708, -8.3363, -8.2576, -8.6444, -8.5673,\n",
      "        -8.3546, -8.4503], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.5329, -8.5337, -8.5500, -8.3807, -8.5500, -8.2279, -8.3672, -8.5500,\n",
      "        -8.5192, -8.5022], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.020677991211414337\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.4597, -8.4694, -8.3777, -8.2448], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-8.5719, -8.2710, -8.3640, -8.3640, -8.4783, -8.3288, -8.6961, -8.6066,\n",
      "        -8.5961, -8.1517], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.5205, -8.5044, -8.5276, -8.5276, -8.5276, -8.4771, -8.4959, -8.4762,\n",
      "        -8.5205, -8.3366], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.023201486095786095\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-8.3351, -8.8644, -8.5937, -8.3644, -8.1528, -8.5247, -8.4624, -8.6299,\n",
      "        -8.4336, -8.3644], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.2406, -8.5060, -8.3375, -8.5279, -8.3375, -8.5279, -8.5103, -8.5124,\n",
      "        -8.3375, -8.5279], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.031600840389728546\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.8235, -8.6443, -8.4161, -8.4104], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-8.6140, -8.7289, -8.6055, -8.4161, -8.2221, -8.6383, -8.4519, -8.6383,\n",
      "        -8.6140, -8.6443], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.5405, -8.5405, -8.6067, -8.3999, -8.3999, -8.5871, -8.4796, -8.5871,\n",
      "        -8.5405, -8.4105], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.013886550441384315\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.7728, -8.5783, -8.3710, -8.4902], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.5841, -8.5841, -8.6057, -8.3710, -8.4902, -8.5766, -8.3710, -8.7450,\n",
      "        -8.5155, -8.3339], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7191, -8.7191, -8.6909, -8.5006, -8.5339, -8.7190, -8.5006, -8.6909,\n",
      "        -8.7191, -8.3713], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.014520701952278614\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.4499, -8.6167, -8.5849, -8.3739], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-8.6296, -8.5487, -8.5888, -8.3739, -8.6013, -8.6031, -8.5888, -8.5821,\n",
      "        -8.5849, -8.5770], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.5664, -8.6938, -8.6938, -8.5365, -8.5349, -8.7197, -8.6938, -8.5349,\n",
      "        -8.5365, -8.7197], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.011650770902633667\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.4283, -8.6230, -8.5922, -8.4081], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-8.5547, -8.4283, -8.5922, -8.4443, -8.4848, -8.6331, -8.5069, -8.6131,\n",
      "        -8.6607, -8.5988], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.5435, -8.5435, -8.5673, -8.6963, -8.5435, -8.7389, -8.6364, -8.7175,\n",
      "        -8.5435, -8.6963], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.014309050515294075\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.4073, -8.6626, -8.5916, -8.4562], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-8.6908, -8.5732, -8.6129, -8.7246, -8.7900, -8.5885, -8.6302, -8.3700,\n",
      "        -8.6129, -8.6626], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7318, -8.6882, -8.6540, -8.6041, -8.8077, -8.5330, -8.5932, -8.5665,\n",
      "        -8.6540, -8.8419], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.010833248496055603\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.6272, -8.6415, -8.3762, -8.6166], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.8895, -8.6591, -8.5872, -8.3762, -8.6591, -8.6786, -8.7339, -8.6947,\n",
      "        -8.6166, -8.7434], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.5333, -8.5993, -8.5993, -8.5333, -8.5993, -8.6752, -8.7382, -8.5385,\n",
      "        -8.5385, -8.5993], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02101432904601097\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.3661, -8.7801, -8.6119, -8.4879], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-8.6100, -8.3890, -8.6909, -8.5471, -8.6909, -8.6100, -8.3951, -8.3951,\n",
      "        -8.6172, -8.6119], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7026, -8.6924, -8.5700, -8.5373, -8.5700, -8.7026, -8.5295, -8.5295,\n",
      "        -8.8281, -8.5295], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.022588638588786125\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.6723, -8.7442, -8.4385, -8.6371], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.3894, -8.6940, -8.6371, -8.4385, -8.8449, -8.8449, -8.6371, -8.8862,\n",
      "        -8.8207, -8.6940], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.5946, -8.5818, -8.5946, -8.5505, -8.7263, -8.7263, -8.5946, -8.7611,\n",
      "        -8.7680, -8.5818], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.012998285703361034\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.4452, -8.7795, -8.6198, -8.4662], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-8.6454, -8.6636, -8.4739, -8.6691, -8.6636, -8.7228, -8.4973, -8.6454,\n",
      "        -8.8271, -8.8271], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.6265, -8.6189, -8.6006, -8.6916, -8.6189, -8.7751, -8.6384, -8.6265,\n",
      "        -8.7751, -8.7751], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.004934924189001322\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-8.6333, -8.8062, -8.4863, -8.5244, -8.8196, -8.6333, -8.6311, -8.5977,\n",
      "        -8.5244, -8.8196], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.6492, -8.8141, -8.6720, -8.6020, -8.8166, -8.6492, -8.6246, -8.6246,\n",
      "        -8.6020, -8.8166], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.004786573350429535\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-8.5398, -8.6094, -8.6725, -8.5398, -8.8559, -8.6094, -8.8229, -8.8229,\n",
      "        -8.6947, -8.6573], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7149, -8.6885, -8.7870, -8.7149, -8.8353, -8.6885, -8.8353, -8.8353,\n",
      "        -8.8048, -8.7149], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.010310156270861626\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-8.6083, -8.5087, -8.9386, -8.7960, -8.6639, -8.4383, -8.8461, -8.6083,\n",
      "        -8.6438, -8.6639], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.6828, -8.7082, -8.8262, -8.8262, -8.7082, -8.5945, -8.8409, -8.6828,\n",
      "        -8.5945, -8.7082], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0095206368714571\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-8.5525, -8.7250, -8.5939, -8.8715, -8.6203, -8.5500, -8.7219, -8.6939,\n",
      "        -8.4803, -8.9999], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7680, -8.5123, -8.8318, -8.8128, -8.6973, -8.6777, -8.7680, -8.7295,\n",
      "        -8.6323, -8.8525], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.022213760763406754\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.8840, -8.8310, -8.5842, -8.7897], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.5792, -8.8515, -8.9011, -8.6318, -8.5888, -8.6568, -8.6731, -8.5792,\n",
      "        -8.6242, -8.4753], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7722, -8.8704, -8.7618, -8.6278, -8.6870, -8.7404, -8.5631, -8.7722,\n",
      "        -8.7618, -8.6870], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.018677841871976852\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.7827, -8.4255, -8.6141, -8.4375], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-8.6076, -8.6934, -8.7420, -8.4517, -8.5493, -8.9454, -8.5140, -8.9979,\n",
      "        -8.7713, -8.4375], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7265, -8.7468, -8.6321, -8.7265, -8.6944, -8.7468, -8.7938, -8.6649,\n",
      "        -8.7468, -8.7411], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04470745474100113\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.9422, -8.9108, -8.7303, -8.8133], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.7571, -8.7445, -8.4924, -8.7133, -8.9642, -8.9642, -8.7689, -8.9422,\n",
      "        -8.9642, -8.7927], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7608, -8.7608, -8.7136, -8.6431, -8.7538, -8.7538, -8.6586, -8.7608,\n",
      "        -8.7538, -8.6380], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.025599082931876183\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-8.8092, -8.5552, -8.8092, -8.9488, -8.6942, -8.5552, -8.5552, -8.8038,\n",
      "        -8.9088, -8.7980], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.6997, -8.7432, -8.6997, -8.8105, -8.7507, -8.7432, -8.7432, -8.9594,\n",
      "        -8.8247, -8.6997], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.019342519342899323\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-8.9243, -8.9017, -8.7431, -8.7372, -8.8791, -8.8099, -8.8560, -8.7613,\n",
      "        -8.9462, -8.7613], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.9222, -8.9600, -8.8458, -8.6868, -8.9222, -8.8458, -8.8274, -8.8274,\n",
      "        -8.8037, -8.8274], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0049455007538199425\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-8.9080, -8.8115, -8.8073, -8.5294, -8.7953, -9.0242, -8.9992, -8.9080,\n",
      "        -8.9080, -8.8115], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.9005, -8.9303, -8.8639, -8.8110, -8.8270, -8.8600, -8.8270, -8.9005,\n",
      "        -8.9005, -8.9303], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.016854386776685715\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.7706, -8.7533, -8.9531, -8.6615], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-8.7822, -8.8450, -8.6204, -8.7698, -9.0222, -8.7822, -8.9884, -8.8450,\n",
      "        -8.7698, -8.9884], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.8609, -8.9605, -8.7954, -8.8609, -8.9064, -8.8609, -8.8385, -8.9605,\n",
      "        -8.8609, -8.8385], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.014461278915405273\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.7481, -8.6702, -8.9554, -8.6876], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-8.8712, -8.6007, -9.2463, -9.0407, -8.8130, -8.8130, -8.9982, -9.0882,\n",
      "        -8.6597, -9.0775], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.9841, -8.9433, -8.9944, -8.8719, -8.8805, -8.8805, -8.7834, -8.8805,\n",
      "        -8.7092, -8.8971], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03554265946149826\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.6752, -8.7134, -8.9675, -8.7266], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-8.7134, -8.6895, -8.7559, -8.7559, -8.8818, -8.7825, -8.8250, -8.9059,\n",
      "        -8.5668, -8.8250], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.9713, -8.8077, -8.8361, -8.8361, -8.8763, -8.8892, -8.8892, -8.8361,\n",
      "        -9.0915, -8.8892], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03930731862783432\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-8.9916, -8.7695, -8.8548, -8.7871, -8.7987, -8.6527, -8.7695, -8.7695,\n",
      "        -8.7695, -8.8695], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.9693, -8.8090, -8.9693, -8.9171, -8.9084, -8.7874, -8.8090, -8.8090,\n",
      "        -8.8090, -8.8890], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.006731752306222916\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-8.6355, -8.7651, -8.9645, -8.8287], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-8.5593, -8.8652, -9.0294, -9.0406, -9.0294, -9.0457, -8.6511, -9.0615,\n",
      "        -9.2578, -9.0708], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-9.1412, -8.9413, -8.9413, -8.8900, -8.9413, -9.1554, -8.8900, -8.9085,\n",
      "        -8.8756, -8.7034], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07560525834560394\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-8.9560, -8.6943, -8.7257, -9.0695, -8.8862, -8.8522, -8.9347, -8.9340,\n",
      "        -9.0707, -8.8711], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.8249, -9.0604, -8.8249, -8.9712, -8.9670, -8.9670, -8.9686, -8.9686,\n",
      "        -9.0604, -8.9489], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01989036798477173\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-8.8064, -8.9310, -8.8585, -8.8404, -8.7852, -8.8404, -9.2495, -8.7437,\n",
      "        -9.0265, -9.1895], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.9466, -9.0464, -8.6749, -8.9033, -8.9067, -8.9033, -8.8693, -8.9726,\n",
      "        -8.9519, -8.9726], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03388965129852295\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-8.8391, -8.7846, -8.8614, -8.9729, -8.8256, -8.9813, -8.8614, -8.9430,\n",
      "        -8.8185, -8.9938], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.7057, -8.8856, -9.0756, -8.8745, -8.9430, -9.0756, -9.0756, -9.1036,\n",
      "        -8.8745, -8.8100], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.021483542397618294\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-9.2291, -9.0527, -8.8424, -9.2378], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-9.0682, -9.1781, -8.9935, -8.8762, -9.0536, -9.1088, -9.0101, -9.0996,\n",
      "        -9.0101, -9.0101], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.8017, -8.8393, -8.6999, -8.5221, -8.7525, -8.8445, -8.9609, -8.7144,\n",
      "        -8.9609, -8.9609], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07134576141834259\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-9.1996, -9.1061, -8.8498, -9.1981], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.9359, -8.5169, -8.9021, -9.0545, -8.9930, -8.9133, -8.9972, -8.9133,\n",
      "        -8.9478, -8.9678], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.9073, -8.9225, -8.6652, -8.8851, -8.8184, -8.6878, -8.9073, -8.6878,\n",
      "        -8.9225, -8.9394], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.039179541170597076\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-9.1521, -9.1147, -8.9208, -9.1157], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-8.9483, -8.8669, -9.0428, -8.8721, -8.8687, -8.8514, -8.7946, -8.7888,\n",
      "        -8.9483, -8.7093], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.9173, -8.7051, -8.7464, -8.9818, -8.9818, -8.8121, -8.7615, -8.7043,\n",
      "        -8.9173, -8.9759], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.022162478417158127\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-8.9338, -8.8921, -8.7798, -8.8572, -8.6558, -8.8921, -8.8117, -8.7088,\n",
      "        -8.7047, -8.9894], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.8399, -8.8450, -8.9018, -8.8450, -8.7624, -8.8450, -8.7535, -8.9381,\n",
      "        -8.7691, -8.9311], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.010317658074200153\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-9.0150, -9.1090, -9.1241, -8.9444], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-8.7244, -8.6262, -9.0238, -8.5799, -8.6666, -8.8606, -8.6628, -8.9999,\n",
      "        -8.8929, -8.8443], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.8333, -8.6856, -9.1015, -8.9169, -8.6448, -8.8218, -8.8730, -8.8730,\n",
      "        -8.8730, -8.7347], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0209705401211977\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-8.8234, -8.9581, -8.9128, -8.6245], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-8.9973, -9.0228, -9.1624, -8.7335, -8.9973, -8.7296, -8.9901, -9.0888,\n",
      "        -8.4613, -8.7296], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.8066, -8.6268, -8.7692, -8.7692, -8.8066, -8.5536, -9.0430, -8.5536,\n",
      "        -8.7692, -8.5536], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08313216269016266\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-9.0589, -8.8346, -9.1421, -8.5333, -8.9085, -8.5016, -8.4596, -8.4891,\n",
      "        -8.6991, -8.9085], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.6108, -8.6137, -8.7020, -8.8528, -8.7987, -9.0079, -8.6137, -8.6402,\n",
      "        -8.5790, -8.7987], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08868055790662766\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-8.7333, -8.5640, -8.2943, -8.6942, -8.5465, -8.5640, -8.5640, -8.4591,\n",
      "        -8.4591, -8.6942], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.9461, -8.4648, -8.5294, -8.6918, -8.8075, -8.4648, -8.4648, -8.6748,\n",
      "        -8.6748, -8.6918], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0291338749229908\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-8.6000, -8.7137, -8.8915, -9.0007], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-8.5461, -8.5949, -8.4418, -8.4399, -8.3481, -8.4399, -8.5949, -8.8745,\n",
      "        -8.4532, -8.6636], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.5976, -8.5589, -8.5589, -8.3831, -8.4558, -8.3831, -8.5589, -8.7244,\n",
      "        -8.8655, -8.8655], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.027030816301703453\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-8.5361, -8.6314, -8.8820, -9.1200], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-8.4538, -8.4107, -8.3756, -8.4850, -8.4387, -8.5494, -8.9509, -8.8718,\n",
      "        -8.8976, -8.5415], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.5696, -8.4795, -8.4795, -8.5810, -8.4560, -8.5949, -8.6365, -8.6874,\n",
      "        -8.5345, -8.6084], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.030970897525548935\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-8.5310, -8.5423, -8.8333, -9.1989], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-8.9380, -8.6040, -8.2332, -8.4763, -8.2332, -8.8523, -8.5310, -8.6040,\n",
      "        -8.4646, -8.8885], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.5223, -8.6079, -8.2936, -8.7365, -8.2936, -8.6079, -8.6779, -8.6079,\n",
      "        -8.6182, -8.7365], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03758227452635765\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-8.5236, -8.4991, -8.7725, -9.2246], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-8.5236, -8.9573, -8.1671, -9.0728, -8.9573, -8.7731, -9.2246, -8.1671,\n",
      "        -8.4059, -8.4290], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.6492, -8.4813, -8.2674, -8.4198, -8.4813, -8.2674, -8.6492, -8.2674,\n",
      "        -8.6964, -8.4023], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15872782468795776\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-8.3187, -8.2398, -8.6030, -8.8759], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-8.3741, -8.7914, -8.5914, -8.2282, -8.7070, -8.3741, -8.3766, -8.7914,\n",
      "        -8.5009, -8.7878], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.5367, -8.5389, -8.2463, -8.3164, -8.6373, -8.5367, -8.5064, -8.5389,\n",
      "        -8.6508, -8.5064], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.043058883398771286\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-8.5555, -8.4107, -8.6412, -9.0263], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-8.5249, -9.0263, -8.2089, -8.3676, -8.3447, -8.2984, -8.7215, -8.5378,\n",
      "        -8.3853, -8.2166], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.6724, -8.4143, -8.3098, -8.3098, -8.5853, -8.7450, -8.8328, -8.4697,\n",
      "        -8.4697, -8.2780], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0695120245218277\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-7.9676, -8.0388, -8.2169, -8.2568], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-8.3399, -8.2219, -8.3399, -8.6527, -8.4143, -8.4532, -8.3399, -8.4188,\n",
      "        -8.2556, -8.5611], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.5059, -8.5059, -8.5059, -8.7349, -8.5291, -8.3997, -8.5059, -8.8075,\n",
      "        -8.2930, -8.5492], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03387640044093132\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-7.9128, -8.0189, -8.1058, -7.9979], grad_fn=<ViewBackward0>)\n",
      "selected action: 0\n",
      "Q_EVAL: tensor([-8.1943, -8.4382, -8.4023, -8.5491, -8.2992, -7.9979, -8.1685, -8.3040,\n",
      "        -8.3040, -7.9979], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.3082, -8.4458, -8.3517, -8.6644, -8.3517, -8.6470, -8.3517, -8.3061,\n",
      "        -8.3061, -8.6470], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09078238904476166\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-7.9534, -8.0305, -8.1021, -7.9287], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-8.5034, -7.9957, -8.5664, -8.3069, -8.1382, -8.4841, -8.3069, -8.4426,\n",
      "        -8.3742, -8.8366], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.2466, -8.1962, -8.5863, -8.2466, -8.2466, -8.5863, -8.2466, -8.3936,\n",
      "        -8.2727, -8.3936], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03449324518442154\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-8.4556, -8.3240, -8.3357, -8.3229], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-8.0455, -8.4266, -8.6914, -8.1002, -7.9055, -8.1002, -8.5708, -8.2194,\n",
      "        -8.3851, -8.0455], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.2410, -8.3823, -8.6069, -8.6069, -8.4906, -8.6069, -8.3611, -8.1753,\n",
      "        -8.3673, -8.2410], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09876628965139389\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-8.4179, -8.3398, -8.4452, -8.0895], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-8.2750, -8.4103, -8.2750, -8.2750, -8.3142, -8.2413, -8.2336, -8.4751,\n",
      "        -8.2413, -8.1676], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.1661, -8.3462, -8.1661, -8.1661, -8.2904, -8.0403, -8.1455, -8.0403,\n",
      "        -8.0403, -8.3509], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0351574569940567\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-8.5246, -8.2129, -8.4122, -8.5761, -8.4511, -8.3127, -7.8024, -8.2407,\n",
      "        -8.1520, -8.1691], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.4457, -8.1471, -8.3076, -8.4457, -8.3076, -8.2487, -8.0222, -8.0222,\n",
      "        -8.5643, -8.1450], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03298185020685196\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-8.3998, -8.3813, -8.3436, -8.1216], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-8.1758, -8.2623, -8.5480, -8.0553, -8.1981, -8.1758, -8.4334, -8.2815,\n",
      "        -8.5114, -7.9542], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-8.1638, -8.4107, -8.3735, 10.0000, -8.0510, -8.1638, -8.4107, -8.1188,\n",
      "        -8.1873, -8.1543], grad_fn=<AddBackward0>)\n",
      "LOSS: 32.624019622802734\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-7.9925, -8.0060, -7.9750, -7.4794], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-7.9216, -7.7643, -7.9241, -8.1185, -8.0006, -7.8976, -7.9216, -7.4794,\n",
      "        -7.2964, -7.8310], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.5426, -7.7828, -7.6494, -7.7828, -7.8935, -7.5840, -7.5426, -7.7314,\n",
      "        -7.6882, -7.5289], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08939661085605621\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-7.6009, -7.5378, -7.5485, -7.4423, -7.6368, -7.0673, -6.7589, -7.4508,\n",
      "        -7.6040, -7.4478], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-7.2184, -7.2637, -7.2184, -7.1130, -7.2628, -7.0830, -7.0830, -7.1130,\n",
      "        -7.2628, -7.1130], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10265103727579117\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-7.1580, -7.1795, -6.6165, -7.2884, -7.0638, -7.2679, -6.3688, -7.2150,\n",
      "        -7.2074, -7.4946], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.7084, -6.8371, -6.7319, -6.9548, -6.7498, -6.7084, -6.7319, -6.9031,\n",
      "        -6.8750, -6.9548], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14866697788238525\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-6.8154, -6.9313, -6.8635, -6.2226], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-6.8154, -7.1112, -6.7683, -6.7785, -6.7440, -6.7440, -6.3953, -6.7645,\n",
      "        -6.9038, -6.7785], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.6003, -6.4465, -6.4313, -6.5023, -6.4623, -6.4623, -6.4425, -6.4623,\n",
      "        -6.6158, -6.5023], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10894240438938141\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-6.4375, -6.5416, -6.4860, -5.9533], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.7823, -6.3053, -6.3830, -6.4042, -6.3053, -6.3794, -6.3053, -6.4042,\n",
      "        -6.4704, -5.8367], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.2041, -6.4105, -6.2380, -6.2041, -6.4105, -6.2380, -6.4105, -6.2041,\n",
      "        -6.4105, -6.3580], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06074143201112747\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-6.3362, -6.0078, -5.5354, -5.8232, -6.1190, -5.8883, -6.3671, -6.0188,\n",
      "        -5.8232, -6.0573], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.1784, -6.0133, -5.9118, -5.9118, -6.0482, -5.4857, -6.2995, -6.0133,\n",
      "        -5.9118, -6.0482], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03541944921016693\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.8615, -5.8798, -5.7838, -5.4011], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-6.0228, -5.8618, -5.3752, -6.0264, -5.7762, -5.4011, -5.3719, -5.5472,\n",
      "        -5.9667, -5.9303], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.8180, -5.8825, -5.8377, -5.8180, -5.8825, -5.8610, -6.0207, -5.8377,\n",
      "        -6.0682, -6.1062], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1069066971540451\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.6395, -5.6357, -5.5222, -5.2414], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.5261, -5.6389, -5.3476, -5.5972, -5.6982, -5.5222, -5.2058, -5.2188,\n",
      "        -5.5324, -5.4373], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([10.0000, -5.7398, -5.5405, -5.7455, -5.8382, -5.6852, -5.6852, -5.6962,\n",
      "        -5.7398, -5.6595], grad_fn=<AddBackward0>)\n",
      "LOSS: 21.167381286621094\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3383, -5.3321, -5.2066, -4.8894], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.8894, -4.6724, -5.4509, -5.2684, -5.0701, -5.2172, -5.2684, -4.8073,\n",
      "        -5.1330, -5.2514], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4004, -5.4124, -5.3775, -5.3647, -5.4991, -5.4114, -5.3647, -5.5818,\n",
      "        -5.5972, -5.5972], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.19894471764564514\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-4.8239, -4.6033, -4.5114, -4.9840, -4.8979, -4.7494, -4.9564, -4.9564,\n",
      "        -4.8715, -4.6666], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1282, -5.1429, -5.2573, -5.2981, -5.2981, -5.1999, -5.1282, -5.1282,\n",
      "        -5.0603, -5.0847], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16714736819267273\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8061, -4.8655, -4.7985, -4.3355], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.8968, -4.3975, -4.8968, -4.6954, -4.3213, -4.4534, -4.6092, -4.2588,\n",
      "        -4.7662, -4.7464], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0249, -4.0816, -5.0249, -4.8448, -4.7721, -4.6839, -4.8794, -4.8329,\n",
      "        -4.8870, -4.8794], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08461640775203705\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-4.4886, -4.5783, -4.6057, -3.9392, -4.5783, -4.5982, -4.0659, -4.5885,\n",
      "        -4.5830, -3.5851], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6680, -4.6521, -4.6042, -4.4723, -4.6521, -4.7038, -4.6680, -4.6586,\n",
      "        -4.6083, -3.8417], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07723675668239594\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-4.5548, -4.2914, -4.5548, -4.1503, -3.7285, -4.5548, -3.7900, -3.8431,\n",
      "        -4.4398, -4.4449], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5673, -4.4546, -4.5673, -4.2825, -4.2825, -4.5673, -4.4110, -4.4710,\n",
      "        -4.4633, -4.4758], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1132851392030716\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3255, -4.3228, -4.2436, -3.6895], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.3338, -3.9106, -3.6517, -3.6602, -4.3624, -4.4386, -4.3492, -4.3073,\n",
      "        -4.2729, -3.6602], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.2172, -4.2015, -4.2932, -4.2942, -4.3831, -4.2932, -4.2825, -4.3227,\n",
      "        -4.2825, -4.2942], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13399866223335266\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-4.0962, -3.4981, -4.1268, -4.3031, -4.2109, -3.4211, -4.2237, -4.2237,\n",
      "        -3.5027, -4.2286], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.1342, -4.1482, -4.1412, -4.2230, -4.1342, -3.3196, -4.1342, -4.1342,\n",
      "        -4.1448, -4.1673], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08790189027786255\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-4.0998, -4.1478, -3.1619, -4.1515, -4.0074, -4.1265, -4.2460, -3.3429,\n",
      "        -4.1821, -4.1181], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9733, -4.0777, -3.9344, -3.9733, -4.1776, -4.0086, -4.0357, -4.0086,\n",
      "        -4.0777, -4.0097], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12023158371448517\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-4.0274, -4.0887, -4.0750, -4.0274, -4.1045, -2.8963, -4.0573, -3.9461,\n",
      "        -4.0062, -3.9955], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9026, -3.9631, -3.9631, -3.9026, -3.9802, -3.0870, -3.8649, -3.8968,\n",
      "        -3.8649, -3.9026], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.017931003123521805\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-4.0284, -3.8834, -3.9354, -3.8293, -3.0805, -3.1364, -3.0764, -3.9584,\n",
      "        -3.9353, -3.9354], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.8610, -3.7992, -3.8110, -3.7688, -3.7724, -3.8227, -3.6657, -3.7083,\n",
      "        -3.8002, -3.8110], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14476455748081207\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.8460, -3.9524, -3.8762, -2.9939], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.6445, -3.8965, -3.8106, -3.0220, -3.8410, -3.7468, -3.9397, -3.8454,\n",
      "        -3.0220, -3.8854], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7062, -3.8107, -3.8905, -3.7198, -3.7563, -3.7062, -3.8233, -3.7346,\n",
      "        -3.7198, -3.7774], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1037740707397461\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.7672, -3.8624, -3.7985, -2.9203], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.9465, -3.8599, -3.7594, -3.7672, -2.9465, -3.6936, -3.8146, -2.8321,\n",
      "        -3.7594, -2.9203], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6519, -3.7614, -3.6688, -3.6519, -3.6519, -3.6519, -3.6688, -3.6737,\n",
      "        -3.6688, -3.6282], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.22668269276618958\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-3.6791, -3.6873, -2.6062, -3.6651, -3.7848, -2.8600, -3.7848, -2.8999,\n",
      "        -3.7848, -3.6818], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6161, -3.6360, -2.7984, -3.4993, -3.7118, -3.5740, -3.7118, -3.6099,\n",
      "        -3.7118, -3.5740], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11125357449054718\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.6122, -3.5866, -3.5731, -2.8782], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.8625, -3.6332, -3.6747, -3.6837, -2.8107, -3.7132, -3.6040, -3.6253,\n",
      "        -3.6747, -2.8263], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4314, -3.5527, -3.5436, -3.5716, -3.5296, -3.6700, -3.5716, -3.5982,\n",
      "        -3.5436, -3.5527], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14252376556396484\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.5744, -3.6268, -3.6111, -2.7694], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.6417, -3.5330, -3.5659, -2.8184, -3.5722, -3.5371, -3.5928, -3.5560,\n",
      "        -3.6247, -2.7882], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6340, -3.5366, -3.5529, -3.6512, -3.5579, -3.5112, -3.5098, -3.4925,\n",
      "        -3.6340, -3.5094], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12257909774780273\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-3.5769, -3.4801, -3.5769, -3.5052, -3.5221, -3.4495, -3.4764, -3.2957,\n",
      "        -2.9123, -2.7467], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6037, -3.5247, -3.6037, -3.6211, -3.4720, -3.5382, -3.5136, -3.3578,\n",
      "        -3.4813, -3.4720], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08822719752788544\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4612, -3.3983, -3.4317, -2.7723], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.5255, -3.3903, -2.5865, -3.4248, -2.7261, -2.7039, -3.4484, -3.2359,\n",
      "        -3.4761, -3.5255], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5782, -3.5234, -3.3769, -3.4956, -3.4399, -3.4335, -3.4335, -3.5987,\n",
      "        -3.4951, -3.5782], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1827104538679123\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-2.3269, -3.4425, -2.6745, -2.6767, -3.4852, -2.8631, -3.3320, -3.4048,\n",
      "        -3.4843, -3.4425], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.6260, -3.4070, -3.4070, -3.4091, -3.4091, -3.4348, -3.3485, -3.4717,\n",
      "        -3.5549, -3.4070], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15073643624782562\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4315, -3.3608, -3.3748, -2.6445], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.3949, -3.4513, -3.0104, -3.3782, -2.7821, -2.7383, -3.4147, -3.4658,\n",
      "        -3.3451, -3.3451], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3252, -3.5363, -3.5039, -3.4490, -3.3003, -3.5636, -3.3801, -3.3003,\n",
      "        -3.4490, -3.4490], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12604138255119324\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3988, -3.3327, -3.3486, -2.6180], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.4375, -2.7320, -3.3876, -3.5383, -3.3251, -2.3183, -2.6180, -2.5455,\n",
      "        -3.3876, -3.3678], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3699, -3.5442, -3.3562, -3.4305, -3.4305, -2.5843, -3.3562, -3.3051,\n",
      "        -3.3562, -3.4362], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1886274516582489\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3704, -3.3113, -3.3299, -2.5969], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.4402, -3.4199, -3.0705, -3.3697, -3.5175, -3.3465, -3.3697, -3.2780,\n",
      "        -3.4199, -3.5077], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5060, -3.5060, -3.2125, -3.4614, -3.4171, -3.5224, -3.4614, -3.3372,\n",
      "        -3.5060, -3.4171], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.010887643322348595\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3462, -3.2987, -3.3151, -2.5761], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.2372, -2.5913, -3.3097, -3.4991, -3.5094, -3.3806, -2.5761, -3.3151,\n",
      "        -2.5972, -2.5761], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.1978, -3.3322, -3.4059, -3.4059, -3.4907, -3.4619, -3.3185, -3.3375,\n",
      "        -3.3375, -3.3185], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.31472888588905334\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3268, -3.2990, -3.3056, -2.5574], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.2898, -3.3859, -3.4012, -3.3221, -3.3991, -3.4161, -2.5574, -3.4161,\n",
      "        -3.5015, -3.3056], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4607, -3.3993, -3.4607, -3.3017, -3.3220, -3.4740, -3.3017, -3.4740,\n",
      "        -3.4740, -3.3220], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06009403616189957\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3136, -3.3032, -3.2987, -2.5418], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.3416, -2.6063, -3.3307, -2.7584, -3.2987, -3.3082, -3.3239, -2.6063,\n",
      "        -2.5418, -3.4351], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4826, -3.2876, -3.3852, -3.2814, -3.3079, -3.2876, -3.4826, -3.2876,\n",
      "        -3.2876, -3.4593], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1807187795639038\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.2968, -3.3031, -3.2869, -2.5218], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.2989, -2.4470, -3.0736, -2.5472, -3.4304, -3.2856, -2.5512, -3.3216,\n",
      "        -3.3480, -2.5512], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2696, -3.2925, -3.4873, -3.2925, -3.4456, -3.4594, -3.2961, -3.3475,\n",
      "        -3.3794, -3.2961], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2584022283554077\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.2882, -3.3116, -3.2814, -2.5059], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.4473, -3.1994, -3.4013, -3.4473, -3.2637, -2.5059, -3.3116, -2.5059,\n",
      "        -2.5308, -3.4767], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4357, -3.2777, -3.3732, -3.4357, -3.4333, -3.2553, -3.3497, -3.2553,\n",
      "        -3.1990, -3.4605], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16073529422283173\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.2851, -3.3225, -3.2820, -2.4945], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.4945, -2.4945, -3.2851, -3.4645, -3.2761, -2.5200, -3.3089, -3.4713,\n",
      "        -3.2820, -2.6146], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2451, -3.2451, -3.2680, -3.4284, -3.1912, -3.2680, -3.4627, -3.4284,\n",
      "        -3.2822, -3.2451], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.21179616451263428\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-3.5305, -2.4842, -2.4842, -2.6934, -3.1799, -3.2534, -3.3295, -2.9762,\n",
      "        -2.6261, -3.2824], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4663, -3.2358, -3.2358, -3.0998, -3.5017, -3.2998, -3.3635, -3.0998,\n",
      "        -3.2358, -3.2608], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1793314814567566\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4015, -3.5156, -3.4891, -2.6426], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.4295, -2.6836, -2.4747, -2.4747, -2.2853, -3.4503, -3.4801, -3.6161,\n",
      "        -3.5588, -2.3238], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5049, -3.3784, -3.2273, -3.2273, -2.4998, -3.3709, -3.3868, -3.4152,\n",
      "        -3.4708, -2.4998], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1761065423488617\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-3.5844, -3.3185, -3.2629, -3.2544, -3.5844, -3.3621, -3.3185, -3.4224,\n",
      "        -2.2142, -3.3621], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4784, -3.2228, -3.5158, -3.2552, -3.4784, -3.4784, -3.2228, -3.1872,\n",
      "        -2.4987, -3.4784], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.026812192052602768\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-2.6109, -3.6049, -3.3209, -3.5102, -3.4480, -3.5546, -2.6988, -3.6598,\n",
      "        -3.2895, -3.4260], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.1910, -3.4829, -3.2198, -3.4032, -3.5100, -3.4306, -3.3993, -3.4032,\n",
      "        -3.2788, -3.2788], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0970686823129654\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-2.8089, -2.9802, -3.4851, -2.4662, -2.4662, -3.5581, -3.2855, -2.6728,\n",
      "        -3.5411, -3.6157], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3742, -3.0873, -3.3869, -3.2195, -3.2195, -3.4375, -3.2803, -3.2195,\n",
      "        -3.3869, -3.4874], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.18295447528362274\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3159, -3.4860, -3.4160, -2.5378], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.2562, -3.5461, -3.4817, -3.4012, -3.5461, -3.6229, -3.4160, -3.3159,\n",
      "        -3.3159, -2.2186], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2591, -3.4282, -3.4585, -3.3960, -3.4282, -3.4963, -3.2840, -3.2203,\n",
      "        -3.2203, -2.5010], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.015990231186151505\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-2.5152, -3.6230, -2.7837, -3.2142, -3.5531, -3.2687, -3.5780, -3.7221,\n",
      "        -2.8469, -3.5404], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2637, -3.5053, -3.4743, -3.4164, -3.4414, -3.2637, -3.4556, -3.5053,\n",
      "        -3.4164, -3.4414], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15005473792552948\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.2648, -3.3495, -3.2538, -2.4757], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.4757, -2.4757, -2.5457, -3.5567, -3.7268, -3.4119, -2.5457, -3.2474,\n",
      "        -3.5567, -3.2538], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2281, -3.2281, -3.2911, -3.4569, -3.5246, -3.4315, -3.2911, -3.2281,\n",
      "        -3.4569, -3.2911], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.23066166043281555\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.2641, -3.3441, -3.2420, -2.4861], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.5348, -2.5547, -3.5573, -3.2560, -2.2193, -2.4861, -3.5243, -3.6356,\n",
      "        -2.8298, -3.2988], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2813, -3.2992, -3.4739, -3.2813, -2.5151, -3.2375, -3.6064, -3.5468,\n",
      "        -3.5012, -3.2375], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2240498811006546\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-3.4015, -2.7520, -3.4919, -2.4995, -3.2352, -3.4536, -2.4995, -3.3006,\n",
      "        -2.5654, -3.5299], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6297, -3.2495, -3.5668, -3.2495, -3.3088, -3.4590, -3.2495, -3.2495,\n",
      "        -3.3088, -3.4928], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.19925543665885925\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-3.5451, -3.4681, -3.2719, -3.2317, -3.3033, -3.6650, -3.6258, -3.6769,\n",
      "        -3.2719, -2.7910], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4779, -3.4858, -3.3071, -3.3190, -3.2613, -3.6033, -3.5244, -3.5119,\n",
      "        -3.3071, -3.5796], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06797386705875397\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.1943, -3.2160, -3.3111, -2.5144], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.2329, -3.6764, -2.2687, -3.2160, -3.5784, -3.3049, -3.5784, -2.2329,\n",
      "        -3.3049, -2.5789], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.5376, -3.6288, -2.5376, -3.5047, -3.5290, -3.2740, -3.5290, -2.5376,\n",
      "        -3.2740, -3.3210], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09010038524866104\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.2960, -3.3368, -3.2313, -2.5932], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.6911, -3.3491, -3.5840, -3.3120, -2.8167, -3.6911, -2.8167, -3.5840,\n",
      "        -3.5840, -2.9458], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6512, -3.5350, -3.5415, -3.2854, -3.2854, -3.6512, -3.2854, -3.5415,\n",
      "        -3.5415, -3.5464], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08440002053976059\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.3117, -3.3442, -3.2365, -2.6091], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.0167, -2.6091, -3.5672, -3.5357, -3.5752, -3.7129, -3.5885, -3.4417,\n",
      "        -2.5486, -3.5885], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.1619, -3.3482, -3.2927, -3.5539, -3.5862, -3.5862, -3.5539, -3.7098,\n",
      "        -3.2938, -3.5539], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12887094914913177\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-2.5590, -3.3543, -3.3365, -3.3256, -3.3365, -3.3236, -3.5959, -3.3365,\n",
      "        -2.5590, -3.5959], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3031, -3.5804, -3.5804, -3.3631, -3.5804, -3.3031, -3.5670, -3.5804,\n",
      "        -3.3031, -3.5670], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13404841721057892\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3302, -3.3683, -3.2438, -2.5713], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.2438, -3.7553, -3.5270, -2.8775, -3.3320, -2.5713, -3.4765, -3.6576,\n",
      "        -3.7553, -3.3301], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3610, -3.7282, -3.5897, -3.3142, -3.3142, -3.3142, -3.7398, -3.6327,\n",
      "        -3.7282, -3.3761], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08340869098901749\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-3.6767, -3.7813, -2.5823, -2.5823, -2.5823, -3.7815, -3.3518, -3.2782,\n",
      "        -3.0558, -2.8620], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5911, -3.7489, -3.3241, -3.3241, -3.3241, -3.7489, -3.3671, -3.3241,\n",
      "        -3.5835, -3.6022], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.24888265132904053\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-3.5785, -2.5974, -3.5179, -3.5307, -3.5939, -2.6705, -3.0777, -2.6705,\n",
      "        -3.2805, -2.3488], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7734, -3.3376, -3.5901, -3.6747, -3.6066, -3.4034, -3.5989, -3.4034,\n",
      "        -3.3762, -3.2009], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2693454623222351\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.4023, -3.4371, -3.3228, -2.6851], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.6765, -3.3642, -3.4986, -3.6765, -3.4337, -3.8465, -2.6851, -3.6765,\n",
      "        -3.5506, -3.7321], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6183, -3.3493, -3.6030, -3.6183, -3.6193, -3.7924, -3.4166, -3.6183,\n",
      "        -3.7910, -3.6183], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06645574420690536\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.4237, -3.4705, -3.3462, -2.6995], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.6416, -3.7094, -2.6242, -3.7767, -3.4683, -3.1186, -3.3918, -3.6801,\n",
      "        -3.8843, -3.5295], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6135, -3.6299, -3.3618, -3.7027, -3.6240, -3.6135, -3.3873, -3.8086,\n",
      "        -3.8086, -3.6240], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08570303022861481\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.4456, -3.5008, -3.3696, -2.7134], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.9183, -3.9183, -3.3575, -3.4356, -3.9183, -3.1426, -3.3909, -3.5680,\n",
      "        -3.3575, -3.7395], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.8245, -3.8245, -3.3933, -3.4420, -3.8245, -3.6180, -3.3762, -3.6180,\n",
      "        -3.3933, -3.6418], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.026725884526968002\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.4656, -3.5232, -3.3910, -2.7273], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.6581, -3.7031, -3.9436, -3.3846, -2.7273, -3.5614, -3.3846, -2.4383,\n",
      "        -3.4607, -3.9421], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3923, -3.8414, -3.8414, -3.3992, -3.4546, -3.6363, -3.3992, -2.6023,\n",
      "        -3.4546, -3.8414], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11406298726797104\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.4862, -3.5472, -3.4121, -2.7432], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.9438, -2.6777, -3.6914, -3.7515, -2.6777, -3.8590, -3.4167, -2.3812,\n",
      "        -2.6743, -2.7432], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7478, -3.4099, -3.4847, -3.8580, -3.4099, -3.7478, -3.4099, -2.6087,\n",
      "        -3.4068, -3.4688], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.22922363877296448\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-3.9865, -3.6983, -3.7936, -3.8694, -3.5946, -3.4294, -3.9596, -2.9572,\n",
      "        -2.4494, -3.7012], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.8739, -3.5091, -3.6787, -3.9084, -3.6615, -3.4308, -3.7686, -3.4178,\n",
      "        -2.6164, -3.6615], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03457456827163696\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-2.9487, -2.7251, -2.7251, -3.7049, -2.6969, -3.8206, -2.8178, -3.4559,\n",
      "        -3.7734, -2.7821], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4525, -3.4525, -3.4525, -3.6538, -3.4272, -3.6995, -3.5039, -3.4272,\n",
      "        -3.8901, -3.5039], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.28692007064819336\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.6127, -3.6562, -3.7318, -2.8606], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.5247, -4.0130, -2.7973, -3.4717, -3.8896, -3.7486, -3.7871, -3.0037,\n",
      "        -2.7479, -3.8384], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5175, -3.9093, -3.5175, -3.4414, -3.7197, -3.7197, -3.8193, -3.4414,\n",
      "        -3.4731, -3.7197], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12928135693073273\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-3.5319, -3.2564, -3.4622, -3.6923, -4.0225, -3.4427, -3.8282, -3.7588,\n",
      "        -4.0225, -3.5563], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5339, -3.7483, -3.4537, -3.7483, -3.9308, -3.4955, -3.9308, -3.7435,\n",
      "        -3.9308, -3.5339], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02760108932852745\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.5712, -3.6628, -3.4938, -2.7960], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.9086, -4.0331, -3.6964, -3.6255, -3.9645, -2.7960, -4.0331, -3.0754,\n",
      "        -2.7399, -3.8278], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.0300, -3.9555, -3.7679, -3.7592, -3.8712, -3.5164, -3.9555, -3.7059,\n",
      "        -3.4659, -3.9555], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15183135867118835\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.5892, -3.6765, -3.5074, -2.8192], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.8782, -3.5074, -2.8192, -3.8782, -3.7192, -3.0193, -2.8192, -1.8529,\n",
      "        -2.8192, -3.4597], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7886, -3.4806, -3.5373, -3.7886, -3.7842, -3.5373, -3.5373, 10.0000,\n",
      "        -3.5373, -3.5373], grad_fn=<AddBackward0>)\n",
      "LOSS: 14.23328971862793\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-4.0003, -3.8947, -3.8402, -3.5560, -4.0003, -2.7655, -3.7996, -3.1967,\n",
      "        -2.7999, -4.0003], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9297, -3.7414, -3.9297, -3.5199, -3.9297, -3.4890, -3.6688, -3.7414,\n",
      "        -3.5199, -3.9297], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14033612608909607\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-3.9127, -3.8009, -3.3988, -2.9980, -3.7921, -3.7039, -3.8153, -2.7180,\n",
      "        -3.6624, -2.9091], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.8126, -3.6955, -3.4462, -3.3968, -3.8852, -3.6955, -3.8852, -3.4462,\n",
      "        -3.6182, -3.4809], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10552890598773956\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.5030, -3.5752, -3.4011, -2.6787], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.4674, -3.2735, -3.5854, -2.8903, -3.6621, -2.6237, -3.9279, -2.9891,\n",
      "        -2.4142, -2.6237], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4430, -3.5761, -3.9461, -3.4430, -3.5761, -3.3614, -3.8477, -3.5927,\n",
      "        -2.5142, -3.3614], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2004004269838333\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4835, -3.5558, -3.3766, -2.6470], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.3472, -3.8499, -2.3948, -3.8103, -3.9863, -3.7688, -2.6735, -3.7229,\n",
      "        -3.3766, -3.7461], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3346, -3.7575, -2.4765, -3.6296, -3.8181, -3.8181, -3.4062, -3.7575,\n",
      "        -3.3346, -3.6296], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06320525705814362\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-3.6109, -3.4099, -3.3424, -2.8121, -3.8795, -3.5561, -3.3455, -3.3455,\n",
      "        -3.7659, -3.4637], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6080, -3.3761, -3.3585, -3.3585, -3.7936, -3.9023, -3.3129, -3.3129,\n",
      "        -3.7936, -3.5980], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04480795934796333\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-2.6117, -3.7023, -3.7023, -3.7501, -3.3285, -2.5901, -3.0776, -2.5901,\n",
      "        -2.8744, -2.5901], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3505, -3.6531, -3.6531, -3.5869, -3.3311, -3.3311, -3.6531, -3.3311,\n",
      "        -3.1312, -3.3311], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.262176513671875\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.7055, -3.7256, -3.7417, -2.8865], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.1817, -3.3180, -3.8311, -2.5929, -3.6838, -3.8311, -2.5714, -3.5493,\n",
      "        -3.8544, -3.8311], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5075, -3.3143, -3.7034, -3.3336, -3.7034, -3.7034, -3.3143, -3.6049,\n",
      "        -3.7602, -3.7034], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1267942637205124\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.3624, -3.4169, -3.2074, -2.5804], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.8415, -3.4159, -3.4881, -2.5238, -3.8415, -3.3091, -2.5804, -3.3091,\n",
      "        -2.5590, -3.5414], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7559, -3.3223, -3.4851, -3.2715, -3.7559, -3.3031, -3.3223, -3.3031,\n",
      "        -3.3031, -3.8584], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17870917916297913\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-3.2431, -3.6932, -3.7317, -3.1913, -2.5498, -2.5498, -3.4642, -3.3545,\n",
      "        -2.9867, -2.8574], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2657, -3.5716, -3.4778, -3.2948, -3.2948, -3.2948, -3.5983, -3.3160,\n",
      "        -3.5465, -3.1116], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15980127453804016\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.3501, -3.4028, -3.1781, -2.5721], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.9087, -3.6600, -3.0445, -3.6589, -3.7465, -3.6381, -3.5348, -3.8239,\n",
      "        -3.4354, -3.2210], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3149, -3.6380, -3.1068, -3.5740, -3.7537, -3.4986, -3.5356, -3.7537,\n",
      "        -3.5974, -3.2643], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02291814610362053\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.3402, -3.3924, -3.1628, -2.5670], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.8156, -3.8156, -2.5441, -3.2135, -3.5387, -3.6888, -3.7101, -2.5670,\n",
      "        -3.3903, -2.5670], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7551, -3.7551, -3.2897, -3.2643, -3.8474, -3.5786, -3.7551, -3.3103,\n",
      "        -3.3103, -3.3103], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17866069078445435\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-3.7888, -3.8093, -3.7088, -3.7888, -2.5676, -2.8847, -3.8093, -2.7880,\n",
      "        -3.7888, -2.9372], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6990, -3.7571, -3.7571, -3.6990, -3.3108, -3.2655, -3.7571, -3.6435,\n",
      "        -3.6990, -3.3108], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16009005904197693\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-3.6863, -3.7437, -3.1956, -3.7988, -3.6504, -3.7988, -3.4443, -3.1451,\n",
      "        -2.5684, -2.5684], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5936, -3.5300, -3.2668, -3.7629, -3.6465, -3.7629, -3.6057, -3.2939,\n",
      "        -3.3115, -3.3115], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12147227674722672\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-3.7118, -3.6910, -3.1927, -3.3316, -3.6802, -3.7861, -3.6802, -3.6585,\n",
      "        -3.7861, -3.6802], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7691, -3.5286, -3.2707, -3.3172, -3.6029, -3.7691, -3.6029, -3.7169,\n",
      "        -3.7691, -3.6029], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.005783010274171829\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.5331, -3.6336, -3.4171, -2.7783], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.1926, -2.5574, -3.7143, -3.6336, -3.1905, -3.6676, -3.1905, -3.5794,\n",
      "        -3.7679, -2.9658], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2743, -3.3017, -3.7754, -3.6605, -3.2743, -3.6121, -3.2743, -3.8555,\n",
      "        -3.7754, -3.3228], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07858521491289139\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3842, -3.4233, -3.1983, -2.5652], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.5389, -2.9803, -3.4233, -2.5652, -3.5137, -3.4233, -3.1737, -3.1983,\n",
      "        -3.6387, -3.2876], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6823, -3.3241, -3.5071, -3.3087, -3.6212, -3.5071, -3.5963, -3.2768,\n",
      "        -3.6212, -3.3087], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09025716036558151\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-3.2152, -3.1486, -2.5806, -2.3641, -2.9717, -2.9717, -3.7027, -2.5772,\n",
      "        -2.5772, -2.5806], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2797, -3.3195, -3.3226, -2.3041, -3.5144, -3.5144, -3.7463, -3.3195,\n",
      "        -3.3195, -3.3226], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.28307998180389404\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.5545, -3.6271, -3.4457, -2.7939], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.5391, -2.5863, -3.6814, -2.9359, -3.3043, -3.3443, -3.2378, -2.5863,\n",
      "        -3.7559, -2.5863], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2852, -3.3277, -3.7582, -3.2852, -3.3327, -3.7193, -3.2852, -3.3277,\n",
      "        -3.7972, -3.3277], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.24788916110992432\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-2.6074, -2.6074, -3.2647, -3.3145, -3.1897, -3.3653, -3.3145, -3.7757,\n",
      "        -2.0410, -2.6074], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3467, -3.3467, -3.2890, -3.3467, -3.3467, -3.3418, -3.3467, -3.7695,\n",
      "        -2.3038, -3.3467], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17364531755447388\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3298, -3.4324, -3.2549, -2.5491], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.3909, -3.3298, -3.7063, -3.7757, -2.6238, -3.7317, -3.7317, -2.6238,\n",
      "        -3.7317, -3.4324], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.3051, -3.3614, -3.6750, -3.6753, -3.3614, -3.7813, -3.5208, -3.3614,\n",
      "        -3.7813, -3.6730], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12148658186197281\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3457, -3.4465, -3.2788, -2.5583], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.8030, -3.7433, -2.6410, -2.6410, -2.6410, -3.6333, -3.3263, -2.6410,\n",
      "        -3.8232, -3.7220], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7917, -3.6885, -3.3769, -3.3769, -3.3769, -3.6885, -3.3024, -3.3769,\n",
      "        -3.9134, -3.5903], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2198370397090912\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3625, -3.4609, -3.3024, -2.5705], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.4416, -2.6562, -3.7408, -3.7661, -2.5705, -2.6562, -3.5221, -2.6562,\n",
      "        -3.5221, -3.8080], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3906, -3.3906, -3.7054, -3.7054, -3.3134, -3.3906, -3.5358, -3.3906,\n",
      "        -3.5358, -3.7162], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.21862800419330597\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.3796, -3.4750, -3.3253, -2.5863], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.7574, -3.3796, -3.6158, -3.7574, -3.1110, -3.3910, -3.8281, -3.8261,\n",
      "        -2.6803, -2.6889], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7250, -3.4200, -3.7948, -3.7250, -3.5471, -3.3277, -3.8674, -3.8181,\n",
      "        -3.4122, -3.4200], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13019254803657532\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-3.9440, -3.8564, -3.4198, -3.8649, -3.8580, -2.6040, -2.6040, -3.4198,\n",
      "        -3.7727, -3.4198], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.8850, -3.8328, -3.3436, -3.9674, -3.7651, -3.3436, -3.3436, -3.3436,\n",
      "        -3.7469, -3.3436], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11352884769439697\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-3.8057, -2.7353, -3.8558, -3.5203, -3.5203, -3.8747, -3.3702, -3.6854,\n",
      "        -3.7862, -3.8462], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9865, -3.4618, -3.9056, -3.4567, -3.4567, -3.9056, -3.4618, -3.8639,\n",
      "        -3.7702, -3.8526], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06125596910715103\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.6075, -3.6025, -3.4580, -2.7533], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.7521, -2.7521, -2.7533, -2.7521, -3.8756, -3.8384, -3.2435, -3.5436,\n",
      "        -3.9086, -3.8763], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4769, -3.4769, -3.4780, -3.4769, -3.8833, -3.9951, -3.8833, -3.4769,\n",
      "        -3.8168, -3.9191], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.25497621297836304\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.6260, -3.6293, -3.4803, -2.7759], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.5576, -3.4803, -2.7759, -2.7807, -3.1735, -3.8892, -3.8892, -2.6686,\n",
      "        -3.4253, -3.8892], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.8561, -3.4017, -3.4983, -3.5026, -3.5026, -3.8795, -3.8795, -3.4017,\n",
      "        -3.4983, -3.8795], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1789759397506714\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.6445, -3.6587, -3.5026, -2.7980], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.9189, -2.7980, -3.9459, -3.9169, -3.9169, -4.0002, -3.8619, -3.5026,\n",
      "        -3.9459, -3.9251], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9712, -3.5182, -3.9712, -3.8976, -3.8976, -3.9712, -3.8198, -3.4233,\n",
      "        -3.9712, -3.8976], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.053311217576265335\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.6616, -3.6847, -3.5209, -2.8193], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.8193, -2.0866, -2.7164, -3.9787, -3.6616, -3.8867, -3.6350, -3.5209,\n",
      "        -3.1037, -3.8900], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5373, -2.3514, -3.4448, -3.9957, -3.5565, -3.8379, -3.8919, -3.4448,\n",
      "        -3.4448, -3.8379], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1320827752351761\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.6751, -3.7111, -3.5377, -2.8386], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.8386, -3.8806, -4.0277, -3.2466, -2.8386, -2.8744, -3.9143, -4.0277,\n",
      "        -3.9973, -3.7122], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5547, -3.7377, -4.0201, -3.9054, -3.5547, -3.5869, -3.8563, -4.0201,\n",
      "        -4.0499, -3.9054], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.20315344631671906\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.6945, -3.7366, -3.5568, -2.8607], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.1214, -1.5192, -3.5568, -4.0428, -3.9948, -3.7214, -2.9092, -3.3833,\n",
      "        -2.8607, -3.2317], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.3673, 10.0000, -3.4825, -4.0450, -3.9524, -3.9086, -3.6183, -3.9506,\n",
      "        -3.5747, -3.6183], grad_fn=<AddBackward0>)\n",
      "LOSS: 13.4278564453125\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.6701, -3.7184, -3.5308, -2.8148], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.8152, -2.8772, -4.0375, -3.1708, -4.0426, -3.5229, -4.0045, -2.7151,\n",
      "        -3.5229, -2.8148], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.8991, -3.5895, -4.0105, -3.5895, -4.0187, -3.5333, -4.0187, -3.4436,\n",
      "        -3.5333, -3.5333], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17383496463298798\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.6527, -3.7060, -3.5114, -2.7782], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.4586, -3.8722, -2.7782, -3.1732, -3.6761, -3.8752, -3.9465, -2.6814,\n",
      "        -2.7782, -2.8471], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.2679, -3.6795, -3.5004, -3.5887, -3.5624, -3.7952, -3.5887, -3.4132,\n",
      "        -3.5004, -3.5624], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.24838118255138397\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.6358, -3.6927, -3.4873, -2.7521], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.0249, -2.8479, -3.2776, -3.1072, -3.9785, -3.9077, -3.4937, -3.7114,\n",
      "        -3.7114, -3.4873], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9636, -3.4769, -3.8844, -3.1828, -3.9636, -3.8472, -3.4769, -3.7813,\n",
      "        -3.7813, -3.3914], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07964317500591278\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.6110, -3.6732, -3.4569, -2.7214], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.4232, -3.8717, -2.7214, -2.7214, -3.8717, -3.2777, -3.4804, -3.8865,\n",
      "        -2.7214, -3.8717], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3737, -3.7447, -3.4493, -3.4493, -3.7447, -3.8030, -3.4493, -3.8256,\n",
      "        -3.4493, -3.7447], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.19207869470119476\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-3.8495, -2.7032, -4.0199, -2.6244, -3.8642, -2.7032, -3.1259, -3.6521,\n",
      "        -4.0185, -3.4965], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7267, -3.4328, -3.9509, -3.3619, -3.8132, -3.4328, -3.1393, -3.5223,\n",
      "        -3.9301, -3.4328], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16601839661598206\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-1.9007, -3.4123, -3.9422, -3.8994, -3.4123, -2.6179, -3.2567, -3.8238,\n",
      "        -3.7783, -3.6955], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.1583, -3.3561, -3.7650, -3.8072, -3.3561, -3.3561, -3.8408, -3.7140,\n",
      "        -3.7140, -3.7112], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10151676833629608\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.5567, -3.6159, -3.3884, -2.6874], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.5951, -3.9062, -2.7990, -3.9705, -2.3508, -3.6159, -3.6543, -4.0080,\n",
      "        -2.6874, -3.8794], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6635, -3.9672, -3.5191, -3.9446, -2.1431, -3.5453, -3.7059, -3.9672,\n",
      "        -3.4187, -3.8045], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1120414137840271\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-3.0525, -2.6827, -2.8326, -2.8010, -2.8326, -3.6303, -3.7723, -3.9992,\n",
      "        -3.4374, -3.9992], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5494, -3.4144, -3.4144, -3.5209, -3.4144, -3.6991, -3.6991, -3.9765,\n",
      "        -3.4144, -3.9765], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1989104449748993\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-2.8086, -3.3429, -2.6697, -2.6232, -3.8559, -3.9926, -3.9138, -3.5167,\n",
      "        -2.9835, -3.5730], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5277, -3.3609, -3.4028, -3.3609, -3.7516, -3.9904, -3.9785, -3.5277,\n",
      "        -3.5277, -3.6558], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.19171565771102905\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-3.9915, -3.8317, -3.6687, -2.6631, -3.7218, -3.8075, -3.3489, -3.9657,\n",
      "        -2.6631, -2.6631], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.0064, -3.8112, -3.6937, -3.3968, -3.7606, -4.0064, -3.3700, -3.8053,\n",
      "        -3.3968, -3.3968], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1683368980884552\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.6139, -3.6482, -3.4358, -2.8164], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.6625, -2.3235, -3.6482, -1.8263, -3.9903, -2.6625, -2.6625, -2.9408,\n",
      "        -3.3564, -3.5381], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3963, -2.1135, -3.7035, -2.1135, -4.0207, -3.3963, -3.3963, -3.0911,\n",
      "        -3.7701, -3.8219], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.20200219750404358\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.6167, -3.6522, -3.4390, -2.8228], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.1168, -3.6167, -3.3082, -3.0195, -2.6658, -3.0753, -4.0061, -3.8315,\n",
      "        -3.3082, -3.4444], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7123, -3.5405, -3.3902, -3.0854, -3.3992, -3.7024, -4.0406, -3.8288,\n",
      "        -3.3902, -3.3992], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1312505155801773\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.6206, -3.6658, -3.4468, -2.8270], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.4495, -3.6206, -3.6658, -3.7182, -2.9341, -3.4923, -2.6695, -2.6695,\n",
      "        -3.6206, -4.0320], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4026, -3.5443, -3.7091, -3.8129, -3.4026, -3.5443, -3.4026, -3.4026,\n",
      "        -3.5443, -4.0569], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1322164088487625\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.6232, -3.6818, -3.4555, -2.8334], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.6730, -2.6667, -3.3737, -3.4164, -2.6730, -2.6730, -2.9684, -3.9170,\n",
      "        -3.4528, -3.7235], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4057, -3.4000, -3.4000, -3.7834, -3.4057, -3.4057, -3.1063, -4.0783,\n",
      "        -3.4057, -3.6897], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.23321203887462616\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.6295, -3.7027, -3.4701, -2.8439], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.0515, -3.7330, -3.7439, -2.6842, -3.6710, -2.6842, -3.4701, -3.6295,\n",
      "        -2.6762, -3.7465], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.0987, -3.6889, -3.8342, -3.4158, -3.6795, -3.4158, -3.4158, -3.5595,\n",
      "        -3.4086, -3.6889], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16303594410419464\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.6352, -3.7224, -3.4820, -2.8554], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.3708, -3.0190, -2.9910, -4.0344, -3.7446, -2.7004, -4.1375, -3.9886,\n",
      "        -4.0461, -3.7446], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.1197, -3.4304, -3.1337, -4.1282, -3.6919, -3.4304, -4.1282, -3.8478,\n",
      "        -4.1193, -3.6919], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0825158879160881\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-2.8668, -2.7121, -3.6396, -3.9277, -3.9277, -3.8196, -2.7011, -3.1808,\n",
      "        -3.8469, -4.1698], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5801, -3.4409, -3.5801, -3.8756, -3.8756, -3.7293, -3.4310, -3.7293,\n",
      "        -4.1531, -4.1531], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.19847196340560913\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.7453, -3.7847, -3.7383, -3.0367], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.8486, -3.7071, -3.7640, -3.4871, -3.0367, -2.7268, -3.5088, -3.9500,\n",
      "        -2.7268, -3.8192], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7330, -3.8940, -3.7041, -3.4542, -3.5933, -3.4542, -3.4542, -3.8940,\n",
      "        -3.4542, -3.8104], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1427016407251358\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.6468, -3.7617, -3.5119, -2.8877], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.8709, -3.9708, -4.2200, -2.8877, -4.2200, -3.7617, -3.5441, -3.7424,\n",
      "        -3.9708, -4.1126], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7531, -3.9119, -4.1973, -3.5990, -4.1973, -3.7531, -3.9159, -3.9119,\n",
      "        -3.9119, -4.1973], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07018347084522247\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-3.5182, -2.8963, -3.6725, -4.2407, -4.2407, -2.7512, -3.0575, -3.8078,\n",
      "        -3.5078, -3.5781], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4913, -3.6067, -3.8323, -4.2203, -4.2203, -3.4761, -3.1675, -3.9369,\n",
      "        -3.4913, -3.8364], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11529342085123062\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.7986, -3.8547, -3.8043, -3.1086], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.6956, -3.8078, -3.4410, -2.7918, -3.9039, -2.7918, -2.7918, -3.7770,\n",
      "        -1.8993, -3.4834], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.8560, -3.7475, -3.4929, -3.5126, -4.2485, -3.5126, -3.5126, -3.7977,\n",
      "        -2.1533, -3.4929], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17745614051818848\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.6692, -3.7920, -3.5459, -2.9169], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.8199, -3.5466, -4.1175, -3.9368, -2.8199, -2.9169, -3.1319, -4.1755,\n",
      "        -1.2887, -3.1945], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5379, -3.5379, -3.8580, -4.2680, -3.5379, -3.6252, -3.1878, -3.9651,\n",
      "        10.0000, -3.5379], grad_fn=<AddBackward0>)\n",
      "LOSS: 12.93089485168457\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.6538, -3.7635, -3.5236, -2.8721], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.8721, -2.7875, -2.8721, -3.5383, -3.5741, -2.8721, -2.7463, -3.6102,\n",
      "        -3.8066, -3.7136], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5849, -3.5087, -3.5849, -3.5087, -3.8159, -3.5849, -3.4717, -3.9277,\n",
      "        -3.7266, -3.8373], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.27523958683013916\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.6484, -3.7471, -3.5124, -2.8430], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.7471, -4.1331, -3.7930, -1.9095, -3.6191, -2.7688, -4.1803, -2.7172,\n",
      "        -2.8430, -4.2199], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7587, -3.9208, -3.9208, -3.1093, -3.6855, -3.4919, -4.2320, -3.4455,\n",
      "        -3.5587, -4.1849], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.30747637152671814\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-2.8130, -2.7483, -3.8828, -4.0219, -4.0219, -3.6452, -3.5004, -3.0859,\n",
      "        -3.9628, -2.8130], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5317, -3.4734, -3.6720, -3.9006, -3.9006, -3.5317, -3.4734, -3.4734,\n",
      "        -3.7437, -3.5317], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1844606250524521\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.6413, -3.7370, -3.4824, -2.7944], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.6659, -3.8664, -3.0687, -4.0620, -3.8664, -4.0166, -3.6413, -4.2098,\n",
      "        -3.5493, -2.9670], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3993, -3.6548, -3.4593, -3.7193, -3.6548, -3.8853, -3.5149, -4.1226,\n",
      "        -3.8152, -3.0055], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10104839503765106\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.6318, -3.7186, -3.4550, -2.7828], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.0613, -3.7807, -4.1808, -3.4299, -3.7705, -4.1782, -3.8235, -2.7194,\n",
      "        -3.5222, -3.9893], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4475, -3.8709, -4.1066, -3.3885, -3.6514, -4.1066, -3.7872, -3.4475,\n",
      "        -3.4475, -3.8709], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07347841560840607\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.6235, -3.6966, -3.4280, -2.7764], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.7555, -2.8754, -2.6461, -4.1533, -2.7077, -2.8754, -3.7910, -3.7555,\n",
      "        -2.7077, -2.7077], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6526, -3.3815, -3.3815, -4.0976, -3.4370, -3.3815, -3.6526, -3.6526,\n",
      "        -3.4370, -3.4370], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.26918041706085205\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-4.0139, -3.9213, -2.6321, -2.7775, -4.1195, -3.0006, -3.4959, -3.6441,\n",
      "        -4.1195, -3.0774], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7005, -3.8551, -3.3689, -3.4997, -4.0996, -3.7697, -3.4365, -3.5991,\n",
      "        -4.0996, -3.4365], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.18939292430877686\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.8000, -3.7481, -3.6775, -3.0049], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.4777, -3.8744, -3.6499, -2.7828, -3.0049, -3.4777, -3.6129, -4.0842,\n",
      "        -3.0818, -3.0049], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4351, -4.2218, -3.7044, -3.5045, -3.5045, -3.4351, -3.5045, -4.1054,\n",
      "        -3.4351, -3.5045], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12844735383987427\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-3.3174, -3.0349, -3.9078, -3.5925, -3.7166, -3.7166, -3.8561, -3.7513,\n",
      "        -4.0625, -3.7019], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3563, -3.4971, -3.7407, -3.7839, -3.6939, -3.6939, -3.8563, -3.7521,\n",
      "        -4.1125, -3.7343], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02842102386057377\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.8359, -3.7670, -3.6827, -3.0727], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.7593, -2.7593, -2.7593, -3.0698, -3.5994, -3.5643, -2.9408, -3.0727,\n",
      "        -3.0698, -2.7046], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4834, -3.4834, -3.4834, -3.7905, -3.7554, -3.4834, -3.3521, -3.4834,\n",
      "        -3.7905, -3.4342], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.351237952709198\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-3.4674, -3.2773, -3.5425, -3.2859, -2.6107, -2.7125, -3.7191, -3.4231,\n",
      "        -3.5727, -3.7191], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6746, -3.4413, -3.6746, -3.3496, -3.3496, -3.4413, -3.7338, -3.4413,\n",
      "        -3.7836, -3.7338], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1213720291852951\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-3.9017, -3.8290, -3.7323, -3.1715], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.7331, -2.7377, -3.8290, -3.5783, -4.2167, -2.7207, -3.1715, -3.1927,\n",
      "        -2.7207, -3.4923], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7542, -3.4639, -3.7712, -3.7728, -4.2703, -3.4486, -3.4639, -3.7785,\n",
      "        -3.4486, -3.8519], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.21897199749946594\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.5068, -3.5437, -3.2500, -2.7317], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.6125, -2.6125, -2.7395, -3.7682, -2.7395, -4.0584, -2.7317, -2.7395,\n",
      "        -3.4103, -3.5380], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3513, -3.3513, -3.4656, -3.7657, -3.4656, -4.1656, -3.4585, -3.4656,\n",
      "        -3.4656, -3.7208], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3249173164367676\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-2.7643, -3.7145, -2.7318, -3.7784, -3.8662, -3.8662, -3.9059, -2.8289,\n",
      "        -2.7643, -3.7784], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4878, -3.9263, -3.4586, -3.8016, -4.3107, -4.3107, -3.9187, -2.8696,\n",
      "        -3.4878, -3.8016], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.20184020698070526\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.5058, -3.5695, -3.2523, -2.7299], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.7299, -4.1267, -2.7854, -2.7299, -2.6345, -4.1267, -2.7043, -4.1267,\n",
      "        -2.6345, -3.2819], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4569, -4.2041, -3.5069, -3.4569, -3.3710, -4.2041, -2.8701, -4.2041,\n",
      "        -3.3710, -3.7553], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.29320988059043884\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.5160, -3.6006, -3.2674, -2.7339], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.6006, -2.7339, -4.0317, -3.3632, -4.1768, -3.6265, -3.0501, -4.1768,\n",
      "        -4.0847, -3.7205], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.0269, -3.4605, -3.8559, -3.4605, -4.2201, -3.8420, -3.5285, -4.2201,\n",
      "        -4.2201, -3.7450], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10480304062366486\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.5345, -3.6491, -3.2991, -2.7437], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.6325, -4.2342, -3.9333, -2.8216, -3.7442, -3.4175, -4.0058, -4.0058,\n",
      "        -2.7437, -3.5894], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7952, -4.2304, -3.8576, -3.5394, -3.8576, -3.3963, -3.9661, -3.9661,\n",
      "        -3.4693, -4.0204], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12763063609600067\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.5561, -3.6953, -3.3365, -2.7580], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.7580, -3.8200, -1.6372, -2.6764, -4.2945, -3.7192, -4.2945, -3.0492,\n",
      "        -3.4939, -3.9827], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.4822, -3.7443, -1.8611, -3.4088, -4.2517, -3.8098, -4.2517, -3.5535,\n",
      "        -3.5535, -3.8720], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13985861837863922\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.5815, -3.7376, -3.3736, -2.7760], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.8446, -2.7760, -3.8528, -2.8446, -3.9820, -2.7760, -3.5203, -3.4849,\n",
      "        -3.5815, -2.8446], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5602, -3.4984, -3.7590, -3.5602, -4.0041, -3.4984, -3.5602, -3.4263,\n",
      "        -3.4984, -3.5602], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.26009243726730347\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-3.8838, -2.8583, -3.6093, -3.6093, -4.3963, -4.0550, -3.5121, -2.8583,\n",
      "        -3.8838, -2.8010], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7794, -3.5725, -3.5209, -3.5209, -4.3021, -4.3021, -3.4489, -3.5725,\n",
      "        -3.7794, -3.5209], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16497254371643066\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.6373, -3.8146, -3.4448, -2.8291], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.8146, -2.8732, -2.8291, -3.5779, -4.4315, -4.1861, -3.4679, -3.8146,\n",
      "        -4.0928, -2.8291], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.1746, -3.5859, -3.5462, -3.5859, -4.3274, -4.0572, -3.4740, -4.1746,\n",
      "        -3.9269, -3.5462], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.18506905436515808\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.6836, -3.8719, -3.4959, -2.8719], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.8878, -3.8719, -3.6836, -3.6058, -3.6836, -3.5036, -3.7580, -4.4692,\n",
      "        -2.7738, -3.5005], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5990, -4.1896, -3.5847, -3.5990, -3.5847, -3.8287, -3.5847, -4.3504,\n",
      "        -3.4964, -3.4964], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12983883917331696\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.7277, -3.9360, -3.5505, -2.9217], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.5741, -3.8633, -4.1548, -3.5741, -3.7642, -2.9217, -3.5505, -2.9016,\n",
      "        -4.5056, -3.7487], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5201, -3.9632, -3.9632, -3.5201, -3.6295, -3.6295, -3.6114, -3.6114,\n",
      "        -4.3738, -4.1841], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1286221295595169\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.7715, -3.9949, -3.6066, -2.9780], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.1756, -3.7639, -4.0737, -3.2528, -3.9674, -2.9780, -4.0082, -3.6440,\n",
      "        -3.5875, -2.9161], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9844, -3.6802, -4.1390, -3.5473, -3.8716, -3.6802, -3.9844, -3.6245,\n",
      "        -3.5473, -3.6245], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11412099748849869\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.8139, -4.0471, -3.6593, -3.0374], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.2887, -3.0374, -4.4248, -4.5671, -3.0374, -3.6540, -3.9755, -3.8139,\n",
      "        -4.1963, -4.4411], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.5703, -3.7337, -4.1118, -4.4492, -3.7337, -3.6388, -4.2331, -3.7337,\n",
      "        -4.2382, -4.4366], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12355241924524307\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.8589, -4.0892, -3.7109, -3.0984], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.6602, -4.5815, -3.7109, -4.5815, -4.1933, -4.2519, -4.5469, -3.8775,\n",
      "        -3.9248, -4.5815], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6553, -4.4898, -3.6553, -4.4898, -4.0352, -4.2670, -4.4898, -4.2670,\n",
      "        -4.0352, -4.4898], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.022077830508351326\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.8993, -4.1170, -3.7580, -3.1570], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.5900, -3.1570, -3.2961, -4.3818, -4.5900, -4.3706, -4.3208, -2.8967,\n",
      "        -4.1851, -4.5900], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5406, -3.8413, -3.6751, -4.1925, -4.5406, -4.2331, -4.2331, -3.6070,\n",
      "        -4.0619, -4.5406], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12013968080282211\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.9365, -4.1316, -3.7977, -3.2187], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.9927, -4.2539, -4.3073, -2.9451, -3.9377, -3.9309, -3.6111, -4.3932,\n",
      "        -2.9927, -2.9228], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6934, -4.3160, -4.2729, -3.0211, -4.0822, -4.2402, -3.6305, -4.2729,\n",
      "        -3.6934, -3.6305], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16251854598522186\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-3.9732, -4.1498, -3.8353, -3.2786], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.9541, -3.6463, -3.2786, -4.4966, -3.6137, -4.0468, -3.0178, -3.2786,\n",
      "        -3.3959, -4.5739], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6587, -3.9507, -3.9507, -4.6421, -3.6587, -4.3421, -3.7160, -3.9507,\n",
      "        -3.7160, -4.6421], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2197762429714203\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.0095, -4.1731, -3.8733, -3.3367], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.6769, -4.0058, -4.1731, -4.4750, -3.3367, -3.4546, -3.9493, -3.3367,\n",
      "        -4.0377, -4.1467], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.0030, -4.1636, -4.3092, -4.6339, -4.0030, -3.7390, -4.3612, -4.0030,\n",
      "        -4.3225, -4.1551], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13948646187782288\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-4.3309, -4.2863, -4.4815, -3.0607, -4.4571, -4.1382, -4.3309, -3.5571,\n",
      "        -4.1649, -4.3127], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3925, -4.7469, -4.3925, -3.7546, -4.3574, -4.3387, -4.3925, -3.7179,\n",
      "        -4.1849, -4.3574], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07876352965831757\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.0970, -4.2545, -3.9574, -3.4317], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.4317, -4.3322, -3.0751, -3.7442, -4.0204, -3.0751, -3.0103, -4.6433,\n",
      "        -4.5090, -3.4317], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.0885, -4.3861, -3.7676, -3.7676, -4.4304, -3.7676, -3.1083, -4.7781,\n",
      "        -4.4304, -4.0885], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.20274074375629425\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.1497, -4.3113, -4.0015, -3.4744], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.6859, -4.6859, -4.0015, -3.4744, -4.3113, -4.6859, -3.6300, -4.3113,\n",
      "        -4.0015, -3.4744], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8041, -4.8041, -3.7818, -4.1269, -4.3802, -4.8041, -3.7584, -4.3802,\n",
      "        -3.7818, -4.1269], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1016053706407547\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-3.1145, -1.8574, -4.3708, -3.7285, -4.2542, -3.1145, -4.7081, -3.8043,\n",
      "        -3.5109, -3.1145], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.8030, -3.1436, -4.3972, -3.7753, -4.4511, -3.8030, -4.7944, -3.8030,\n",
      "        -4.1598, -3.8030], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.35469427704811096\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-4.6692, -3.5415, -3.1359, -3.7856, -3.6973, -4.5790, -3.5415, -3.5415,\n",
      "        -3.6921, -3.5415], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8498, -4.1873, -3.8223, -4.1873, -3.8223, -4.5009, -4.1873, -4.1873,\n",
      "        -3.7863, -4.1873], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.23642882704734802\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.2827, -4.5122, -4.0881, -3.5737], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.8146, -3.1611, -4.1937, -4.6092, -3.1119, -3.0862, -4.3956, -3.1611,\n",
      "        -3.5737, -3.8146], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.2163, -3.8450, -4.3524, -4.5256, -3.8007, -3.1139, -4.2907, -3.8450,\n",
      "        -4.2163, -4.2163], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2189549207687378\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.3116, -4.5616, -4.1068, -3.5943], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.6261, -3.8714, -3.5943, -3.8847, -3.5943, -3.5943, -2.3438, -4.3179,\n",
      "        -4.6236, -4.5616], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5563, -4.2349, -4.2349, -4.4843, -4.2349, -4.2349, -2.0214, -4.4962,\n",
      "        -4.4962, -4.4843], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.18853852152824402\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-4.2587, -3.2300, -3.2300, -4.3849, -3.9813, -4.3541, -4.6858, -3.8345,\n",
      "        -4.9923, -3.7694], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4223, -3.9070, -3.9070, -4.5922, -4.2521, -4.4223, -4.3191, -3.9070,\n",
      "        -4.9282, -3.8463], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12141820043325424\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0364, -4.2645, -3.8067, -3.2717], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.6251, -3.6251, -4.4061, -3.9731, -4.7479, -3.1900, -5.0314, -4.6197,\n",
      "        -4.3935, -4.6447], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.2626, -4.2626, -4.3839, -4.2626, -4.5827, -3.8710, -4.9541, -4.5758,\n",
      "        -4.5519, -4.9541], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1516885608434677\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0973, -4.3073, -3.8429, -3.3152], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.8183, -3.6367, -3.8429, -4.4442, -3.8502, -3.8429, -4.0170, -3.8429,\n",
      "        -4.6433, -3.6367], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6607, -4.2731, -3.8997, -4.4222, -3.8997, -3.8997, -3.9837, -3.8997,\n",
      "        -4.5791, -4.2731], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08525173366069794\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1576, -4.3480, -3.8821, -3.3621], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.6542, -4.5604, -3.0150, -3.3621, -4.0846, -3.3621, -4.1464, -4.7742,\n",
      "        -4.4546, -3.8784], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.2887, -4.6999, -3.0991, -4.0259, -4.2887, -4.0259, -4.0259, -5.0092,\n",
      "        -4.6111, -3.9236], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14483432471752167\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2322, -4.3944, -3.9263, -3.4132], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.5951, -4.5951, -3.6674, -4.7402, -2.1059, -3.4132, -3.6674, -3.6674,\n",
      "        -4.0226, -4.8945], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6203, -4.6203, -4.3007, -4.7415, -3.1024, -4.0719, -4.3007, -4.3007,\n",
      "        -4.0719, -4.7414], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2657165825366974\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3039, -4.4451, -3.9715, -3.4606], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.6843, -3.6843, -4.1568, -3.9715, -4.7125, -3.9715, -5.1376, -3.6843,\n",
      "        -4.9518, -5.1373], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3159, -4.3159, -4.1146, -3.9687, -4.5415, -3.9687, -5.0627, -4.3159,\n",
      "        -4.7574, -5.1506], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1271289438009262\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-4.6877, -4.9653, -4.6877, -4.7806, -3.5114, -3.7115, -3.5114, -4.7410,\n",
      "        -5.1530, -4.3697], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8280, -4.8191, -4.8280, -4.7001, -4.1602, -4.3404, -4.1602, -4.5822,\n",
      "        -5.0935, -4.3404], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13343651592731476\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-4.1244, -4.4145, -5.1645, -3.5648, -4.7095, -4.1607, -4.2990, -4.4433,\n",
      "        -4.5932, -3.7508], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7446, -4.5959, -5.1267, -4.2083, -4.8691, -4.2083, -4.3757, -4.5959,\n",
      "        -4.3757, -4.3757], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1327902376651764\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9630, -5.0617, -4.5913, -4.1717], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.6145, -4.6624, -5.1826, -1.3731, -4.8390, -4.0195, -4.0195, -5.1826,\n",
      "        -3.3854, -4.0041], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.2531, -4.6617, -5.1567, 10.0000, -4.8995, -4.0468, -4.0468, -5.1567,\n",
      "        -4.0468, -4.0468], grad_fn=<AddBackward0>)\n",
      "LOSS: 13.020187377929688\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9860, -5.0776, -4.6228, -4.1156], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.7089, -4.4236, -3.5638, -3.3226, -4.6333, -4.9404, -4.6105, -4.1156,\n",
      "        -4.7617, -3.5638], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8452, -4.6111, -4.2074, -3.9903, -4.6111, -4.8384, -4.3572, -4.2074,\n",
      "        -4.6111, -4.2074], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14343011379241943\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.4465, -4.5116, -4.0728, -3.5229], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.5229, -3.6844, -3.5229, -4.9388, -5.0226, -4.9388, -3.6844, -4.0541,\n",
      "        -4.6850, -5.0827], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.1706, -4.3160, -4.1706, -4.7877, -5.0202, -4.7877, -4.3160, -4.4846,\n",
      "        -4.7914, -5.0202], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1882990151643753\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-2.8392, -5.0341, -3.4970, -4.5679, -4.1818, -4.1947, -4.4082, -5.0341,\n",
      "        -3.4970, -3.6575], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.8529, -4.9725, -4.1473, -4.2917, -4.7505, -4.1473, -4.5428, -4.9725,\n",
      "        -4.1473, -4.2917], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16758808493614197\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1672, -4.3570, -3.9475, -3.2070], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.9869, -3.2070, -3.6405, -4.8638, -4.8573, -4.6830, -4.0023, -3.4838,\n",
      "        -3.2070, -3.2070], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9356, -3.8863, -4.2764, -4.7265, -4.7580, -4.5241, -4.5701, -4.1354,\n",
      "        -3.8863, -3.8863], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2592371106147766\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1514, -4.3310, -3.9477, -3.1971], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.4117, -2.8768, -4.8247, -4.6509, -3.7242, -4.6509, -4.6216, -4.0831,\n",
      "        -4.6216, -4.4148], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5185, -2.7725, -4.6748, -4.5185, -3.8774, -4.5185, -4.6748, -4.2724,\n",
      "        -4.6748, -4.2724], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.016502745449543\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-3.6373, -4.1235, -4.6155, -4.4147, -4.5129, -4.6155, -4.7752, -4.7425,\n",
      "        -4.2218, -3.6373], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.2735, -4.1352, -4.5177, -4.5177, -4.2735, -4.5177, -4.7056, -4.7056,\n",
      "        -4.1352, -4.2735], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09105166047811508\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-3.6451, -4.8245, -4.2644, -3.6451, -4.7278, -3.4910, -5.0823, -4.4946,\n",
      "        -4.3170, -3.6451], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.2805, -4.8853, -4.3171, -4.2805, -4.7054, -4.1419, -5.0447, -4.3171,\n",
      "        -4.6198, -4.2805], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17667844891548157\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0692, -4.2380, -3.9385, -3.1856], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.6867, -5.0598, -3.6653, -4.4700, -4.5410, -4.1015, -4.7112, -3.5065,\n",
      "        -3.1856, -4.6671], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7124, -5.0498, -4.2988, -4.3143, -4.5321, -3.8670, -4.7124, -4.1558,\n",
      "        -3.8670, -4.6436], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13678807020187378\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0559, -4.2207, -3.9342, -3.2027], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.7511, -3.5233, -4.6174, -4.0927, -3.2027, -4.6535, -4.5119, -4.8030,\n",
      "        -3.5233, -4.7511], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9050, -4.1709, -4.6479, -3.8825, -3.8825, -4.7240, -4.8448, -4.9050,\n",
      "        -4.1709, -4.9050], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15197421610355377\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0544, -4.2243, -3.9356, -3.2272], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.6513, -3.6974, -4.0544, -3.5386, -4.8407, -4.3484, -3.5386, -4.7435,\n",
      "        -4.6513, -4.6394], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7353, -3.9045, -4.1848, -4.1848, -5.0830, -4.6572, -4.1848, -4.8599,\n",
      "        -4.7353, -4.7353], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10857989639043808\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0725, -4.2315, -3.9439, -3.2458], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.7384, -4.0725, -4.6251, -5.0207, -3.9439, -5.0207, -4.9094, -3.5519,\n",
      "        -4.5399, -4.0604], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3646, -4.1967, -4.7373, -5.0939, -3.9212, -5.0939, -4.6648, -4.1967,\n",
      "        -4.6543, -4.3646], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10126221179962158\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0975, -4.2514, -3.9510, -3.2680], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.0707, -4.5543, -4.5291, -3.2680, -3.9510, -3.2680, -3.7560, -3.5640,\n",
      "        -3.2680, -3.8229], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3804, -4.6636, -4.4406, -3.9412, -3.9412, -3.9412, -4.3804, -4.2076,\n",
      "        -3.9412, -4.2076], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.24275419116020203\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1298, -4.2801, -3.9637, -3.3016], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.5881, -3.8360, -3.7444, -4.8013, -4.2404, -3.7760, -3.3016, -4.5738,\n",
      "        -4.5130, -4.6644], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8879, -4.2182, -3.9715, -4.9865, -4.2182, -4.3984, -3.9715, -4.8879,\n",
      "        -4.3984, -4.7498], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1277417242527008\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1608, -4.3126, -3.9772, -3.3279], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.4112, -4.8237, -3.5789, -4.3168, -3.3279, -5.0447, -4.8267, -4.3707,\n",
      "        -4.8267, -3.5789], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6418, -5.0133, -4.2210, -4.7107, -3.9951, -5.1433, -5.0133, -4.4742,\n",
      "        -5.0133, -4.2210], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16041865944862366\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-4.7064, -3.8134, -4.8819, -1.6361, -4.8819, -5.0937, -3.5836, -4.7064,\n",
      "        -4.6467, -4.0041], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7763, -4.4321, -5.0262, -1.7920, -5.0262, -4.7348, -4.2252, -4.7763,\n",
      "        -4.7205, -4.0159], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10045753419399261\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2329, -4.4219, -4.0299, -3.3720], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.8320, -3.8895, -3.8320, -4.2060, -4.2329, -4.9500, -4.0837, -3.8320,\n",
      "        -3.5890, -4.3973], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4488, -4.2301, -4.4488, -4.8170, -4.2301, -5.0365, -4.0348, -4.4488,\n",
      "        -4.2301, -4.4271], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2052375078201294\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2647, -4.4810, -4.0570, -3.3966], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.4159, -4.4289, -4.6358, -4.6968, -4.8428, -4.7190, -4.7813, -5.0206,\n",
      "        -5.0206, -1.8334], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4771, -4.4462, -4.5278, -4.7368, -4.8173, -4.6803, -4.9115, -5.0536,\n",
      "        -5.0536, -1.7882], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.004065416753292084\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-4.2222, -5.0893, -4.1043, -4.4289, -4.1632, -3.8884, -4.9166, -5.0893,\n",
      "        -4.5408, -3.8884], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5544, -5.0670, -4.0756, -4.4996, -4.4996, -4.4996, -4.8445, -5.0670,\n",
      "        -4.5544, -4.4996], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09827728569507599\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.4462, -4.5813, -4.1169, -3.6061], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.9071, -3.4429, -3.6061, -3.6061, -4.8728, -4.9826, -4.1169, -3.6061,\n",
      "        -5.1501, -4.6959], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5164, -4.0986, -4.2455, -4.2455, -4.8752, -4.8752, -4.0986, -4.2455,\n",
      "        -5.0846, -4.5164], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.20760512351989746\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-5.2306, -4.7173, -3.9350, -5.0388, -4.7324, -4.7173, -4.1332, -4.8933,\n",
      "        -4.8658, -4.9362], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1103, -4.5415, -4.5415, -4.9127, -4.9471, -4.5415, -4.1296, -4.9127,\n",
      "        -4.7292, -4.8047], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05424647405743599\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-4.6388, -3.6581, -3.6581, -4.4694, -1.6710, -3.5161, -3.6581, -4.9555,\n",
      "        -5.2327, -5.3607], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6383, -4.2923, -4.2923, -4.5697, -1.7936, -4.1645, -4.2923, -4.8467,\n",
      "        -5.1470, -5.3030], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16746163368225098\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.4871, -4.6597, -4.1647, -3.6990], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.6990, -4.5458, -4.4359, -4.9768, -3.5604, -3.6990, -5.2559, -5.2559,\n",
      "        -3.6990, -4.9624], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3291, -4.5574, -4.8950, -4.9566, -4.2043, -4.3291, -5.1876, -5.1876,\n",
      "        -4.3291, -4.8950], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1830945909023285\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5108, -4.6802, -4.1891, -3.7498], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.6569, -4.4170, -4.1338, -4.1338, -4.3737, -5.2717, -3.6113, -4.8970,\n",
      "        -3.7498, -4.0266], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.6938, -4.3748, -4.3748, -4.3748, -4.6239, -5.2328, -4.2502, -5.0378,\n",
      "        -4.3748, -4.6239], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13588765263557434\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5265, -4.6933, -4.2075, -3.7944], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.7944, -4.7726, -5.1378, -5.1444, -4.0576, -1.6767, -3.6657, -4.9462,\n",
      "        -4.6023, -5.0555], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4150, -4.7814, -5.1092, -5.2832, -4.6518, -1.8106, -4.2991, -5.1092,\n",
      "        -4.6351, -5.0833], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12059788405895233\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-4.7655, -4.2645, -3.8380, -4.8963, -4.9512, -4.9512, -3.8380, -3.8380,\n",
      "        -4.9141, -3.8380], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6716, -4.4542, -4.4542, -4.8760, -4.9277, -4.9277, -4.4542, -4.4542,\n",
      "        -5.0410, -4.4542], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15812498331069946\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.7627, -4.9686, -4.4505, -4.1240], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.1240, -4.7627, -3.8864, -4.4180, -4.3435, -5.3205, -4.1240, -4.5184,\n",
      "        -4.4505, -4.7285], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7116, -4.7116, -4.4978, -2.7489, -4.7131, -5.3713, -4.7116, -4.4978,\n",
      "        -4.4978, -4.9050], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.4025706648826599\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.7798, -4.9778, -4.4655, -4.1462], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.3889, -4.1462, -1.6737, -4.8611, -5.2010, -4.5420, -3.7980, -3.7980,\n",
      "        -4.7798, -4.7798], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.8002, -4.7316, -1.8294, -4.9475, -5.2430, -4.5228, -4.4182, -4.4182,\n",
      "        -4.7316, -4.7316], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.36743563413619995\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.7890, -4.9839, -4.4777, -4.1550], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.8105, -4.1550, -5.0432, -3.9208, -4.7297, -4.1550, -4.2974, -4.5566,\n",
      "        -4.1359, -4.8785], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4294, -4.7395, -5.1232, -4.5287, -4.9475, -4.7395, -4.4294, -4.5287,\n",
      "        -4.4294, -4.9716], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16028359532356262\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.8099, -4.9996, -4.5054, -4.1736], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.1736, -4.1736, -4.1736, -4.1736, -3.9372, -3.9372, -4.7749, -4.8400,\n",
      "        -4.5939, -4.5646], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7563, -4.7563, -4.7563, -4.7563, -4.5435, -4.5435, -4.9983, -4.9983,\n",
      "        -4.7563, -4.5435], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2194812297821045\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.8428, -5.0284, -4.5400, -4.1996], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.3718, -3.9697, -1.7232, -4.5001, -4.1996, -3.8197, -4.9851, -5.3880,\n",
      "        -4.4635, -4.9477], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4377, -4.5728, -1.8452, -4.7515, -4.7796, -4.4377, -4.7515, -5.4303,\n",
      "        -4.5728, -5.0171], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12376173585653305\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.8782, -5.0612, -4.5814, -4.2277], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.7417, -5.2163, -4.2277, -5.3078, -5.4856, -4.6007, -4.0025, -4.0025,\n",
      "        -4.2277, -4.0025], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7118, -5.2412, -4.8050, -5.3305, -5.4477, -4.6023, -4.6023, -4.6023,\n",
      "        -4.8050, -4.6023], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17489294707775116\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.9193, -5.0985, -4.6271, -4.2653], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.4505, -5.4505, -4.7229, -5.4520, -3.8435, -5.3393, -4.2653, -4.7229,\n",
      "        -5.3745, -4.9193], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4721, -5.4721, -4.6831, -5.7401, -4.4592, -5.3570, -4.8387, -4.6831,\n",
      "        -5.2610, -4.8387], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08146838843822479\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.9648, -5.1361, -4.6729, -4.3034], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.5279, -4.0867, -4.0867, -3.8606, -5.4901, -5.1079, -4.0867, -5.0916,\n",
      "        -4.3034, -5.4901], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4746, -4.6781, -4.6781, -4.4746, -5.5029, -4.6587, -4.6781, -5.0875,\n",
      "        -4.8731, -5.5029], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.19553899765014648\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-3.8904, -5.5384, -5.0088, -5.0997, -4.5632, -3.8904, -4.0658, -3.8904,\n",
      "        -4.3496, -4.9948], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5014, -5.5408, -4.9147, -5.4027, -4.5014, -4.5014, -3.2247, -4.5014,\n",
      "        -4.9147, -5.1188], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.22664251923561096\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.0465, -5.2195, -4.7231, -4.3877], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.3877, -4.3877, -4.3877, -3.2543, -4.6963, -4.3877, -5.0763, -5.5728,\n",
      "        -5.5728, -5.4060], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9489, -4.9489, -4.9489, -3.2889, -4.6322, -4.9489, -5.3536, -5.5686,\n",
      "        -5.5686, -5.3060], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13521644473075867\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.0896, -5.2711, -4.7561, -4.4398], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.7357, -3.9536, -3.9536, -4.2290, -4.7357, -4.2290, -4.4398, -4.5277,\n",
      "        -4.2290, -3.3095], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8061, -4.5582, -4.5582, -4.8061, -4.8061, -4.8061, -4.9958, -4.9107,\n",
      "        -4.8061, -3.3519], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.21978798508644104\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.1413, -5.3256, -4.7945, -4.4974], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-1.0049, -5.5147, -5.3387, -4.0016, -4.6644, -5.1413, -4.6886, -4.4974,\n",
      "        -4.4974, -4.5320], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([10.0000, -5.3988, -5.3988, -4.6015, -4.8542, -5.0477, -4.6098, -5.0477,\n",
      "        -5.0477, -4.6015], grad_fn=<AddBackward0>)\n",
      "LOSS: 12.214588165283203\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.1416, -5.3251, -4.7931, -4.4856], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.9443, -4.6528, -5.1421, -4.6528, -4.6360, -4.4856, -5.4881, -4.3236,\n",
      "        -4.6360, -5.4146], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([10.0000, -4.5862, -5.1655, -4.5862, -4.5426, -5.0370, -5.6309, -4.5862,\n",
      "        -4.5426, -5.3925], grad_fn=<AddBackward0>)\n",
      "LOSS: 12.01988697052002\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.1163, -5.2845, -4.7611, -4.4214], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.7602, -4.9903, -5.4185, -5.6024, -4.7496, -4.7442, -5.4296, -5.6024,\n",
      "        -4.1670, -5.6024], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2698, -4.8093, -5.2698, -5.5473, -5.0846, -4.9792, -5.2842, -5.5473,\n",
      "        -4.7503, -5.5473], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08524896204471588\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.0732, -5.2284, -4.7222, -4.3648], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.1025, -3.3300, -4.1025, -4.9493, -4.3648, -5.2429, -5.5315, -4.1679,\n",
      "        -4.7089, -4.4640], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6922, -3.3265, -4.6922, -4.3583, -4.9283, -5.2155, -5.4836, -4.4716,\n",
      "        -4.6922, -4.3583], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14692017436027527\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.0343, -5.1736, -4.6653, -4.3286], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.0617, -3.8066, -4.9317, -4.0617, -4.9380, -3.8066, -4.6653, -3.4419,\n",
      "        -1.5507, -4.0617], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6555, -4.4259, -5.1739, -4.6555, -4.9581, -4.4259, -4.6555, -3.3012,\n",
      "        -1.6302, -4.6555], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.19103392958641052\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.0018, -5.1284, -4.6185, -4.3056], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.5978, -4.6206, -4.3704, -5.0018, -4.8610, -4.5717, -4.6206, -4.3056,\n",
      "        -5.0830, -5.2112], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9168, -4.6367, -4.6999, -4.8750, -4.8750, -4.8750, -4.6367, -4.8750,\n",
      "        -5.1086, -5.4154], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06857429444789886\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.9684, -5.0811, -4.5695, -4.2762], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.5949, -4.3042, -5.1554, -4.2762, -3.7492, -4.2862, -5.0213, -4.0255,\n",
      "        -4.0255, -5.1554], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6229, -4.3743, -5.1206, -4.8486, -4.3743, -4.2420, -5.0722, -4.6229,\n",
      "        -4.6229, -5.1206], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14448878169059753\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.9452, -5.0425, -4.5326, -4.2583], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.4027, -4.2583, -4.2583, -5.0425, -5.3133, -4.2583, -5.3133, -4.8659,\n",
      "        -3.7356, -3.7356], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3621, -4.8324, -4.8324, -5.0908, -5.3749, -4.8324, -5.3749, -4.8380,\n",
      "        -4.3621, -4.3621], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17862346768379211\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.9384, -5.0250, -4.5108, -4.2581], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.4075, -4.2362, -4.9308, -4.2581, -4.9543, -4.2581, -5.0250, -4.9384,\n",
      "        -4.8181, -4.6075], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4986, -4.6194, -5.0281, -4.8323, -5.0281, -4.8323, -5.0879, -4.8323,\n",
      "        -4.8126, -4.2063], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10055781900882721\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-5.2696, -4.5680, -4.0148, -4.0148, -4.0148, -4.5680, -4.5410, -4.7273,\n",
      "        -4.8915, -4.5680], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3639, -4.6133, -4.6133, -4.6133, -4.6133, -4.6133, -4.8431, -4.6415,\n",
      "        -5.0033, -4.6133], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12008919566869736\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-5.1825, -5.0683, -4.7590, -4.5569], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.5789, -4.0142, -4.0470, -4.8657, -4.1991, -5.2656, -4.2718, -5.2656,\n",
      "        -4.0142, -4.9300], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8031, -4.6127, -4.3731, -4.9835, -4.2086, -5.3646, -4.8446, -5.3646,\n",
      "        -4.6127, -4.8446], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12421778589487076\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.9289, -5.0327, -4.4402, -4.2798], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.0214, -4.2087, -4.2798, -4.3016, -4.2087, -4.5972, -4.5754, -4.8579,\n",
      "        -4.5681, -4.2798], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6192, -4.2138, -4.8518, -4.3735, -4.2138, -4.7971, -4.8518, -4.9683,\n",
      "        -4.6192, -4.8518], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1148233413696289\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.9284, -5.0474, -4.4252, -4.2847], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.0347, -4.0347, -4.0347, -4.6095, -3.5803, -4.2248, -4.2847, -5.0474,\n",
      "        -4.2847, -4.2847], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6312, -4.6312, -4.6312, -4.8562, -3.2801, -4.2223, -4.8562, -5.1485,\n",
      "        -4.8562, -4.8562], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.22087781131267548\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-3.5868, -4.3211, -4.5762, -4.8415, -3.1698, -4.2990, -4.0608, -4.1083,\n",
      "        -3.7621, -4.2990], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2998, -4.9559, -4.6547, -4.9559, -3.2998, -4.8691, -4.6547, -4.3859,\n",
      "        -4.3859, -4.8691], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.199030801653862\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-4.9749, -5.1041, -4.4314, -4.3125], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.5982, -4.2679, -4.0792, -4.8775, -5.3672, -4.1825, -4.3164, -1.2768,\n",
      "        -4.3125, -4.0792], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3033, -4.2372, -4.6713, -4.8812, -5.3699, -4.3859, -4.3859, -1.3756,\n",
      "        -4.8812, -4.6713], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11684270203113556\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.0044, -5.1379, -4.4553, -4.3330], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.1379, -4.3475, -4.2955, -4.4500, -1.2740, -4.6194, -4.1078, -4.9158,\n",
      "        -3.7628, -5.0482], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2555, -4.3865, -4.2497, -4.7533, -1.3660, -4.6970, -4.6970, -4.8997,\n",
      "        -4.3865, -5.2442], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08988148719072342\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.0444, -5.1824, -4.5070, -4.3518], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.7515, -4.3518, -4.3518, -4.2338, -5.4419, -5.4419, -4.2338, -3.9475,\n",
      "        -4.9583, -4.1287], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9166, -4.9166, -4.9166, -4.3863, -5.4260, -5.4260, -4.3863, -4.2571,\n",
      "        -4.9166, -4.7158], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11546182632446289\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.0849, -5.2272, -4.5772, -4.3690], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.5679, -3.7600, -5.4823, -4.2162, -4.3690, -4.5772, -5.1840, -3.7600,\n",
      "        -4.0062, -4.1485], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3795, -4.3840, -5.4327, -4.7337, -4.9321, -4.7337, -5.2682, -4.3840,\n",
      "        -4.2619, -4.7337], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.18409235775470734\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.1323, -5.2804, -4.6704, -4.3979], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.8884, -4.8884, -5.5230, -4.1495, -4.1495, -5.2804, -4.3979, -4.1495,\n",
      "        -5.0245, -5.0861], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8430, -4.8430, -5.4432, -4.7345, -4.7345, -5.3288, -4.9582, -4.7345,\n",
      "        -5.3822, -4.8430], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15406212210655212\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.1819, -5.3306, -4.7509, -4.4416], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.8489, -4.1687, -4.2746, -4.4416, -4.7175, -4.7509, -4.4416, -4.2322,\n",
      "        -3.7811, -4.4416], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9974, -4.7518, -5.2153, -4.9974, -4.8089, -4.7518, -4.9974, -4.4030,\n",
      "        -4.4030, -4.9974], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2598304748535156\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.2354, -5.3838, -4.8349, -4.4930], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.4853, -4.7356, -4.9452, -4.4930, -4.1957, -4.4930, -4.7561, -4.8349,\n",
      "        -5.5798, -5.5798], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4170, -4.8387, -4.8867, -5.0437, -4.7761, -5.0437, -4.7761, -4.7761,\n",
      "        -5.4850, -5.4850], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09839823096990585\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.2886, -5.4268, -4.9135, -4.5514], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.7296, -4.5514, -4.9560, -5.4268, -4.5514, -4.5514, -5.5976, -4.7296,\n",
      "        -4.2319, -5.1807], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4371, -5.0963, -5.0963, -5.4604, -5.0963, -5.0963, -5.5166, -4.4371,\n",
      "        -4.8087, -4.9423], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1478656679391861\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.3375, -5.4561, -4.9633, -4.6246], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.5996, -3.8758, -4.4696, -2.7871, -4.5821, -4.4696, -5.3418, -5.5996,\n",
      "        -4.6246, -5.5996], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5621, -4.4882, -4.3469, -1.3309, -4.4882, -4.3469, -4.9847, -5.5621,\n",
      "        -5.1621, -5.5621], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2955109179019928\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.3663, -5.4452, -4.9903, -4.6794], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.8467, -3.9187, -5.5580, -5.6309, -5.3663, -4.2698, -3.9187, -4.2698,\n",
      "        -4.2710, -4.2698], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8428, -4.5268, -5.5907, -5.5907, -5.2115, -4.8428, -4.5268, -4.8428,\n",
      "        -4.3642, -4.8428], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17600591480731964\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-4.2956, -5.5251, -4.7389, -4.6337, -4.4114, -5.0836, -5.3892, -4.7290,\n",
      "        -5.1065, -2.7498], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8661, -5.6270, -5.2650, -4.5735, -4.3871, -5.1019, -5.2650, -4.5735,\n",
      "        -5.2650, -1.3335], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2687694728374481\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-5.7566, -5.5537, -5.4253, -5.1853], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.7653, -4.3023, -5.1853, -5.0479, -4.7653, -4.7150, -4.7653, -4.0056,\n",
      "        -4.6479, -4.3023], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2888, -4.8721, -5.2888, -5.1273, -5.2888, -4.6050, -5.2888, -4.6050,\n",
      "        -4.6050, -4.8721], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.18616357445716858\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-5.2900, -4.6199, -4.6199, -4.9106, -4.8042, -4.9106, -4.3231, -5.4102,\n",
      "        -4.3231, -5.4621], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1397, -4.8908, -4.8908, -4.8908, -5.3238, -4.8908, -4.8908, -5.7118,\n",
      "        -4.8908, -5.6582], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1214052066206932\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0735, -5.0154, -4.6835, -4.3306], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.4594, -4.3306, -5.4304, -5.4409, -4.6835, -4.8574, -4.6754, -5.0436,\n",
      "        -4.8574, -4.8382], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6733, -4.8975, -5.3716, -5.7363, -4.6885, -5.3716, -4.8975, -4.8975,\n",
      "        -5.3716, -5.0333], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10956175625324249\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0642, -5.0283, -4.6718, -4.3348], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.0642, -4.9911, -4.5364, -3.8048, -5.3627, -4.3348, -5.2136, -5.4160,\n",
      "        -5.5732, -4.9152], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4237, -5.2049, -5.0482, -3.2838, -5.7412, -4.9013, -5.2594, -5.2594,\n",
      "        -5.5949, -5.4237], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1458117812871933\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-4.5455, -5.0649, -4.9419, -4.3421, -4.1610, -4.6736, -4.3421, -5.5195,\n",
      "        -4.3421, -4.9419], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0351, -5.3130, -5.4477, -4.9079, -4.7449, -4.7449, -4.9079, -5.6800,\n",
      "        -4.9079, -5.4477], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2145083248615265\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.1618, -5.1345, -4.7006, -4.3706], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.0635, -4.9765, -5.0635, -5.3058, -5.1345, -5.6384, -4.9765, -4.3706,\n",
      "        -5.1269, -4.7952], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9335, -5.4788, -4.9335, -5.6802, -5.3587, -5.5997, -5.4788, -4.9335,\n",
      "        -4.9335, -4.4022], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12391921132802963\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.2410, -5.2248, -4.7223, -4.4266], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.6205, -5.0110, -5.2410, -5.1069, -5.0110, -5.7200, -5.4697, -5.0110,\n",
      "        -4.4266, -5.2209], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6722, -5.5099, -5.5099, -4.9839, -5.5099, -5.6913, -5.5272, -5.5099,\n",
      "        -4.9839, -5.2730], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11542898416519165\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3384, -5.3281, -4.7614, -4.4947], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.1512, -4.4947, -5.7921, -1.3475, -5.2718, -4.4947, -5.0391, -4.7205,\n",
      "        -5.0391, -4.9277], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0452, -5.0452, -5.6894, -1.3608, -4.9794, -5.0452, -5.5352, -4.8084,\n",
      "        -5.5352, -5.0452], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12273355573415756\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-5.1735, -5.8088, -4.5729, -5.9525, -5.1396, -5.7730, -5.4733, -4.5729,\n",
      "        -1.3521, -5.2943], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1156, -5.6786, -5.1156, -5.9800, -5.1156, -5.5722, -5.6786, -5.1156,\n",
      "        -1.3674, -5.3196], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06940373033285141\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.5159, -5.4961, -4.8419, -4.6569], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.5890, -4.6569, -4.5890, -5.3452, -4.6569, -5.1234, -5.8912, -5.8190,\n",
      "        -4.6569, -4.8090], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4734, -5.1912, -4.4734, -5.3281, -5.1912, -5.6110, -5.7284, -5.6110,\n",
      "        -5.1912, -4.4734], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13036377727985382\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.5899, -5.5597, -4.8648, -4.7583], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.2919, -4.9052, -4.8648, -5.1760, -5.1189, -4.7583, -4.7583, -4.7583,\n",
      "        -5.8471, -5.7865], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8628, -5.0322, -4.8628, -5.6070, -5.2825, -5.2825, -5.2825, -5.2825,\n",
      "        -5.6365, -5.7409], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14251771569252014\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.6619, -5.6115, -4.8864, -4.8662], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.2546, -5.2546, -5.2546, -5.2062, -5.1090, -5.2546, -4.8662, -5.6115,\n",
      "        -5.7358, -4.8662], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5981, -5.5981, -5.5981, -5.3796, -5.3796, -5.5981, -5.3796, -5.6581,\n",
      "        -5.6157, -5.3796], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11187447607517242\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.7275, -5.6484, -4.8601, -4.9813], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.2319, -5.0658, -5.3842, -4.5796, -4.8601, -4.6360, -5.3842, -5.3842,\n",
      "        -4.7212, -5.3842], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3741, -5.2491, -5.5728, -4.9645, -4.9645, -4.6408, -5.5728, -5.5728,\n",
      "        -4.6408, -5.5728], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03615473955869675\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.2665, -5.3131, -4.5249, -4.4751], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.2665, -4.4751, -5.7449, -5.7954, -5.2665, -4.6434, -5.4598, -5.7449,\n",
      "        -5.5248, -4.9969], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3367, -5.0276, -5.4972, -5.5390, -5.3367, -5.0276, -5.4593, -5.4972,\n",
      "        -5.5390, -5.1848], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06867276877164841\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-4.5318, -5.1907, -4.5264, -6.0205, -5.7348, -4.4923, -5.6652, -5.6652,\n",
      "        -5.6195, -5.6195], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0431, -5.3147, -5.0737, -5.7348, -5.4692, -5.0431, -5.5388, -5.5388,\n",
      "        -5.5388, -5.5388], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10768647491931915\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.4722, -5.2739, -4.5296, -4.8026], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.2749, -4.5296, -5.2749, -4.1013, -4.6374, -6.0544, -4.8026, -4.5922,\n",
      "        -5.2749, -5.5814], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3154, -5.0767, -5.3154, -4.6911, -4.6911, -5.5419, -5.0367, -5.0367,\n",
      "        -5.3154, -5.3154], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12407892942428589\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.5216, -5.2594, -4.5479, -4.8957], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.9611, -6.0227, -1.3504, -5.8875, -5.9279, -5.7128, -4.8078, -5.3705,\n",
      "        -4.6587, -5.8875], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1211, -5.5477, -1.4569, -5.5704, -5.7397, -5.4650, -5.0300, -5.3270,\n",
      "        -5.0300, -5.5704], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07495570927858353\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.5477, -5.2270, -4.6167, -4.9700], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.5486, -4.8807, -5.2118, -5.6226, -5.9276, -5.9276, -5.6226, -4.7102,\n",
      "        -5.6226, -5.4506], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5880, -5.0624, -5.1531, -5.5306, -5.6107, -5.6107, -5.5306, -5.0624,\n",
      "        -5.5306, -5.3926], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03917495533823967\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-5.6803, -5.8773, -4.9819, -5.9407, -5.9407, -5.1316, -4.7504, -6.3916,\n",
      "        -5.9407, -4.5712], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2368, -5.8886, -5.1141, -5.6716, -5.6716, -5.2084, -5.1141, -5.8921,\n",
      "        -5.6716, -5.1141], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11139073222875595\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-5.9101, -4.7681, -5.1585, -5.9101, -5.5401, -5.9101, -5.9101, -5.9101,\n",
      "        -5.5125, -5.3008], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.7768, -5.2084, -5.8241, -5.7768, -5.7813, -5.7768, -5.7768, -5.7768,\n",
      "        -5.7585, -5.6233], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09482957422733307\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.6258, -5.1789, -5.0017, -5.1351], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.9426, -5.8641, -5.7900, -5.4058, -5.9426, -6.0574, -5.5578, -5.4922,\n",
      "        -4.8013, -5.8465], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.8482, -5.8482, -6.0892, -5.5016, -5.8482, -5.9668, -5.8652, -5.8687,\n",
      "        -5.2909, -5.6831], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06275570392608643\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.6585, -5.2094, -5.1034, -5.1940], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.8359, -6.0555, -5.8359, -5.8359, -5.4848, -5.8359, -5.5450, -5.4359,\n",
      "        -4.8470, -5.7804], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.8923, -5.8923, -5.8923, -5.8923, -5.9360, -5.8923, -6.2024, -5.8084,\n",
      "        -5.3480, -5.7461], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10661053657531738\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-5.5329, -4.8870, -5.6462, -4.9939, -5.8459, -4.9103, -5.3117, -5.5329,\n",
      "        -4.9103, -5.4310], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.9371, -5.0475, -5.8109, -5.3983, -5.8879, -5.3496, -5.8109, -5.9371,\n",
      "        -5.3496, -5.8109], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13244052231311798\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3637, -5.2009, -4.7755, -5.0149], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.0149, -5.9000, -5.9000, -5.6338, -5.2009, -5.6762, -5.6762, -4.4056,\n",
      "        -5.0682, -5.3637], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2980, -5.8113, -5.8113, -5.8485, -5.5611, -5.7068, -5.7068, -4.9650,\n",
      "        -5.3067, -5.7068], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07612435519695282\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.4543, -5.3565, -4.7075, -5.1436], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.3891, -5.6998, -4.6539, -5.4543, -5.6998, -5.9493, -4.2901, -2.8110,\n",
      "        -5.6918, -5.1436], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.7270, -5.5681, -4.8610, -5.5681, -5.5681, -5.7000, -4.8610, -1.5463,\n",
      "        -5.7270, -5.2367], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2202220857143402\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-5.5402, -6.0120, -5.1166, -5.9570, -5.6853, -5.7603, -5.2676, -5.2581,\n",
      "        -5.3581, -5.7886], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4490, -5.4551, -5.4490, -5.6049, -5.4490, -5.4490, -5.6183, -5.0956,\n",
      "        -5.4551, -5.5523], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09202947467565536\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-5.3864, -6.1037, -5.1074, -4.6786, -5.6334, -5.6334, -6.2063, -2.9470,\n",
      "        -5.3297, -5.6334], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4181, -5.5966, -5.4113, -5.2107, -5.4113, -5.4113, -5.5966, -3.4413,\n",
      "        -5.5235, -5.4113], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14351773262023926\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.7520, -5.5173, -4.9442, -5.4438], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.8421, -5.6877, -5.3094, -5.5322, -6.1435, -6.1037, -5.3948, -5.3920,\n",
      "        -5.8944, -5.8944], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7867, -5.5638, -5.2947, -5.4369, -5.8573, -5.6824, -5.5638, -5.2947,\n",
      "        -5.6824, -5.6824], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04150523617863655\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.6761, -5.5548, -5.0257, -5.3385], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.8387, -6.0472, -5.6283, -5.4147, -6.0428, -5.1593, -6.2746, -1.5419,\n",
      "        -5.8195, -5.4147], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.9649, -5.6728, -5.5041, -5.5041, -5.8001, -5.4937, -5.9649, -3.2993,\n",
      "        -5.8001, -5.5041], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.35429346561431885\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.5986, -5.5638, -5.1172, -5.2115], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.2604, -5.9332, -5.9076, -5.7327, -6.3657, -5.7327, -5.6206, -5.4889,\n",
      "        -5.7327, -5.2905], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5463, -5.7432, -5.9330, -5.9330, -6.0617, -5.9330, -5.5866, -5.3125,\n",
      "        -5.9330, -5.5866], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0451195165514946\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-5.2020, -6.3803, -4.9114, -5.6754, -5.6754, -5.1344, -4.9114, -5.1617,\n",
      "        -5.5449, -5.2020], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6383, -6.1401, -4.9208, -6.0261, -6.0261, -5.6455, -4.9208, -5.6209,\n",
      "        -5.3817, -5.6383], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11833502352237701\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-5.6606, -2.2410, -5.6606, -5.6606, -5.3889, -6.0830, -5.8738, -5.6606,\n",
      "        -5.4747, -5.3272], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.0682, -1.5908, -6.0682, -6.0682, -5.6131, -5.8352, -5.7945, -6.0682,\n",
      "        -5.4138, -5.6496], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13131287693977356\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.2738, -5.4432, -4.8950, -5.0697], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.5018, -5.2901, -5.6147, -5.8049, -5.6942, -5.6942, -2.8650, -5.4432,\n",
      "        -5.1300, -5.1456], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.0739, -5.6847, -6.1038, -6.0739, -6.0739, -6.0739, -2.9503, -5.4055,\n",
      "        -5.6847, -5.6170], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16216932237148285\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-5.0485, -5.2066, -5.2066, -5.7493, -5.5765, -5.6926, -4.8031, -6.0092,\n",
      "        -6.0388, -4.8322], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5437, -5.5390, -5.5390, -5.9756, -5.5390, -5.7486, -4.9291, -5.7453,\n",
      "        -5.7486, -4.9291], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07009652256965637\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3017, -5.3737, -4.7165, -5.1727], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.5600, -5.8627, -4.7879, -5.5526, -5.0079, -5.8218, -5.5600, -5.2879,\n",
      "        -5.8218, -5.2879], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6625, -6.0346, -4.8502, -5.8798, -5.5071, -5.8798, -5.6811, -5.4635,\n",
      "        -5.8798, -5.4635], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04832485690712929\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7642, -4.7516, -4.1705, -4.4772], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.8318, -5.9671, -5.2864, -6.1434, -5.3987, -5.9317, -5.9671, -2.0660,\n",
      "        -4.7516, -4.7642], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5678, -5.7516, -5.1442, -5.9152, -5.3745, -5.9152, -5.7516, -1.6785,\n",
      "        -4.7534, -5.1442], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.053022246807813644\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7745, -4.7134, -4.1000, -4.5370], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.5144, -5.4921, -5.6186, -5.7857, -5.6364, -5.6186, -5.7857, -5.9561,\n",
      "        -4.9921, -5.8791], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6900, -5.3117, -5.3117, -5.4928, -5.6513, -5.3117, -5.4928, -5.6513,\n",
      "        -5.0630, -5.4724], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06868292391300201\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7524, -4.6396, -4.0836, -4.5734], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.5733, -2.8328, -5.5733, -6.0068, -5.9840, -4.4949, -1.4837, -4.4949,\n",
      "        -4.0836, -5.6980], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3185, -4.6752, -5.3185, -5.8048, -5.6167, -4.6752, -1.7203, -4.6752,\n",
      "        -4.6752, -5.4816], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.42178091406822205\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6790, -4.5358, -4.0213, -4.5378], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.6093, -5.3202, -4.7931, -5.6305, -5.9768, -5.6093, -5.9100, -5.8459,\n",
      "        -5.8459, -5.3147], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3138, -5.4543, -5.2282, -5.5793, -5.5793, -5.3138, -5.7832, -5.5793,\n",
      "        -5.5793, -5.4120], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07102279365062714\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5892, -4.4396, -4.0099, -4.4852], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.5588, -5.5317, -5.6171, -4.7991, -5.5317, -5.5274, -5.9286, -5.9286,\n",
      "        -5.4123, -5.6171], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5067, -5.2279, -5.3608, -5.3192, -5.2279, -5.8221, -5.5873, -5.5873,\n",
      "        -5.3608, -5.3608], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09115475416183472\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5158, -4.3531, -4.0369, -4.4044], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.8325, -4.9935, -5.7354, -5.5766, -5.2020, -5.3317, -5.8325, -5.5766,\n",
      "        -4.4044, -5.4962], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6325, -5.3880, -5.5662, -5.4429, -5.5845, -5.3880, -5.6325, -5.4429,\n",
      "        -2.7537, -5.5845], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3181977868080139\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.4150, -4.2392, -4.0260, -4.2480], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.5120, -5.0169, -5.4257, -5.7019, -5.4928, -5.6076, -5.4432, -5.7019,\n",
      "        -5.7019, -5.2598], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5678, -5.2941, -5.6432, -5.6722, -5.5152, -5.9608, -5.5152, -5.6722,\n",
      "        -5.6722, -5.5152], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03255458548665047\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3352, -4.1572, -3.9972, -4.1113], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.9972, -4.9810, -5.5993, -5.7561, -5.9022, -5.2121, -5.5879, -5.4206,\n",
      "        -5.5993, -5.8035], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5974, -5.2898, -5.6968, -6.0401, -5.6968, -5.5610, -5.6161, -5.5610,\n",
      "        -5.6968, -5.8114], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07398547232151031\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2778, -4.0950, -3.9492, -4.0003], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.0950, -5.3615, -5.1098, -5.3615, -5.3615, -5.5278, -5.1433, -5.5278,\n",
      "        -5.5278, -5.5278], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5543, -5.5706, -5.2960, -5.5706, -5.5706, -5.6984, -5.2960, -5.6984,\n",
      "        -5.6984, -5.6984], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05166052654385567\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2278, -4.0499, -3.8648, -3.9286], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.3634, -5.5075, -5.3488, -5.5075, -4.2278, -5.3291, -5.0029, -5.3634,\n",
      "        -5.5075, -4.0499], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6328, -5.6546, -5.5332, -5.6546, -5.2899, -5.6328, -5.2899, -5.6328,\n",
      "        -5.6546, -4.4783], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17300395667552948\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-2.7597, -5.0921, -5.7334, -5.3551, -5.3947, -5.0323, -5.5084, -5.0921,\n",
      "        -5.3947, -5.7110], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.7979, -5.1335, -5.8906, -5.4395, -5.5189, -5.1809, -5.5524, -5.1335,\n",
      "        -5.5189, -5.7074], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.009156020358204842\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.7995, -2.9740, -2.5810, -2.0242], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.6585, -5.0737, -5.6303, -5.0737, -4.0972, -5.5175, -5.3694, -5.6642,\n",
      "        -5.5175, -4.2550], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.2927, -5.0669, -5.4431, -5.0669, -4.2927, -5.4431, -5.3382, -5.7702,\n",
      "        -5.4431, -5.0669], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11579886823892593\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.6130, -1.7565, -1.5125, -0.8318], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.8032, -4.8176, -5.6781, -1.7565, -5.5261, -5.6136, -5.8032, -5.6917,\n",
      "        -5.7506, -4.3465], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6474, -4.9363, -5.3346, -2.8433, -5.3346, -5.3750, -5.6474, -5.5374,\n",
      "        -5.5374, -4.9118], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.18442612886428833\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.6943, -5.7318, -4.7352, -5.4920], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.4920, -4.7352, -5.5607, -5.3526, -5.3526, -5.4345, -4.5005, -5.4920,\n",
      "        -4.2025, -4.8661], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2617, -5.1676, -5.2330, -5.1676, -5.1676, -4.8608, -5.0504, -5.2617,\n",
      "        -4.1765, -4.8493], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11013554036617279\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.7257, -5.6457, -4.6040, -5.2936], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.5407, -5.5407, -4.4674, -5.2936, -5.7540, -5.2936, -5.4426, -5.4426,\n",
      "        -5.4426, -5.4426], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1795, -5.1795, -5.0206, -5.1436, -5.4543, -5.1436, -5.2462, -5.2462,\n",
      "        -5.2462, -5.2462], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0856173187494278\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-4.6301, -5.2359, -5.0961, -5.5069, -4.9690, -4.2810, -5.7167, -5.7167,\n",
      "        -5.3512, -4.6301], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8529, -5.3431, -5.3431, -5.2015, -5.2015, -4.8529, -5.4994, -5.4994,\n",
      "        -5.2676, -4.8529], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07475898414850235\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3213, -5.2234, -4.3222, -4.8703], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.6108, -3.6108, -1.5733, -4.4179, -1.5733, -4.5316, -5.4448, -5.9497,\n",
      "        -5.1504, -5.1504], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.2497, -4.2497, -1.7484, -4.8561, -1.7484, -5.0785, -5.2382, -5.5411,\n",
      "        -5.2188, -5.2188], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15879395604133606\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-4.7528, -5.3427, -5.3427, -4.4709, -5.1008, -5.1008, -5.1726, -4.7510,\n",
      "        -4.7510, -4.9369], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0541, -5.2759, -5.2759, -4.8847, -5.2759, -5.2759, -5.3344, -4.9326,\n",
      "        -4.9326, -4.8847], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04270442947745323\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3573, -5.0884, -4.3800, -4.7664], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.4823, -4.7986, -5.2839, -1.8834, -5.0884, -5.3573, -5.1182, -5.5090,\n",
      "        -5.2991, -4.7542], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4024, -4.9420, -5.4024, -1.7450, -5.3188, -5.3188, -5.3485, -5.3485,\n",
      "        -5.2987, -4.9420], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.022880394011735916\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3591, -5.0129, -4.3701, -4.7239], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.0868, -5.0918, -4.7834, -5.9473, -5.0918, -5.0868, -5.9473, -3.7712,\n",
      "        -5.4183, -5.0868], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3558, -5.3498, -4.8842, -5.7431, -5.3498, -5.3558, -5.7431, -4.3726,\n",
      "        -5.7368, -5.3558], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09069975465536118\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3450, -4.9553, -4.3461, -4.7002], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.1716, -4.1716, -5.1895, -5.3853, -5.7236, -4.7002, -4.3461, -4.1716,\n",
      "        -5.1170, -5.0952], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3887, -4.3887, -5.2994, -5.7374, -5.7379, -4.9115, -4.9115, -4.3887,\n",
      "        -5.3654, -5.3595], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07735107839107513\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3385, -4.9400, -4.3044, -4.6888], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.8426, -6.2615, -5.1873, -5.1232, -5.1232, -5.1232, -5.3385, -5.4994,\n",
      "        -5.1232, -5.1489], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8740, -5.7301, -5.2739, -5.3505, -5.3505, -5.3505, -5.3583, -5.3505,\n",
      "        -5.3505, -5.3583], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05639712139964104\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3098, -4.9262, -4.2543, -4.6921], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.2543, -3.7959, -4.2543, -5.1854, -5.1786, -4.6921, -5.1306, -5.2568,\n",
      "        -5.1960, -4.6921], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8288, -4.4163, -4.8288, -5.2382, -5.3366, -4.8288, -5.3398, -5.2382,\n",
      "        -5.3398, -4.8288], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11751075834035873\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.2928, -4.9217, -4.2017, -4.7173], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.2532, -5.4536, -5.2481, -6.2605, -5.1882, -4.6978, -5.1881, -3.7812,\n",
      "        -5.5306, -5.2928], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3081, -5.3487, -5.3132, -5.6694, -5.1954, -4.7291, -5.1954, -4.4031,\n",
      "        -5.3132, -5.3081], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08030177652835846\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.2513, -4.9143, -4.1718, -4.7358], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.2157, -5.5695, -5.3122, -4.7738, -5.3002, -5.3122, -5.6165, -5.1704,\n",
      "        -5.1861, -5.5362], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2964, -5.6518, -5.3133, -4.7547, -5.2964, -5.3133, -5.6517, -5.0442,\n",
      "        -5.1734, -5.3133], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.008066978305578232\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.2139, -4.9077, -4.1481, -4.7499], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.3728, -5.0329, -5.7892, -5.1841, -5.3728, -5.1841, -5.3728, -5.5511,\n",
      "        -5.3728, -5.5272], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3208, -5.3098, -5.5296, -5.1568, -5.3208, -5.1568, -5.3208, -5.6390,\n",
      "        -5.3208, -5.3208], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.020669467747211456\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-5.3608, -4.3263, -5.5183, -5.5421, -5.4203, -4.7540, -4.7540, -5.1618,\n",
      "        -5.7590, -5.2516], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2938, -4.4259, -5.3459, -5.5286, -5.3459, -4.7276, -4.7276, -5.1578,\n",
      "        -5.6428, -5.2938], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.006655402481555939\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.1226, -4.9001, -4.1436, -4.7502], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.5185, -4.7502, -5.3731, -4.8461, -4.8665, -5.1226, -5.1870, -4.9001,\n",
      "        -4.7502, -3.9357], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0667, -4.7293, -5.3075, -4.7293, -5.3075, -5.3075, -5.1670, -5.0667,\n",
      "        -4.7293, -2.7047], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.20916616916656494\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0938, -4.9092, -4.1507, -4.7413], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.4536, -5.4519, -5.5419, -4.3439, -4.7413, -4.1507, -4.7413, -5.4892,\n",
      "        -3.8263, -5.1895], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4174, -5.1733, -5.6677, -4.4437, -4.7356, -4.7356, -4.7356, -5.4174,\n",
      "        -4.4437, -5.1733], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08334919065237045\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0703, -4.9176, -4.1656, -4.7335], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.5158, -5.3295, -5.3295, -5.3295, -5.5273, -5.8013, -1.9220, -4.7335,\n",
      "        -5.5158, -5.2027], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4604, -5.3087, -5.3087, -5.3087, -5.3192, -5.4604, -1.7983, -4.7491,\n",
      "        -5.4604, -5.1933], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0182623453438282\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-5.5285, -5.5082, -4.6425, -5.2980, -5.7828, -5.2980, -5.5439, -4.8028,\n",
      "        -5.1629, -0.8846], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5117, -5.3464, -4.6826, -5.3225, -5.5117, -5.3225, -5.7084, -4.7736,\n",
      "        -5.1263, 10.0000], grad_fn=<AddBackward0>)\n",
      "LOSS: 11.86073112487793\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9295, -4.8516, -4.1673, -4.6164], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.4238, -5.4473, -5.4238, -5.4473, -5.1674, -5.4473, -4.5248, -4.8516,\n",
      "        -5.1674, -5.7126], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3327, -5.5172, -5.3327, -5.5172, -5.2878, -5.5172, -5.0723, -5.0723,\n",
      "        -5.2878, -5.7072], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04087735712528229\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8445, -4.8006, -4.1405, -4.5375], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.5375, -3.7745, -5.0554, -5.3867, -5.2080, -4.1405, -4.7198, -4.7198,\n",
      "        -4.8006, -5.0554], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7264, -4.2273, -5.2478, -5.5136, -5.5164, -4.7264, -4.7264, -4.7264,\n",
      "        -5.0387, -5.2478], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08261527866125107\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7804, -4.7714, -4.1094, -4.4897], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.3964, -4.7667, -4.4897, -5.3489, -4.9760, -5.0538, -4.6260, -5.3769,\n",
      "        -4.9076, -4.4897], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4946, -4.9891, -4.6985, -5.4946, -5.1979, -5.1634, -4.6029, -5.6104,\n",
      "        -5.2901, -4.6985], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.043003350496292114\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7310, -4.7494, -4.0517, -4.4754], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.3414, -4.9375, -5.0187, -5.0187, -4.6135, -4.7210, -5.3414, -4.9733,\n",
      "        -4.4754, -3.9350], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4419, -5.1225, -5.1004, -5.1004, -4.8436, -5.1522, -5.4419, -5.4419,\n",
      "        -4.6465, -4.0992], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.058246683329343796\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6900, -4.7353, -3.9771, -4.4846], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.9771, -5.4609, -5.3500, -5.2045, -5.2530, -4.3193, -5.3500, -5.3500,\n",
      "        -3.5740, -4.8457], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5794, -5.3612, -5.3612, -5.1504, -5.3612, -4.8153, -5.3612, -5.3612,\n",
      "        -4.0654, -5.0316], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09097722917795181\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6578, -4.7267, -3.9355, -4.4851], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.9507, -4.9980, -5.3498, -5.3498, -4.4851, -1.4272, -5.1467, -4.4851,\n",
      "        -4.4078, -4.6578], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3161, -4.9953, -5.3161, -5.3161, -4.5419, -1.4075, -4.9953, -4.5419,\n",
      "        -4.4277, -4.9740], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.026598211377859116\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6500, -4.7239, -3.8942, -4.5042], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.1542, -3.8942, -4.5042, -5.4562, -4.8914, -5.3480, -3.8942, -1.6238,\n",
      "        -4.8914, -5.3480], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6925, -4.5047, -4.5047, -5.4100, -4.9050, -5.2551, -4.5047, -1.3814,\n",
      "        -4.9050, -5.2551], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1113806813955307\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6474, -4.7236, -3.8575, -4.5301], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.8456, -5.3689, -4.3691, -5.1925, -3.4146, -4.6474, -4.8834, -5.1108,\n",
      "        -3.4146, -5.2238], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9115, -5.3610, -4.3347, -4.9093, -3.9679, -4.8403, -4.8403, -5.1968,\n",
      "        -3.9679, -5.2648], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07463470101356506\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6621, -4.7238, -3.8508, -4.5478], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.3324, -5.3348, -4.8374, -5.0577, -5.3348, -5.3932, -3.8508, -3.9607,\n",
      "        -4.2265, -4.9635], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1645, -5.1645, -4.9742, -4.9742, -5.1645, -5.3446, -4.4657, -3.9294,\n",
      "        -4.4657, -4.8957], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05551224946975708\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6662, -4.7149, -3.8567, -4.5559], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.4022, -5.3110, -5.3110, -4.8303, -5.1254, -2.5885, -4.2085, -3.3823,\n",
      "        -4.8363, -4.8363], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5571, -5.1484, -5.1484, -4.8960, -4.8879, -2.4006, -4.4710, -3.8924,\n",
      "        -4.7877, -4.7877], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.050672758370637894\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-1.5285, -5.2716, -3.8831, -5.2716, -4.8079, -3.8831, -4.8079, -4.9728,\n",
      "        -5.2716, -5.0415], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.3032, -5.1579, -4.4948, -5.1579, -4.8033, -4.4948, -4.8033, -5.1579,\n",
      "        -5.1579, -5.1579], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08857909590005875\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6683, -4.6857, -3.9142, -4.5340], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.7815, -4.7815, -4.7815, -5.2187, -1.4960, -4.2488, -4.1989, -3.9142,\n",
      "        -5.3038, -4.6857], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8239, -4.8239, -4.8239, -5.1624, -1.2876, -4.5228, -4.7053, -4.5228,\n",
      "        -5.2349, -4.5570], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07751922309398651\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6650, -4.6618, -3.9397, -4.5168], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.9397, -4.7924, -4.6325, -4.7670, -3.4325, -4.7670, -3.4325, -5.0136,\n",
      "        -4.2613, -4.2665], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5457, -5.0072, -4.8527, -4.8527, -3.7623, -4.8527, -3.7623, -4.9772,\n",
      "        -4.3771, -4.6610], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08645801246166229\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6676, -4.6484, -3.9800, -4.4979], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.4979, -4.7467, -4.8114, -4.6676, -4.7467, -4.7843, -4.3238, -4.4979,\n",
      "        -5.1264, -4.9481], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5820, -4.8914, -5.0342, -4.8914, -4.8914, -5.0342, -4.5820, -4.5820,\n",
      "        -5.1954, -5.1954], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03507755696773529\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6814, -4.6375, -3.9943, -4.5062], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.5062, -4.9836, -5.1041, -4.7310, -4.6814, -4.5062, -3.8237, -5.1041,\n",
      "        -4.3402, -4.7575], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5949, -5.0330, -5.1914, -5.0558, -4.9062, -4.5949, -3.6917, -5.1914,\n",
      "        -4.5949, -4.9062], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02938113547861576\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7159, -4.6309, -3.9852, -4.5314], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.5314, -4.9074, -5.0994, -4.3322, -3.8151, -4.7852, -5.0994, -3.9852,\n",
      "        -3.9852, -3.9852], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5867, -5.0536, -5.1662, -4.5867, -3.6819, -4.8990, -5.1662, -4.5867,\n",
      "        -4.5867, -4.5867], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12141413986682892\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7498, -4.6279, -3.9743, -4.5641], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.0156, -4.8803, -1.3898, -5.1781, -4.3297, -4.8276, -3.9743, -4.5641,\n",
      "        -3.8092, -4.2467], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0139, -4.9706, -1.2380, -5.5141, -4.5768, -5.0139, -4.5768, -4.5768,\n",
      "        -3.6809, -4.4327], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06542131304740906\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7783, -4.6411, -3.9502, -4.5921], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.8419, -4.8779, -4.8419, -4.5921, -3.9502, -4.8779, -4.4361, -3.8187,\n",
      "        -2.3968, -4.3211], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9925, -4.8890, -4.9925, -4.5551, -4.5551, -4.8890, -4.4188, -3.6823,\n",
      "        -2.2451, -4.5551], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05096706002950668\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8013, -4.6564, -3.9206, -4.6172], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.8640, -4.4186, -3.8316, -4.9647, -4.8013, -5.1296, -3.4456, -4.9897,\n",
      "        -5.1485, -4.8640], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9696, -4.6291, -3.6874, -4.8982, -4.8823, -5.0794, -3.6874, -5.3864,\n",
      "        -5.3864, -4.9696], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03734022006392479\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-4.2718, -4.9028, -5.1834, -3.8841, -4.9028, -4.6385, -4.8426, -4.9740,\n",
      "        -4.2908, -4.6385], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6263, -4.9383, -5.3519, -4.4957, -4.9383, -4.4957, -4.8617, -4.8617,\n",
      "        -4.4957, -4.4957], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06263262033462524\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3368, -5.0735, -4.2911, -5.0014], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.2791, -2.3562, -4.2911, -5.0014, -4.6580, -5.0014, -5.1498, -5.1498,\n",
      "        -5.1498, -4.7746], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4548, -2.2319, -4.4777, -4.8620, -4.9262, -4.8620, -5.0302, -5.0302,\n",
      "        -5.0302, -4.8620], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.024253977462649345\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9166, -4.7537, -3.8501, -4.6240], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.9785, -4.7794, -4.9908, -5.3690, -5.0105, -4.7794, -4.7167, -4.6240,\n",
      "        -3.8501, -4.6240], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8518, -4.8713, -4.9177, -5.0236, -4.9612, -4.8713, -4.9177, -4.4651,\n",
      "        -4.4651, -4.4651], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06291736662387848\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9426, -4.7762, -3.8500, -4.6011], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.9338, -4.6011, -4.2990, -5.1239, -5.3060, -4.2627, -4.6011, -3.8500,\n",
      "        -5.1239, -5.1239], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6472, -4.4650, -4.4484, -5.0270, -5.3342, -4.5489, -4.4650, -4.4650,\n",
      "        -5.0270, -5.0270], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06305204331874847\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9658, -4.7926, -3.8709, -4.5653], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.3290, -4.3745, -3.8709, -4.5505, -5.0936, -5.0936, -3.8140, -5.3205,\n",
      "        -5.0389, -4.3597], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3620, -4.4326, -4.4838, -4.5066, -5.0485, -5.0485, -3.6255, -5.2810,\n",
      "        -4.9371, -4.4838], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.044900570064783096\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9855, -4.8031, -3.8904, -4.5330], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.9976, -4.9855, -5.0547, -4.5351, -3.8904, -3.8672, -5.0664, -0.2309,\n",
      "        -4.5330, -4.5351], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9606, -4.9606, -4.9576, -4.4629, -4.5014, -4.4704, -5.0706, 10.0000,\n",
      "        -4.5014, -4.4629], grad_fn=<AddBackward0>)\n",
      "LOSS: 10.543055534362793\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9826, -4.7835, -3.9147, -4.4545], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.0372, -5.0372, -5.0372, -5.0690, -5.3322, -4.9406, -4.4545, -4.4545,\n",
      "        -5.3322, -4.9986], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9827, -4.9827, -4.9827, -5.0938, -5.4201, -5.0023, -4.5232, -4.5232,\n",
      "        -5.4201, -5.0938], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00472856592386961\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9807, -4.7665, -3.9289, -4.3927], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.8042, -4.5274, -4.3927, -4.9439, -4.5274, -4.5129, -0.1352, -5.2134,\n",
      "        -5.3274, -4.4421], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0192, -4.5212, -4.5360, -5.1168, -4.5212, -4.3077, 10.0000, -4.9979,\n",
      "        -5.4363, -4.5212], grad_fn=<AddBackward0>)\n",
      "LOSS: 10.29251480102539\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-3.9188, -4.8806, -3.9188, -5.0089, -1.2962, -4.8806, -4.8388, -4.4883,\n",
      "        -4.3203, -5.4597], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5269, -5.1187, -4.5269, -4.9949, -1.0537, -5.1187, -5.0395, -4.5269,\n",
      "        -4.5269, -5.1187], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11127251386642456\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6151, -4.3953, -3.8521, -3.5284], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.8107, -4.9287, -5.1790, -4.8493, -3.9009, -4.4461, -4.4512, -5.2582,\n",
      "        -4.8107, -4.6757], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0339, -4.9799, -5.0155, -5.1087, -4.5108, -5.0528, -4.5095, -5.3709,\n",
      "        -5.0339, -4.1756], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12025884538888931\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9081, -4.6078, -3.8443, -4.2601], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.4255, -4.8675, -4.6078, -4.4406, -5.2162, -4.8911, -4.8268, -3.8443,\n",
      "        -4.2601, -4.4025], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.1670, -5.0750, -4.1670, -4.4599, -5.3445, -4.9531, -4.9966, -4.4599,\n",
      "        -4.4599, -4.4732], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07774941623210907\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8644, -4.5290, -3.7600, -4.2578], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.9169, -4.8430, -4.5202, -4.5202, -1.1828, -3.7600, -4.3515, -4.9169,\n",
      "        -3.3993, -4.8393], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0234, -5.0234, -4.9214, -4.9214, -0.9093, -4.3840, -4.4177, -5.0234,\n",
      "        -3.3594, -4.9044], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08516242355108261\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8297, -4.4597, -3.6578, -4.2849], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.8297, -4.9256, -4.9956, -5.0305, -4.3837, -4.3092, -4.8343, -4.2831,\n",
      "        -4.2831, -4.8335], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8548, -4.8087, -4.9453, -4.8388, -4.8548, -4.3440, -4.8475, -4.2920,\n",
      "        -4.2920, -4.8475], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.027728896588087082\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7996, -4.3895, -3.5720, -4.3086], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.0715, -5.1042, -5.1058, -4.3291, -4.7508, -4.9929, -4.7175, -3.6455,\n",
      "        -4.7996, -5.0715], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8962, -5.1942, -5.0831, -4.7750, -4.7750, -4.7750, -4.7723, -3.3927,\n",
      "        -4.7750, -4.8962], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03845691680908203\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7693, -4.3222, -3.5029, -4.3230], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.1902, -5.6149, -5.0168, -3.5977, -3.5029, -5.0168, -3.5029, -5.0476,\n",
      "        -3.5964, -3.2112], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.2380, -5.0207, -4.7046, -3.4047, -4.1526, -4.7046, -4.1526, -5.1291,\n",
      "        -3.4047, -3.4047], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15125644207000732\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7216, -4.2632, -3.4685, -4.3249], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.1808, -4.2632, -5.1808, -4.0722, -5.4444, -5.0070, -4.3249, -4.3249,\n",
      "        -4.3011, -5.4677], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8710, -4.2448, -4.8710, -4.1216, -4.8710, -4.6650, -4.1216, -4.1216,\n",
      "        -4.6650, -4.8710], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12114045768976212\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6220, -4.2013, -3.4795, -4.2801], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.7361, -5.1896, -3.9222, -5.1225, -4.2801, -4.9397, -5.4113, -4.9498,\n",
      "        -5.1896, -3.2047], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6199, -4.9320, -4.2358, -5.0253, -4.1316, -4.9320, -4.9320, -5.1190,\n",
      "        -4.9320, -3.3899], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05688167363405228\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5205, -4.1469, -3.5087, -4.2189], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.0852, -4.7208, -5.1605, -4.7208, -4.8368, -0.9279, -4.8368, -3.4842,\n",
      "        -3.5087, -5.4019], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.1578, -4.8819, -5.0009, -4.8819, -4.6767, -1.8369, -4.6767, -3.3633,\n",
      "        -4.1578, -5.1529], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14582720398902893\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.4197, -4.1091, -3.5439, -4.1463], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.2821, -5.1107, -3.7486, -4.7276, -4.7276, -3.4707, -4.1091, -4.5115,\n",
      "        -4.7276, -4.0483], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3245, -5.0696, -4.3481, -4.6993, -4.6993, -3.3245, -4.3481, -4.9504,\n",
      "        -4.6993, -4.3017], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07005833089351654\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-4.0757, -4.1371, -5.1833, -4.0489, -3.4691, -4.6149, -4.5003, -3.4691,\n",
      "        -5.0253, -4.0069], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.2273, -4.2273, -5.1339, -4.3499, -3.2809, -4.7234, -5.0090, -3.2809,\n",
      "        -5.1381, -4.3499], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0596037395298481\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8395, -4.5611, -4.1313, -4.5300], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.0113, -5.0113, -1.0053, -5.0113, -4.1066, -3.7303, -3.8117, -2.5047,\n",
      "        -4.0246, -3.4860], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1662, -5.1662, -0.6917, -5.1662, -4.2402, -4.3602, -4.2402, -1.6836,\n",
      "        -4.2357, -3.2542], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1540994495153427\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2531, -4.1174, -3.5888, -3.9864], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.3371, -3.9864, -4.9837, -3.5888, -4.2531, -4.1045, -4.0691, -3.8033,\n",
      "        -3.8033, -4.5258], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2230, -4.2299, -5.1781, -4.2299, -4.6940, -4.2299, -4.3391, -4.1878,\n",
      "        -4.1878, -4.6940], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11281950771808624\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2495, -4.1490, -3.5881, -3.9734], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.5103, -4.7199,  0.3728, -4.6475, -3.5881, -3.5881, -4.8121, -4.7816,\n",
      "        -3.5881, -4.5742], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.1981, -5.1828, 10.0000, -4.6586, -4.2293, -4.2293, -5.0234, -4.9999,\n",
      "        -4.2293, -4.8743], grad_fn=<AddBackward0>)\n",
      "LOSS: 9.441142082214355\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2598, -4.1805, -3.5807, -3.9457], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.9249, -4.7381, -0.9740, -3.7693, -4.9249, -3.5185, -4.6247, -4.2784,\n",
      "        -4.3066, -4.3066], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1622, -4.7187, -0.6206, -4.0609, -5.1622, -3.1537, -4.6250, -4.0609,\n",
      "        -4.6250, -4.6250], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07061117142438889\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2663, -4.1968, -3.5499, -3.9448], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.9202, -4.9202, -3.5499, -4.8077, -4.0849, -4.2793, -4.8547, -3.9448,\n",
      "        -4.9202, -4.3722], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1210, -5.1210, -4.1949, -4.9268, -4.2335, -4.5720, -4.9350, -4.1949,\n",
      "        -5.1210, -5.0553], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11945698410272598\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2771, -4.2238, -3.5008, -3.9725], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.6718, -4.9481, -4.2847, -3.5008, -4.2862, -4.0905, -4.2847,  0.4987,\n",
      "        -4.5540, -3.6718], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.0047, -5.0615, -4.5021, -4.1508, -4.1666, -4.1666, -4.5021, 10.0000,\n",
      "        -4.6192, -4.0047], grad_fn=<AddBackward0>)\n",
      "LOSS: 9.105024337768555\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2802, -4.2420, -3.4541, -3.9780], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.8796, -4.8243, -4.9617, -4.9617, -4.8243, -4.2790, -4.2790, -3.4511,\n",
      "        -3.5060, -4.8243], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.4932, -4.7899, -5.0024, -5.0024, -4.7899, -4.4380, -4.4380, -3.1084,\n",
      "        -3.1084, -4.7899], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04822816327214241\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2788, -4.2425, -3.3939, -4.0000], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.9902, -4.2983, -4.0000, -4.2983, -2.2190, -4.2983, -4.8037, -4.6582,\n",
      "        -4.7151, -0.5476], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9320, -4.3628, -4.0545, -4.3628, -3.1123, -4.3628, -4.7054, -4.8299,\n",
      "        -4.4912, -0.4463], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09163513779640198\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2813, -4.2413, -3.3364, -4.0105], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.0562, -3.3364, -3.3364, -5.0057, -3.3364, -5.0057, -5.0057, -4.0403,\n",
      "        -4.6379, -4.3088], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8636, -4.0028, -4.0028, -4.8636, -4.0028, -4.8636, -4.8636, -3.9509,\n",
      "        -4.7487, -4.2941], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1450316160917282\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-5.0010, -4.3100, -3.4380, -5.0010, -4.5765, -4.7690, -3.3087, -5.1622,\n",
      "        -4.1505, -4.4432], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8227, -4.2489, -3.8459, -4.8227, -4.8976, -4.5828, -3.9778, -4.8976,\n",
      "        -4.5828, -4.6909], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11374646425247192\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2963, -4.2479, -3.2997, -4.0165], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.2997, -3.4130, -4.0165, -4.3002, -4.2210, -4.3002, -4.2963, -3.4130,\n",
      "        -3.2997, -3.7626], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9697, -3.0165, -3.9697, -4.2239, -4.2239, -4.2239, -4.2239, -3.0165,\n",
      "        -3.9697, -3.6977], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12355989217758179\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3015, -4.2333, -3.2963, -4.0225], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.9599, -3.2963, -2.9575, -4.4433, -3.7215, -3.3737, -3.2963, -4.9599,\n",
      "        -1.8862, -4.0225], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7823, -3.9666, -2.9990, -4.6183, -3.6794, -2.9990, -3.9666, -4.7823,\n",
      "        -1.4139, -3.9666], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13626420497894287\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3078, -4.2145, -3.2972, -4.0336], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.2942, -3.3266, -3.7956, -4.0096, -3.2972, -4.9966, -4.2145, -4.2984,\n",
      "        -3.2972, -3.2972], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.1904, -2.9907, -3.7212, -3.6702, -3.9675, -4.7675, -3.7212, -4.1904,\n",
      "        -3.9675, -3.9675], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.18996116518974304\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2942, -4.1582, -3.2980, -4.0481], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.7855, -4.3140, -4.9318, -4.1582, -4.3140, -4.9782, -3.7046, -4.0481,\n",
      "        -0.6848, -2.0524], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7197, -4.1841, -4.7602, -3.7197, -4.1841, -4.7081, -3.7018, -3.9682,\n",
      "        -0.2527, -1.4124], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09353803098201752\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2546, -4.0703, -3.2960, -4.0456], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.2960, -4.3204, -4.9210, -4.9210, -3.1351, -4.0456, -3.0458, -3.1732,\n",
      "        -4.3204, -4.1583], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9664, -4.1850, -4.7610, -4.7610, -3.0109, -3.9664, -3.9664, -3.0109,\n",
      "        -4.1850, -4.7610], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17961111664772034\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-3.2613, -4.3849, -4.8482, -3.2613, -3.6991, -3.0929, -4.3849, -4.2007,\n",
      "        -3.1340, -4.3409], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9352, -4.4723, -4.5382, -3.9352, -3.7908, -3.0443, -4.4723, -4.1672,\n",
      "        -3.9352, -4.1672], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17034587264060974\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1078, -3.8366, -3.4589, -3.2639], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.4589, -4.6424, -3.0162, -4.4482, -4.2997, -3.2152, -4.1090, -4.4482,\n",
      "        -3.7446, -3.9735], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9375, -4.5330, -3.0857, -4.5330, -4.4538, -3.8937, -4.1482, -4.5330,\n",
      "        -3.9375, -4.4538], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10139014571905136\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1101, -3.7836, -3.1825, -4.0295], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.8922, -0.4783, -3.1825, -3.3512, -4.6580, -3.1825, -4.7755, -4.3791,\n",
      "        -4.1274, -2.3468], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6741, -1.5003, -3.8642, -3.8642, -4.7147, -3.8642, -4.7147, -4.1361,\n",
      "        -4.4369, -1.5003], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3163141906261444\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0692, -3.7063, -3.1472, -3.9889], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.1472, -3.4065, -4.8474, -3.1472, -3.1472, -1.9689, -4.1829, -3.9889,\n",
      "        -4.0585, -4.3584], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.8325, -3.7835, -4.6527, -3.8325, -3.8325, -1.5016, -4.8508, -3.8325,\n",
      "        -4.1245, -4.1245], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.23369145393371582\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-2.8874, -3.5781, -4.3662, -4.3247, -3.7077, -3.1170, -3.1170, -3.4564,\n",
      "        -4.7952, -4.6639], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.0748, -4.1479, -4.6318, -4.1108, -3.9274, -3.8053, -3.8053, -3.8053,\n",
      "        -4.6318, -4.8290], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16474483907222748\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0174, -3.6262, -3.1084, -3.8918], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.1192, -3.1084, -4.4650, -4.1192, -4.7227, -3.3858, -3.9452, -4.4777,\n",
      "        -4.8328, -3.8918], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4113, -3.7975, -4.3900, -4.4113, -4.6302, -3.7870, -4.3900, -4.5973,\n",
      "        -4.7685, -3.7975], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10459347069263458\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0093, -3.6256, -3.0854, -3.8389], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.9080, -4.6503, -4.6938, -3.4041, -4.2521, -0.5510, -3.4781, -4.4339,\n",
      "        -4.6503, -4.6503], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.0308, -4.6103, -4.6103, -3.7767, -4.1303, -0.1753, -3.7768, -4.6103,\n",
      "        -4.6103, -4.6103], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04420972615480423\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0073, -3.6234, -3.0596, -3.7810], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.6592, -3.4914, -3.5443, -3.0596, -4.5702, -2.9450, -3.9867, -3.7810,\n",
      "        -4.5702, -4.0193], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8036, -3.7536, -3.7605, -3.7536, -4.5880, -3.0185, -4.1423, -3.7536,\n",
      "        -4.5880, -4.3848], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07826307415962219\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0038, -3.6358, -3.0259, -3.7292], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.9936, -3.0259, -4.2109, -4.5097, -4.2215, -4.2215, -4.4352, -4.5097,\n",
      "        -4.0038, -3.5043], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.0104, -3.7233, -4.3784, -4.5639, -4.1539, -4.1539, -4.5639, -4.5639,\n",
      "        -4.1539, -3.7233], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06168090179562569\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0088, -3.6547, -2.9939, -3.6886], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.2591, -3.6886, -3.0375, -4.2192, -4.4608, -4.2192, -2.9939, -4.2591,\n",
      "        -4.2373, -4.4608], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3611, -3.6945, -3.0070, -4.1617, -4.5369, -4.1617, -3.6945, -4.3611,\n",
      "        -4.1617, -4.5369], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05365123599767685\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-4.2163, -2.9667, -4.3109, -2.9667, -2.9667, -4.4222, -2.9667, -3.6545,\n",
      "        -4.2163, -4.2163], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.1686, -3.6701, -4.3448, -3.6701, -3.6701, -4.5125, -3.6701, -3.6701,\n",
      "        -4.1686, -4.1686], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1995035707950592\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-2.9559, -4.2116, -3.7448, -3.6319, -4.7385, -4.2116, -4.3922, -4.0846,\n",
      "        -4.3628, -4.0331], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6603, -4.1823, -3.8791, -3.6603, -4.4988, -4.1823, -4.4988, -4.3386,\n",
      "        -4.3386, -4.1823], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06729069352149963\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-4.2031, -4.2031, -4.4032, -2.9519, -4.0950, -2.8503, -4.6714, -4.4042,\n",
      "        -4.2031, -4.7276], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.1930, -4.1930, -3.8752, -3.6567, -4.3090, -3.0032, -4.6855, -4.3289,\n",
      "        -4.1930, -4.8168], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08588254451751709\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-4.3577, -4.3500, -3.6167, -4.2031, -2.9537, -2.9537, -4.3249, -4.6829,\n",
      "        -4.0901, -4.8003], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3004, -4.4781, -3.6584, -4.2063, -3.6584, -3.6584, -4.3553, -4.7081,\n",
      "        -4.2063, -4.8261], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1030183807015419\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1302, -3.7922, -2.9595, -3.6273], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.8140, -4.2066, -4.3470, -4.2066, -4.3470, -4.2066, -4.3470, -3.5725,\n",
      "        -4.3470, -2.9595], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.5051, -4.2152, -4.4706, -4.2152, -4.4706, -4.2152, -4.4706, -3.6635,\n",
      "        -4.4706, -3.6635], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06607263535261154\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.1673, -3.8156, -2.9545, -3.6509], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.3810, -4.3776, -4.3776, -4.3776, -4.3776, -2.9545, -3.8400, -3.6719,\n",
      "        -3.8400, -3.6509], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.2736, -4.4560, -4.4560, -4.4560, -4.4560, -3.6590, -4.2141, -3.6806,\n",
      "        -4.2141, -3.6590], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08125313371419907\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2038, -3.8392, -2.9500, -3.6794], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.2582, -4.2419, -4.4422, -4.4883, -4.7181, -4.4883, -2.9500, -3.8448,\n",
      "        -5.2827, -4.2443], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.0575, -4.2913, -4.4603, -4.2913, -4.4603, -4.2913, -3.6550, -4.2021,\n",
      "        -4.6579, -4.2021], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12038829177618027\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2129, -3.8447, -2.9691, -3.6984], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.8820, -4.7917, -4.5061, -4.2129, -2.8816, -4.7917, -3.4595, -2.9691,\n",
      "        -4.5061, -3.2610], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6642, -4.7552, -4.4997, -4.2097, -3.0724, -4.7552, -4.4303, -3.6722,\n",
      "        -4.4997, -3.0724], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15589231252670288\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2251, -3.8463, -2.9807, -3.7349], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.9807, -3.2588, -4.4477, -3.5650, -4.5815, -4.5815, -3.9178, -4.8121,\n",
      "        -4.2758, -4.2469], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.6826, -3.1023, -4.2782, -3.6826, -4.5261, -4.5261, -4.2085, -4.7719,\n",
      "        -4.3028, -4.2085], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06542034447193146\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2356, -3.8389, -2.9991, -3.7666], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.7575, -3.5689, -2.9991, -3.7666, -4.4506, -3.8009, -4.2770, -4.6519,\n",
      "        -4.4506, -4.2216], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7050, -3.6992, -3.6992, -3.6992, -4.3129, -3.7050, -4.3129, -4.5647,\n",
      "        -4.3129, -4.2949], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05757785961031914\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2435, -3.8216, -3.0265, -3.7890], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.7092, -4.2674, -4.8657, -0.7036, -3.7890, -4.7597, -4.2668, -3.0265,\n",
      "        -3.0265, -3.8345], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6117, -4.3209, -4.6117, -0.1419, -3.7238, -4.8983, -4.2297, -3.7238,\n",
      "        -3.7238, -4.4403], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17567622661590576\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2425, -3.8100, -3.0486, -3.8142], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.0486, -3.0486, -4.7667, -3.5944, -4.7667, -4.7667, -2.9411, -3.8142,\n",
      "        -3.8142, -4.3784], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7437, -3.7437, -4.6527, -3.7437, -4.6527, -4.6527, -3.1759, -3.7437,\n",
      "        -3.7437, -3.7478], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14903314411640167\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2393, -3.7818, -3.0932, -3.8183], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.0932, -4.7899, -4.7899, -3.0932, -3.0932, -3.0932, -4.3073, -4.3718,\n",
      "        -4.7899, -4.6990], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7839, -4.7139, -4.7139, -3.7839, -3.7839, -3.7839, -4.3779, -3.7969,\n",
      "        -4.7139, -4.9610], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2329699993133545\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-4.9991, -4.2335, -4.2636, -3.1520, -4.2335, -1.8626, -4.2636, -4.4519,\n",
      "        -4.2335, -4.7976], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8298, -4.3133, -4.4239, -3.8368, -4.3133, -1.6184, -4.4239, -4.4281,\n",
      "        -4.3133, -4.7805], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0628650113940239\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8031, -4.2457, -3.7280, -4.2413], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.7280, -4.2663, -3.8376, -3.2002, -3.7280, -4.8080, -3.8352, -3.7455,\n",
      "        -3.2002, -4.2413], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.8802, -4.4621, -3.8802, -3.8802, -3.8802, -4.8332, -3.8802, -4.4538,\n",
      "        -3.8802, -4.3552], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15285637974739075\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.2775, -3.7760, -3.2558, -3.8559], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.8559, -4.8223, -4.2532, -3.7760, -3.8809, -3.8809, -4.8106, -3.2558,\n",
      "        -3.7779, -4.6655], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9302, -4.9267, -4.4002, -4.4118, -3.9657, -3.9657, -4.8810, -3.9302,\n",
      "        -3.9302, -5.0542], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10907167196273804\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3193, -3.8517, -3.3093, -3.8887], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.8105, -4.7309, -4.8105, -4.6718, -4.2684, -4.8105, -4.3835, -3.8193,\n",
      "        -3.8193, -4.3110], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9146, -5.0515, -4.9146, -5.1304, -4.4355, -4.9146, -4.5134, -3.9996,\n",
      "        -3.9996, -4.5134], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.049638718366622925\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-4.8848, -4.8369, -4.3757, -3.3229, -3.2099, -3.3358, -4.3787, -3.8805,\n",
      "        -4.9364, -3.8150], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9093, -4.9093, -4.4925, -3.9906, -3.1886, -3.1886, -4.4335, -4.0022,\n",
      "        -5.1064, -3.9906], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05649659037590027\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.4364, -4.0396, -3.3266, -3.9769], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.3266, -4.3460, -3.3266, -4.8746, -3.9769, -3.3266, -3.3245, -3.9791,\n",
      "        -4.5002, -4.3460], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9939, -4.4269, -3.9939, -4.9004, -3.9939, -3.9939, -3.2037, -3.9921,\n",
      "        -4.4562, -4.4269], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1366785317659378\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-4.3973, -4.4964, -4.0489, -5.0273, -4.9177, -4.5157, -3.3337, -4.3243,\n",
      "        -2.0624, -4.0297], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4217, -4.4217, -3.9801, -4.8926, -4.8919, -4.4467, -4.0003, -4.4217,\n",
      "        -1.6169, -4.0003], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06877203285694122\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5399, -4.2056, -3.3464, -4.0773], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.2817, -4.2884, -4.9614, -4.2959, -4.3640, -3.0304, -4.4461, -3.5838,\n",
      "        -5.1853, -4.9614], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8939, -4.4345, -4.8939, -4.8664, -4.4243, -3.2490, -4.4243, -4.0118,\n",
      "        -5.0191, -4.8939], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07689423859119415\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5552, -4.2593, -3.3605, -4.1134], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.2593, -4.9194, -3.3820, -5.2333, -4.7139, -4.1457, -4.3346, -2.0557,\n",
      "        -5.2845, -4.9889], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.2281, -4.9579, -3.2685, -5.0192, -4.4418, -3.9810, -4.4369, -1.6388,\n",
      "        -4.9012, -4.9012], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.050122521817684174\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5445, -4.2804, -3.3887, -4.1440], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.3887, -4.1440, -5.0003, -3.3887, -3.3330, -4.5168, -5.0003, -3.3875,\n",
      "        -5.0003, -5.0003], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.0498, -4.0498, -4.9202, -4.0498, -3.2868, -4.4569, -4.9202, -3.2868,\n",
      "        -4.9202, -4.9202], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09246194362640381\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5377, -4.3009, -3.4288, -4.1642], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.5293, -4.5377, -4.9909, -5.1234, -4.6141, -3.8976, -3.4288, -5.1234,\n",
      "        -4.5293, -4.5293], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4865, -4.4865, -4.9481, -4.9481, -4.4757, -4.2261, -4.0859, -4.9481,\n",
      "        -4.4865, -4.4865], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06302111595869064\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5295, -4.3171, -3.4883, -4.1627], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.9678, -4.9715, -4.1627, -4.9678, -3.4883, -0.7265, -4.5162, -3.4883,\n",
      "        -4.6145, -4.1627], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0058, -5.0058, -4.1395, -5.0058, -4.1395, -0.1693, -4.5367, -4.1395,\n",
      "        -4.5234, -4.1395], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11723126471042633\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-0.4956, -4.9412, -5.0468, -3.5458, -4.7255, -4.5002, -3.5458, -4.9412,\n",
      "        -3.5458, -4.5002], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.1715, -5.0574, -5.0574, -4.1912, -4.5659, -4.5833, -4.1912, -5.0574,\n",
      "        -4.1912, -4.5833], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1421164721250534\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-4.7371, -4.7371, -4.9383, -4.5046, -4.5241, -4.9383, -4.9383, -4.9383,\n",
      "        -4.9383, -4.7371], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6859, -4.6859, -5.1048, -4.6266, -4.6266, -5.1048, -5.1048, -5.1048,\n",
      "        -5.1048, -4.6859], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.017187634482979774\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-4.1816, -4.5534, -4.0910, -3.6364, -4.4343, -3.6364, -4.5315, -3.6364,\n",
      "        -4.5315, -4.9877], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0779, -5.0779, -5.0779, -4.2727, -4.6237, -4.2727, -4.6421, -4.2727,\n",
      "        -4.6421, -5.1322], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.33484089374542236\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-5.2437, -4.2252, -4.5558, -5.0073, -3.7208, -4.5453, -5.1611, -4.5823,\n",
      "        -4.5383, -5.0073], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2726, -4.3341, -4.6897, -5.1904, -4.2351, -4.6897, -5.1904, -4.6681,\n",
      "        -4.6897, -5.1904], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04142271727323532\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6047, -4.3920, -3.7410, -4.2672], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.4595, -3.3829, -5.2024, -5.0597, -5.2576, -4.1981, -5.2576, -2.5773,\n",
      "        -5.0597, -5.0597], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3669, -3.3196, -5.2148, -5.2148, -5.2809, -4.2592, -5.2809, -1.5962,\n",
      "        -5.2148, -5.2148], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.18670538067817688\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6229, -4.3900, -3.7197, -4.3015], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.3653, -4.6107, -4.6324, -5.1495, -4.6107, -4.3638, -4.7150, -4.0842,\n",
      "        -5.1495, -4.0842], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3223, -4.6758, -4.8039, -5.2090, -4.6758, -4.8039, -4.8039, -4.3478,\n",
      "        -5.2090, -4.3478], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.038741566240787506\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6448, -4.3808, -3.6752, -4.3338], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.2495, -4.6448, -5.3091, -3.3533, -5.2495, -3.6752, -5.0734, -4.3338,\n",
      "        -5.2495, -5.2495], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1876, -4.6524, -5.1876, -3.3358, -5.1876, -4.3077, -5.1876, -4.3077,\n",
      "        -5.1876, -5.1876], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04441981762647629\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6619, -4.3744, -3.6441, -4.3610], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.3099, -3.1728, -4.4797, -4.5748, -4.5748, -3.6441, -5.3532, -4.3610,\n",
      "        -3.3429, -4.3610], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1799, -3.3460, -4.7469, -4.6006, -4.6006, -4.2797, -5.1727, -4.2797,\n",
      "        -3.3460, -4.2797], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05694937705993652\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-5.1121, -3.5769, -4.8066, -4.4112, -5.3902, -3.6261, -4.3773, -3.5769,\n",
      "        -5.3639, -5.3639], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2089, -3.3494, -4.6337, -4.5892, -5.1652, -4.2635, -4.2635, -3.3494,\n",
      "        -5.1652, -5.1652], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07232867181301117\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6855, -4.3638, -3.6185, -4.3814], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.5727, -3.6185, -4.5727, -4.8378, -3.7963, -5.4016, -3.3365, -3.6185,\n",
      "        -4.8038, -5.3815], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5827, -4.2566, -4.5827, -4.6356, -4.2139, -5.1632, -3.3598, -4.2566,\n",
      "        -4.6356, -5.1632], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11633148044347763\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6966, -4.3665, -3.6284, -4.3757], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.7730, -4.5732, -4.8406, -5.1632, -4.7869, -3.6284, -5.3751, -4.4276,\n",
      "        -5.1258, -3.6284], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7172, -4.5893, -4.5893, -5.1715, -4.6536, -4.2656, -5.1715, -4.5893,\n",
      "        -5.2230, -4.2656], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09733410179615021\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-2.6417, -0.8556, -3.6557, -3.6557, -4.8533, -3.6557, -4.3680, -2.0245,\n",
      "        -3.6557, -4.8533], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7700, -0.2138, -4.2902, -4.2902, -4.6874, -4.2902, -4.4690, -1.7700,\n",
      "        -4.2902, -4.6874], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2911751866340637\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-4.1379, -4.3445, -4.2353, -5.3182, -4.8188, -3.8466, -3.3558, -4.3445,\n",
      "        -3.6967, -4.8188], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3270, -4.3270, -4.2482, -5.2218, -4.7241, -4.3270, -3.3357, -4.3270,\n",
      "        -4.3270, -4.7241], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0692378506064415\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7232, -4.3736, -3.7218, -4.3133], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.2302, -3.7218, -5.2857, -5.3955, -4.5725, -4.5725, -4.7930, -4.2302,\n",
      "        -4.7901, -4.7930], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.2683, -4.3496, -5.2488, -5.2488, -4.6716, -4.6716, -4.7609, -4.2683,\n",
      "        -4.7661, -4.7609], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04421910271048546\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7186, -4.3805, -3.7484, -4.2851], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.7484, -5.0813, -4.7650, -4.0713, -2.5246, -4.1120, -3.7484, -5.2522,\n",
      "        -3.7484, -5.2362], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3735, -5.2746, -4.7959, -4.4939, -1.7781, -4.2887, -4.3735, -5.2746,\n",
      "        -4.3735, -5.1356], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1988489031791687\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7207, -4.3893, -3.7891, -4.2566], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.1618, -5.2129, -3.7891, -4.5877, -4.2566, -4.8438, -0.8848, -5.1942,\n",
      "        -4.2566, -3.7891], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3167, -5.3062, -4.4102, -4.8399, -4.4102, -5.3908, -0.2276, -5.3062,\n",
      "        -4.4102, -4.4102], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16586828231811523\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7312, -4.3993, -3.8020, -4.2622], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.3993, -4.7312, -4.7312, -4.7251, -5.3551, -3.9051, -4.7251, -4.2622,\n",
      "        -5.2933, -4.2278], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.5146, -4.8485, -4.8485, -4.8485, -5.3047, -4.4218, -4.8485, -4.4218,\n",
      "        -5.4103, -4.3125], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03871441259980202\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7484, -4.4178, -3.7851, -4.2903], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.7413, -4.2518, -4.7413, -4.7413, -3.7851, -4.7413, -3.7851, -5.3021,\n",
      "        -5.2170, -4.6396], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8266, -4.4066, -4.8266, -4.8266, -4.4066, -4.8266, -4.4066, -5.3931,\n",
      "        -5.3384, -4.7527], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0861431434750557\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7665, -4.4472, -3.7612, -4.3261], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.7813, -4.2471, -4.8544, -5.2271, -5.3668, -5.2271, -3.7612, -4.0273,\n",
      "        -5.2271, -4.4472], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8032, -4.2505, -4.7897, -5.2546, -5.2546, -5.2546, -4.3851, -4.3851,\n",
      "        -5.2546, -4.6246], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05682230740785599\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-5.2551, -4.4788, -4.8264, -3.7328, -3.5728, -4.1968, -5.3605, -0.6818,\n",
      "        -4.8967, -4.7608], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2260, -4.6917, -4.7771, -4.3595, -3.1563, -4.3595, -5.3395, -0.2614,\n",
      "        -4.7659, -4.9035], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08559757471084595\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-4.8710, -4.4085, -4.5116, -4.7580, -4.8710, -3.1594, -3.3256, -4.4941,\n",
      "        -4.1512, -5.2822], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7461, -4.3323, -4.7361, -4.6904, -4.7461, -3.1754, -3.1754, -4.3890,\n",
      "        -4.7361, -5.1926], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04759598150849342\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.2516, -4.9555, -4.1384, -4.8955], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.7958, -4.4458, -4.4458, -4.9999, -4.8955, -2.4328, -2.4328, -4.4458,\n",
      "        -4.6329, -3.6881], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6760, -4.3193, -4.3193, -4.7226, -4.7246, -1.8932, -1.8932, -4.3193,\n",
      "        -4.7246, -4.3193], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11575444787740707\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8057, -4.5726, -3.6908, -4.4435], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.4435, -5.2906, -4.4435, -5.4318, -4.4435, -0.6522, -3.4595, -3.6908,\n",
      "        -4.4435, -5.2906], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3217, -5.1695, -4.3217, -5.2801, -4.3217, -0.2904, -3.1449, -4.3217,\n",
      "        -4.3217, -5.1695], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0739680603146553\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7961, -4.5835, -3.7030, -4.4193], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.1409, -5.2625, -4.1409, -3.7030, -4.9244, -5.2625, -4.1409, -4.5835,\n",
      "        -4.4193, -4.1548], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3327, -5.1750, -4.3327, -4.3327, -4.7517, -5.1750, -4.3327, -4.7394,\n",
      "        -4.3327, -4.7394], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09254594892263412\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7833, -4.5952, -3.7217, -4.3821], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.8414, -4.3821, -3.7217, -5.2284, -3.7217, -4.8414, -4.5536, -5.2284,\n",
      "        -4.6615, -4.3821], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7601, -4.3495, -4.3495, -5.1954, -4.3495, -4.7601, -4.7256, -5.1954,\n",
      "        -4.7601, -4.3495], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08451403677463531\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.7828, -4.6104, -3.7487, -4.3458], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.8827, -5.1950, -5.1950, -4.8162, -3.7487, -5.0285, -5.0631, -5.4451,\n",
      "        -4.6104, -5.1950], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7564, -5.2232, -5.2232, -4.7953, -4.3738, -5.3616, -4.7564, -5.1590,\n",
      "        -4.7658, -5.2232], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07206409424543381\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8022, -4.6110, -3.7858, -4.3181], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.8176, -4.2613, -3.4833, -5.1652, -4.7305, -4.3144, -3.7858, -4.8155,\n",
      "        -3.4833, -4.5475], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9152, -4.4073, -2.9988, -5.2575, -4.8352, -4.7456, -4.4073, -4.7982,\n",
      "        -2.9988, -4.7982], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11550639569759369\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-0.5775, -3.2727, -4.8263, -4.7823, -4.2806, -4.8132, -4.8821, -5.1523,\n",
      "        -3.7986, -5.1523], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.0072, -3.0033, -4.8526, -4.8526, -4.4187, -4.9127, -4.8120, -5.2728,\n",
      "        -4.4187, -5.2728], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.25699055194854736\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8421, -4.5968, -3.7996, -4.2799], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.2719, -4.8421, -4.1714, -5.2120, -3.2719, -3.4369, -4.8712, -5.1851,\n",
      "        -4.2799, -3.7996], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.0010, -4.8599, -4.0932, -5.3603, -3.0010, -3.0010, -4.8158, -5.3688,\n",
      "        -4.4197, -4.4197], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08060108870267868\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8662, -4.5765, -3.7842, -4.2821], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.8444, -5.1356, -4.2821, -4.8444, -4.7779, -4.8444, -4.7779, -4.0913,\n",
      "        -4.2821, -3.2658], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7980, -5.2580, -4.4058, -4.7980, -4.8463, -4.7980, -4.8463, -4.4058,\n",
      "        -4.4058, -3.0270], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.021731160581111908\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8828, -4.5422, -3.7496, -4.3034], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.2438, -5.1586, -4.9579, -5.1586, -4.8071, -4.3034, -5.4944, -3.2502,\n",
      "        -4.8828, -1.9811], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3746, -5.2290, -4.9489, -5.2290, -4.7669, -4.3746, -5.1388, -3.0676,\n",
      "        -4.8194, -1.9606], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.019804496318101883\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8744, -4.5002, -3.7182, -4.3241], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.3241, -5.1872, -4.8529, -3.7182, -4.3241, -3.7182, -5.1872, -4.9944,\n",
      "        -4.8529, -3.7182], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3464, -5.2084, -4.8029, -4.3464, -4.3464, -4.3464, -5.2084, -4.8029,\n",
      "        -4.8029, -4.3464], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12273912131786346\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8756, -4.4702, -3.7059, -4.3455], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.7308, -4.2249, -3.7059, -4.8678, -3.0754, -3.0754, -3.2132, -5.0321,\n",
      "        -4.7308, -4.8885], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7328, -4.3353, -4.3353, -5.2127, -3.1451, -3.1451, -3.1451, -4.8024,\n",
      "        -4.7328, -4.8024], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.060180384665727615\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8769, -4.4420, -3.6980, -4.3641], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.1984, -3.6980, -4.1883, -5.6613, -4.3641, -3.1984, -3.6980, -4.4135,\n",
      "        -4.5643, -3.6980], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.1777, -4.3282, -4.3282, -5.2557, -4.3282, -3.1777, -4.3282, -4.7271,\n",
      "        -4.3147, -4.3282], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15382707118988037\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8740, -4.4101, -3.6973, -4.3906], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.2574, -3.1977, -5.2574, -4.2503, -4.9715, -4.2503, -5.2574, -4.5582,\n",
      "        -3.1977, -3.6973], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1987, -3.2149, -5.1987, -4.3275, -4.8252, -4.3275, -5.1987, -4.7336,\n",
      "        -3.2149, -4.3275], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.047226496040821075\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8712, -4.3817, -3.7074, -4.4061], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.4061, -5.2721, -5.0048, -4.6148, -4.4061, -5.2721, -5.2721, -5.0048,\n",
      "        -4.1476, -5.2721], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3367, -5.2131, -4.8533, -4.7268, -4.3367, -5.2131, -5.2131, -4.8533,\n",
      "        -4.3753, -5.2131], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01338447816669941\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8667, -4.3601, -3.7340, -4.3966], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.0039, -3.1523, -3.7340, -4.1500, -5.0039, -3.7846, -4.3287, -5.4157,\n",
      "        -2.1183, -5.0039], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8959, -3.2536, -4.3606, -4.7350, -4.8959, -3.8954, -4.3606, -5.2438,\n",
      "        -1.9525, -4.8959], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08505040407180786\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8583, -4.3454, -3.7727, -4.3817], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.7727, -4.9870, -4.9870, -5.0847, -4.5665, -5.4006, -3.7727, -5.2404,\n",
      "        -3.7727, -3.7727], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.3954, -4.9470, -4.9470, -5.2840, -4.7887, -5.2840, -4.3954, -5.2840,\n",
      "        -4.3954, -4.3954], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16589787602424622\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8571, -4.3488, -3.8173, -4.3817], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.9773, -4.9773, -4.6901, -4.2492, -3.1535, -5.1200, -5.2236, -4.9773,\n",
      "        -4.5611, -4.5611], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9957, -4.9957, -5.0920, -3.9398, -3.2695, -5.2452, -5.3175, -4.9957,\n",
      "        -4.8243, -4.8243], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.043478500097990036\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8544, -4.3746, -3.8277, -4.3916], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.8277, -4.4540, -3.8277, -3.8277, -3.2700, -4.9013, -3.8277, -4.9809,\n",
      "        -3.1749, -5.3607], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4449, -4.4449, -4.4449, -4.4449, -3.2843, -5.2925, -4.4449, -5.0086,\n",
      "        -3.2843, -5.3183], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.16917604207992554\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8619, -4.4243, -3.8452, -4.4099], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.4099, -4.8619, -4.9873, -5.2255, -4.4099, -4.2380, -4.4687, -3.8452,\n",
      "        -4.9035, -5.1003], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4606, -5.0218, -5.0218, -5.3201, -4.4606, -3.9476, -4.4606, -4.4606,\n",
      "        -5.0883, -5.3201], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.058651626110076904\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8844, -4.4829, -3.8497, -4.4479], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.2444, -5.0079, -4.2047, -5.2337, -4.4479, -5.2337, -5.2337, -3.2465,\n",
      "        -3.2711, -5.0580], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4647, -5.0154, -3.9440, -5.2965, -4.4647, -5.2965, -5.2965, -3.3187,\n",
      "        -3.3187, -5.0154], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.013797560706734657\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8990, -4.5323, -3.8389, -4.4835], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.2540, -3.2797, -3.8389, -5.2511, -3.8389, -4.8125, -5.1158, -3.8389,\n",
      "        -3.2797, -5.4482], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2348, -3.3401, -4.4551, -5.2671, -4.4551, -4.7413, -5.2348, -4.4551,\n",
      "        -3.3401, -5.2671], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11986857652664185\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9111, -4.5853, -3.8404, -4.5232], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.4418, -4.5232, -4.5232, -4.6910, -5.0562, -3.8404, -4.5232, -3.8404,\n",
      "        -5.0562, -4.5232], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2523, -4.4564, -4.4564, -4.7134, -4.9891, -4.4564, -4.4564, -4.4564,\n",
      "        -4.9891, -4.4564], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08220575749874115\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9147, -4.6362, -3.8596, -4.5470], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.8073, -5.0553, -5.3080, -4.1160, -4.4337, -5.2925, -5.2925, -5.0451,\n",
      "        -4.5470, -3.8596], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.9539, -4.9903, -5.2591, -3.9539, -4.4736, -5.2591, -5.2591, -4.8257,\n",
      "        -4.4736, -4.4736], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.048877764493227005\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-5.0482, -5.3015, -5.0482, -5.2691, -4.7921, -4.0114, -4.5628, -5.3015,\n",
      "        -5.3015, -4.5628], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0001, -5.2719, -5.0001, -5.3129, -4.6999, -3.9723, -4.4970, -5.2719,\n",
      "        -5.2719, -4.4970], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.002791126724332571\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9224, -4.7095, -3.9141, -4.5663], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.4437, -3.9141, -3.3260, -5.3951, -4.7095, -4.7319, -5.3002, -4.9070,\n",
      "        -4.3448, -3.2367], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8035, -4.5227, -3.3876, -5.2755, -4.8035, -4.4477, -5.2886, -4.8700,\n",
      "        -4.5227, -3.3876], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0663415864109993\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-4.7376, -4.4641, -4.7376, -5.3027, -3.9456, -5.0225, -4.7376, -5.0764,\n",
      "        -3.9456, -4.7376], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7100, -4.8129, -4.7100, -5.3077, -4.5510, -5.0329, -4.7100, -4.8992,\n",
      "        -4.5510, -4.7100], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08893280476331711\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3612, -5.1405, -4.5126, -5.0173], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.3613, -4.9553, -3.9883, -5.0494, -3.9883, -5.3075, -3.8140, -5.4412,\n",
      "        -5.3514, -4.9355], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3369, -5.0504, -4.5894, -5.0614, -4.5894, -5.3369, -4.0514, -5.3618,\n",
      "        -5.3369, -5.0614], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08122336119413376\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9611, -4.7697, -4.0307, -4.6161], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.8668, -4.0307, -3.4150, -5.0848, -0.6442, -5.3177, -4.0307, -2.0611,\n",
      "        -4.7180, -4.0307], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0748, -4.6276, -3.4150, -4.9794, -0.3839, -5.3610, -4.6276, -1.9704,\n",
      "        -4.7373, -4.6276], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12015960365533829\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.9905, -4.7938, -4.0813, -4.6486], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.9905, -5.3319, -5.3460, -4.3619, -5.2747, -5.3319, -4.0813, -1.9711,\n",
      "        -4.0813, -5.3319], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1060, -5.3898, -5.4062, -4.5431, -5.3898, -5.3898, -4.6731, -1.9775,\n",
      "        -4.6731, -5.3898], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0773785263299942\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0378, -4.8266, -4.1282, -4.6918], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.1282, -4.2811, -4.1282, -4.6918, -5.5064, -4.1282, -5.0203, -5.3552,\n",
      "        -5.2955, -4.6918], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7154, -4.8530, -4.7154, -4.7154, -5.4299, -4.7154, -5.1207, -5.4108,\n",
      "        -5.4108, -4.7154], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13948430120944977\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.0961, -4.8651, -4.1798, -4.7435], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.6394, -0.7089, -5.5187, -5.3849, -4.1798, -5.0346, -5.3849, -3.4985,\n",
      "        -5.3849, -4.1798], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.3971, -1.9978, -5.4591, -5.4342, -4.7618, -5.1387, -5.4342, -3.4745,\n",
      "        -5.4342, -4.7618], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.24197249114513397\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.1476, -4.9036, -4.2245, -4.7816], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.3999, -5.1938, -5.0357, -4.7816, -5.3999, -5.3576, -5.1476, -5.3953,\n",
      "        -5.1986, -4.3602], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4485, -5.4354, -5.1492, -4.8020, -5.4485, -5.5200, -5.1492, -5.4354,\n",
      "        -5.5200, -4.8020], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04028499871492386\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.1975, -4.9380, -4.2188, -4.8257], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.2188, -5.0634, -4.9219, -4.8257, -4.8257, -5.1248, -4.5840, -4.3382,\n",
      "        -5.1975, -4.2188], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7969, -5.1256, -5.1256, -4.7969, -4.7969, -5.1256, -4.7969, -4.1536,\n",
      "        -5.1256, -4.7969], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08000743389129639\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.2357, -4.9600, -4.2205, -4.8613], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.2205, -3.5063, -5.4631, -4.2205, -4.2205, -4.1565, -5.0954, -5.4326,\n",
      "        -5.4815, -4.8613], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7984, -3.5303, -5.4777, -4.7984, -4.7984, -4.1557, -5.1179, -5.4299,\n",
      "        -5.4299, -4.7984], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10099951922893524\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.2775, -4.9867, -4.2355, -4.8970], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.4977, -5.1251, -5.5204, -4.9792, -4.2355, -4.5779, -4.7341, -5.1251,\n",
      "        -5.5204, -4.8970], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4385, -5.1201, -5.4385, -5.1144, -4.8119, -4.8119, -4.7377, -5.1201,\n",
      "        -5.4385, -4.8119], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.042961180210113525\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3088, -5.0082, -4.2567, -4.9181], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.1626, -1.0141, -5.2159, -4.2567, -4.2103, -4.5981, -4.7879, -5.5101,\n",
      "        -0.6356, -5.2895], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.1841, -0.4310, -5.1366, -4.8311, -4.1841, -4.8311, -4.6715, -5.4594,\n",
      "        -0.4310, -5.1366], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08129797130823135\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-4.6267, -5.6344, -4.9241, -4.5188, -5.5660, -4.2780, -5.1694, -4.6080,\n",
      "        -4.2780, -5.3749], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8502, -5.4843, -4.8502, -4.8502, -5.4843, -4.8502, -5.1641, -4.6781,\n",
      "        -4.8502, -5.5011], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08700709044933319\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3386, -5.0085, -4.2914, -4.9175], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.1699, -5.1302, -5.5857, -5.1940, -2.1848, -5.5857, -5.5857, -5.0085,\n",
      "        -4.9175, -4.9175], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7776, -5.1994, -5.5166, -5.1980, -3.5793, -5.5166, -5.5166, -4.9126,\n",
      "        -4.8623, -4.8623], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2132870852947235\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3423, -4.9983, -4.3139, -4.8702], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.5685, -4.7175, -5.5685, -5.1784, -5.5685, -5.1784, -5.3423, -5.1784,\n",
      "        -4.7604, -5.1784], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5607, -4.9575, -5.5607, -5.2429, -5.5607, -5.2429, -5.2429, -5.2429,\n",
      "        -4.8191, -5.2429], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.008777020499110222\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3377, -4.9883, -4.3279, -4.8307], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.5599, -5.5599, -5.1809,  0.5897, -5.1091, -5.5599, -5.5599, -4.3279,\n",
      "        -5.5599, -5.1572], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5982, -5.5982, -5.2840, 10.0000, -5.2840, -5.5982, -5.5982, -4.8951,\n",
      "        -5.5982, -4.8537], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.901655197143555\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3238, -4.9599, -4.3332, -4.7765], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.2788, -4.8033, -4.3332, -4.8033, -4.5472, -4.7745, -5.1618, -5.1672,\n",
      "        -4.7745, -5.5479], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3230, -5.0112, -4.8999, -5.0112, -4.5722, -4.8839, -5.3117, -5.3230,\n",
      "        -4.8839, -5.6274], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0487116277217865\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3151, -4.9468, -4.3289, -4.7402], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.2504, -5.3151, -5.2504, -4.3289, -5.2896, -5.5986, -3.4003, -5.5435,\n",
      "        -5.3151, -5.1622], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3285, -5.3285, -5.3285, -4.8960, -5.3485, -5.4939, -3.3508, -5.5602,\n",
      "        -5.3285, -5.3285], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03790280222892761\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3128, -4.9354, -4.3202, -4.7200], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.4973, -4.3202, -4.9354, -4.7200, -4.8172, -4.3202, -3.4973, -4.3202,\n",
      "        -4.3202, -4.3202], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3090, -4.8881, -5.0059, -4.8881, -4.8881, -4.8881, -3.3090, -4.8881,\n",
      "        -4.8881, -4.8881], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17221909761428833\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3262, -4.9307, -4.3266, -4.7281], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.6989, -4.9307, -5.3262, -4.2358, -4.8322, -5.2044, -5.4307, -5.2044,\n",
      "        -4.6683, -4.3266], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.2850, -5.0039, -5.3490, -4.3290, -4.8940, -5.3490, -5.7148, -5.3490,\n",
      "        -4.4999, -4.8940], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06624209880828857\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3425, -4.9324, -4.3037, -4.7643], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.7677, -4.7643, -5.6391, -4.3037, -4.2268, -4.3037, -5.6391, -4.8043,\n",
      "        -5.2690, -3.3293], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9088, -4.8733, -5.6501, -4.8733, -4.3014, -4.8733, -5.6501, -4.9088,\n",
      "        -5.3327, -3.2949], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07026951014995575\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3707, -4.9458, -4.2792, -4.8107], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.6916, -4.2792, -4.2792, -4.8107, -3.6385, -4.2510, -5.6916, -5.3707,\n",
      "        -5.1400, -4.2792], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6260, -4.8513, -4.8513, -4.8513, -3.3098, -4.5601, -5.6260, -5.3110,\n",
      "        -5.3110, -4.8513], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12285002321004868\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3995, -4.9636, -4.2717, -4.8553], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.2717, -4.5733, -5.1869, -5.3995, -5.5003, -2.0686, -5.7337, -5.3434,\n",
      "        -5.3995, -4.8269], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8445, -4.8445, -5.3659, -5.3005, -5.3005, -1.7543, -5.4435, -5.3659,\n",
      "        -5.3005, -4.8701], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06786493957042694\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3821, -4.9694, -4.2586, -4.8893], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.3690, -4.9694, -4.2586, -4.7827, -4.2586, -4.2586, -5.3690, -5.4672,\n",
      "        -5.3690, -4.9391], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3465, -4.9078, -4.8328, -4.8328, -4.8328, -4.8328, -5.3465, -5.3044,\n",
      "        -5.3465, -4.8637], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10289038717746735\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3711, -4.9740, -4.2629, -4.9195], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.3711, -4.2629, -5.1985, -4.9488, -5.5241, -4.9740, -4.8006, -5.5241,\n",
      "        -4.8006, -5.2785], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3206, -4.8366, -5.0850, -4.8727, -5.3206, -4.9153, -4.8366, -5.3206,\n",
      "        -4.8366, -5.4540], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04700619727373123\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3556, -4.9819, -4.2779, -4.9284], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.8585, -4.9607, -0.8579, -5.8585, -5.2729, -5.5510, -5.5486, -5.5510,\n",
      "        -4.2779, -4.2779], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6589, -5.3566, -0.1961, -5.6589, -5.3566, -5.3490, -5.3490, -5.3490,\n",
      "        -4.8501, -4.8501], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14576756954193115\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3413, -4.9919, -4.3211, -4.9067], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.4803, -4.3211, -5.5295, -5.8335, -4.0151, -5.8335, -5.8335, -5.8335,\n",
      "        -5.5295, -4.9067], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4020, -4.8890, -5.4020, -5.7041, -4.9768, -5.7041, -5.7041, -5.7041,\n",
      "        -5.4020, -4.8890], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13533984124660492\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3315, -5.0035, -4.3641, -4.8928], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.5989, -5.8353, -4.3641, -5.3315, -4.3641, -5.5080, -4.8928, -4.8928,\n",
      "        -3.5204, -5.8030], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.7457, -5.7457, -4.9277, -5.4520, -4.9277, -5.4520, -4.9277, -4.9277,\n",
      "        -3.3705, -5.7457], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07106982916593552\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3344, -5.0184, -4.4110, -4.8973], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.7793, -4.4110, -3.9346, -5.0184, -4.6106, -5.7793, -4.4110, -3.5282,\n",
      "        -5.7793, -4.4573], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.7820, -4.9699, -4.3113, -5.0116, -4.9699, -5.7820, -4.9699, -3.3726,\n",
      "        -5.7820, -5.0116], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12271963059902191\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.3467, -5.0238, -4.4388, -4.9101], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.7102, -5.4876, -4.1794, -5.4876, -5.3467, -4.6256, -4.2224, -5.4876,\n",
      "        -5.7740, -4.9101], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6904, -5.5203, -5.0311, -5.5203, -5.5203, -4.9949, -4.3179, -5.5203,\n",
      "        -5.8067, -4.9949], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09127464145421982\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-4.9070, -4.4203, -4.6760, -5.5287, -4.4203, -4.9594, -4.4203, -0.5540,\n",
      "        -4.5711, -4.4203], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0131, -4.9783, -4.9783, -5.5020, -4.9783, -4.9783, -4.9783, -0.1623,\n",
      "        -4.2968, -4.9783], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1577679067850113\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.8660, -5.4602, -4.9882, -5.5778], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-1.8844, -4.4060, -4.4060, -5.3130, -4.7483, -4.4060, -5.8533, -5.0138,\n",
      "        -4.9069, -5.1947], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.7068, -4.9654, -4.9654, -5.4894, -4.9654, -4.9654, -5.7817, -4.9654,\n",
      "        -5.0038, -5.3476], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10888560116291046\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.4059, -5.0245, -4.3900, -5.0684], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.9308, -4.9695, -5.2321, -5.0684, -5.0684, -4.3900, -5.4059, -0.5400,\n",
      "        -5.4059, -5.9058], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.7102, -4.9510, -5.3199, -4.9510, -4.9510, -4.9510, -5.4725, -0.1675,\n",
      "        -5.4725, -5.7764], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05633504316210747\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-4.7546, -5.9345, -5.4298, -5.9345, -4.3942, -5.6468, -3.4298, -5.9345,\n",
      "        -5.4377, -5.6468], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9993, -5.7841, -5.4679, -5.7841, -4.9548, -5.4679, -3.6110, -5.7841,\n",
      "        -5.4679, -5.4679], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05413011461496353\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.9352, -5.4528, -4.9780, -5.6277], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-6.0308, -5.1185, -4.4213, -4.4213, -4.4213, -5.9264, -5.1185, -4.4213,\n",
      "        -0.8082, -5.9264], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.8078, -4.9792, -4.9792, -4.9792, -4.9792, -5.8078, -4.9792, -4.9792,\n",
      "        -1.7426, -5.8078], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.22344744205474854\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.5249, -5.0629, -4.4754, -5.0984], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.4754, -5.9173, -4.4754, -5.8864, -5.3962, -5.8864, -4.9348, -4.9391,\n",
      "        -5.5502, -5.5734], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0279, -5.8566, -5.0279, -5.8566, -5.5157, -5.8566, -5.0532, -5.0279,\n",
      "        -5.8275, -5.5157], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07323391735553741\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.5766, -5.0879, -4.5228, -5.0829], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.8554, -4.1895, -5.8554, -4.5228, -3.5318, -4.9433, -4.5228, -1.8693,\n",
      "        -4.5228, -4.5228], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.9019, -4.3760, -5.9019, -5.0705, -3.6208, -5.0705, -5.0705, -1.6685,\n",
      "        -5.0705, -5.0705], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1303454041481018\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.6386, -5.1169, -4.5694, -5.0822], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.5694, -4.5694, -5.8013, -4.5694, -5.4862, -4.9641, -4.6222, -4.5694,\n",
      "        -3.5697, -4.5694], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1124, -5.1124, -5.8986, -5.1124, -5.5670, -5.1124, -5.1599, -5.1124,\n",
      "        -3.6282, -5.1124], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1805230975151062\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.7060, -5.1564, -4.6231, -5.0964], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.8999, -5.8408, -4.6231, -4.6231, -4.9325, -4.6231, -3.8162, -4.6231,\n",
      "        -4.6231, -5.9393], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.9331, -5.9911, -5.1608, -5.1608, -4.4346, -5.1608, -3.6421, -5.1608,\n",
      "        -5.1608, -5.9911], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.17500841617584229\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.7795, -5.1803, -4.6899, -5.1305], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.4659, -4.9371, -5.8545, -4.9965, -4.6899, -5.8545, -3.8647, -4.6899,\n",
      "        -4.6899, -2.9650], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6342, -4.4782, -6.0418, -5.1868, -5.2209, -6.0418, -3.6685, -5.2209,\n",
      "        -5.2209, -1.6178], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.3044494390487671\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.8432, -5.1869, -4.7375, -5.1627], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.4662, -5.1869, -1.8961, -4.7375, -4.9828, -4.7375, -5.1627, -5.8430,\n",
      "        -5.3300, -5.4662], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6522, -5.3038, -1.6447, -5.2638, -5.2078, -5.2638, -5.2638, -5.9897,\n",
      "        -5.6522, -5.6522], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08860567957162857\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.9115, -5.2193, -4.7541, -5.2270], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.0443, -4.7541, -5.2270, -4.7541, -5.0231, -4.7541, -4.7541, -5.6400,\n",
      "        -5.1457, -5.1547], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-1.6819, -5.2787, -5.2787, -5.2787, -5.4000, -5.2787, -5.2787, -5.6312,\n",
      "        -5.2787, -5.2787], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14099036157131195\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.9728, -5.2369, -4.7862, -5.2983], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.2369, -6.3085, -3.6559, -5.6611, -5.5389, -5.5389, -5.2306, -4.7862,\n",
      "        -5.9864, -3.6215], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3219, -6.0950, -3.6620, -5.6332, -5.6332, -5.6332, -5.3076, -5.3076,\n",
      "        -6.0950, -3.6620], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.036261700093746185\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.0192, -5.2583, -4.8129, -5.3745], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.8129, -6.0481, -4.9886, -5.8861, -6.0192, -6.0192, -4.8475, -4.8129,\n",
      "        -4.8129, -4.3618], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3316, -6.1063, -5.2059, -5.9576, -5.6331, -5.6331, -4.5452, -5.3316,\n",
      "        -5.3316, -4.5452], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12862160801887512\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.9890, -5.2619, -4.8341, -5.4144], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.6711, -5.6711, -5.9076, -5.6711, -6.1096, -4.8341, -6.1096, -6.1096,\n",
      "        -4.8341, -5.7126], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6709, -5.6709, -5.9689, -5.6709, -6.1396, -5.3507, -6.1396, -6.1396,\n",
      "        -5.3507, -5.5588], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05638350173830986\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.9676, -5.2730, -4.8617, -5.4538], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.6663, -4.8617, -4.4168, -5.9676, -5.4538, -4.2910, -6.1387, -4.2910,\n",
      "        -5.9676, -5.9676], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.7103, -5.3755, -4.6184, -5.7103, -5.3755, -4.6184, -5.8266, -4.6184,\n",
      "        -5.7103, -5.7103], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08233408629894257\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-5.9698, -4.8736, -5.1471, -5.3392, -5.8786, -4.8736, -4.8736, -5.8221,\n",
      "        -6.2220,  0.8911], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.0070, -5.3863, -5.3241, -5.5319, -5.7757, -5.3863, -5.3863, -5.7757,\n",
      "        -6.2240, 10.0000], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.384260177612305\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.7512, -5.1814, -4.9701, -5.5310], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.8786, -5.8694, -5.0569, -6.2470, -6.2470, -6.2470, -5.4227, -5.0569,\n",
      "        -4.8786, -4.8786], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3908, -5.8327, -5.3007, -6.2589, -6.2589, -6.2589, -5.3908, -5.3007,\n",
      "        -5.3908, -5.3908], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0908561497926712\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.7100, -5.2382, -5.0004, -5.5538], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-6.2718, -5.7802, -5.9123, -5.7788, -4.8837, -4.8837, -6.1121, -5.8385,\n",
      "        -5.1066, -3.6838], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.2858, -5.8804, -5.8804, -6.0245, -5.3954, -5.3954, -6.1453, -5.8886,\n",
      "        -5.3154, -3.7153], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06433393806219101\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.6900, -5.3055, -5.0154, -5.5799], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-6.3007, -5.3370, -5.9528, -5.3370, -5.6569, -4.8794, -6.3007, -6.1358,\n",
      "        -5.9528, -5.6569], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.2979, -5.5139, -5.9089, -5.5139, -5.9089, -5.3914, -6.2979, -6.0080,\n",
      "        -5.9089, -5.9089], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04719867557287216\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-5.7473, -3.7643, -5.3510, -4.7829, -5.9679, -5.4123, -6.0450, -5.4123,\n",
      "        -5.9679, -5.8327], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.1385, -3.7082, -5.3046, -4.6438, -5.9061, -5.4002, -5.9820, -5.4002,\n",
      "        -5.9061, -5.7498], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.019646454602479935\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.8271, -5.6994, -5.2873, -5.9093], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.8946, -6.3305, -4.8946, -4.9929, -5.1703, -5.4413, -4.8946, -6.1372,\n",
      "        -5.9707, -5.7252], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4051, -6.2751, -5.4051, -5.4306, -5.4878, -5.4051, -5.4051, -6.2751,\n",
      "        -5.8972, -5.7586], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11042870581150055\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.6812, -5.4701, -4.9739, -5.5992], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.3425, -6.3529, -3.8384, -4.9160, -5.9817, -6.3529, -5.5992, -4.9160,\n",
      "        -4.9160, -4.1882], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2809, -6.2753, -3.7052, -5.4244, -5.8982, -6.2753, -5.4244, -5.4244,\n",
      "        -5.4244, -4.6292], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10410115867853165\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.6922, -5.4915, -4.9669, -5.5858], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-6.3611, -5.4930, -3.8728, -4.9669, -5.9811, -5.6562, -6.3611, -4.9669,\n",
      "        -5.3763, -5.9811], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.2835, -5.4604, -3.7179, -5.4702, -5.9088, -5.5118, -6.2835, -5.4702,\n",
      "        -5.2821, -5.9088], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05839567258954048\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.7061, -5.5055, -4.9795, -5.5711], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.9696, -5.0076, -5.3984, -3.8973, -5.0076, -5.9696, -6.3583, -5.4805,\n",
      "        -6.3583, -5.5055], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.9324, -5.5068, -5.2973, -3.7273, -5.5068, -5.9324, -6.3054, -5.5068,\n",
      "        -6.3054, -5.4693], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05479755252599716\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.7190, -5.5076, -5.0030, -5.5559], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-6.3523, -5.5183, -5.0697, -6.3523, -6.2274, -5.9573, -6.3523, -5.4281,\n",
      "        -5.9573, -5.0697], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.3372, -5.5628, -5.5628, -6.3372, -6.0391, -5.9665, -6.3372, -5.5027,\n",
      "        -5.9665, -5.5628], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05299835279583931\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.7309, -5.5101, -5.0350, -5.5345], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-6.1282, -5.9429, -5.0399, -5.5345, -4.9624, -4.3324, -5.7572, -5.4834,\n",
      "        -5.1413, -5.1413], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.3793, -6.0117, -5.3559, -5.6272, -4.6677, -4.6677, -5.5315, -5.9288,\n",
      "        -5.6272, -5.6272], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10970108211040497\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.8113, -5.4989, -5.0491, -5.5473], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-6.3435, -5.9879,  1.1439, -0.8292, -4.7045, -5.1756, -5.1756, -6.3198,\n",
      "        -3.8464, -2.1333], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.3850, -6.1298, 10.0000, -1.8580, -4.6789, -5.6581, -5.6581, -6.3850,\n",
      "        -3.7719, -1.8580], grad_fn=<AddBackward0>)\n",
      "LOSS: 8.006291389465332\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.8706, -5.4899, -5.0443, -5.5171], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.8706, -5.1945, -6.0991, -5.6054, -5.6054, -5.5297, -6.0371, -6.3085,\n",
      "        -5.9335, -5.1945], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.9768, -5.6751, -6.0321, -5.6500, -5.6500, -5.5399, -5.9768, -6.3703,\n",
      "        -5.6500, -5.6751], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05695149302482605\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.9326, -5.4689, -5.0522, -5.5000], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-6.2867, -6.2867, -5.7682, -5.9061, -5.5738, -6.3368, -5.6035, -4.8672,\n",
      "        -5.4011, -6.2748], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.3657, -6.3657, -5.5470, -6.0431, -5.6994, -6.1685, -5.6994, -4.6775,\n",
      "        -5.3804, -6.1685], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.018117595463991165\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-5.3607, -5.9571, -6.4520, -5.9107, -5.9536, -6.2761, -5.2381, -5.9107,\n",
      "        -5.2381, -5.9107], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3848, -6.0691, -6.3582, -6.0579, -6.0579, -6.3582, -5.7143, -6.0579,\n",
      "        -5.7143, -6.0579], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.055810779333114624\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.2855, -5.7036, -5.2433, -5.6028], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.5998, -6.2855, -5.2433, -5.2433, -6.2912, -5.9656, -6.2855, -6.2668,\n",
      "        -5.3265, -5.9841], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6889, -6.0594, -5.7190, -5.7190, -6.3432, -6.0398, -6.0594, -6.3432,\n",
      "        -5.3783, -6.2227], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0636376440525055\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-6.4956, -5.3403, -6.2920, -6.0060, -5.6395, -6.3162, -5.6395, -5.6395,\n",
      "        -6.3162, -3.8981], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.3307, -5.3752, -6.0756, -6.0756, -5.7132, -6.3307, -5.7132, -5.7132,\n",
      "        -6.3307, -3.7902], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0108501510694623\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.2588, -5.6233, -5.2249, -5.6230], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.9211, -5.5991, -6.3425, -2.0842, -6.3425, -3.8859, -5.2249, -3.9211,\n",
      "        -5.6792, -5.6230], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.8151, -5.7024, -6.3281, -1.6358, -6.3281, -3.8151, -5.7024, -3.8151,\n",
      "        -5.7024, -5.7024], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04744568094611168\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.2179, -5.5742, -5.2022, -5.6406], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-6.4702, -3.9236, -5.2022, -5.2022, -6.1028, -5.7138, -6.4702, -5.2022,\n",
      "        -5.7051, -0.5122], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.3170, -3.8536, -5.6820, -5.6820, -6.2781, -5.7099, -6.3170, -5.6820,\n",
      "        -5.6820,  0.2520], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.13576549291610718\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.1879, -5.5447, -5.1921, -5.6566], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.7018, -5.2326, -6.4263, -6.2485, -2.0433, -5.2326, -5.1921, -5.6566,\n",
      "        -3.8614, -5.1921], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6729, -5.3757, -6.3294, -6.1605, -1.6298, -5.3757, -5.6729, -5.6729,\n",
      "        -3.8862, -5.6729], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06931183487176895\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.1584, -5.5375, -5.1896, -5.6738], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-6.7110, -5.6738, -1.8807, -6.7110, -6.4630, -5.1896, -5.1896, -6.1584,\n",
      "        -5.1896, -0.7011], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.3418, -5.6706, -1.6310, -6.3418, -6.3418, -5.6706, -5.6706, -6.1856,\n",
      "        -5.6706,  0.2784], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.20040766894817352\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.0790, -5.5263, -5.2094, -5.6725], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.6725, -4.5462, -6.3024, -5.2279, -5.2094, -5.6824, -5.2279, -5.5263,\n",
      "        -5.2094, -5.2279], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6885, -4.7263, -6.2013, -5.3938, -5.6885, -5.7504, -5.3938, -5.6155,\n",
      "        -5.6885, -5.3938], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.059699177742004395\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.0164, -5.5642, -5.2184, -5.6760], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.9757, -4.1424, -5.2184, -6.5228, -6.5346, -5.2662, -6.4855, -6.4855,\n",
      "        -6.5228, -5.7773], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.9930, -3.9232, -5.6966, -6.4256, -6.4256, -5.3939, -6.4256, -6.4256,\n",
      "        -6.4256, -5.6966], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03378145396709442\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.9565, -5.6084, -5.2341, -5.6774], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.4493, -4.1370, -0.9065, -5.6774, -6.2621, -5.7315, -5.2341, -6.5438,\n",
      "        -5.8175, -5.2341], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.3040, -3.9304, -1.4927, -5.7107, -6.1961, -5.7818, -5.7107, -6.4643,\n",
      "        -6.0179, -5.7107], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1462630331516266\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.9140, -5.6523, -5.2386, -5.6760], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.4433, -5.2386, -5.3428, -6.0217, -6.2675, -6.5496, -5.2386, -5.6760,\n",
      "        -5.2386, -5.7017], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1686, -5.7148, -5.3836, -5.9939, -6.4851, -6.4851, -5.7148, -5.7148,\n",
      "        -5.7148, -5.7595], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08144260942935944\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.9023, -5.7000, -5.2484, -5.6835], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-6.5575, -1.6509, -5.6835, -5.9023, -5.2484, -5.2484, -5.3776, -0.4096,\n",
      "        -5.6835, -4.6669], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.5043, -1.3733, -5.7235, -6.1627, -5.7235, -5.7235, -5.3718,  0.3149,\n",
      "        -5.7235, -4.6788], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.11275939643383026\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-4.6562, -5.2603, -6.1299, -5.8003, -6.1651, -6.1651, -5.7165, -6.1651,\n",
      "        -5.8003, -5.7165], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6529, -5.7342, -6.1311, -5.7107, -6.1311, -6.1311, -5.7342, -6.1311,\n",
      "        -5.7107, -5.7342], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02448219619691372\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.9479, -5.8004, -5.2839, -5.7482], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.7907, -4.0397, -6.1424, -5.2839, -5.0607, -6.5789, -6.5789, -5.6800,\n",
      "        -5.2839, -5.2839], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.9723, -3.9773, -6.1120, -5.7555, -5.5546, -6.5282, -6.5282, -5.7555,\n",
      "        -5.7555, -5.7555], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09598396718502045\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.9771, -5.8451, -5.3245, -5.7828], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-6.5862, -5.7828, -5.8518, -6.5862, -5.0646, -6.1094, -6.5862, -4.0413,\n",
      "        -5.3245, -4.0624], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.5576, -5.7921, -5.9215, -6.5576, -5.5582, -6.1137, -6.5576, -3.9936,\n",
      "        -5.7921, -3.9936], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.047658082097768784\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.0074, -5.8860, -5.3674, -5.8211], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.3674, -6.5951, -6.5345, -5.8211, -6.5951, -4.8464, -6.1005, -5.3674,\n",
      "        -5.3674, -6.5345], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.8307, -6.5877, -6.2421, -5.8307, -6.5877, -4.6378, -6.1190, -5.8307,\n",
      "        -5.8307, -6.2421], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08587954938411713\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.0440, -5.8979, -5.4379, -5.8656], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.0803, -5.4379, -6.0918, -5.4379, -0.9526, -5.4379, -6.0440, -5.2902,\n",
      "        -5.7195, -0.2499], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.0375, -5.8941, -6.1475, -5.8941, -1.2249, -5.8941, -6.1475, -5.3848,\n",
      "        -5.8941,  0.3151], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10727627575397491\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.0834, -5.9078, -5.5032, -5.9018], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.1926, -3.3893, -4.0960, -4.8872, -5.1326, -5.9078, -4.6418, -6.5541,\n",
      "        -6.5541, -5.7487], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.3151, -1.1734, -4.0504, -4.6864, -5.6194, -5.6194, -4.6864, -6.6417,\n",
      "        -6.6417, -5.9529], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.5589739084243774\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.0743, -5.8669, -5.5213, -5.8566], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-6.4889, -5.5213, -6.0193, -4.1058, -5.5213, -1.4036, -5.5998, -5.4412,\n",
      "        -6.4889, -5.5998], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.6469, -5.9691, -6.1834, -3.9336, -5.9691, -1.1587, -5.9691, -5.2158,\n",
      "        -6.6469, -5.9691], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08913106471300125\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.0261, -5.8069, -5.4807, -5.8377], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-6.3495, -5.8069, -6.0261, -6.4833, -5.4807, -6.0201, -5.4807, -5.1586,\n",
      "        -5.7350, -6.0201], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.3874, -5.6428, -6.1615, -6.6118, -5.9326, -6.1615, -5.9326, -5.6428,\n",
      "        -5.9326, -6.1615], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07851653546094894\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.9851, -5.7452, -5.4403, -5.8348], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.7280, -5.4402, -6.0472, -5.8416, -5.1741, -5.4402, -5.7280, -5.7186,\n",
      "        -6.5008, -6.0516], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.8962, -5.8962, -6.1467, -6.3813, -5.6567, -5.8962, -5.8962, -5.8962,\n",
      "        -6.5878, -6.1467], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10546629130840302\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.9320, -5.6771, -5.3618, -5.8565], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-6.5598, -6.1316, -5.8565, -0.2113, -5.9320, -6.5598, -5.3618, -3.7164,\n",
      "        -5.3618, -5.6792], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.5608, -6.1113, -5.8256,  0.3001, -6.1113, -6.5608, -5.8256, -3.8103,\n",
      "        -5.8256, -5.8256], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07555653154850006\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.9042, -5.6227, -5.2999, -5.8821], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.2999, -5.2999, -5.2320, -6.2031, -5.2999, -3.7425, -5.9042, -6.2031,\n",
      "        -6.2031, -5.2999], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.7699, -5.7699, -5.2255, -6.0821, -5.7699, -3.7986, -6.0821, -6.0821,\n",
      "        -6.0821, -5.7699], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09624052792787552\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-5.2751, -6.6508, -6.0785, -5.4845, -5.4253, -6.2383, -3.9733, -6.6508,\n",
      "        -5.9112, -6.1912], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.7476, -6.5085, -5.7476, -5.6819, -5.0872, -6.0633, -3.7803, -6.5085,\n",
      "        -5.7476, -5.9360], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06862679868936539\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.1555, -5.7818, -5.6431, -6.2248], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.6713, -5.8544, -3.5821, -4.4182, -6.2248, -5.2997, -5.6431, -6.6404,\n",
      "        -5.5180, -5.2997], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7470, -5.9748, -3.7470, -4.5817, -6.0788, -5.7698, -5.7698, -6.5275,\n",
      "        -5.7174, -5.7698], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06058571860194206\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.9445, -5.5811, -5.3264, -5.8975], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.8975, -5.3264, -5.3264,  1.4234, -6.6165, -5.5447, -5.3264, -5.3264,\n",
      "        -5.3264, -4.4163], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.7938, -5.7938, -5.7938, 10.0000, -6.5486, -5.7316, -5.7938, -5.7938,\n",
      "        -5.7938, -4.5888], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.472995758056641\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.9767, -5.6215, -5.3652, -5.8709], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-6.1555, -5.9866, -6.5828, -5.6059, -5.3652, -3.6763, -4.6102, -5.2676,\n",
      "        -5.3652, -3.6763], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.1266, -5.9835, -6.5727, -5.7598, -5.8287, -3.6434, -4.5978, -5.2286,\n",
      "        -5.8287, -3.6434], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04580948129296303\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.0116, -5.6604, -5.4109, -5.8528], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.4109, -5.7138, -5.4109, -5.8528, -6.1170, -6.5544, -5.4109, -5.4109,\n",
      "        -5.1375, -5.4109], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.8698, -5.8306, -5.8698, -5.8698, -6.1570, -6.5904, -5.8698, -5.8698,\n",
      "        -5.2464, -5.8698], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.10816758871078491\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.0577, -5.7174, -5.4660, -5.8541], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-0.2220, -5.8541, -6.0913, -4.0305, -5.4660, -5.3092, -6.2298, -5.4660,\n",
      "        -6.2624, -5.4660], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.3719, -5.9194, -6.1893, -3.5457, -5.9194, -5.7680, -6.1893, -5.9194,\n",
      "        -6.1893, -5.9194], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14358840882778168\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.1279, -5.7697, -5.4895, -5.8691], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.2129, -6.0001, -4.5003, -5.4895, -6.5209, -6.5209, -3.5718, -5.4895,\n",
      "        -5.4895, -6.5209], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2688, -5.9405, -4.5956, -5.9405, -6.6279, -6.6279, -3.5267, -5.9405,\n",
      "        -5.9405, -6.6279], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06624852120876312\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.2023, -5.8327, -5.5115, -5.9022], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.2591, -5.5115, -3.7602, -6.3198, -5.7612, -5.5115, -5.7209, -5.2591,\n",
      "        -6.5250, -6.3198], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2650, -5.9603, -3.5174, -6.6148, -5.9603, -5.9603, -5.8036, -5.2650,\n",
      "        -6.6148, -6.6148], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06906719505786896\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.2710, -5.8797, -5.5125, -5.9630], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.5125, -6.4077, -0.4212, -3.5420, -6.5398, -6.2710, -4.8855, -4.2367,\n",
      "        -6.1300, -5.5125], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.9613, -6.5662,  0.4317, -3.5360, -6.5662, -6.1720, -5.3421, -4.5244,\n",
      "        -6.1720, -5.9613], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.14587634801864624\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.3398, -5.9026, -5.5263, -6.0344], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.7144, -6.3470, -6.5297, -3.7879, -6.5615, -3.5455, -5.5263, -5.9026,\n",
      "        -3.5456, -3.8775], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4897, -6.5283, -6.3524, -3.5753, -6.5283, -3.5753, -5.9736, -5.7614,\n",
      "        -3.5753, -3.5753], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04742081090807915\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.3984, -5.9022, -5.5338, -6.1057], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.9022, -5.5338, -6.3984, -6.5890, -5.5338, -6.2664, -5.5338, -6.6362,\n",
      "        -5.7050, -5.5338], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.7504, -5.9804, -6.1758, -6.4917, -5.9804, -6.1758, -5.9804, -6.4917,\n",
      "        -5.7504, -5.9804], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09110825508832932\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.4311, -5.8889, -5.5693, -6.1526], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-6.6091, -2.9782, -6.5428, -6.6091, -5.8889, -6.3268, -5.8723, -6.4311,\n",
      "        -6.0652, -6.6586], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.4976, -1.1344, -6.4976, -6.4976, -5.7763, -6.2160, -5.5469, -6.2160,\n",
      "        -6.1675, -6.4148], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.36735832691192627\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.3898, -5.8265, -5.6108, -6.1246], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.7801, -5.8031, -6.5793, -5.2311, -5.6108, -5.8665, -6.5793, -5.2311,\n",
      "        -6.1405, -4.3764], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.1603, -5.8273, -6.5265, -5.2816, -6.0498, -6.0498, -6.5265, -5.2816,\n",
      "        -6.4680, -4.4512], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04949180409312248\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.3492, -5.7816, -5.6287, -6.1053], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-6.3568, -6.1129, -5.5018, -5.6287, -6.1053, -5.7816, -5.6287, -6.3568,\n",
      "        -5.6287, -5.5341], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.3316, -6.4361, -5.2644, -6.0658, -6.0658, -5.8599, -6.0658, -6.3316,\n",
      "        -6.0658, -5.6339], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07529316842556\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-5.1956, -5.6577, -5.4283, -5.6577, -6.5377, -6.5878, -0.0833, -6.7284,\n",
      "        -5.9814, -6.5878], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3377, -6.0919, -5.8854, -6.0919, -6.4966, -6.4966,  0.4707, -6.4966,\n",
      "        -6.0919, -6.4966], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09976162016391754\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-6.3871, -6.5897, -5.6836, -5.4562, -5.6836, -6.1451, -5.7226, -0.3259,\n",
      "        -6.5448, -6.3871], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.3984, -6.5306, -6.1153, -5.9106, -6.1153, -6.3973, -5.6983,  0.4727,\n",
      "        -6.5306, -6.3984], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12849922478199005\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.2861, -5.7682, -5.7111, -6.0438], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-6.3798, -3.5374, -5.7111, -6.5854, -5.9747, -5.7111, -5.7111, -6.3798,\n",
      "        -6.0438, -3.5899], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.4240, -3.4070, -6.1400, -6.5682, -5.9341, -6.1400, -6.1400, -6.4240,\n",
      "        -6.1400, -3.4070], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06174200773239136\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.2763, -5.7708, -5.7433, -6.0429], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.7433, -5.7433, -6.5956, -6.1295, -6.1295, -2.6370, -5.7433, -5.7433,\n",
      "        -6.5472, -6.3946], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.1690, -6.1690, -6.5941, -6.1690, -6.1690, -1.2913, -6.1690, -6.1690,\n",
      "        -6.5941, -6.4387], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2543005645275116\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-6.2672, -5.5351, -5.7734, -6.0414, -6.5894, -5.3086, -4.7065, -5.7734,\n",
      "        -5.7677, -6.0256], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.4469, -5.9442, -6.1909, -5.9442, -6.6107, -5.9442, -5.4322, -6.1909,\n",
      "        -5.9442, -6.1909], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.15473683178424835\n",
      "---\n",
      "Agent's Location: [1 0]\n",
      "state: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.3485, -5.6805, -5.8633, -6.0895], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.8633, -5.8633, -6.4332, -6.6256, -6.4332, -5.6805, -5.7877, -6.6256,\n",
      "        -6.1288, -6.3243], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.1125, -6.1125, -6.3538, -6.5331, -6.3538, -5.8326, -5.8199, -6.5331,\n",
      "        -6.1125, -6.2103], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01913592591881752\n",
      "---\n",
      "Agent's Location: [1 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-6.1606, -5.2562, -5.6297, -6.1205], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-3.3102, -5.9803, -6.4476, -6.4355, -6.4355, -6.0591, -5.6113, -3.9316,\n",
      "        -6.6376, -4.1413], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.1851, -6.0502, -6.2718, -6.2718, -6.2718, -6.1274, -5.7306, -4.5691,\n",
      "        -6.4646, -4.5691], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.07433632761240005\n",
      "---\n",
      "Agent's Location: [1 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.9611, -4.5948, -5.4558, -5.5239], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.0485, -6.5036, -6.0485, -6.0485, -6.0485, -6.3864, -6.4500, -6.4500,\n",
      "        -6.6308, -6.0485], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.0262, -6.2457, -6.0262, -6.0262, -6.0262, -6.0262, -6.2457, -6.2457,\n",
      "        -6.4412, -6.0262], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.031809061765670776\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8692, -4.1206, -3.9565, -4.8204], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-6.0776, -6.3885, -6.3885, -6.0776, -6.0393, -6.0776, -6.3885, -5.9876,\n",
      "        -6.0776, -5.6267], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.0644, -6.2765, -6.2765, -6.0644, -6.0644, -6.0644, -6.2765, -5.8796,\n",
      "        -6.0644, -5.6806], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.005357490852475166\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.9290, -3.2909, -3.4373, -2.2962], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-6.5335, -5.6106, -6.0995, -6.3711, -6.4821, -6.3036, -6.0995, -4.9988,\n",
      "        -5.7373, -6.5335], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.5301, -5.7059, -6.1173, -6.1173, -6.1177, -6.3219, -6.1173, -5.4390,\n",
      "        -5.7731, -6.5302], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.040238115936517715\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.7291, -2.1563, -1.5672, -0.5387], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-6.1065, -5.5708, -5.5708, -3.3304, -5.0633, -6.2084, -5.5708, -6.1016,\n",
      "        -3.9026, -3.9026], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.2111, -5.7668, -5.7668, -3.0176, -5.3925, -6.3955, -5.7668, -6.2111,\n",
      "        -3.0176, -3.0176], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.19462451338768005\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3262, -0.9836, -0.0187,  1.6186], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-6.0718, -6.0718, -6.4729, -2.3064, -6.2776, -6.4641, -6.4729, -6.0718,\n",
      "        -5.0714, -6.4729], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.2519, -6.2519, -6.6498, -1.5097, -6.2310, -6.6498, -6.6498, -6.2519,\n",
      "        -5.3118, -6.6498], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09204081445932388\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-6.4599, -6.2601, -6.5019, -6.4949], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.0674, -5.3990, -6.5484, -6.2601, -5.4632, -6.1918, -5.8254, -6.0674,\n",
      "        -6.0674, -6.1785], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.2429, -5.6948, -6.3840, -6.2086, -5.7453, -6.2429, -5.7453, -6.2429,\n",
      "        -6.2429, -6.3840], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03403762727975845\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-6.4224, -5.7097, -6.1227, -6.3891], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.2036, -6.1917, -4.9654, -6.1875, -6.1046, -6.5297, -6.1046, -2.3743,\n",
      "        -6.5297, -6.5297], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.3107, -6.1787, -5.2411, -6.1387, -6.1787, -6.5688, -6.1787, -1.6109,\n",
      "        -6.5688, -6.5688], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06883098185062408\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-6.1154, -5.3579, -5.6972, -6.1532], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.1330, -6.3058, -6.3058, -6.1330, -6.1330, -6.1330, -6.1330, -6.1330,\n",
      "        -5.4733, -6.1330], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.1133, -6.1133, -6.1133, -6.1133, -6.1133, -6.1133, -6.1133, -6.1133,\n",
      "        -5.5978, -6.1133], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.009239166975021362\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-5.7641, -4.9798, -5.3332, -5.9212], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.4737, -3.8361, -6.1467, -4.6447, -3.4853, -6.5740, -5.6338, -6.5740,\n",
      "        -6.1467, -6.0064], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5498, -4.1368, -6.0704, -4.1368, -3.1247, -6.4599, -5.5498, -6.4599,\n",
      "        -6.0704, -5.9892], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.052925385534763336\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-5.7459, -4.9198, -5.3000, -5.9134], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.7459, -3.3554, -6.0262, -4.8130, -6.1406, -6.0181, -6.1406, -5.5350,\n",
      "        -6.1406, -6.2216], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4278, -3.1243, -6.0245, -5.1839, -6.0245, -5.9815, -6.0245, -5.7214,\n",
      "        -6.0245, -6.1446], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03746415302157402\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-5.6903, -4.9142, -5.2215, -5.8883], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.0875, -6.2070, -3.8812, -3.8812, -6.0875, -6.3122, -4.5680, -3.3111,\n",
      "        -6.2070, -6.0440], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.0294, -6.1507, -4.0129, -4.0129, -6.0294, -6.0294, -4.0129, -3.1201,\n",
      "        -6.1507, -5.7167], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05795425921678543\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-5.6202, -4.9130, -5.1390, -5.8482], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.0125, -6.5399, -3.7411, -6.1720, -5.4913, -6.2778, -6.5399, -6.0215,\n",
      "        -3.2667, -6.0263], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.0384, -6.4363, -3.9523, -6.1622, -5.6157, -6.1622, -6.4363, -6.0384,\n",
      "        -3.1136, -6.0384], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.011958264745771885\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-5.5556, -4.9335, -5.0559, -5.8019], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.9528, -5.2597, -5.9528, -4.3058, -6.5027, -5.9579, -6.5027, -5.9528,\n",
      "        -3.8255, -5.9579], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.0658, -5.4402, -6.0658, -5.0141, -6.4671, -6.0658, -6.4671, -6.0658,\n",
      "        -3.8919, -6.0658], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06027306243777275\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-3.1829, -5.8954, -3.1829, -6.0834, -6.4694, -5.8954, -6.1128, -4.3727,\n",
      "        -6.1128, -6.4694], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.1009, -6.0671, -3.1009, -6.0661, -6.4750, -6.0671, -6.2137, -3.8586,\n",
      "        -6.2137, -6.4750], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.035739801824092865\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-5.8533, -3.7641, -5.8533,  0.0633, -5.7813, -6.4486, -4.7967, -5.8533,\n",
      "        -5.8533, -5.2445], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.0425, -3.8435, -6.0425,  0.4160, -5.9463, -6.4543, -4.8447, -6.0425,\n",
      "        -6.0425, -5.4010], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03280653804540634\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-5.4840, -4.8271, -4.8648, -5.6972], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.1915, -3.1410, -5.8634, -5.1915, -5.4411, -4.7263, -5.8634, -6.4505,\n",
      "        -5.8634, -6.1273], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4830, -3.1195, -5.9982, -5.4830, -5.5686, -4.7871, -5.9982, -6.4118,\n",
      "        -5.9982, -6.1736], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02485920488834381\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-5.9265, -6.1992, -6.4339, -2.3757, -5.9265, -5.9265, -6.4339, -6.1516,\n",
      "        -5.9265, -3.7082], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.9231, -5.9231, -6.3399, -1.9763, -5.9231, -5.9231, -6.3399, -6.1103,\n",
      "        -5.9231, -3.8701], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02814057469367981\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-5.9014, -5.0938, -5.3574, -5.9979], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.5732, -4.1632, -6.1427, -5.6679, -6.4038, -4.4075, -6.2222, -5.4228,\n",
      "        -5.6448, -4.5732], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7468, -3.8867, -6.0801, -5.5844, -6.3012, -4.7468, -6.0801, -5.3733,\n",
      "        -5.5844, -4.7468], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02995908260345459\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-5.4362, -4.6576, -4.8333, -5.5879], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.3544, -2.9988, -3.1486, -4.2180, -6.1214, -4.5324, -6.1477, -5.9681,\n",
      "        -2.9988, -5.9737], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.2910, -3.1018, -3.1018, -3.8986, -6.0868, -4.6791, -5.8717, -5.8717,\n",
      "        -3.1018, -5.8717], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.024805430322885513\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-5.4152, -4.6582, -4.7846, -5.5153], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.6472, -5.6793, -6.3079, -6.0074, -5.9606, -6.0733, -4.9613, -6.2886,\n",
      "        -5.9221, -5.8896], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6142, -5.7876, -6.3006, -6.1113, -5.8848, -6.1113, -4.7859, -6.3006,\n",
      "        -5.8848, -5.9264], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.006452202796936035\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-5.3931, -4.6666, -4.7294, -5.4531], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.0371, -3.0047, -3.0047, -5.6675, -5.9361, -5.9086, -5.9361, -6.2628,\n",
      "        -5.4495, -5.9361], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.9045, -3.0327, -3.0327, -5.8068, -5.9045, -5.9498, -5.9045, -6.3178,\n",
      "        -5.4020, -5.9045], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.004849078133702278\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-5.3731, -4.6766, -4.6759, -5.3984], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.5667, -5.9037, -5.9037, -5.2014, -5.9996, -5.9037, -5.8762, -5.9834,\n",
      "        -5.9037, -0.3590], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.4765, -5.9229, -5.9229, -5.4229, -6.1711, -5.9229, -5.9745, -5.9229,\n",
      "        -5.9229,  0.4003], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06778918206691742\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.8737, -4.4722, -3.8149, -4.6527], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.8906, -5.6134, -5.9882, -5.8935, -5.8906, -0.8909, -5.1994, -5.8935,\n",
      "        -3.8792, -6.4322], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.9173, -5.8157, -6.1763, -5.9173, -5.9173, -2.0562, -5.4212, -5.9173,\n",
      "        -3.9280, -6.1763], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.1553703397512436\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-5.3195, -5.8924, -5.8768, -5.9802, -6.1106, -5.4251, -5.8768, -4.4322,\n",
      "        -5.8768, -5.8768], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6687, -5.8826, -5.8826, -6.1670, -6.3008, -5.3975, -5.8826, -4.3929,\n",
      "        -5.8826, -5.8826], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.019556868821382523\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.5841, -3.8537, -3.2704, -4.3848], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.8666, -6.1034, -6.0103, -3.2704, -4.8275, -2.9538, -3.8537, -6.1034,\n",
      "        -5.8666, -6.1738], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.8180, -6.2435, -6.1254, -2.9521, -4.6587, -2.9521, -3.9433, -6.2435,\n",
      "        -5.8180, -6.2435], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.019996119663119316\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.7321, -2.9184, -2.9945, -2.1879], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.2719, -5.8424, -5.8424, -2.9945, -6.1220, -5.8691, -5.8424, -5.1654,\n",
      "        -5.2023, -5.8424], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2778, -5.7447, -5.7447, -2.9691, -6.1751, -5.7447, -5.7447, -5.3415,\n",
      "        -5.2778, -5.7447], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.009386749938130379\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.0047, -2.2210, -1.6313, -1.1036], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-6.1368, -4.7043, -6.1368, -5.3092, -5.7930, -6.2555, -3.6387, -6.2555,\n",
      "        -4.2446, -3.2408], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.1305, -4.5461, -6.1305, -5.5017, -5.6949, -6.0409, -3.9167, -6.0409,\n",
      "        -4.2749, -2.9811], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03095916472375393\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.2989, -0.9375,  0.1713,  1.5366], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.7170, -6.1517, -5.7170, -5.1806, -5.1415, -4.4825, -5.7170, -4.2280,\n",
      "        -5.7170, -6.4595], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6625, -6.1155, -5.6625, -5.2137, -5.2137, -5.0342, -5.6625, -4.2368,\n",
      "        -5.6625, -6.1155], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0442349798977375\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-6.0540, -5.6343, -6.1710, -5.6343, -6.2790, -1.5767, -5.7767, -6.1710,\n",
      "        -6.1859, -5.6333], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.0700, -5.6565, -6.1361, -5.6565, -6.1361, -1.9797, -5.6565, -6.1361,\n",
      "        -6.0700, -5.6006], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.021544545888900757\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-5.5660, -6.1698, -5.5660, -2.9394, -5.5660, -5.8293, -5.5660, -6.1791,\n",
      "        -5.5660, -5.5660], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6675, -6.1748, -5.6675, -3.0120, -5.6675, -5.6675, -5.6675, -6.1061,\n",
      "        -5.6675, -5.6675], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.009863331913948059\n",
      "---\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.8964, -5.7748, -5.8757, -6.1667], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.8964, -6.1623, -5.5415, -5.8784, -6.1667, -6.1623, -5.2511, -4.3677,\n",
      "        -5.5415, -5.8964], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.1973, -6.1213, -5.6704, -6.1213, -6.1973, -6.1213, -5.2446, -4.1520,\n",
      "        -5.6704, -6.1973], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03242127224802971\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-5.9852, -5.3446, -5.6195, -6.2313], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-6.1781, -5.5571, -5.5571, -5.6582, -5.5571, -5.5571, -6.1443, -3.7625,\n",
      "        -5.5571, -5.5571], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-6.1778, -5.6406, -5.6406, -5.6406, -5.6406, -5.6406, -6.0863, -3.7898,\n",
      "        -5.6406, -5.6406], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.004628215916454792\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-5.5564, -4.9229, -5.1038, -5.8298], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.6044, -6.1275, -6.1884, -4.2099, -1.0213, -5.8533, -5.8533, -5.6044,\n",
      "        -4.2099, -2.9238], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6004, -6.0393, -6.1457, -4.1750,  0.3792, -5.6004, -5.6004, -5.6004,\n",
      "        -4.1750, -3.0128], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.21093256771564484\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-5.0762, -4.4744, -4.5613, -5.4616], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.6174, -6.0384,  1.5305, -5.9580, -5.6174, -3.5702, -6.2041, -6.0384,\n",
      "        -6.1650, -5.6174], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5898, -5.9974, 10.0000, -5.9974, -5.5898, -3.7670, -6.1252, -5.9974,\n",
      "        -6.1252, -5.5898], grad_fn=<AddBackward0>)\n",
      "LOSS: 7.178590297698975\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-5.0601, -4.4709, -4.5411, -5.3920], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.1813, -5.1054, -5.2613, -5.6173, -5.9237, -5.9237, -6.1548, -5.6173,\n",
      "        -5.9237, -5.7138], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.1346, -5.1289, -5.3884, -5.5949, -5.9727, -5.9727, -6.1210, -5.5949,\n",
      "        -5.9727, -5.5949], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.004236062988638878\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-5.0429, -4.4682, -4.5219, -5.3303], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.6191, -6.0037, -5.7053, -6.0037, -5.6191, -5.6191, -5.6191, -6.0672,\n",
      "        -6.0672, -4.1763], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6040, -6.1195, -5.6040, -6.1195, -5.6040, -5.6040, -5.6040, -6.1195,\n",
      "        -6.1195, -4.1122], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.004758227616548538\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-5.0280, -4.4548, -4.5003, -5.2867], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.6169, -5.4833, -5.4833, -5.0265, -5.6169, -6.0404, -5.0265, -5.6169,\n",
      "        -5.6169, -4.2009], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6048, -5.6048, -5.6048, -5.0944, -5.6048, -6.1014, -5.0944, -5.6048,\n",
      "        -5.6048, -4.0897], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.005545228254050016\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-5.0119, -4.4269, -4.4897, -5.2594], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-2.9129, -5.9024, -5.6892, -4.4897, -2.0266, -4.5119, -5.6271, -5.6271,\n",
      "        -4.1394, -5.6125], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.8240, -6.0705, -5.8765, -4.0787, -1.5409, -4.4372, -5.5909, -5.5909,\n",
      "        -4.0787, -5.5909], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.04885470122098923\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.9810, -4.3669, -4.4420, -5.2090], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.5333, -4.1133, -5.6226, -5.6431, -4.1133, -5.6431, -5.1907, -5.5333,\n",
      "        -5.0674, -4.7420], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.8187, -4.0763, -5.5607, -5.8187, -4.0763, -5.8187, -5.2678, -5.8187,\n",
      "        -5.0121, -4.9302], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.027561625465750694\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-6.0542, -5.6194, -2.8626, -5.6403, -5.6194, -6.0542, -5.5004, -5.6194,\n",
      "        -5.6194, -3.5043], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.9503, -5.4946, -2.7753, -5.7187, -5.4946, -5.9503, -5.6022, -5.4946,\n",
      "        -5.4946, -3.5903], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.011545125395059586\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3879, -4.0238, -3.4032, -4.2444], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.4539, -4.0238, -5.5761, -6.0738, -3.4933, -4.1078, -5.5761, -5.5243,\n",
      "        -5.1706, -5.5761], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5613, -4.0629, -5.4572, -5.9085, -3.5545, -4.0629, -5.4572, -5.4572,\n",
      "        -5.3515, -5.4572], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01258312352001667\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0868, -3.4144, -2.7900, -3.8940], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.8991, -6.0799, -5.6184, -5.6184, -5.8327, -4.4756, -5.4431, -6.0036,\n",
      "        -5.4429, -5.5120], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7556, -5.8988, -5.6253, -5.6253, -5.6253, -4.0392, -5.5485, -5.8988,\n",
      "        -5.3459, -5.4498], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03222274035215378\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "randomly selected action: 0\n",
      "Q_EVAL: tensor([-5.3112, -3.3681, -4.9501, -5.1049, -5.5951, -5.4485, -2.7842, -5.6833,\n",
      "        -5.4485, -5.3112], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4551, -3.4725, -4.8632, -5.1214, -5.6173, -5.4551, -2.7223, -5.6173,\n",
      "        -5.4551, -5.4551], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.006889957934617996\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0329, -3.3776, -2.7153, -3.8266], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.3995, -1.8945, -5.3995, -6.0499, -5.5928, -5.3995, -5.4414, -5.3995,\n",
      "        -5.3995, -5.3995], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4401, -1.3864, -5.4401, -5.8947, -5.6064, -5.4401, -5.4401, -5.4401,\n",
      "        -5.4401, -5.4401], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.02922889217734337\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4249, -2.7559, -2.6787, -1.8495], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.5793, -5.8521, -2.7559, -5.3680, -5.4262, -5.7976, -5.8646, -5.3680,\n",
      "        -5.6899, -5.2625], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5919, -5.5068, -2.6645, -5.4223, -5.5068, -5.5919, -5.8835, -5.4223,\n",
      "        -5.5919, -5.4223], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.021795064210891724\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.5310, -1.9535, -1.1826, -0.4049], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.3393, -1.8082, -5.2679, -5.8737, -4.0595, -4.0595, -5.2679, -4.5379,\n",
      "        -5.3393, -1.9535], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4094, -1.3644, -5.4094, -5.8883, -4.0659, -4.0659, -5.4094, -4.6625,\n",
      "        -5.4094, -2.6274], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.071680948138237\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.2763, -1.1272,  0.2007,  1.8164], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.6643, -2.7208, -5.6643, -5.9620, -5.7864, -5.3184, -5.3184, -5.3184,\n",
      "        -3.3102, -1.1272], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6127, -2.5434, -5.6127, -5.8913, -5.8913, -5.3894, -5.3894, -5.3894,\n",
      "        -3.3975, -1.3653], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.013220992870628834\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.7508, -5.4266, -5.7294, -5.9256], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.3072, -4.5288, -5.3072, -4.0811, -5.3253, -4.2366, -5.9256, -5.3072,\n",
      "        -4.6717, -5.4446], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3600, -4.6306, -5.3600, -4.1098, -5.3600, -4.2154, -5.8839, -5.3600,\n",
      "        -4.9217, -5.4748], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.008634472265839577\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "randomly selected action: 1\n",
      "Q_EVAL: tensor([-4.0784, -5.1989, -0.3940, -4.9097, -5.3161, -4.8071, -4.5662, -5.3161,\n",
      "        -5.3735, -5.8910], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.1390, -5.3243,  0.6565, -5.0697, -5.3243, -5.0697, -4.6206, -5.3243,\n",
      "        -5.3243, -5.8664], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.12237407267093658\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-5.1287, -4.4737, -4.7228, -5.2478], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.8882, -5.3184, -5.3184, -4.7443, -5.2460, -5.3184, -4.7806, -5.5417,\n",
      "        -5.8501, -5.7624], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0263, -5.2698, -5.2698, -4.7475, -5.2698, -5.2698, -5.0263, -5.5765,\n",
      "        -5.8229, -5.5765], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.012365497648715973\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.5391, -3.9485, -4.0456, -4.7922], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.1270, -3.5129, -5.2153, -4.4216, -4.7922, -5.4922, -5.5468, -5.3034,\n",
      "        -5.5468, -5.8130], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2232, -3.3672, -5.3610, -4.5537, -4.9794, -5.5531, -5.5531, -5.2232,\n",
      "        -5.5531, -5.7820], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.011538684368133545\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.5478, -3.9096, -4.0587, -4.8345], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.8356, -3.9096, -4.6312, -5.5674, -5.4698, -5.1124, -5.2894, -5.5674,\n",
      "        -5.0163, -5.2894], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9260, -4.5186, -4.6621, -5.5147, -5.5147, -4.6621, -5.1681, -5.5147,\n",
      "        -5.2057, -5.1681], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06556590646505356\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.5591, -3.9036, -4.0418, -4.8645], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-3.9784, -5.0978, -5.2439, -5.7648, -5.7648, -5.2439, -2.4393, -5.4126,\n",
      "        -5.2439, -5.7648], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.1411, -5.1471, -5.1471, -5.7097, -5.7097, -5.1471, -2.2277, -5.1471,\n",
      "        -5.1471, -5.7097], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.018144182860851288\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.5621, -3.9255, -3.9906, -4.8831], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.3423, -5.1669, -5.3154, -1.9290, -5.5611, -5.5611, -4.1569, -5.2413,\n",
      "        -5.5611, -5.5909], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1594, -5.1594, -5.2957, -2.1908, -5.5305, -5.5305, -4.0989, -5.2957,\n",
      "        -5.5305, -5.7172], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.012753461487591267\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.5675, -3.9529, -3.9358, -4.8887], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.0924, -5.5263, -4.7902, -5.0924, -5.0924, -5.5263, -5.0463, -3.1754,\n",
      "        -4.8657, -4.1085], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1814, -5.5516, -4.9198, -5.1814, -5.1814, -5.5516, -5.1814, -3.2591,\n",
      "        -4.6976, -4.0671], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.009700237773358822\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3369, -4.0282, -3.3585, -4.2191], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.6632, -5.0383, -5.6632, -5.5058, -5.0383, -5.0383, -4.1765, -5.0162,\n",
      "        -5.0383, -3.8893], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.7409, -5.1948, -5.7409, -5.5631, -5.1948, -5.1948, -4.0227, -4.9207,\n",
      "        -5.1948, -4.0227], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01638835482299328\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.7152, -3.1632, -2.4590, -3.4506], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.0358, -5.4253, -5.0573, -5.6555, -5.6555, -4.7768, -5.5052, -5.0358,\n",
      "        -4.0140, -2.4879], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1775, -5.5411, -5.1775, -5.7179, -5.7179, -4.8913, -5.5411, -5.1775,\n",
      "        -4.0130, -2.0773], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.025882374495267868\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.9370, -2.4432, -2.1932, -1.1963], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.0613, -5.0613, -5.5289, -5.5289, -5.0613, -5.0613, -4.5919, -5.2178,\n",
      "        -5.5289, -5.0613], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1327, -5.1327, -5.4845, -5.4845, -5.1327, -5.1327, -4.6573, -5.2477,\n",
      "        -5.4845, -5.1327], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0036596450954675674\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.4166, -1.9049, -1.0504, -0.2023], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.4632, -5.5429, -4.1864,  0.2118, -5.4900, -5.0341, -5.1085, -5.1255,\n",
      "        -1.0504, -5.1085], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6208, -5.4253, -4.4788,  0.6834, -5.4253, -4.7678, -5.0866, -5.1930,\n",
      "        -1.1821, -5.0866], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.044446300715208054\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.2849, -1.1787,  0.2188,  1.8689], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.2043, -3.9036, -5.1443, -4.6267, -5.1443, -3.9827, -3.0587, -4.8832,\n",
      "        -4.7709, -5.5337], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1640, -4.0264, -5.0638, -4.7460, -5.0638, -3.9246, -3.2072, -4.9840,\n",
      "        -4.7460, -5.3949], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.00993812270462513\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "randomly selected action: 2\n",
      "Q_EVAL: tensor([-0.1754, -1.0387, -5.5122, -4.4148, -4.6773, -5.2107, -5.5122, -5.5122,\n",
      "        -5.6754, -5.1595], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([ 0.6819, -1.1579, -5.3930, -4.5754, -4.5838, -5.0654, -5.3930, -5.3930,\n",
      "        -5.5881, -5.0654], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08639129251241684\n",
      "---\n",
      "Agent's Location: [2 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.7085, -4.9060, -5.1263, -5.4310], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.6225, -5.1563, -5.1563, -5.1563, -3.9268, -5.1563, -5.1503, -5.1563,\n",
      "        -5.1263, -5.1563], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.6092, -5.0946, -5.0946, -5.0946, -3.9989, -5.0946, -5.0946, -5.0946,\n",
      "        -5.0946, -5.0946], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0032337703742086887\n",
      "---\n",
      "Agent's Location: [2 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-5.1235, -4.4639, -4.5795, -5.1923], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.1267, -5.1235, -5.1267, -2.3764, -4.4947, -5.1489, -5.1267, -5.5214,\n",
      "        -3.9620, -5.5690], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1401, -5.2368, -5.1401, -1.9657, -4.6394, -5.1401, -5.1401, -5.4534,\n",
      "        -3.9667, -5.6455], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.021351326256990433\n",
      "---\n",
      "Agent's Location: [2 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.6762, -4.0114, -4.0881, -4.8057], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-3.3100, -5.5293, -4.6223, -4.1184, -5.1272, -2.8439, -5.5293, -2.3700,\n",
      "        -5.3681, -5.1080], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.1293, -5.6623, -4.6508, -4.6102, -5.1601, -3.1293, -5.6623, -1.9444,\n",
      "        -5.6623, -5.1601], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.06636250764131546\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3403, -3.9488, -3.2746, -4.1775], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.2989, -5.5550, -5.0037, -5.0037, -5.1028,  1.8630, -3.8024, -5.2989,\n",
      "        -5.5550, -5.6551], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4223, -5.6161, -5.1232, -5.1232, -5.1232, 10.0000, -4.3678, -5.4223,\n",
      "        -5.6161, -5.6161], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.659820556640625\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.6577, -3.0423, -2.3226, -3.3219], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.0850, -5.0850, -5.0850, -4.4212, -5.3127, -5.3127, -5.3127, -3.7653,\n",
      "        -5.3127, -5.1664], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0907, -5.0907, -5.0907, -4.5706, -5.3937, -5.3937, -5.3937, -4.3287,\n",
      "        -5.3937, -5.3937], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.041775695979595184\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.8778, -2.2761, -2.0275, -1.0977], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.3543, -4.6167, -4.8434, -3.9176, -5.0474, -5.6231, -4.7875, -5.6231,\n",
      "        -5.0605, -5.2089], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3590, -4.7120, -4.9242, -3.9306, -5.1550, -5.5427, -4.9242, -5.5427,\n",
      "        -5.0547, -5.3590], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.008158540353178978\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.2828, -1.7932, -0.8390,  0.1019], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-2.2495, -5.3937, -5.5508, -5.2607, -5.6666, -5.0391, -5.6666, -5.2582,\n",
      "        -4.0868, -5.0391], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.0035, -5.3231, -5.5102, -5.3231, -5.5102, -5.0196, -5.5102, -5.0196,\n",
      "        -4.2291, -5.0196], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01979057863354683\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3154, -1.1978,  0.2953,  2.0000], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.4038, -5.4038, -5.1519, -4.4787, -5.2643, -5.0041, -2.9880, -5.5733,\n",
      "        -2.2350, -3.9035], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3127, -5.3127, -5.1210, -4.1686, -5.0096, -5.0096, -3.0224, -5.5038,\n",
      "        -2.0123, -3.9072], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.023424264043569565\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.5371, -5.0160, -5.1702, -5.6758], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-1.7788, -4.9716, -3.9213, -4.7966, -2.9921, -4.9716, -4.9716, -4.8605,\n",
      "        -5.1900, -4.5900], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.0148, -5.0145, -3.8986, -4.9097, -3.0034, -5.0145, -5.0145, -5.0145,\n",
      "        -4.9097, -4.6822], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.018544143065810204\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-5.4556, -4.6020, -4.7556, -5.5662], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.6714, -5.6714, -4.8040, -2.2129, -4.2852, -3.9367, -4.3927, -1.9658,\n",
      "        -5.1075, -4.9523], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.5240, -5.5240, -4.9239, -2.0070, -4.4917, -3.8979, -4.4917, -2.0070,\n",
      "        -5.0140, -5.0140], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01684301719069481\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-4.8926, -4.1035, -4.1492, -5.0725], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.3141, -5.0351, -5.0351, -5.6350, -4.9505, -5.3597, -4.9505, -4.9505,\n",
      "         0.2095,  0.2095], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.3337, -5.1530, -5.1530, -5.5316, -5.0170, -5.3337, -5.0170, -5.0170,\n",
      "         0.8681,  0.8681], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09200556576251984\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.2730, -3.6624, -3.3773, -4.5955], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.0131, -4.9498, -3.2207, -5.3135, -4.9498, -2.9824, -4.9498, -4.0960,\n",
      "        -4.8543, -4.9498], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0099, -5.0099, -2.9617, -5.3309, -5.0099, -2.9617, -5.0099, -4.0396,\n",
      "        -5.0099, -5.0099], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.010971532203257084\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.3303, -3.9285, -3.2176, -4.2938], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.8796, -4.2626, -4.9621, -4.9688, -2.1690, -2.9632, -4.2938, -1.0642,\n",
      "        -3.3679, -4.3642], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9839, -4.4356, -4.9839, -4.9839, -1.9578, -2.9580, -4.3825, -0.6681,\n",
      "        -3.8958, -4.3825], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.05299283191561699\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4926, -2.9334, -2.1806, -3.1680], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-2.9334, -5.0006, -5.0006,  2.1239, -3.6159, -4.2876, -5.0006, -5.2730,\n",
      "        -5.0006, -3.1680], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-2.9625, -4.9396, -4.9396, 10.0000, -4.0667, -4.3890, -4.9396, -5.2830,\n",
      "        -4.9396, -3.6969], grad_fn=<AddBackward0>)\n",
      "LOSS: 6.254277229309082\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.7780, -2.0949, -1.9725, -0.9918], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.9989, -2.1876, -5.4328, -4.3406, -4.4763, -3.4105, -5.4328, -3.8552,\n",
      "        -4.9434, -5.2500], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.9065, -1.8926, -5.4638, -4.3529, -4.3529, -3.8713, -5.4638, -3.8713,\n",
      "        -4.9065, -5.2646], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.03270522505044937\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-1.0673, -1.6621, -0.6362,  0.5520], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.9846, -4.9846, -5.0055, -3.8235, -5.0055, -5.3993, -5.3993, -5.3993,\n",
      "        -3.8235, -3.8235], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8832, -4.8832, -4.8832, -3.8288, -4.8832, -5.4510, -5.4510, -5.4510,\n",
      "        -3.8288, -3.8288], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.005862629506736994\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "randomly selected action: 3\n",
      "Q_EVAL: tensor([-4.9454, -4.9498, -2.9209, -3.8037, -5.2662, -5.3746, -1.9552, -4.1572,\n",
      "        -5.2151, -4.9361], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.0176, -4.8714, -2.9465, -3.7843, -5.2526, -5.4509, -1.8417, -4.0982,\n",
      "        -5.2526, -4.8714], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.004034988582134247\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.2601, -4.9486, -5.1302, -5.3652], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.2288, -4.8009, -4.9098, -4.4514, -2.7438, -5.1302, -5.2017, -4.9098,\n",
      "        -4.9185, -4.9098], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.2523, -4.8622, -4.8622, -4.5760, -2.9316, -5.2523, -5.2523, -4.8622,\n",
      "        -4.8622, -4.8622], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.008254112675786018\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-5.1818, -4.4441, -4.6856, -5.2715], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.3699, -4.2122, -4.8657, -5.2018, -4.8677, -4.8677, -5.1911, -4.9535,\n",
      "        -4.8657, -4.8934], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4581, -4.2937, -4.8525, -5.2468, -4.8525, -4.8525, -5.2468, -4.9997,\n",
      "        -4.8525, -4.8525], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.0024160812608897686\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-4.6556, -3.9485, -4.0683, -4.7985], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.8377, -2.0315, -4.8285, -5.1877, -3.6842, -4.8713, -2.1024, -5.3879,\n",
      "        -5.1877, -5.3879], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8392, -1.8061, -4.8392, -5.2339, -4.1066, -4.8392, -1.8061, -5.4574,\n",
      "        -5.2339, -5.4574], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.033215202391147614\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.2516, -3.6932, -3.4218, -4.6064], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.8292, -4.2516, -5.4307, -5.1670, -5.4307, -4.9542, -2.0225, -4.7777,\n",
      "        -3.4375, -4.7777], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.8284, -4.0796, -5.4588, -5.2230, -5.4588, -4.9733, -1.8266, -4.8284,\n",
      "        -3.6046, -4.8284], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.010608971118927002\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-4.0021, -3.6889, -2.8423, -3.8888], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.4821, -5.2323, -4.8363, -4.8324, -5.1601, -3.1430, -4.7450, -3.2704,\n",
      "        -5.1601, -4.3873], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4481, -5.1988, -4.8081, -4.8354, -5.1988, -3.4662, -4.8081, -3.4662,\n",
      "        -5.1988, -4.5200], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.017041120678186417\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4506, -2.8526, -2.0267, -3.1893], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-5.0895, -4.7320, -5.3052, -2.8049, -3.6436, -3.8653, -4.7320, -3.8653,\n",
      "        -2.7961, -5.2623], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.1576, -4.7742, -5.4199, -2.8240, -3.5244, -4.0589, -4.7742, -4.0589,\n",
      "        -2.8240, -5.1576], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.01226592343300581\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.8179, -1.9464, -1.9068, -1.0204], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.7184, -4.8727, -4.1172, -5.0922, -4.7184, -5.2770, -4.8704, -0.3755,\n",
      "        -5.2770, -4.7184], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7496, -4.7496, -4.1709, -5.1248, -4.7496, -5.1248, -4.7496,  1.2684,\n",
      "        -5.1248, -4.7496], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.2785433828830719\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9626, -1.5491, -0.4517,  0.8244], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.6090, -4.7124, -2.0073, -3.5838, -5.1004, -4.7412,  2.5267, -5.6090,\n",
      "        -3.5838, -5.2547], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4001, -4.7417, -1.9385, -3.4659, -5.1058, -4.7417, 10.0000, -5.4001,\n",
      "        -3.4659, -5.1058], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.599279403686523\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.3135, -1.2927,  0.4347,  2.5939], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-5.5542, -5.5542, -4.1956, -5.2097, -5.5542, -4.1702, -4.3116, -3.5795,\n",
      "        -4.7031, -5.1724], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4158, -5.4158, -4.1710, -5.1079, -5.4158, -4.1521, -4.4636, -3.4379,\n",
      "        -4.7532, -5.1079], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.011857724748551846\n",
      "---\n",
      "end game\n",
      "Agent's Location: [3 0]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "four actions: tensor([-5.3298, -4.9404, -5.0672, -5.4502], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-5.4502, -5.0674, -4.6964, -5.0674, -4.6964, -5.0674, -4.6964, -5.4502,\n",
      "        -3.9024, -5.0674], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-5.4463, -5.1289, -4.7798, -5.1289, -4.7798, -5.1289, -4.7798, -5.4463,\n",
      "        -3.9641, -5.1289], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.003984423819929361\n",
      "---\n",
      "Agent's Location: [3 1]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "four actions: tensor([-5.1616, -4.3571, -4.5335, -5.1700], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-4.7091, -3.6575, -4.6987, -5.0503, -0.9442, -4.6987, -4.3571, -3.6575,\n",
      "        -4.7091, -1.9454], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7870, -3.9503, -4.7870, -5.1315, -0.1099, -4.7870, -4.4939, -3.9503,\n",
      "        -4.7870, -1.8498], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.09297265112400055\n",
      "---\n",
      "Agent's Location: [3 2]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "four actions: tensor([-4.6185, -3.8919, -3.9120, -4.6443], grad_fn=<ViewBackward0>)\n",
      "selected action: 1\n",
      "Q_EVAL: tensor([-3.5882, -4.9392, -4.0597, -0.8801, -1.8901, -4.5971, -4.6539, -5.2931,\n",
      "        -4.7118, -4.6682], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.3788, -5.1374, -4.1692, -0.0931, -1.7921, -4.7765, -4.7982, -5.4691,\n",
      "        -4.7982, -4.7982], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.08325609564781189\n",
      "---\n",
      "Agent's Location: [3 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "four actions: tensor([-4.1499, -3.6485, -3.2079, -4.3223], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.7264, -3.5638, -5.2580, -2.7126, -5.2580, -4.7264, -4.6587, -4.1953,\n",
      "        -5.2580, -5.1806], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.7757, -3.3663, -5.4538, -2.7617, -5.4538, -4.7757, -4.7757, -4.1422,\n",
      "        -5.4538, -5.1184], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.018164869397878647\n",
      "---\n",
      "Agent's Location: [2 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.8414, -3.5108, -2.6271, -3.5533], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-3.8102, -5.2999, -4.7548, -4.1753, -3.5954, -4.5186, -5.0507, -4.6852,\n",
      "        -4.7548, -4.1336], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.8747, -5.4072, -4.7202, -4.0869, -3.8747, -4.7327, -5.0667, -4.7202,\n",
      "        -4.7202, -4.0869], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.015346484258770943\n",
      "---\n",
      "Agent's Location: [1 3]\n",
      "state: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-3.4400, -2.7797, -1.9488, -3.1090], grad_fn=<ViewBackward0>)\n",
      "selected action: 2\n",
      "Q_EVAL: tensor([-4.7621, -4.9475, -4.9591, -2.7797, -4.9591, -4.7099, -2.6144, -4.8653,\n",
      "        -4.0575, -4.7621], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6820, -5.0378, -5.0378, -2.7539, -5.0378, -4.6820, -2.7539, -4.8474,\n",
      "        -4.0530, -4.6820], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.005464869551360607\n",
      "---\n",
      "Agent's Location: [0 3]\n",
      "state: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-2.6079, -1.7712, -1.7643, -0.6675], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-4.7841, -4.7587, -2.7517, -4.0484, -5.3425, -4.7587, -4.7587, -4.7587,\n",
      "        -4.7587,  2.9050], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-4.6512, -4.6436, -2.7427, -4.0195, -5.3480, -4.6436, -4.6436, -4.6436,\n",
      "        -4.6436, 10.0000], grad_fn=<AddBackward0>)\n",
      "LOSS: 5.042352676391602\n",
      "---\n",
      "Agent's Location: [0 2]\n",
      "state: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.9084, -1.5660, -0.4328,  0.9460], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-3.7411, -4.6363, -4.7028, -4.6363, -5.4081, -4.7028, -3.5267, -4.0424,\n",
      "        -3.4446, -4.7028], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-3.7547, -4.6382, -4.6382, -4.6382, -5.3519, -4.6382, -3.7547, -4.0157,\n",
      "        -3.3182, -4.6382], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.008451960049569607\n",
      "---\n",
      "Agent's Location: [0 1]\n",
      "state: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "four actions: tensor([-0.1662, -1.3829,  0.5126,  3.0793], grad_fn=<ViewBackward0>)\n",
      "selected action: 3\n",
      "Q_EVAL: tensor([-0.5116, -4.6316, -4.0257, -4.6316, -4.2672, -1.6496, -2.5304, -4.6316,\n",
      "        -4.6316, -2.5304], grad_fn=<IndexBackward0>)\n",
      "Q_TARGET: tensor([-0.1311, -4.6452, -3.6913, -4.6452, -4.3816, -1.4604, -2.6834, -4.6452,\n",
      "        -4.6452, -2.6834], grad_fn=<AddBackward0>)\n",
      "LOSS: 0.035299692302942276\n",
      "---\n",
      "end game\n"
     ]
    }
   ],
   "source": [
    "# Deep Q Network\n",
    "import gym \n",
    "import gym_examples\n",
    "from dqn import Agent\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('gym_examples/GridWorld-v0', size=4)\n",
    "agent = Agent(gamma=0.9, epsilon=0.5, batch_size=10, n_actions=4, eps_end=0.01, input_dims=[16], lr=0.003)\n",
    "scores, eps_hist,avg_scores = [], [], []\n",
    "n_games = 10\n",
    "\n",
    "for i in range(n_games):\n",
    "    score = 0\n",
    "    done=False\n",
    "    observation = env.reset()[0]['agent']\n",
    "    counter = 0\n",
    "\n",
    "    while not done:\n",
    "        agent_location = env.get_agent_location()\n",
    "        print(f\"Agent's Location: {agent_location}\")\n",
    "\n",
    "        # choose an action based on current state of env\n",
    "        action = agent.choose_action(observation)\n",
    "\n",
    "        observation_, reward, done, truncated, info = env.step(action)\n",
    "        observation_ = observation_['agent'] # since one hot encoded state is nested in dictionary\n",
    "\n",
    "        score += reward\n",
    "\n",
    "        # store transition and update weights\n",
    "        agent.store_transitions(observation, action, reward, observation_, done)\n",
    "        agent.learn()\n",
    "\n",
    "        #update state\n",
    "        observation = observation_\n",
    "        \n",
    "        # end of an episode\n",
    "        counter += 1\n",
    "        print('---')\n",
    "\n",
    "    scores.append(score)\n",
    "    eps_hist.append(agent.epsilon)\n",
    "    print('end game')\n",
    "\n",
    "    avg_score = np.mean(scores)\n",
    "    \n",
    "    avg_scores.append(avg_score)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the average score per episode\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "episodes = list(range(1, n_games + 1))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(episodes, avg_scores, marker='o', linestyle='-', color='b', label='Average Score per Episode')\n",
    "plt.title('Average Scores Over Episodes')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Score')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
